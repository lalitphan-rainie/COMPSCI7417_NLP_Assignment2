Uni-directional Transformer VS Bi-directional BERT,"<p>I just finished reading the <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""noreferrer"">Transformer</a> paper and <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT</a> paper. But couldn't figure out why Transformer is uni-directional and BERT is bi-directional as mentioned in BERT paper. As they don't use recurrent networks, it's not so straightforward to interpret the directions. Can anyone give some clue? Thanks.</p>
","nlp, transformer-model, pre-trained-model, bert-language-model","<p>To clarify, the original Transformer model from Vaswani et al. is an encoder-decoder architecture. Therefore the statement ""Transformer is uni-directional"" is misleading.</p>

<p>In fact, the transformer <em>encoder</em> is bi-directional, which means that the self-attention can attend to tokens both on the left and right. In contrast, the <em>decoder</em> is uni-directional, since while generating text one token at a time, you cannot allow the decoder to attend to the right of the current token. The transformer decoder constrains the self-attention by masking the tokens to the right.</p>

<p>BERT uses the transformer encoder architecture and can therefore attend both to the left and right, resulting in ""bi-directionality"".</p>

<p>From the BERT paper itself:</p>

<blockquote>
  <p>We note that in the literature the bidirectional Transformer is often referred to as a “Transformer encoder” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.</p>
</blockquote>

<p>Recommended reading: <a href=""https://jalammar.github.io/illustrated-bert/"" rel=""noreferrer"">this article</a>.</p>
",16,9,5127,2019-03-12 04:23:33,https://stackoverflow.com/questions/55114128/uni-directional-transformer-vs-bi-directional-bert
How to cluster similar sentences using BERT,"<p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>

<p>A good example of the implementation can be seen in this short article: <a href=""http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/"" rel=""noreferrer"">http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/</a></p>

<p>I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?</p>

<p>Any information would be helpful.</p>
","python, nlp, artificial-intelligence, word-embedding, bert-language-model","<p>As <a href=""https://stackoverflow.com/users/4935974/subham-kumar"">Subham Kumar</a> <a href=""https://stackoverflow.com/a/62859000/395857"">mentioned</a>, one can use this Python 3 library to compute sentence similarity: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>The library has a few <a href=""https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/clustering"" rel=""noreferrer"">code examples</a> to perform clustering:</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py"" rel=""noreferrer""><code>fast_clustering.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a more complex example on performing clustering on large scale dataset.

This examples find in a large set of sentences local communities, i.e., groups of sentences that are highly
similar. You can freely configure the threshold what is considered as similar. A high threshold will
only find extremely similar sentences, a lower threshold will find more sentence that are less similar.

A second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.

The method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).

In this example, we download a large set of questions from Quora and then find similar questions in this set.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer, util
import os
import csv
import time


# Model for computing sentence embeddings. We use one trained for similar questions detection
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)
# and find similar question in it
url = &quot;http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv&quot;
dataset_path = &quot;quora_duplicate_questions.tsv&quot;
max_corpus_size = 50000 # We limit our corpus to only the first 50k questions


# Check if the dataset exists. If not, download and extract
# Download dataset if needed
if not os.path.exists(dataset_path):
    print(&quot;Download dataset&quot;)
    util.http_get(url, dataset_path)

# Get all unique sentences from the file
corpus_sentences = set()
with open(dataset_path, encoding='utf8') as fIn:
    reader = csv.DictReader(fIn, delimiter='\t', quoting=csv.QUOTE_MINIMAL)
    for row in reader:
        corpus_sentences.add(row['question1'])
        corpus_sentences.add(row['question2'])
        if len(corpus_sentences) &gt;= max_corpus_size:
            break

corpus_sentences = list(corpus_sentences)
print(&quot;Encode the corpus. This might take a while&quot;)
corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)


print(&quot;Start clustering&quot;)
start_time = time.time()

#Two parameters to tune:
#min_cluster_size: Only consider cluster that have at least 25 elements
#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar
clusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)

print(&quot;Clustering done after {:.2f} sec&quot;.format(time.time() - start_time))

#Print for all clusters the top 3 and bottom 3 elements
for i, cluster in enumerate(clusters):
    print(&quot;\nCluster {}, #{} Elements &quot;.format(i+1, len(cluster)))
    for sentence_id in cluster[0:3]:
        print(&quot;\t&quot;, corpus_sentences[sentence_id])
    print(&quot;\t&quot;, &quot;...&quot;)
    for sentence_id in cluster[-3:]:
        print(&quot;\t&quot;, corpus_sentences[sentence_id])

</code></pre>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py"" rel=""noreferrer""><code>kmeans.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then k-mean clustering is applied.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Corpus with example sentences
corpus = ['A man is eating food.',
          'A man is eating a piece of bread.',
          'A man is eating pasta.',
          'The girl is carrying a baby.',
          'The baby is carried by the woman',
          'A man is riding a horse.',
          'A man is riding a white horse on an enclosed ground.',
          'A monkey is playing drums.',
          'Someone in a gorilla costume is playing a set of drums.',
          'A cheetah is running behind its prey.',
          'A cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# Perform kmean clustering
num_clusters = 5
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print(&quot;Cluster &quot;, i+1)
    print(cluster)
    print(&quot;&quot;)
</code></pre>
<p><a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/agglomerative.py"" rel=""noreferrer""><code>agglomerative.py</code></a>:</p>
<pre><code>&quot;&quot;&quot;
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.
&quot;&quot;&quot;
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
import numpy as np

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Corpus with example sentences
corpus = ['A man is eating food.',
          'A man is eating a piece of bread.',
          'A man is eating pasta.',
          'The girl is carrying a baby.',
          'The baby is carried by the woman',
          'A man is riding a horse.',
          'A man is riding a white horse on an enclosed ground.',
          'A monkey is playing drums.',
          'Someone in a gorilla costume is playing a set of drums.',
          'A cheetah is running behind its prey.',
          'A cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# Normalize the embeddings to unit length
corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)

# Perform kmean clustering
clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = {}
for sentence_id, cluster_id in enumerate(cluster_assignment):
    if cluster_id not in clustered_sentences:
        clustered_sentences[cluster_id] = []

    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in clustered_sentences.items():
    print(&quot;Cluster &quot;, i+1)
    print(cluster)
    print(&quot;&quot;)
</code></pre>
",8,35,47696,2019-04-10 18:31:20,https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert
How to feed Bert embeddings to LSTM,"<p>I am working on a Bert + MLP model for text classification problem. Essentially, I am trying to replace the MLP model with a basic LSTM model.</p>

<p>Is it possible to create a LSTM with embedding? Or, is best to create a LSTM with embedded layer?</p>

<p>More specifically, I am having a hard time trying to create embedded matrix so I can create embedding layer using Bert embedding. </p>

<pre><code>def get_bert_embeddings(dataset='gap_corrected_train',
                        dataset_path=TRAIN_PATH,
                        bert_path=BERT_UNCASED_LARGE_PATH,
                        bert_layers=BERT_LAYERS):
    """"""Get BERT embeddings for all files in dataset_path and specified BERT layers and write them to file.""""""
    df = None
    for file in os.listdir(dataset_path):
        if df is None:
            df = pd.read_csv(dataset_path+'/'+file, sep='\t')
        else:
            next_df = pd.read_csv(dataset_path+'/'+file, sep='\t')
            df = pd.concat([df, next_df], axis=0)
            df.reset_index(inplace=True, drop=True)

    for i, layer in enumerate(bert_layers):
        embeddings_file = INTERIM_PATH + 'emb_bert' + str(layer) + '_' + dataset + '.h5'
        if not os.path.exists(embeddings_file):
            print('Embeddings file: ', embeddings_file)
            print('Extracting BERT Layer {0} embeddings for {1}...'.format(layer, dataset))
            print(""Started at "", time.ctime())

            emb = get_bert_token_embeddings(df, bert_path, layer)
            emb.to_hdf(embeddings_file, 'table')

            print(""Finished at "", time.ctime())

</code></pre>

<pre><code>def build_mlp_model(input_shape):
    input_layer = layers.Input(input_shape)



    input_features = layers.Input((len(FEATURES),))
    x = layers.Concatenate(axis=1, name=""concate_layer"")([input_layer, input_features]) 


    x = layers.Dense(HIDDEN_SIZE, name='dense1')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(DROPOUT, seed=RANDOM)(x)

    x = layers.Dense(HIDDEN_SIZE//2, name='dense2')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(DROPOUT//2, seed=RANDOM)(x)

    x = layers.Dense(HIDDEN_SIZE//4, name='dense3')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(DROPOUT//2, seed=RANDOM)(x)

    output_layer = layers.Dense(3, name='output', kernel_regularizer = regularizers.l2(LAMBDA))(x)
    output_layer = layers.Activation('softmax')(output_layer)

    model = models.Model(input=[input_layer, input_features], output=output_layer, name=""mlp"")
    return model

</code></pre>
","keras, lstm, keras-layer, mlp, bert-language-model","<p>You can create model that uses first the Embedding layer which is followed by LSTM and then Dense.
Such as here:</p>

<pre><code>deep_inputs = Input(shape=(length_of_your_data,))
embedding_layer = Embedding(vocab_size, output_dim = 3000, trainable=True)(deep_inputs)
LSTM_Layer_1 = LSTM(512)(embedding_layer) 
dense_layer_1 = Dense(number_of_classes, activation='softmax')(LSTM_Layer_1) 
model_AdGroups = Model(inputs=deep_inputs, outputs=dense_layer_1) 
</code></pre>
",1,6,8759,2019-04-13 20:42:43,https://stackoverflow.com/questions/55669695/how-to-feed-bert-embeddings-to-lstm
Why is a throw-away column required in Bert format?,"<p>I have recently come across Bert(Bidirectional Encoder Representations from Transformers). I saw that Bert requires a strict format for the train data. The third column needed is described as follows:</p>

<p><b>Column 3:</b> A column of all the same letter — this is a throw-away column that you need to include because the BERT model expects it.</p>

<p>What is a throw-away column and why is this column needed in the dataset  since it is stated that it contains the same letter?</p>

<p>Thank you.</p>
","machine-learning, deep-learning, nlp, bert-language-model","<p>BERT was pre-trained on two tasks - Masked Language Modelling &amp; Next Sentence Prediction.</p>

<p>The third column as you refer to it as is used only in Next Sentence Prediction and downstream tasks that require multiple sentences such as question answering. In these cases the value of the column won't just be A or 0 for everything. Sentence 1 will be all 0 while sentence 2 will be all 1 indicating that the former is sentence A and the latter is sentence B.</p>
",1,2,132,2019-04-29 20:49:15,https://stackoverflow.com/questions/55910635/why-is-a-throw-away-column-required-in-bert-format
Why can&#39;t I import functions in bert after pip install bert,"<p>I am a beginner for bert, and I am trying to use files of bert given on the GitHub:<a href=""https://github.com/google-research/bert"" rel=""noreferrer"">https://github.com/google-research/bert</a></p>

<p>However I cannot import files(such as run_classifier, optimisation and so on) from bert after using <code>pip install bert</code> to install bert in terminal. I tried to run following codes in jupiter notebook:</p>

<pre><code>import bert
from bert import run_classifier
</code></pre>

<p>And the error is:</p>

<pre><code>ImportError: cannot import name 'run_classifier'
</code></pre>

<p>Then I found the file named 'bert' in <code>\anaconda3\lib\python3.6\site-packages</code>, and there were no python files named 'run_classifier', 'optimization' etc inside it. So I downloaded those files from GitHub and put them into file 'bert' by myself. After doing this I could import run_classifier.</p>

<p>However, another problem occurred. I couldn't use the functions inside the files although I could import them.
For example, there's a function <code>convert_to_unicode</code> in tokenization.py:</p>

<pre><code>Help on module bert.tokenization in bert:

NAME

    bert.tokenization - Tokenization classes.    
FUNCTIONS

    convert_to_unicode(text)
    Converts `text` to Unicode (if it's not already), assuming utf-8 input.
</code></pre>

<p>Then I tried this:</p>

<pre><code>import tokenization from bert
convert_to_unicode('input.txt')
</code></pre>

<p>And the error is:</p>

<pre><code>NameError: name 'convert_to_unicode' is not defined
</code></pre>

<p>Then I tried:</p>

<pre><code>from tokenization import convert_to_unicode
</code></pre>

<p>And the error is:</p>

<pre><code>ModuleNotFoundError: No module named 'tokenization'
</code></pre>

<p>I am really confused about this. </p>
","python, nlp, bert-language-model","<p>The package you're looking for is <code>bert-tensorflow</code>, not <code>bert</code>.</p>

<p><a href=""https://pypi.org/project/bert-tensorflow/"" rel=""noreferrer"">bert-tensorflow</a> is the Python package for Google's BERT implementation.<br>
<a href=""https://pypi.org/project/bert"" rel=""noreferrer"">bert</a> is a serialization library.</p>
",12,8,20017,2019-06-12 03:43:26,https://stackoverflow.com/questions/56554380/why-cant-i-import-functions-in-bert-after-pip-install-bert
BERT output not deterministic,"<p>BERT output is not deterministic.
I expect the output values are deterministic when I put a same input, but my bert model the values are changing. Sounds awkwardly, the same value is returned twice, once. That is, once another value comes out, the same value comes out and it repeats.
How I can make the output deterministic?
let me show snippets of my code.
I use the model as below.</p>

<p>For the BERT implementation, I use huggingface implemented BERT pytorch implementation. which is quite fameous model ri implementation in the pytorch area. [link] <a href=""https://github.com/huggingface/pytorch-pretrained-BERT/"" rel=""noreferrer"">https://github.com/huggingface/pytorch-pretrained-BERT/</a></p>

<pre><code>        tokenizer = BertTokenizer.from_pretrained(self.bert_type, do_lower_case=self.do_lower_case, cache_dir=self.bert_cache_path)
        pretrain_bert = BertModel.from_pretrained(self.bert_type, cache_dir=self.bert_cache_path)
        bert_config = pretrain_bert.config
</code></pre>

<p>Get the output like this</p>

<pre><code>        all_encoder_layer, pooled_output = self.model_bert(all_input_ids, all_segment_ids, all_input_mask)

        # all_encoder_layer: BERT outputs from all layers.
        # pooled_output: output of [CLS] vec.

</code></pre>

<p>pooled_output</p>

<pre><code>tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,

tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,
</code></pre>

<p>for the all encoder layer, the situation is same, - same in twice an once.</p>

<p>I extract word embedding feature from the bert, and the situation is same. </p>

<pre><code>wemb_n
tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],

tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],
</code></pre>
","deep-learning, nlp, transformer-model, bert-language-model","<p>Please try to set the seed. I faced the same issue and set the seed to make sure we get same values every time. One of the possible reasons could be dropout taking place in BERT.</p>
",9,9,3488,2019-06-17 23:17:12,https://stackoverflow.com/questions/56639938/bert-output-not-deterministic
Remove loops for sentence comparison in NLP,"<p>I'm using BERT to compare text similarity, with the following code: </p>

<pre><code>from bert_embedding import BertEmbedding
import numpy as np
from scipy.spatial.distance  import cosine as cosine_similarity

bert_embedding = BertEmbedding()
TEXT1 = ""As expected from MIT-level of course: it's interesting, challenging, engaging, and for me personally quite enlightening. This course is second part of 5 courses in  micromasters program. I was interested in learning about supply chain (purely personal interest, my work touch this topic but not directly) and stumbled upon this course, took it, and man-oh-man...I just couldn't stop learning. Now I'm planning to take the rest of the courses. Average time/effort per week should be around 8-10 hours, but I tried to squeeze everything into just 5 hours since I have very limited free time. You will need 2-3 hours per week for the lecture videos, 2 hours for practice problems, and another 2 hours for the weekly homework. This course offers several topics around demand forecasting and inventory. Basic knowledge of probability and statistics is needed. It will help if you take the prerequisite course: supply chain analytics. But if you've already familiar with basic concept of statistics, you can pick yourself along the way. The lectures are very interesting and engaging, it gives you a lot of knowledge but also throw in some business perspective, so it's very relatable and applicable! The practice problems can help strengthen the understanding of the given knowledge and the homework are very challenging compared to other online-courses I have taken. This course is the best quality I have taken so far, and I have taken several (3-4 MOOCs) from other provider.""
TEXT1 = TEXT1.split('.')

sentence2 = [""CHALLENGING COURSE ""]
</code></pre>

<p>from there I want to find the best match of sentence2 in one of the sentences of TEXT1, using cosine distance</p>

<pre><code>best_match = {'sentence':'','score':''}
best = 0
for sentence in TEXT1: 
  #sentence = sentence.replace('SUPPLY CHAIN','')
  if len(sentence) &lt; 5:
    continue
  avg_vec1 = calculate_avg_vec([sentence])
  avg_vec2 = calculate_avg_vec(sentence2)

  score = cosine_similarity(avg_vec1,avg_vec2)
  if score &gt; best:
    best_match['sentence'] =  sentence
    best_match['score'] =  score
    best = score

best_match
</code></pre>

<p>The code is working, but since I want to compare the sentence2 not only with TEXT1 with but N texts I need to improve the speed. is it possible to vectorice this loop? or any way to speed it up? </p>
","python, numpy, bert-language-model","<p><code>cosine_similarity</code> is defined as a dot product of two normalized vectors.</p>

<p>This is essentially a matrix multiplication, followed by an <code>argmax</code> to get the best index.</p>

<p>I'll be using <code>numpy</code>, even though - as mentioned in the comments - you could probably plug it in to the <code>BERT</code> model with <code>pytorch</code> or <code>tensorflow</code>.</p>

<p>First, we define a <strong>normalized</strong> average vector:</p>

<pre><code>def calculate_avg_norm_vec(sentence):
    vs = sentence2vectors(sentence) # TODO: use Bert embedding
    vm = vs.mean(axis=0)
    return vm/np.linalg.norm(vm)
</code></pre>

<p>Then, we build a matrix of all sentences and their vectors</p>

<pre><code>X = np.apply_along_axis(calculate_avg_norm_vec, 1, all_sentences)
target = calculate_avg_norm_vec(target_sentence)
</code></pre>

<p>Finally, we'll need to multiply the <code>target</code> vector with the <code>X</code> matrix, and take the <code>argmax</code></p>

<pre><code>index_of_sentence = np.dot(X,target.T).argmax(axis=1)
</code></pre>

<p>You might want to make sure that the <code>axis</code> and indexing fit your data, but this is the overall scheme</p>
",3,1,257,2019-06-18 19:52:42,https://stackoverflow.com/questions/56656153/remove-loops-for-sentence-comparison-in-nlp
Bert-multilingual in pytorch,"<p>I am using bert embedding for french text data. and I have problem with loading model and vocabulary.</p>

<p>I used the following code for tokenization that works well, but to get the vocabulary, it gives me Chinese words!!</p>

<pre class=""lang-py prettyprint-override""><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
text = ""La Banque Nationale du Canada fête cette année le 110e anniversaire de son bureau de Paris.""
marked_text = ""[CLS] "" + text + "" [SEP]""
tokenized_text = tokenizer.tokenize(marked_text)
list(tokenizer.vocab.keys())[5000:5020]
</code></pre>

<p>I expected french words in vocabulary but i get chinese words, should i specify the language somewhere in the code?</p>
","python, pytorch, multilingual, bert-language-model","<p>You are Getting Chinese text because, you are looking for a specific range of the words from the vocabulary <code>[5000:5020]</code>, which corresponds to the Chinese text. Also,<code>bert -base-multilingual-cased</code> is trained on 104 languages.</p>

<p>If you further want to verify your code, you can use this:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
text = ""La Banque Nationale du Canada fête cette année le 110e anniversaire de son bureau de Paris.""
marked_text = ""[CLS] "" + text + "" [SEP]""
tokenized_text = tokenizer.tokenize(marked_text)
</code></pre>

<p>which is same as your code, followed by:</p>

<pre><code>token_no=[]
for token in tokenized_text:
    #print(tokenizer.vocab[token]) ### you can use this to check the corresponding index of the token
    token_no.append(tokenizer.vocab[token])


### The below code obtains the tokens from the index, which is similar to what you were trying, but on the correct range.
new_token_list=[]
for i in token_no:
    new_token_list.append(list(tokenizer.vocab.keys())[i])

#print(new_token_list); ### you can use it if you want to check back the tokens.
</code></pre>
",4,1,2953,2019-06-21 17:51:48,https://stackoverflow.com/questions/56708296/bert-multilingual-in-pytorch
Using BERT in order to detect language of a given word,"<p>I have words in the Hebrew language. Part of them are originally in English, and part of them are 'Hebrew English', meaning that those are words that are originally from English but are written with Hebrew words. 
For example: 'insulin' in Hebrew is ""אינסולין"" (Same phonetic sound). </p>

<p>I have a simple binary dataset. 
X: words (Written with Hebrew characters)
y: label 1 if the word is originally in English and is written with Hebrew characters, else 0</p>

<p>I've tried using the classifier, but the input for it is full text, and my input is just words. </p>

<p>I don't want any MASKING to happen, I just want simple classification.</p>

<p>Is it possible to use BERT for this mission? Thanks</p>
","deep-learning, nlp, classification, bert-language-model","<p>BERT is intended to work with words in context. Without context, a BERT-like model is equivalent to simple word2vec lookup (there is fancy tokenization, but I don't know how it works with Hebrew - probably, not very efficiently). So if you really really want to use distributional features in your classifier, you can take a pretrained word2vec model instead - it's simpler than BERT, and no less powerful.</p>

<p>But I'm not sure it will work anyway. Word2vec and its equivalents (like BERT without context) don't know much about inner structure of a word - only about contexts it is used in. In your problem, however, word structure is more important than possible contexts. For example, words בלוטת (gland) or דם (blood) or סוכר (sugar) often occur in the same context as insulin, but בלוטת and דם are Hebrew, whereas סוכר is English (okay, originally Arabic, but we are probably not interested in too ancient origins). You just cannot predict it from context only.</p>

<p>So why not start with some simple model (e.g. logistic regression or even naive bayes) over simple features (e.g. character n-grams)? Distributional features (I mean w2v) may be added as well, because they tell about topic, and topics may be informative (e.g. in medicine, and technology in general, there are probably relatively more English words than in other domains). </p>
",4,2,1521,2019-06-23 10:48:19,https://stackoverflow.com/questions/56723111/using-bert-in-order-to-detect-language-of-a-given-word
"Why are the matrices in BERT called Query, Key, and Value?","<p>Within the transformer units of <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a>, there are modules called Query, Key, and Value, or simply Q,K,V.</p>

<p>Based on the BERT <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> and <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">code</a> (particularly in <a href=""https://github.com/google-research/bert/blob/master/modeling.py"" rel=""nofollow noreferrer"">modeling.py</a>), my pseudocode understanding of the forward-pass of an attention module (using Q,K,V) with a single attention-head is as follows:</p>

<pre><code>q_param = a matrix of learned parameters
k_param = a matrix of learned parameters
v_param = a matrix of learned parameters
d = one of the matrix dimensions (scalar value)

def attention(to_tensor, from_tensor, attention_mask):
    q = from_tensor * q_param
    k = to_tensor * k_param
    v = to_tensor * v_param

    attention_scores = q * transpose(k) / sqrt(d)
    attention_scores += some_function(attention_mask) #attention_mask is usually just ones
    attention_probs = dropout(softmax(attention_scores))
    context = attention_probs * v

    return context
</code></pre>

<p>Note that BERT uses ""self-attention,"" so <code>from_tensor</code> and <code>to_tensor</code> are the same in BERT; I think both of these are simply the output from the previous layer.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Why are the matrices called Query, Key, and Value?</li>
<li>Did I make any mistakes in my pseudocode representation of the algorithm?</li>
</ol>
","python, tensorflow, deep-learning, nlp, bert-language-model","<p>For your first question, BERT is based on the encoder of the transformer model from the <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">2017 Vaswani et al ""Attention is all you need""</a> paper. The queries, keys, and values metaphor appears already in that paper (although I have learned it is not the source of this idea since the comments above). However, the metaphor actually works best for the other part of the transformer, namely the decoder; this is because as you say the encoder uses self attention, and it seems to me that the queries and keys play a symmetric role in BERT. So perhaps it would be easier to understand this metaphor for the transformer's decoder rather than for BERT.</p>

<p>To my understanding, in the Vaswani et al transformer model, the queries and keys allow all positions of the decoder layer <code>j-1</code> to attend to all positions of the encoder layer <code>j</code> via the attention scores. The values are then selected by the queries and keys: the result of the attention layer is the sum of values weighted by the attention scores. The projections of queries and keys determine where the attention for each position is placed. For example, an extreme case could be that the queries are projected by the identity function and the keys are projected to a permutation which moves position <code>i</code> to position <code>i+1</code>. The dot product of the keys and queries would allow each position of decoder layer <code>j-1</code> to attend to the position before it in encoder layer <code>j</code>. So the decoder layer <code>j-1</code> is referred to as the queries when, together with the keys, it decides how much each position in decoder layer <code>j-1</code> (again, but not referred to as the values) will contribute. </p>
",5,5,3913,2019-06-25 02:49:08,https://stackoverflow.com/questions/56746191/why-are-the-matrices-in-bert-called-query-key-and-value
How to use trained BERT model checkpoints for prediction?,"<p>I trained the BERT with SQUAD 2.0 and got the <code>model.ckpt.data</code>, <code>model.ckpt.meta</code>, <code>model.ckpt.index</code> (F1 score : 81) in the output directory along with <code>predictions.json</code>, etc. using the <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">BERT-master</a><code>/run_squad.py</code></p>
<pre><code>python run_squad.py \
  --vocab_file=$BERT_LARGE_DIR/vocab.txt \
  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \
  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \
  --do_train=True \
  --train_file=$SQUAD_DIR/train-v2.0.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True
</code></pre>
<p>I tried to copy the <code>model.ckpt.meta</code>, <code>model.ckpt.index</code>, <code>model.ckpt.data</code> to the <code>$BERT_LARGE_DIR</code> directory and changed the <code>run_squad.py</code> flags as follows to only predict the answer and not train using a dataset:</p>
<pre><code>python run_squad.py \
  --vocab_file=$BERT_LARGE_DIR/vocab.txt \
  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \
  --init_checkpoint=$BERT_LARGE_DIR/model.ckpt \
  --do_train=False \
  --train_file=$SQUAD_DIR/train-v2.0.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True
</code></pre>
<p>It throws bucket directory/model.ckpt does not exist error.</p>
<p>How to utilize the checkpoints generated after training and use it for prediction?</p>
","python, tensorflow, neural-network, google-cloud-tpu, bert-language-model","<p>Usually, the trained checkpoints are created in the directory specified by <code>--output_dir</code> parameter while training. (Which is <code>gs://some_bucket/squad_large/</code> in your case). Every checkpoint will have a number. You have to identify the biggest number; example: <code>model.ckpt-12345</code>. Now, set the <code>--init_checkpoint</code> parameter in your evaluation/prediction, using the output directory and the last saved checkpoint (The model with the highest number). (In your case, it shall be something like <code>--init_checkpoint=gs://some_bucket/squad_large/model.ckpt-&lt;highest number&gt;</code>)</p>
",3,5,5604,2019-06-28 04:28:46,https://stackoverflow.com/questions/56800910/how-to-use-trained-bert-model-checkpoints-for-prediction
ImportError: cannot import name &#39;warmup_linear&#39;,"<p>While trying to import <code>warmup_linear</code>, I'm getting this error</p>

<pre><code>ImportError: cannot import name 'warmup_linear'
</code></pre>

<p>Import - </p>

<pre><code>from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear
</code></pre>

<p>Requirements file</p>

<pre><code>boto3==1.9.198
botocore==1.12.198
certifi==2019.6.16
chardet==3.0.4
docutils==0.14
h5py==2.9.0
idna==2.8
jmespath==0.9.4
Keras==2.2.4
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
numpy==1.17.0
Pillow==6.1.0
python-dateutil==2.8.0
pytorch-pretrained-bert==0.6.2
PyYAML==5.1.1
regex==2019.6.8
requests==2.22.0
s3transfer==0.2.1
scipy==1.3.0
seqeval==0.0.12
six==1.12.0
torch==1.1.0
torchvision==0.3.0
tqdm==4.32.2
urllib3==1.25.3
</code></pre>

<p>What needs to be done to import 'warmup_linear'? </p>
","pytorch, torch, bert-language-model","<p>The version 0.4.0 doesn't give this issue.
<code>pip install pytorch_pretrained_bert==0.4.0</code>
or downgrading to 0.6.1 solved it.</p>
",3,3,5795,2019-07-30 07:18:56,https://stackoverflow.com/questions/57266256/importerror-cannot-import-name-warmup-linear
PyTorch BERT TypeError: forward() got an unexpected keyword argument &#39;labels&#39;,"<p>Training a BERT model using PyTorch transformers (following the tutorial <a href=""https://mccormickml.com/2019/07/22/BERT-fine-tuning/"" rel=""noreferrer"">here</a>).</p>

<p>Following statement in the tutorial</p>

<pre><code>loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
</code></pre>

<p>leads to</p>

<pre><code>TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>Here is the full error,</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-53-56aa2f57dcaf&gt; in &lt;module&gt;
     26         optimizer.zero_grad()
     27         # Forward pass
---&gt; 28         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
     29         train_loss_set.append(loss.item())
     30         # Backward pass

~/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>I cant seem to figure out what kind of argument the forward() function expects.</p>

<p>There is a similar problem <a href=""https://github.com/allenai/allennlp/issues/2528"" rel=""noreferrer"">here</a>, but I still do not get what the solution is.</p>

<p>System information:</p>

<ul>
<li>OS: Ubuntu 16.04 LTS</li>
<li>Python version: 3.6.x</li>
<li>Torch version: 1.3.0</li>
<li>Torch Vision version: 0.4.1</li>
<li>PyTorch transformers version: 1.2.0</li>
</ul>
","python, pytorch, bert-language-model, huggingface-transformers","<p>As far as I know, the BertModel does not take labels in the <code>forward()</code> function. Check out the <a href=""https://github.com/huggingface/transformers/blob/7d9a33fb5cf40a87ff7fa9b4b8556b9bd4760461/src/transformers/models/bert/modeling_bert.py#L189"" rel=""noreferrer"">forward</a> function parameters.</p>
<p>I suspect you are trying to fine-tune the BertModel for sequence classification task and the API provides a class for that which is <a href=""https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L849"" rel=""noreferrer"">BertForSequenceClassification</a>. As you can see its forward() function definition:</p>
<pre><code>def forward(self, input_ids, attention_mask=None, token_type_ids=None,
            position_ids=None, head_mask=None, labels=None):
</code></pre>
<p>Please note, the forward() method returns the followings.</p>
<pre><code>Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:
        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:
            Classification (or regression if config.num_labels==1) loss.
        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``
            Classification (or regression if config.num_labels==1) scores (before SoftMax).
        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)
            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)
            of shape ``(batch_size, sequence_length, hidden_size)``:
            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
        **attentions**: (`optional`, returned when ``config.output_attentions=True``)
            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:
            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads. 
</code></pre>
<p>Hope this helps!</p>
",25,26,71862,2019-10-18 15:42:38,https://stackoverflow.com/questions/58454157/pytorch-bert-typeerror-forward-got-an-unexpected-keyword-argument-labels
How to use BERT just for ENTITY extraction from a Sequence without classification in the NER task?,"<p>My requirement here is given a sentence(sequence), I would like to just extract the entities present in the sequence without classifying them to a type in the NER task. I see that BertForTokenClassification for NER does the classification. Can this be adapted for just the extraction?</p>

<p>Can BERT just be used to do <strong>entity extraction/identification</strong>?</p>
","nlp, pytorch, named-entity-recognition, named-entity-extraction, bert-language-model","<p>Regardless BERT, NER tagging is usually done by tagging with the IOB format (inside, outside, beginning) or something similar (often the end is also explicitly tagged). The <em>inside</em> and <em>beggining</em> tags contain the entity type. Something like this:</p>

<pre><code>Alex B-PER
is O
going O
to O
Los B-LOC
Angeles I-LOC
</code></pre>

<p>If you modify your training data, such that there will be only one entity type, the model will only learn to detect the entities without knowing what type the entity is.</p>

<pre><code>Alex B
is O
going O
to O
Los B
Angeles I
</code></pre>
",0,0,613,2019-10-24 12:50:47,https://stackoverflow.com/questions/58541811/how-to-use-bert-just-for-entity-extraction-from-a-sequence-without-classificatio
Bert Embedding Layer raises `Type Error: unsupported operand type(s) for +: &#39;None Type&#39; and &#39;int&#39;` with BiLSTM,"<p>I've problems integrating Bert Embedding Layer in a BiLSTM model for word sense disambiguation task, </p>

<pre><code>Windows 10
Python 3.6.4
TenorFlow 1.12
Keras 2.2.4
No virtual environments were used
PyCharm Professional 2019.2
</code></pre>

<p>
The whole script
<br></p>

<pre><code>import os
import yaml
import numpy as np
from argparse import ArgumentParser

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.layers import (LSTM, Add, Bidirectional, Dense, Input, TimeDistributed, Embedding)

from tensorflow.keras.preprocessing.sequence import pad_sequences

try:
    from bert.tokenization import FullTokenizer
except ModuleNotFoundError:
    os.system('pip install bert-tensorflow')

from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tqdm import tqdm

from keras_bert import BertEmbeddingLayer
from model_utils import visualize_plot_mdl
from parsing_dataset import load_dataset
from utilities import configure_tf, initialize_logger


def parse_args():
    parser = ArgumentParser(description=""WSD"")
    parser.add_argument(""--model_type"", default='baseline', type=str,
                        help=""""""Choose the model: baseline: BiLSTM Model.
                                attention: Attention Stacked BiLSTM Model.
                                seq2seq: Seq2Seq Attention."""""")

    return vars(parser.parse_args())


def train_model(mdl, data, epochs=1, batch_size=32):
    [train_input_ids, train_input_masks, train_segment_ids], train_labels = data
    history = mdl.fit([train_input_ids, train_input_masks, train_segment_ids],
                      train_labels, epochs=epochs, batch_size=batch_size)
    return history


def baseline_model(output_size):
    hidden_size = 128
    max_seq_len = 64

    in_id = Input(shape=(None,), name=""input_ids"")
    in_mask = Input(shape=(None,), name=""input_masks"")
    in_segment = Input(shape=(None,), name=""segment_ids"")
    bert_inputs = [in_id, in_mask, in_segment]

    bert_embedding = BertEmbeddingLayer()(bert_inputs)
    embedding_size = 768

    bilstm = Bidirectional(LSTM(hidden_size, dropout=0.2,
                                recurrent_dropout=0.2,
                                return_sequences=True
                                )
                           )(bert_embedding)

    output = TimeDistributed(Dense(output_size, activation=""softmax""))(bilstm)

    mdl = Model(inputs=bert_inputs, outputs=output, name=""Bert_BiLSTM"")

    mdl.compile(loss=""sparse_categorical_crossentropy"",
                optimizer='adadelta', metrics=[""acc""])

    return mdl


def initialize_vars(sess):
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    K.set_session(sess)


class PaddingInputExample(object):
    """"""Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  batches could cause silent errors.
  """"""

class InputExample(object):
    """"""A single training/test example for simple sequence classification.""""""

    def __init__(self, guid, text_a, text_b=None, label=None):
        """"""Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The un-tokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The un-tokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """"""
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


def create_tokenizer_from_hub_module(bert_path=""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1""):
    """"""Get the vocab file and casing info from the Hub module.""""""
    bert_module = hub.Module(bert_path)
    tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True)
    vocab_file, do_lower_case = sess.run(
        [
            tokenization_info[""vocab_file""],
            tokenization_info[""do_lower_case""],
        ]
    )

    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)


def convert_single_example(tokenizer, example, max_seq_length=256):
    """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

    if isinstance(example, PaddingInputExample):
        input_ids = [0] * max_seq_length
        input_mask = [0] * max_seq_length
        segment_ids = [0] * max_seq_length
        label = [0] * max_seq_length
        return input_ids, input_mask, segment_ids, label

    tokens_a = tokenizer.tokenize(example.text_a)
    if len(tokens_a) &gt; max_seq_length - 2:
        tokens_a = tokens_a[0: (max_seq_length - 2)]

    tokens = []
    segment_ids = []
    tokens.append(""[CLS]"")
    segment_ids.append(0)
    example.label.append(0)
    for token in tokens_a:
        tokens.append(token)
        segment_ids.append(0)
    tokens.append(""[SEP]"")
    segment_ids.append(0)
    example.label.append(0)

    input_ids = tokenizer.convert_tokens_to_ids(tokens)

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    input_mask = [1] * len(input_ids)

    # Zero-pad up to the sequence length.
    while len(input_ids) &lt; max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)
        example.label.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    return input_ids, input_mask, segment_ids, example.label


def convert_examples_to_features(tokenizer, examples, max_seq_length=256):
    """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""

    input_ids, input_masks, segment_ids, labels = [], [], [], []
    for example in tqdm(examples, desc=""Converting examples to features""):
        input_id, input_mask, segment_id, label = convert_single_example(tokenizer, example, max_seq_length)
        input_ids.append(np.array(input_id))
        input_masks.append(np.array(input_mask))
        segment_ids.append(np.array(segment_id))
        labels.append(np.array(label))
    return np.array(input_ids), np.array(input_masks), np.array(segment_ids), np.array(labels).reshape(-1, 1)


def convert_text_to_examples(texts, labels):
    """"""Create InputExamples""""""
    InputExamples = []
    for text, label in zip(texts, labels):
        InputExamples.append(
            InputExample(guid=None, text_a="" "".join(text), text_b=None, label=label)
        )
    return InputExamples


# Initialize session
sess = tf.Session()

params = parse_args()
initialize_logger()
configure_tf()

# Load our config file
config_file_path = os.path.join(os.getcwd(), ""config.yaml"")
config_file = open(config_file_path)
config_params = yaml.load(config_file)

# This parameter allow that train_x to be in form of words, to allow using of your keras-elmo layer
elmo = config_params[""use_elmo""]  
dataset = load_dataset(elmo=elmo)
vocabulary_size = dataset.get(""vocabulary_size"")
output_size = dataset.get(""output_size"")

# Parse data in Bert format
max_seq_length = 64
train_x = dataset.get(""train_x"")
train_text = [' '.join(x) for x in train_x]
train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]
train_text = np.array(train_text, dtype=object)[:, np.newaxis]
# print(train_text.shape)  # (37184, 1)
train_labels = dataset.get(""train_y"")

# Instantiate tokenizer
tokenizer = create_tokenizer_from_hub_module()

# Convert data to InputExample format
train_examples = convert_text_to_examples(train_text, train_labels)

# Extract features
(train_input_ids, train_input_masks, train_segment_ids, train_labels) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)

bert_inputs = [train_input_ids, train_input_masks, train_segment_ids]
data = bert_inputs, train_labels
del dataset

model = baseline_model(output_size)

# Instantiate variables
initialize_vars(sess)

history = train_model(model, data)

</code></pre>

<p>
The layer <code>BertEmbeddingLayer()</code> is imported from <a href=""https://github.com/strongio/keras-bert/blob/b40f8fd52a2100f733f44832ae1cedc7e9f22558/keras-bert.py#L165"" rel=""nofollow noreferrer"">strongio/keras-bert</a>, as well as following the approach in the file to integrate my work however I always have this error, please check the traceback below (exception is raised when building the model)</p>

<pre><code>Traceback (most recent call last):
  File ""code/prova_bert.py"", line 230, in &lt;module&gt;
    model = baseline_model(output_size, max_seq_len, visualize=True)
  File ""code/prova_bert.py"", line 165, in baseline_model
    )(bert_embeddings)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\wrappers.py"", line 473, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 746, in __call__
    self.build(input_shapes)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\wrappers.py"", line 612, in build
    self.forward_layer.build(input_shape)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 149, in wrapper
    output_shape = fn(instance, input_shape)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 552, in build
    self.cell.build(step_input_shape)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 149, in wrapper
    output_shape = fn(instance, input_shape)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 1934, in build
    constraint=self.kernel_constraint)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 609, in add_weight
    aggregation=aggregation)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\training\checkpointable\base.py"", line 639, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1977, in make_variable
    aggregation=aggregation)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py"", line 183, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py"", line 146, in _variable_v1_call
    aggregation=aggregation)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py"", line 125, in &lt;lambda&gt;
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2437, in default_variable_creator
    import_scope=import_scope)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py"", line 187, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 297, in __init__
    constraint=constraint)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 409, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1959, in &lt;lambda&gt;
    shape, dtype=dtype, partition_info=partition_info)
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\init_ops.py"", line 473, in __call__
    scale /= max(1., (fan_in + fan_out) / 2.)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
Exception ignored in: &lt;bound method BaseSession.__del__ of &lt;tensorflow.python.client.session.Session object at 0x0000026396AD0630&gt;&gt;
Traceback (most recent call last):
  File ""C:\Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\client\session.py"", line 738, in __del__
TypeError: 'NoneType' object is not callable
</code></pre>

<p>Please refer to my <a href=""https://github.com/strongio/keras-bert/issues/30"" rel=""nofollow noreferrer"">issue</a> on their repo and for data examples being fed to the model please check this <a href=""https://github.com/strongio/keras-bert/issues/3#issuecomment-547067170"" rel=""nofollow noreferrer"">issue</a></p>
","python, tensorflow, keras, recurrent-neural-network, bert-language-model","<p>First of all, the results by ""mean"" or ""first"" pooling is not for all the tokens, so you got to change in <code>call()</code> function:</p>

<pre><code>elif self.pooling == ""mean"": 
    result = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[""sequence_output"" ] 
    pooled = result
</code></pre>

<p>In build_model, change to:</p>

<pre><code>embedding_size = 768
in_id = Input(shape=(max_seq_length,), name=""input_ids"") 
in_mask = Input(shape=(max_seq_length,), name=""input_masks"")
in_segment = Input(shape=(max_seq_length,), name=""segment_ids"")

bert_inputs = [in_id, in_mask, in_segment] 
bert_output = BertLayer(n_fine_tune_layers=12, pooling=""mean"")(bert_inputs) 
bert_output = Reshape((max_seq_length, embedding_size))(bert_output) 

bilstm = Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2,return_sequences=True))(bert_output)
output = Dense(output_size, activation=""softmax"")(bilstm)

</code></pre>
",4,6,3154,2019-10-29 12:48:02,https://stackoverflow.com/questions/58607787/bert-embedding-layer-raises-type-error-unsupported-operand-types-for-non
BertTokenizer - when encoding and decoding sequences extra spaces appear,"<p>When using Transformers from HuggingFace I am facing a problem with the encoding and decoding method.</p>

<p>I have a the following string:</p>

<pre><code>test_string = 'text with percentage%'
</code></pre>

<p>Then I am running the following code:</p>

<pre><code>import torch
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

test_string = 'text with percentage%'

# encode Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.
input_ids = tokenizer.encode(test_string)
output = tokenizer.decode(input_ids)
</code></pre>

<p>And the output looks like this:</p>

<pre><code>'text with percentage %'
</code></pre>

<p>With an extra space before the %. I have tried the extra arguments like <code>clean_up_tokenization_spaces</code>  but this is for something different.</p>

<p>How what should I use in the decoding and encoding to get exactly the same text before and after. This also happens for other special signs.  </p>
","python, pytorch, tokenize, torch, bert-language-model","<p>If you are trying to use BERT for token classification in order to find a span in your original string, then one workaround is to use <code>BertTokenizerFast</code> with the option <code>return_offsets_mapping=True</code>.</p>
<pre><code>test_string = 'text with percentage%'

tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
tokens = tokenizer(test_string, return_offsets_mapping=True)
input_ids = tokens.data[&quot;input_ids&quot;]

span_start_index, span_stop_index = some_model(input_ids)
</code></pre>
<p>Then once you get the token classification results, you can do something like</p>
<pre><code>predicted_span = test_string[tokens.encodings[0].offsets[span_start_index][0]:tokens.encodings[0].offsets[span_stop_index][1]]
</code></pre>
",4,10,13980,2019-11-21 16:43:31,https://stackoverflow.com/questions/58979779/berttokenizer-when-encoding-and-decoding-sequences-extra-spaces-appear
Get probability of multi-token word in MASK position,"<p>It is relatively easy to get a token's probability according to a language model, as the snippet below shows. You can get the output of a model, restrict yourself to the output of the masked token, and then find the probability of your requested token in the output vector. However, this only works with single-token words, e.g. words that are themselves in the tokenizer's vocabulary. When a word does not exist in the vocabulary, the tokenizer will chunk it up into pieces that it <em>does</em> know (see the bottom of the example). But since the input sentence consists of only one masked position, and the requested token has more tokens than that, how can we get its probability? Ultimately I am looking for a solution that works regardless of the number of subword units a word has.</p>

<p>In the code below I have added many comments explaining what is going on, as well as printing out the given output of print statements. You'll see that predicting tokens such as 'love' and 'hate' is straightforward because they are in the tokenizer's vocabulary. 'reprimand' is not, though, so it cannot be predicted in a single masked position - it consists of three subword units. So how can we predict 'reprimand' in the masked position?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
import torch

# init model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()
# init softmax to get probabilities later on
sm = torch.nn.Softmax(dim=0)
torch.set_grad_enabled(False)

# set sentence with MASK token, convert to token_ids
sentence = f""I {tokenizer.mask_token} you""
token_ids = tokenizer.encode(sentence, return_tensors='pt')
print(token_ids)
# tensor([[ 101, 1045,  103, 2017,  102]])
# get the position of the masked token
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)
# only get output for masked token
# output is the size of the vocabulary
mask_hidden_state = last_hidden_state[masked_position]
# convert to probabilities (softmax)
# giving a probability for each item in the vocabulary
probs = sm(mask_hidden_state)

# get probability of token 'hate'
hate_id = tokenizer.convert_tokens_to_ids('hate')
print('hate probability', probs[hate_id].item())
# hate probability 0.008057191967964172

# get probability of token 'love'
love_id = tokenizer.convert_tokens_to_ids('love')
print('love probability', probs[love_id].item())
# love probability 0.6704086065292358

# get probability of token 'reprimand' (?)
reprimand_id = tokenizer.convert_tokens_to_ids('reprimand')
# reprimand is not in the vocabulary, so it needs to be split into subword units
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# [UNK]

reprimand_id = tokenizer.encode('reprimand', add_special_tokens=False)
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# ['rep', '##rim', '##and']
# but how do we now get the probability of a multi-token word in a single-token position?
</code></pre>
","python, pytorch, transformer-model, bert-language-model, huggingface-transformers","<p>Since the split word is not present in the dictionary, BERT is simply unaware of its probability, so there is no use of masking it before tokenization.</p>
<p>And you can't get its probability by exploiting the rule of chain, see <a href=""https://github.com/google-research/bert/issues/35"" rel=""nofollow noreferrer"">response</a> by J.Devlin. To illustrate it, let's take a more generic example. Try to estimate the probability of some bigram in position <code>i</code>. While you can estimate the probability of each word given the sentence and their positions</p>
<p><code>P(w_i|w_0, w_1... w_i-1, w_i+1, ..., w_N)</code>,</p>
<p><code>P(w_i+1|w_0, w_1... w_i, wi+2, ..., w_N)</code>,</p>
<p>there is no way to get the probability of the bigram</p>
<p><code>P(w_i,w_i+1|w_0, w_1... w_i-1, wi+2, ..., w_N)</code></p>
<p>because BERT does not store such information.</p>
<p>Having said all that, you can get a <strong>very rough</strong> estimate of the probability of your OOV word by multiplying probabilities of seeing it's parts. So you will get</p>
<p><code>P(&quot;reprimand&quot;|...) ~= P(&quot;rep&quot;|...)*P(&quot;##rim&quot;|...)*P(&quot;##and&quot;|...)</code></p>
<p>Since your subwords are not regular words, but a special kind of words, this is not all wrong, because the dependency between them is implicit.</p>
",7,12,5091,2019-12-21 09:24:46,https://stackoverflow.com/questions/59435020/get-probability-of-multi-token-word-in-mask-position
keras LSTM get hidden-state (converting sentece-sequence to document context vectors),"<p>Im trying to create document context vectors from sentence-vectors via LSTM using keras (so each document consist of a sequence of sentence vectors).</p>

<p>My goal is to replicate the following blog post using keras: <a href=""https://andriymulyar.com/blog/bert-document-classification"" rel=""nofollow noreferrer"">https://andriymulyar.com/blog/bert-document-classification</a></p>

<p>I have a (toy-)tensor, that looks like this: <code>X = np.array(features).reshape(5, 200, 768)</code> So 5 documents with each having a 200 sequence of sentence vectors - each sentence vector having 768 features.</p>

<p>So to get an embedding from my sentence vectors, I encoded my documents as one-hot-vectors to learn an LSTM:</p>

<pre><code>y = [1,2,3,4,5] # 5 documents in toy-tensor
y = np.array(y)
yy = to_categorical(y)
yy = yy[0:5,1:6]
</code></pre>

<p>Until now, my code looks like this</p>

<pre><code>inputs1=Input(shape=(200,768))
lstm1, states_h, states_c =LSTM(5,dropout=0.3,recurrent_dropout=0.2, return_state=True)(inputs1)
model1=Model(inputs1,lstm1)
model1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc']) 
model1.summary()
model1.fit(x=X,y=yy,batch_size=100,epochs=10,verbose=1,shuffle=True,validation_split=0.2)
</code></pre>

<p><strong>When I print <code>states_h</code> I get a tensor of shape=(?,5) and I dont really know how to access the vectors inside the tensor, which should represent my documents.</strong></p>

<pre><code>print(states_h)
Tensor(""lstm_51/while/Exit_3:0"", shape=(?, 5), dtype=float32)
</code></pre>

<p>Or am I doing something wrong? To my understanding there should be 5 document vectors e.g. <code>doc1=[...] ; ...; doc5=[...]</code> so that I can reuse the document vectors for a classification task.</p>
","python, keras, lstm, embedding, bert-language-model","<p>Well, printing a tensor shows exactly this: it's a tensor, it has that shape and that type. </p>

<p>If you want to see data, you need to feed data.<br>
States are not weights, they are not persistent, they only exist with input data, just as any other model output. </p>

<p>You should create a model that outputs this information (yours doesn't) in order to grab it. You can have two models:</p>

<pre><code>#this is the model you compile and train - exactly as you are already doing
training_model = Model(inputs1,lstm1)     

#this is just for getting the states, nothing else, don't compile, don't train
state_getting_model = Model(inputs1, [lstm1, states_h, states_c]) 
</code></pre>

<p>(Don't worry, these two models will share the same weights and be updated together, even if you only train the <code>training_model</code>)    </p>

<p>Now you can:</p>

<p><strong>With eager mode off (and probably ""on"" too):</strong></p>

<pre><code>lstm_out, states_h_out, states_c_out = state_getting_model.predict(X)
print(states_h_out)
print(states_c_out)
</code></pre>

<p><strong>With eager mode on:</strong></p>

<pre><code>lstm_out, states_h_out, states_c_out = state_getting_model(X)
print(states_h_out.numpy())
print(states_c_out.numpy())
</code></pre>
",3,2,1403,2019-12-27 09:11:36,https://stackoverflow.com/questions/59498423/keras-lstm-get-hidden-state-converting-sentece-sequence-to-document-context-vec
estimator.train throws ValueError: model_fn should return an EstimatorSpec,"<p>Here's the code I'm using...</p>

<p>I've got a breakpoint installed at what is for me line 304...</p>

<p>estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)</p>

<p>Has anyone seen this?  I'm certain I have the correct versions of TensorFlow and BERT installed.</p>

<p>The complete stack trace is as follows....</p>

<pre><code>    Exception has occurred: ValueError
    model_fn should return an EstimatorSpec.
    File ""C:\Program Files\Python36\Lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1153, in _call_model_fn
 raise ValueError('model_fn should return an EstimatorSpec.')
    File ""C:\Program Files\Python36\Lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1191, in _train_model_default
features, labels, ModeKeys.TRAIN, self.config)
    File ""C:\Program Files\Python36\Lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1161, in _train_model
return self._train_model_default(input_fn, hooks, saving_listeners)
    File ""C:\Program Files\Python36\Lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 370, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
    File ""C:\Users\brownru\eclipse-workspace\tiaaNLPPython\org\tiaa\ai\penelope\bertNLP\sentiment\sentiment.py"", line 304, in &lt;module&gt;
estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
    File ""C:\Program Files\Python36\Lib\runpy.py"", line 85, in _run_code
exec(code, run_globals)
    File ""C:\Program Files\Python36\Lib\runpy.py"", line 96, in _run_module_code
mod_name, mod_spec, pkg_name, script_name)
    File ""C:\Program Files\Python36\Lib\runpy.py"", line 263, in run_path
pkg_name=pkg_name, script_name=fname)
    ValueError: model_fn should return an EstimatorSpec.
</code></pre>

<p>This code is my attempt to run some Google colab code from here - </p>

<p><a href=""https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=t6Nukby2EB6-"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=t6Nukby2EB6-</a></p>

<pre><code># Copyright 2019 Google Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# install --proxy http://proxy.ops.tiaa-cref.org:8080 tensorFlow

import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_estimator as tfe
from datetime import datetime

import bert
from bert import run_classifier
from bert import optimization
from bert import tokenization


# Set the output directory for saving model file
# Optionally, set a GCP bucket location

OUTPUT_DIR = r'C:\Users\brownru\Documents\npsExplanationComplains\sentimentOutput' 
#@markdown Whether or not to clear/delete the directory and create a new one
DO_DELETE = True #@param {type:""boolean""}
#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.
USE_BUCKET = False #@param {type:""boolean""}
BUCKET = 'BUCKET_NAME' #@param {type:""string""}

if USE_BUCKET:
    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)
#from google.colab import auto
#auth.authenticate_user()

if DO_DELETE:
    try:
        tf.gfile.DeleteRecursively(OUTPUT_DIR)
    except:
            # Doesn't matter if the directory didn't exist
            pass
    tf.gfile.MakeDirs(OUTPUT_DIR)
    print('***** Model output directory: {} *****'.format(OUTPUT_DIR))

'''
First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub).
'''
from tensorflow import keras
import os
import re

# Load all files from a directory in a DataFrame.
def load_directory_data(directory):
    data = {}
    data[""sentence""] = []
    data[""sentiment""] = []
    for file_path in os.listdir(directory):
        with tf.gfile.GFile(os.path.join(directory, file_path), ""r"") as f:
            data[""sentence""].append(f.read())
            data[""sentiment""].append(re.match(""\d+_(\d+)\.txt"", file_path).group(1))
    return pd.DataFrame.from_dict(data)

# Merge positive and negative examples, add a polarity column and shuffle.
def load_dataset(directory):
    pos_df = load_directory_data(os.path.join(directory, ""pos""))
    neg_df = load_directory_data(os.path.join(directory, ""neg""))
    pos_df[""polarity""] = 1
    neg_df[""polarity""] = 0
    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)

# Download and process the dataset files.
def download_and_load_datasets():
    #dataset = tf.keras.utils.get_file(fname=""aclImdb.tar.gz"", origin=""http://chapdc3sas51.ops.tiaa-cref.org/nlpAssets/aclImdb_v1.tar.gz"", extract=True)
    trainPath = r'C:\Users\brownru\.keras\datasets\aclImdb\train'
    testPath = r'C:\Users\brownru\.keras\datasets\aclImdb\test'
    train_df = load_dataset(trainPath)
    test_df = load_dataset(testPath)

    return train_df, test_df

train, test = download_and_load_datasets()

#To keep training fast, we'll take a sample of 5000 train and test examples, respectively.

train = train.sample(5000)
test = test.sample(5000)

train.columns

#Index(['sentence', 'sentiment', 'polarity'], dtype='object')

#For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respectively)

DATA_COLUMN = 'sentence'
LABEL_COLUMN = 'polarity'
# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'
label_list = [0, 1]


#Data Preprocessing We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExample's using the constructor provided in the BERT library.
#text_a is the text we want to classify, which in this case, is the Request field in our Dataframe. 
#text_b is used if we're training a model to understand the relationship between sentences (i.e. is text_b a translation of text_a? Is text_b an answer to the question asked by text_a?). This doesn't apply to our task, so we can leave text_b blank.
#label is the label for our example, i.e. True, False

# Use the InputExample class from BERT's run_classifier code to create examples from the data
train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example
                                                                text_a = x[DATA_COLUMN], 
                                                                text_b = None, 
                                                                label = x[LABEL_COLUMN]), axis = 1)

test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                text_a = x[DATA_COLUMN], 
                                                                text_b = None, 
                                                                label = x[LABEL_COLUMN]), axis = 1)



# This is a path to an uncased (all lowercase) version of BERT
BERT_MODEL_HUB = ""http://chapdc3sas51.ops.tiaa-cref.org/nlpAssets/1.tar.gz""

def create_tokenizer_from_hub_module():
    with tf.Graph().as_default():
        bert_module = hub.Module(BERT_MODEL_HUB)
        tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True)
        with tf.Session() as sess:
                vocab_file, do_lower_case = sess.run([tokenization_info[""vocab_file""],tokenization_info[""do_lower_case""]])      
    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

tokenizer = create_tokenizer_from_hub_module()

tokenizer.tokenize(""This here's an example of using the BERT tokenizer"")

# We'll set sequences to be at most 128 tokens long TEST.
MAX_SEQ_LENGTH = 128
# Convert our train and test features to InputFeatures that BERT understands.
train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)
test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

#Creating a model

def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):
#Creates a classification model.
    bert_module = hub.Module(BERT_MODEL_HUB,trainable=True)
    bert_inputs = dict(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)
    bert_outputs = bert_module(inputs=bert_inputs,signature=""tokens"",as_dict=True)

# Use ""pooled_output"" for classification tasks on an entire sentence.
# Use ""sequence_outputs"" for token-level output.
    output_layer = bert_outputs[""pooled_output""]

    hidden_size = output_layer.shape[-1].value

# Create our own layer to tune for politeness data.  
    output_weights = tf.get_variable(""output_weights"", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))
    output_bias = tf.get_variable(""output_bias"", [num_labels], initializer=tf.zeros_initializer())

    with tf.variable_scope(""loss""):
        # Dropout helps prevent overfitting
        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

        logits = tf.matmul(output_layer, output_weights, transpose_b=True)
        logits = tf.nn.bias_add(logits, output_bias)
        log_probs = tf.nn.log_softmax(logits, axis=-1)

        # Convert labels into one-hot encoding
        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)
        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))
        # If we're predicting, we want predicted labels and the probabilities.
        if is_predicting:
            return (predicted_labels, log_probs)

        # If we're train/eval, compute loss between predicted and actual label
    per_example_loss = tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, predicted_labels, log_probs)


'''Next we'll wrap our model function in a model_fn_builder function that adapts our model to work for training, evaluation, and prediction.'''

# model_fn_builder actually creates our model function
# using the passed parameters for num_labels, learning_rate, etc.
def model_fn_builder(num_labels, learning_rate, num_train_steps,
                     num_warmup_steps):
#Returns `model_fn` closure for TPUEstimator.""""""
    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    #""""""The `model_fn` for TPUEstimator.""""""

        input_ids = features[""input_ids""]
        input_mask = features[""input_mask""]
        segment_ids = features[""segment_ids""]
        label_ids = features[""label_ids""]

        is_predicting = (mode == tfe.estimator.ModeKeys.PREDICT)

    # TRAIN and EVAL
        if not is_predicting:

            (loss, predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)
            train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)

# Calculate evaluation metrics. 
            def metric_fn(label_ids, predicted_labels):
                        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)
                        f1_score = tf.contrib.metrics.f1_score(
                            label_ids,
                            predicted_labels)
                        auc = tf.metrics.auc(
                            label_ids,
                            predicted_labels)
                        recall = tf.metrics.recall(
                            label_ids,
                            predicted_labels)
                        precision = tf.metrics.precision(
                            label_ids,
                            predicted_labels) 
                        true_pos = tf.metrics.true_positives(
                            label_ids,
                            predicted_labels)
                        true_neg = tf.metrics.true_negatives(
                            label_ids,
                            predicted_labels)   
                        false_pos = tf.metrics.false_positives(
                            label_ids,
                            predicted_labels)  
                        false_neg = tf.metrics.false_negatives(
                            label_ids,
                            predicted_labels)
                        return {
                            ""eval_accuracy"": accuracy,
                            ""f1_score"": f1_score,
                            ""auc"": auc,
                            ""precision"": precision,
                            ""recall"": recall,
                            ""true_positives"": true_pos,
                            ""true_negatives"": true_neg,
                            ""false_positives"": false_pos,
                            ""false_negatives"": false_neg
                        }

                        eval_metrics = metric_fn(label_ids, predicted_labels)

                        if mode == tfe.estimator.ModeKeys.TRAIN:
                            return tfe.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
                        else:
                            return tfe.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)
        else:
            (predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)
            predictions = {'probabilities': log_probs, 'labels': predicted_labels}
            return tfe.estimator.EstimatorSpec(mode, predictions=predictions)

# Return the actual model function in the closure
    return model_fn

# Compute train and warmup steps from batch size
# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
NUM_TRAIN_EPOCHS = 3.0
# Warmup is a period of time where hte learning rate 
# is small and gradually increases--usually helps training.
WARMUP_PROPORTION = 0.1
# Model configs
SAVE_CHECKPOINTS_STEPS = 500
SAVE_SUMMARY_STEPS = 100

# Compute # train and warmup steps from batch size
num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)
num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)
# Specify outpit directory and number of checkpoint steps to save
run_config = tfe.estimator.RunConfig(
    model_dir=OUTPUT_DIR,
    save_summary_steps=SAVE_SUMMARY_STEPS,
    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)

model_fn = model_fn_builder(
  num_labels=len(label_list),
  learning_rate=LEARNING_RATE,
  num_train_steps=num_train_steps,
  num_warmup_steps=num_warmup_steps)

estimator = tfe.estimator.Estimator(
  model_fn=model_fn,
  config=run_config,
  params={""batch_size"": BATCH_SIZE}
  )

# Create an input function for training. drop_remainder = True for using TPUs.
train_input_fn = bert.run_classifier.input_fn_builder(
    features=train_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=True,
    drop_remainder=False)

#Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes.
print(f'Beginning Training!')
current_time = datetime.now()
estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
print(""Training took time "", datetime.now() - current_time)

#Now let's use our test data to see how well our model did:
test_input_fn = run_classifier.input_fn_builder(
    features=test_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

estimator.evaluate(input_fn=test_input_fn, steps=None)

def getPrediction(in_sentences):
    labels = [""Negative"", ""Positive""]
    input_examples = [run_classifier.InputExample(guid="""", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, """" is just a dummy label
    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)
    predictions = estimator.predict(predict_input_fn)
    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]




pred_sentences = [
  ""That movie was absolutely awful"",
  ""The acting was a bit lacking"",
  ""The film was creative and surprising"",
  ""Absolutely fantastic!""
]

predictions = getPrediction(pred_sentences)

predictions
</code></pre>
","tensorflow, tensorflow-estimator, bert-language-model","<p>Horrifyingly, the answer to this problem was all about indentation.  There is a function in the Google Colab example posted above called def model_fn.  This appears to be  wrapper function for another function that actually creates a model to pass to the TensorFlow Estimator.  While I was debugging this in VS code I'd placed a break-point in the function to try and sort out what was happening and it kept skipping over the middle bit where it was checking for ""false pos, false_neg etc.</p>

<p>Evidently i'd somehow broken the indentation when editing in  VS Code and the functions were nested such that pylint didn't identify any syntax problems - it just skipped over the function.</p>

<p>Fix was to just recopy the entire  def model_fn function from the colab notebook and <em>voila</em> it worked.</p>
",0,1,1019,2020-01-02 13:42:40,https://stackoverflow.com/questions/59564384/estimator-train-throws-valueerror-model-fn-should-return-an-estimatorspec
Multilingual Bert sentence vector captures language used more than meaning - working as interned?,"<p>Playing around with BERT, I downloaded the Huggingface Multilingual Bert and entered three sentences, saving their sentence vectors (the embedding of <code>[CLS]</code>), then translated them via Google Translate, passed them through the model and saved their sentence vectors.</p>

<p>I then compared the results using cosine similarity.</p>

<p>I was surprised to see that each sentence vector was pretty far from the one generated from the sentence translated from it (0.15-0.27 cosine distance) while different sentences from the same language were quite close indeed (0.02-0.04 cosine distance).</p>

<p>So instead of having sentences of similar meaning (but different languages) grouped together (in 768 dimensional space ;) ), dissimilar sentences of the same language are closer. </p>

<p>To my understanding the whole point of Multilingual Bert is inter-language transfer learning - for example training a model (say, and FC net) on representations in one language and having that model be readily used in other languages.  </p>

<p>How can that work if sentences (of different languages) of the exact meaning are mapped to be more apart than dissimilar sentences of the same language?  </p>

<p>My code:</p>

<pre><code>import torch

import transformers
from transformers import AutoModel,AutoTokenizer

bert_name=""bert-base-multilingual-cased""
tokenizer = AutoTokenizer.from_pretrained(bert_name)
MBERT = AutoModel.from_pretrained(bert_name)

#Some silly sentences
eng1='A cat jumped from the trees and startled the tourists'
e=tokenizer.encode(eng1, add_special_tokens=True)
ans_eng1=MBERT(torch.tensor([e]))

eng2='A small snake whispered secrets to large cats'
t=tokenizer.tokenize(eng2)
e=tokenizer.encode(eng2, add_special_tokens=True)
ans_eng2=MBERT(torch.tensor([e]))

eng3='A tiger sprinted from the bushes and frightened the guests'
e=tokenizer.encode(eng3, add_special_tokens=True)
ans_eng3=MBERT(torch.tensor([e]))

# Translated to Hebrew with Google Translate
heb1='חתול קפץ מהעץ והבהיל את התיירים'
e=tokenizer.encode(heb1, add_special_tokens=True)
ans_heb1=MBERT(torch.tensor([e]))

heb2='נחש קטן לחש סודות לחתולים גדולים'
e=tokenizer.encode(heb2, add_special_tokens=True)
ans_heb2=MBERT(torch.tensor([e]))

heb3='נמר רץ מהשיחים והפחיד את האורחים'
e=tokenizer.encode(heb3, add_special_tokens=True)
ans_heb3=MBERT(torch.tensor([e]))


from scipy import spatial
import numpy as np

# Compare Sentence Embeddings

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_heb1[1].data.numpy())

print ('Eng1-Heb1 - Translated sentences',result)


result = spatial.distance.cosine(ans_eng2[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Eng2-Heb2 - Translated sentences',result)

result = spatial.distance.cosine(ans_eng3[1].data.numpy(), ans_heb3[1].data.numpy())

print ('Eng3-Heb3 - Translated sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_heb1[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Heb1-Heb2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Heb1-Heb3 - Similiar sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Eng1-Eng2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng3[1].data.numpy())

print ('Eng1-Eng3 - Similiar sentences',result)

#Output:
""""""
Eng1-Heb1 - Translated sentences 0.2074061632156372
Eng2-Heb2 - Translated sentences 0.15557605028152466
Eng3-Heb3 - Translated sentences 0.275478720664978

---

Heb1-Heb2 - Different sentences 0.044616520404815674
Heb1-Heb3 - Similar sentences 0.027982771396636963

---

Eng1-Eng2 - Different sentences 0.027982771396636963
Eng1-Eng3 - Similar sentences 0.024596810340881348
""""""
</code></pre>

<p>P.S.</p>

<p>At least the Heb1 was closer to Heb3 than to Heb2.
This was also observed for the English equivalents, but less so. </p>
","python, deep-learning, pytorch, multilingual, bert-language-model","<p>The [CLS] Token somehow represents the input sequence, but how exactly is difficult to say. The language is of course an important characteristic of a sentence, probably more than meaning. BERT is a pretrained model which tries to model such characteristics as meaning, structure and also language. If you want to have a model, which helps you identify if two sentences of different language mean the same thing, I can think of two different approaches:</p>

<ol>
<li><p>approach: You can train a classifier (SVM, logistic Regression or even some neuronal nets such as CNN) on that task.  <code>Inputs: two [CLS]-Token, Output: Same meaning, or not same meaning.</code>
As training data, you could choose [CLS]-Token-pairs of sentences of different language which are either of the same meaning  or not. To get meaningful results, you would need a lot of such sentence pairs. Luckily you can either generate them via google translate, or use a parallel texts such as the bible which exists in a lot of languages, and extract sentence pairs from there.</p></li>
<li><p>approach: Fine-tune the bert model on exactly that task:
As in the previous approach, you need a lot of training data.
A sample input to the BERT model would look like that:
<code>A cat jumped from the trees and startled the tourists [SEP] חתול קפץ מהעץ והבהיל את התיירים</code></p>

<p>To classify if those sentences are of the same meaning, you would add a classification layer on top of the [CLS]-Token and Fine tune the whole Model on that task.</p></li>
</ol>

<p>Note: I have never worked with a multilingual BERT-model, those approaches are what comes to my mind to accomplish the mentioned task. If you try those approaches, I would be interested to know how they perform 😊.</p>
",3,3,1538,2020-01-06 22:19:23,https://stackoverflow.com/questions/59619760/multilingual-bert-sentence-vector-captures-language-used-more-than-meaning-wor
Sequence labeling with BERT for words position,"<p>If I have a set of sentences and in these sentences there are some dependencies between words.
I want to train BERT to predict which words have dependencies with others.</p>
<p>Example, If I have this sentence:</p>
<blockquote>
<p>We were moving around in Paris, which is the capital of France.</p>
<p>0------1-------2-------3------4----5------6-----7---8-----9----10---11  (words indices)</p>
</blockquote>
<p>I want BERT to predict, for word <code>Paris</code>, the position of <code>France</code>. So, to shape the task as sequence labeling task.</p>
<p><a href=""https://i.sstatic.net/9eGLw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9eGLw.png"" alt=""enter image description here"" /></a></p>
<p>where the label for a word could be -1, if there is no relation between this word and any other words in the sentence, or the index of the other word; for our example above, <code>Paris</code> word should have 11 as the index of word <code>France</code>.</p>
<p>Is it a right way to place the indices as labels?</p>
","python, tensorflow, machine-learning, neural-network, bert-language-model","<p>No. The problem is that n every sentence the position index has an entirely different meaning, so it would be extremely for the network to learn what to do. You can imagine the parameter matrix in the final projection as embeddings of the target classes and the classification as measuring similarity of the output state from the class embeddings.</p>

<p>I suggest doing the classification similarly to what people sometimes do in dependency parsers, i.e., <em>for each pair of words, classify if there is a relation between the words or not</em>.</p>

<p>BERT gives you a matrix with contextual embeddings for each sentence. Create a 3D tensor out of it, where position <code>[i, j]</code> contains a concatenation of representation of words <code>i</code> and <code>j</code>. Then, classify each of these pairs as true/false, telling if the is a dependency link between these two words or not.</p>
",1,0,620,2020-01-09 17:02:54,https://stackoverflow.com/questions/59668900/sequence-labeling-with-bert-for-words-position
BERT tokenizer &amp; model download,"<p>I`m beginner.. I'm working with Bert. However, due to the security of the company network, the following code does not receive the bert model directly.</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
model = BertForSequenceClassification.from_pretrained(""bert-base-multilingual-cased"", num_labels=2) 
</code></pre>

<p>So I think I have to download these files and enter the location manually.
But I'm new to this, and I'm wondering if it's simple to download a format like .py from github and put it in a location.</p>

<p>I'm currently using the bert model implemented by hugging face's pytorch, and the address of the source file I found is:</p>

<p><a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">https://github.com/huggingface/transformers</a></p>

<p>Please let me know if the method I thought is correct, and if so, what file to get.</p>

<p>Thanks in advance for the comment.</p>
","python, github, pytorch, huggingface-transformers, bert-language-model","<p>As described <a href=""https://github.com/huggingface/transformers/issues/856"" rel=""noreferrer"">here</a>, what you need to do are download <code>pre_train</code> and <code>configs</code>, then putting them in the same folder. Every model has a pair of links, you might want to take a look at lib code. </p>

<p>For instance</p>

<pre><code>import torch
from transformers import *
model = BertModel.from_pretrained('/Users/yourname/workplace/berts/')
</code></pre>

<p>with <code>/Users/yourname/workplace/berts/</code> refer to your folder</p>

<p>Below are what I found</p>

<p>at <code>src/transformers/configuration_bert.py</code> there are a list of models' configs</p>

<pre><code>BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {
    ""bert-base-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json"",
    ""bert-large-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json"",
    ""bert-base-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json"",
    ""bert-large-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json"",
    ""bert-base-multilingual-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json"",
    ""bert-base-multilingual-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json"",
    ""bert-base-chinese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json"",
    ""bert-base-german-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json"",
    ""bert-large-uncased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json"",
    ""bert-large-cased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json"",
    ""bert-large-uncased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json"",
    ""bert-large-cased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json"",
    ""bert-base-cased-finetuned-mrpc"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json"",
    ""bert-base-german-dbmdz-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json"",
    ""bert-base-german-dbmdz-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json"",
    ""bert-base-japanese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json"",
    ""bert-base-japanese-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json"",
    ""bert-base-japanese-char"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json"",
    ""bert-base-japanese-char-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json"",
    ""bert-base-finnish-cased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json"",
    ""bert-base-finnish-uncased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json"",
}
</code></pre>

<p>and at <code>src/transformers/modeling_bert.py</code> there are links to pre_trains</p>

<pre><code>BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {
    ""bert-base-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin"",
    ""bert-large-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin"",
    ""bert-base-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin"",
    ""bert-large-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin"",
    ""bert-base-multilingual-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin"",
    ""bert-base-multilingual-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin"",
    ""bert-base-chinese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin"",
    ""bert-base-german-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin"",
    ""bert-large-uncased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin"",
    ""bert-large-cased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin"",
    ""bert-large-uncased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin"",
    ""bert-large-cased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin"",
    ""bert-base-cased-finetuned-mrpc"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin"",
    ""bert-base-german-dbmdz-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-pytorch_model.bin"",
    ""bert-base-german-dbmdz-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-pytorch_model.bin"",
    ""bert-base-japanese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-pytorch_model.bin"",
    ""bert-base-japanese-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin"",
    ""bert-base-japanese-char"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-pytorch_model.bin"",
    ""bert-base-japanese-char-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-pytorch_model.bin"",
    ""bert-base-finnish-cased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/pytorch_model.bin"",
    ""bert-base-finnish-uncased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/pytorch_model.bin"",
}
</code></pre>
",14,9,24790,2020-01-12 07:56:32,https://stackoverflow.com/questions/59701981/bert-tokenizer-model-download
Removing SEP token in Bert for text classification,"<p>Given a sentiment classification dataset, I want to fine-tune Bert.</p>

<p>As you know that BERT created to predict the next sentence given the current sentence. Thus, to make the network aware of this, they inserted a <code>[CLS]</code> token in the beginning of the first sentence then they add <code>[SEP]</code> token to separate the first from the second sentence and finally another <code>[SEP]</code> at the end of the second sentence (it's not clear to me why they append another token at the end).</p>

<p>Anyway, for text classification, what I noticed in some of the examples online (see <a href=""https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"" rel=""noreferrer"">BERT in Keras with Tensorflow hub</a>) is that they add <code>[CLS]</code> token and then the sentence and at the end another <code>[SEP]</code> token.</p>

<p>Where in other research works (e.g. <a href=""https://arxiv.org/pdf/1905.08284.pdf"" rel=""noreferrer"">Enriching Pre-trained Language Model with Entity Information for Relation Classification</a>) they remove the last <code>[SEP]</code> token.</p>

<p>Why is it/not beneficial to add the <code>[SEP]</code> token at the end of the input text when my task uses only single sentence?</p>
","python, bert-language-model","<p>Im not quite sure why BERT needs the separation token <code>[SEP]</code> at the end for single-sentence tasks, but my guess is that BERT is an autoencoding model that, as mentioned, originally was designed for Language Modelling and Next Sentence Prediction. So BERT was trained that way to always expect the <code>[SEP]</code> token, which means that the token is involved in the underlying knowledge that BERT built up during training.</p>
<p>Downstream tasks that followed later, such as single-sentence use-cases (e.g. text classification), turned out to work too with BERT, however the <code>[SEP]</code> was left as a relict for BERT to work properly and thus is needed even for these tasks.</p>
<p>BERT might learn faster, if <code>[SEP]</code> is appended at the end of a single sentence, because it encodes somewhat of a knowledge in that token, that this marks the end of the input. Without it, BERT would still know where the sentence ends (due to the padding tokens), which explains that fore mentioned research leaves away the token, but this might slow down training slightly, since BERT might be able to learn faster with appended <code>[SEP]</code> token, especially if there are no padding tokens in a truncated input.</p>
",6,12,3427,2020-01-13 15:15:36,https://stackoverflow.com/questions/59719323/removing-sep-token-in-bert-for-text-classification
What is the difference between Sentence Encodings and Contextualized Word Embeddings?,"<p>I have seen both terms used while reading papers about BERT and ELMo so I wonder if there is a difference between them.</p>
","nlp, word-embedding, elmo, bert-language-model","<ul>
<li>A <strong>contextualized word embeding</strong> is a vector representing a word in a special context. The <em>traditional word embeddings</em> such as Word2Vec and GloVe generate one vector for each word, whereas a contextualized word embedding generates a vector for a word depending on the context. Consider the sentences <code>The duck is swimming</code>and <code>You shall duck when someone shoots at you</code>. With traditional word embeddings, the word vector for <code>duck</code>would be the same in both sentences, whereas it should be a different one in the contextualized case. </li>
<li>While word embeddings encode words into a vector representation, there is also the question on how to represent a whole sentence in a way a computer can easily work with. These <strong>sentence encodings</strong> can embedd a whole sentence as one vector , doc2vec for example which generate a vector for a sentence. But also BERT generates a representation for the whole sentence, the [CLS]-token. </li>
</ul>

<p>So in short, a conextualized word embedding represents a word in a context, whereas a sentence encoding represents a whole sentence.</p>
",21,13,8893,2020-01-23 11:20:54,https://stackoverflow.com/questions/59877385/what-is-the-difference-between-sentence-encodings-and-contextualized-word-embedd
Getting word-level encodings from sub-word tokens encodings,"<p>I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.</p>

<p>Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.</p>

<p>Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?</p>
","nlp, tokenize, bert-language-model, huggingface-transformers","<p>Intuitively, your problem seems similar to ""how to get a good sentence representation"", with the exception that these days you could also use a classification token of a sentence to get a sentence representation in most transformer-based models. Such token is not available for token-level representations, though.</p>

<p>In your case, I think there are a few options but from what I've seen, people most often use either an average or a max value. In other words: take the average of your subword units, or take the max values. Averaging is the most intuitive place to start, in my opinion.</p>

<p>Note that averages are only just that, an average over a sequence. This implies that it is not super accurate (one high and one low value will have the same mean as two medium values), but it's probably the most straightforward.</p>
",1,2,577,2020-01-28 19:02:15,https://stackoverflow.com/questions/59955402/getting-word-level-encodings-from-sub-word-tokens-encodings
"tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 512], but got 891 [Op:Slice]","<p>I am reading text files stored in my database and they all have different sizes. When I run my code it suddenly stops and giving this error. Not finding any relevant answers anywhere.
I have tried changing the max_seq_embeddings but still does not work.
As soon as I encounter a file of length 3619 it raised an error.</p>

<pre><code>YES &lt;class 'str'&gt; 1814
YES &lt;class 'str'&gt; 1334
YES &lt;class 'str'&gt; 3619
Traceback (most recent call last):
  File ""C:/Users/DeLL/PycharmProjects/Phase1/venv/src/Main.py"", line 24, in &lt;module&gt;
    output = model.predict(data.text)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\bert.py"", line 77, in predict
    logits = self.model(input_ids, segment_ids, input_mask,valid_ids)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\model.py"", line 55, in call
    sequence_output = self.bert([input_word_ids, input_mask, input_type_ids],**kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\bert_modeling.py"", line 197, in __call__
    return super(BertModel, self).__call__(inputs, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\bert_modeling.py"", line 217, in call
    word_embeddings=word_embeddings, token_type_ids=input_type_ids)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\bert_modeling.py"", line 329, in __call__
    return super(EmbeddingPostprocessor, self).__call__(inputs, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\src\bert_modeling.py"", line 355, in call
    tf.slice(self.position_embeddings, [0, 0], [seq_length, width]),
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\ops\array_ops.py"", line 866, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\ops\gen_array_ops.py"", line 9212, in _slice
    input, begin, size, name=name, ctx=_ctx)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\ops\gen_array_ops.py"", line 9251, in _slice_eager_fallback
    ctx=_ctx, name=name)
  File ""C:\Users\DeLL\PycharmProjects\Phase1\venv\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""&lt;string&gt;"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 512], but got 891 [Op:Slice]

Process finished with exit code 1
</code></pre>

<p>Here is the link to all the file used.
<a href=""https://github.com/kamalkraj/BERT-NER-TF"" rel=""nofollow noreferrer"">https://github.com/kamalkraj/BERT-NER-TF</a></p>
","python, tensorflow, bert-language-model","<p>You are using <a href=""https://www.tensorflow.org/api_docs/python/tf/slice"" rel=""nofollow noreferrer"">tf.slice</a> in your program.  <code>tf.slice</code> accepts below arguments -</p>

<pre><code>tf.slice(
    input_, begin, size, name=None
)
</code></pre>

<p>As the error clearly states, the <code>size</code> argument is expecting the value from the range of <code>[0, 512]</code>, but got <code>891</code>. </p>

<p>The range of <code>size</code> depends on the <code>begin</code> parameter, if <code>begin</code> is <code>0</code> then you have complete length of input as <code>size</code> .i.e. <code>size</code> can have value from the range of <code>[0,len(input)]</code>, else if <code>begin</code> parameter is set greater than <code>0</code> ,then <code>size</code> can have value form the range of <code>[Begin, len(input)-Begin]</code>.</p>

<p>Let me explain with a example -</p>

<p><strong>Example 1 :</strong> Here I have set <code>begin=[5]</code> and <code>size=[5]</code>, meaning start after 5th position for size of 5. </p>

<pre><code>x = tf.constant((""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j""))
print(x)
y = tf.slice(x, [5], [5])
print(y)
</code></pre>

<p><strong>Output -</strong></p>

<pre><code>tf.Tensor([b'a' b'b' b'c' b'd' b'e' b'f' b'g' b'h' b'i' b'j'], shape=(10,), dtype=string)
tf.Tensor([b'f' b'g' b'h' b'i' b'j'], shape=(5,), dtype=string)
</code></pre>

<p><strong>Example 2 :</strong> Here I have set <code>begin=[5]</code> and <code>size=[10]</code>, meaning start after 5th position for size of 10. But as <code>begin=[5]</code> I don't have any input after <code>size=[5]</code> as the input shape is <code>shape=(10,)</code>. <strong>This will reproduce the error your are facing.</strong></p>

<pre><code>x = tf.constant((""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j""))
print(x)
y = tf.slice(x, [5], [10])
print(y)
</code></pre>

<p><strong>Output -</strong></p>

<pre><code>tf.Tensor([b'a' b'b' b'c' b'd' b'e' b'f' b'g' b'h' b'i' b'j'], shape=(10,), dtype=string)
---------------------------------------------------------------------------
_FallbackException                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in _slice(input, begin, size, name)
   9082         _ctx._context_handle, tld.device_name, ""Slice"", name,
-&gt; 9083         tld.op_callbacks, input, begin, size)
   9084       return _result

_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError: Expected size[0] in [0, 5], but got 10 [Op:Slice]
</code></pre>

<p>Hope this answers your question. Happy Learning.</p>
",3,1,2835,2020-01-30 15:06:07,https://stackoverflow.com/questions/59988919/tensorflow-python-framework-errors-impl-invalidargumenterror-expected-size0-i
Outputting attention for bert-base-uncased with huggingface/transformers (torch),"<p>I was following <a href=""https://www.aclweb.org/anthology/P19-1328/"" rel=""noreferrer"">a paper</a> on BERT-based lexical substitution (specifically trying to implement equation (2) - if someone has already implemented the whole paper that would also be great). Thus, I wanted to obtain both the last hidden layers (only thing I am unsure is the ordering of the layers in the output: last first or first first?) and the attention from a basic BERT model (bert-base-uncased). </p>

<p>However, I am a bit unsure whether the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">huggingface/transformers library</a> actually outputs the attention (I was using torch, but am open to using TF instead) for bert-base-uncased?</p>

<p>From <a href=""https://github.com/huggingface/transformers/issues/1073"" rel=""noreferrer"">what I had read</a>, I was expected to get a tuple of (logits, hidden_states, attentions), but with the example below (runs e.g. in Google Colab), I get of length 2 instead. </p>

<p>Am I misinterpreting what I am getting or going about this the wrong way? I did the obvious test and used <code>output_attention=False</code> instead of <code>output_attention=True</code> (while <code>output_hidden_states=True</code> does indeed seem to add the hidden states, as expected) and nothing change in the output I got. That's clearly a bad sign about my understanding of the library or indicates an issue.</p>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import (AutoModelWithLMHead, 
                          AutoTokenizer, 
                          BertConfig)

bert_tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attention=True) # Nothign changes, when I switch to output_attention=False
bert_model = AutoModelWithLMHead.from_config(config)

sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)

indexed_tokens = bert_tokenizer.encode(bert_tokenized_sequence, return_tensors='pt')

predictions = bert_model(indexed_tokens)

########## Now let's have a look at what the predictions look like #############
print(len(predictions)) # Length is 2, I expected 3: logits, hidden_layers, attention

print(predictions[0].shape) # torch.Size([1, 16, 30522]) - seems to be logits (shape is 1 x sequence length x vocabulary

print(len(predictions[1])) # Length is 13 - the hidden layers?! There are meant to be 12, right? Is one somehow the attention?

for k in range(len(predictions[1])):
  print(predictions[1][k].shape) # These all seem to be torch.Size([1, 16, 768]), so presumably the hidden layers?
</code></pre>

<h1>Explanation of what worked in the end inspired by accepted answer</h1>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import BertModel, BertConfig, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True)
model = BertModel.from_pretrained('bert-base-uncased', config=config)
sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
tokenized_sequence = tokenizer.tokenize(sequence)
indexed_tokens = tokenizer.encode(tokenized_sequence, return_tensors='pt'
enter code here`outputs = model(indexed_tokens)
print( len(outputs) ) # 4 
print( outputs[0].shape ) #1, 16, 768 
print( outputs[1].shape ) # 1, 768
print( len(outputs[2]) ) # 13  = input embedding (index 0) + 12 hidden layers (indices 1 to 12)
print( outputs[2][0].shape ) # for each of these 13: 1,16,768 = input sequence, index of each input id in sequence, size of hidden layer
print( len(outputs[3]) ) # 12 (=attenion for each layer)
print( outputs[3][0].shape ) # 0 index = first layer, 1,12,16,16 = , layer, index of each input id in sequence, index of each input id in sequence
</code></pre>
","python, attention-model, huggingface-transformers, bert-language-model","<p>The reason is that you are using <code>AutoModelWithLMHead</code> which is a wrapper for the actual model. It calls the BERT model (i.e., an instance of <code>BERTModel</code>) and then it uses the embedding matrix as a weight matrix for the word prediction. In between the underlying model indeed returns attentions, but the wrapper does not care and only returns the logits.</p>

<p>You can either get the BERT model directly by calling <code>AutoModel</code>. Note that this model does not return the logits, but the hidden states.</p>

<pre class=""lang-py prettyprint-override""><code>bert_model = AutoModel.from_config(config)
</code></pre>

<p>Or you can get it from the <code>BertWithLMHead</code> object by calling:</p>

<pre class=""lang-py prettyprint-override""><code>wrapped_model = bert_model.base_model
</code></pre>
",3,10,15610,2020-02-07 20:46:37,https://stackoverflow.com/questions/60120849/outputting-attention-for-bert-base-uncased-with-huggingface-transformers-torch
What does BERT&#39;s special characters appearance in SQuAD&#39;s QA answers mean?,"<p>I'm running a fine-tuned model of BERT and ALBERT for Questing Answering. And, I'm evaluating the performance of these models on a subset of questions from <a href=""https://rajpurkar.github.io/SQuAD-explorer/"" rel=""nofollow noreferrer"">SQuAD v2.0</a>. I use <a href=""https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/"" rel=""nofollow noreferrer"">SQuAD's official evaluation script</a> for evaluation. </p>

<p>I use Huggingface <code>transformers</code> and in the following you can find an actual code and example I'm running (might be also helpful for some folks who are trying to run fine-tuned model of ALBERT on SQuAD v2.0):</p>

<pre><code>tokenizer = AutoTokenizer.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")
model = AutoModelForQuestionAnswering.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")

question = ""Why aren't the examples of bouregois architecture visible today?""
text = """"""Exceptional examples of the bourgeois architecture of the later periods were not restored by the communist authorities after the war (like mentioned Kronenberg Palace and Insurance Company Rosja building) or they were rebuilt in socialist realism style (like Warsaw Philharmony edifice originally inspired by Palais Garnier in Paris). Despite that the Warsaw University of Technology building (1899\u20131902) is the most interesting of the late 19th-century architecture. Some 19th-century buildings in the Praga district (the Vistula\u2019s right bank) have been restored although many have been poorly maintained. Warsaw\u2019s municipal government authorities have decided to rebuild the Saxon Palace and the Br\u00fchl Palace, the most distinctive buildings in prewar Warsaw.""""""

input_dict = tokenizer.encode_plus(question, text, return_tensors=""pt"")
input_ids = input_dict[""input_ids""].tolist()
start_scores, end_scores = model(**input_dict)

all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]).replace('▁', '')
print(answer)
</code></pre>

<p>And the output is like the following:</p>

<pre><code>[CLS] why aren ' t the examples of bour ego is architecture visible today ? [SEP] exceptional examples of the  bourgeois architecture of the later periods were not restored by the communist authorities after the war
</code></pre>

<p>As you can see there are BERT's special tokens in the answer including <code>[CLS]</code> and <code>[SEP]</code>.</p>

<p>I understand that in cases where the answer is just <code>[CLS]</code> (having two <code>tensor(0)</code> for <code>start_scores</code> and <code>end_scores</code>) it basically means model thinks there's no answer to the question in context which makes sense. And in these cases I just simply set the answer to that question to a null string when running the evaluation script.</p>

<p><strong>But</strong> I wonder in cases like the example above, should I again assume that model could not find an answer and set the answer to empty string? or should I just leave the answer like that when I'm evaluating the model performance?</p>

<p>I'm asking this question because as far as I understand, the performance calculated using the evaluation script can change (correct me if I'm wrong) if I have such cases as answers and I may not get a realistic sense of the performance of these models. </p>
","nlp-question-answering, bert-language-model, huggingface-transformers, squad","<p>You should simply treat them as invalid because you try to predict a proper answer span from the variable <code>text</code>. Everything else should be invalid. This is also the way how huggingface <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L429"" rel=""nofollow noreferrer"">treats</a> this predictions:</p>

<blockquote>
  <p>We could hypothetically create invalid predictions, e.g., predict that the start of the span is in the question. We throw out all invalid predictions.</p>
</blockquote>

<p>You should also note that they use a <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_squad.py#L295"" rel=""nofollow noreferrer"">more sopisticated method</a> to get the predictions for each question (don't ask me why they show torch.argmax in their example). Please have a look at the example below:</p>

<pre><code>from transformers.data.processors.squad import SquadResult, SquadExample, SquadFeatures,SquadV2Processor, squad_convert_examples_to_features
from transformers.data.metrics.squad_metrics import compute_predictions_logits, squad_evaluate

###
#your example code
###

outputs = model(**input_dict)

def to_list(tensor):
    return tensor.detach().cpu().tolist()

output = [to_list(output[0]) for output in outputs]
start_logits, end_logits = output

all_results = []
all_results.append(SquadResult(1000000000, start_logits, end_logits))

#this is the answers section from the evaluation dataset
answers = [{'text':'not restored by the communist authorities', 'answer_start':77}, {'text':'were not restored', 'answer_start':72}, {'text':'not restored by the communist authorities after the war', 'answer_start':77}]

examples = [SquadExample('0', question, text, 'not restored by the communist authorities', 75, 'Warsaw', answers,False)]

#this does basically the same as tokenizer.encode_plus() but stores them in a SquadFeatures Object and splits if neccessary
features = squad_convert_examples_to_features(examples, tokenizer, 512, 100, 64, True)

predictions = compute_predictions_logits(
            examples,
            features,
            all_results,
            20,
            30,
            True,
            'pred.file',
            'nbest_file',
            'null_log_odds_file',
            False,
            True,
            0.0,
            tokenizer
            )

result = squad_evaluate(examples, predictions)

print(predictions)
for x in result.items():
  print(x)
</code></pre>

<p>Output:</p>

<pre><code>OrderedDict([('0', 'communist authorities after the war')])
('exact', 0.0)
('f1', 72.72727272727273)
('total', 1)
('HasAns_exact', 0.0)
('HasAns_f1', 72.72727272727273)
('HasAns_total', 1)
('best_exact', 0.0)
('best_exact_thresh', 0.0)
('best_f1', 72.72727272727273)
('best_f1_thresh', 0.0)
</code></pre>
",1,2,1331,2020-02-09 03:35:08,https://stackoverflow.com/questions/60133236/what-does-berts-special-characters-appearance-in-squads-qa-answers-mean
"Pre-training BERT/RoBERTa language model using domain text, how long it gonna take estimately? which is faster?","<p>I want to pre-train BERT and RoBERTa MLM using domain corpus (sentiment-related text). How long it gonna take for using 50k~100k words. Since RoBERTa is not trained on predicting the next sentence objective, one training objective less than BERT and with larger mini-batches and learning rates, I assume RoBERTa will be much faster?</p>
","language-model, bert-language-model, huggingface-transformers","<p>100k words it too few to train such a large model as BERT or RoBERTa. The main claim of <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer"">the RoBERTa paper</a> is that BERT is actually undertrained. Whereas BERT was trained on 16 GB of text data, RoBERTa used 160 GB of plain text.</p>

<p>For a small domain-specific data as you describe, you can try fine-tuning an existing model. In this case, I would go for RoBERTa because it seems to be better pre-trained, does not have the next-sentence-objective (which is a hassle to pre-process data for it) and it uses SentencePiece for tokenization, which allows loss-less detokenization.</p>
",6,2,1324,2020-02-09 13:33:21,https://stackoverflow.com/questions/60137162/pre-training-bert-roberta-language-model-using-domain-text-how-long-it-gonna-ta
How should properly formatted data for NER in BERT look like?,"<p>I am using Huggingface's <code>transformers</code> library and want to perform NER using BERT. I tried to find an explicit example of how to properly format the data for NER using BERT. It is not entirely clear to me from the paper and the comments I've found.</p>

<p>Let's say we have a following sentence and labels:</p>

<pre><code>sent = ""John Johanson lives in Ramat Gan.""
labels = ['B-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC']
</code></pre>

<p>Would data that we input to the model be something like this:</p>

<pre><code>sent = ['[CLS]', 'john', 'johan',  '##son', 'lives',  'in', 'ramat', 'gan', '.', '[SEP]']
labels = ['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O']
attention_mask = [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]
sentence_id = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>

<p>?</p>

<p>Thank you!</p>
","python, nlp, format, bert-language-model, huggingface-transformers","<p>Update 2021-08-27: The tutorial link points to a legacy tutorial, which I don't fully recommend anymore, since it does not use Huggingface's convenience library <code>datasets</code>.</p>
<p>There is actually <a href=""https://github.com/huggingface/transformers/tree/master/examples/legacy/token-classification"" rel=""nofollow noreferrer"">a great tutorial</a> for the NER example on the huggingface documentation page. Specifically, it also goes into detail how the <a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py"" rel=""nofollow noreferrer"">provided script</a> does the preprocessing. Specifically, there is a link to an external contributor's <code>preprocess.py</code> script, that basically takes the data from the CoNLL 2003 format to whatever is required by the huggingface library. I found this to be the easiest way to assert I have proper formatting, and unless you have some specific changes that you might want to incorporate, this gets you started super quick without worrying about implementation details.</p>
<p>The linked example script also provides more than enough detail on how to feed the respective inputs into the model itself, but generally, you are correct in your above-mentioned input pattern.</p>
",2,2,1751,2020-02-14 06:02:09,https://stackoverflow.com/questions/60220842/how-should-properly-formatted-data-for-ner-in-bert-look-like
How to feed the output of a finetuned bert model as inpunt to another finetuned bert model?,"<p>I finetuned two separate bert model (bert-base-uncased) on sentiment analysis and pos tagging tasks. Now, I want to feed the output of the pos tagger (batch, seqlength, hiddensize) as input to the sentiment model.The original bert-base-uncased model is in 'bertModel/' folder which contains 'model.bin' and 'config.json'. Here is my code:</p>

<pre><code>class DeepSequentialModel(nn.Module):
def __init__(self, sentiment_model_file, postag_model_file, device):
    super(DeepSequentialModel, self).__init__()

    self.sentiment_model = SentimentModel().to(device)
    self.sentiment_model.load_state_dict(torch.load(sentiment_model_file, map_location=device))
    self.postag_model = PosTagModel().to(device)
    self.postag_model.load_state_dict(torch.load(postag_model_file, map_location=device))

    self.classificationLayer = nn.Linear(768, 1)

def forward(self, seq, attn_masks):
    postag_context = self.postag_model(seq, attn_masks)
    sent_context = self.sentiment_model(postag_context, attn_masks)
    logits = self.classificationLayer(sent_context)
    return logits

class PosTagModel(nn.Module):
def __init__(self,):
    super(PosTagModel, self).__init__()
    self.bert_layer = BertModel.from_pretrained('bertModel/')
    self.classificationLayer = nn.Linear(768, 43)

def forward(self, seq, attn_masks):
    cont_reps, _ = self.bert_layer(seq, attention_mask=attn_masks)
    return cont_reps

class SentimentModel(nn.Module):
def __init__(self,):
    super(SentimentModel, self).__init__()
    self.bert_layer = BertModel.from_pretrained('bertModel/')
    self.cls_layer = nn.Linear(768, 1)

def forward(self, input, attn_masks):
    cont_reps, _ = self.bert_layer(encoder_hidden_states=input, encoder_attention_mask=attn_masks)
    cls_rep = cont_reps[:, 0]
    return cls_rep
</code></pre>

<p>But I get the below error. I appreciate it if someone could help me. Thanks!</p>

<pre><code>    cont_reps, _ = self.bert_layer(encoder_hidden_states=input, encoder_attention_mask=attn_masks)
    result = self.forward(*input, **kwargs)
    TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'
</code></pre>
","pytorch, pre-trained-model, bert-language-model, huggingface-transformers","<p>To formulate this as an answer, too, and keep it properly visible for future visitors, the <code>forward()</code> call of transformers <a href=""https://github.com/huggingface/transformers/blob/v2.1.1/transformers/modeling_bert.py#L201"" rel=""nofollow noreferrer"">does not support these arguments in version 2.1.1</a>, or any earlier version, for that matter. note that the link in my comment is in fact pointing to a different forward function, but otherwise the point still holds.</p>

<p>Passing <code>encoder_hidden_states</code> to <code>forward()</code> was <a href=""https://github.com/huggingface/transformers/blob/v2.2.0/transformers/modeling_bert.py#L210"" rel=""nofollow noreferrer"">first possible in version 2.2.0</a>.</p>
",1,2,845,2020-02-19 10:13:53,https://stackoverflow.com/questions/60297908/how-to-feed-the-output-of-a-finetuned-bert-model-as-inpunt-to-another-finetuned
"If BERT&#39;s [CLS] can be retrained for a variety of sentence classification objectives, what about [SEP]?","<p>In BERT pretraining, the [CLS] token is embedded into the input of a classifier tasked with the Next Sentence Prediction task (or, in some BERT variants, with other tasks, such as ALBERT's Sentence Order Prediction); this helps in the pretraining of the entire transformer, and it also helps to make the [CLS] position readily available for retraining to other ""sentence scale"" tasks.</p>

<p>I wonder whether [SEP] could also be retrained in the same manner.
While [CLS] will probably be easier to retrain as the transformer is already trained to imbue its embedding with meaning from across the sentence, while [SEP] does not have these ""connections"" (one would assume), this might still work with sufficient fine-tuning. </p>

<p>With this one could retrain the same model for two different classification tasks,   one using [CLS] and one using [SEP].</p>

<p>Am I missing anything? 
Is there a reason why this would not work? </p>
","transformer-model, bert-language-model, huggingface-transformers","<p>In theory it can give 'some' results so it would work (it's just a token), but the question is why you would want to that. These tokens have been pretrained for a specific purpose. I suppose that by 'retrain' you mean finetuning, so if you would finetune the SEP token suddenly as a classification token, I think you won't get good results because you are only fine-tuning one token in the whole language model for a task that it wasn't even pretrained for.</p>
",2,1,1246,2020-02-24 14:54:01,https://stackoverflow.com/questions/60378466/if-berts-cls-can-be-retrained-for-a-variety-of-sentence-classification-object
BERT fine tuning,"<p>I'm trying to create my model for question answering based on BERT und can't understand what is the meaning of fine tuning. Do I understand it right, that it is like adaption for specific domain? And if I want to use it with Wikipedia corpora, I just need to integrate unchanged pre-trained model in my network?</p>
","nlp, bert-language-model","<p>Fine tuning is adopting (refining) the pre-trained BERT model to two things:</p>

<ol>
<li>Domain</li>
<li>Task (e.g. classification, entity extraction, etc.).</li>
</ol>

<p>You can use pre-trained models as-is at first and if the performance is sufficient, fine tuning for your use case may not be needed.</p>
",2,2,698,2020-02-26 16:16:03,https://stackoverflow.com/questions/60418179/bert-fine-tuning
Need to Fine Tune a BERT Model to Predict Missing Words,"<p>I'm aware that BERT has a capability in predicting a missing word within a sentence, which can be syntactically correct and semantically coherent. Below is a sample code:</p>

<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval(); # turning off the dropout

def fill_the_gaps(text):
   text = '[CLS] ' + text + ' [SEP]'
   tokenized_text = tokenizer.tokenize(text)
   indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
   segments_ids = [0] * len(tokenized_text)
   tokens_tensor = torch.tensor([indexed_tokens])
   segments_tensors = torch.tensor([segments_ids])
   with torch.no_grad():
      predictions = model(tokens_tensor, segments_tensors)
   results = []
   for i, t in enumerate(tokenized_text):
       if t == '[MASK]':
           predicted_index = torch.argmax(predictions[0, i]).item()
           predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
           results.append(predicted_token)
   return results

 print(fill_the_gaps(text = 'I bought an [MASK] because its rainy .'))
 print(fill_the_gaps(text = 'Im sad because you are [MASK] .'))
 print(fill_the_gaps(text = 'Im worried because you are [MASK] .'))
 print(fill_the_gaps(text = 'Im [MASK] because you are [MASK] .'))
</code></pre>

<p>Can someone explain to me, do I need to fine Tune a BERT Model to predict missing words or just use the pre-trained BERT model? Thanks.</p>
","python, nlp, bert-language-model","<p>BERT is a masked Language Model, meaning it is trained on exactly this task. That is why it can do it. So in that sense, no fine tuning is needed.</p>

<p>However, if the text you will see at runtime is different than the text BERT was trained on, your performance may be much better if you fine tune on the type of text you expect to see. </p>
",4,1,1430,2020-03-02 10:14:51,https://stackoverflow.com/questions/60486655/need-to-fine-tune-a-bert-model-to-predict-missing-words
Why can Bert&#39;s three embeddings be added?,"<p>I already know the meaning of Token Embedding, Segment Embedding, and Position Embedding. But why can these three vectors be added together? The Size and direction of vectors will change after the addition, and the semantics of the word will also change. (It's the same question for the Transformer model which has two Embeddings named Input Embedding and Position Embedding.)</p>
","vector, nlp, embedding, transformer-model, bert-language-model","<p>Firstly, these vectors are added element-wise -> The size of the embeddings stays the same.</p>

<p>Secondly, position plays a significant role in the meaning of a token, so it should somehow be part of the embedding. Attention: The token embeddinng does not necessarily hold semantic information as we now it from word2vec, all those embeddings(token, segment and position) are learned together in pre-training, so that they best accomplish the tasks together. In pre-training, they are already added together, so they are trained especially for this case. Direction of vectors do change with this addition, but the new direction gives important information to the model, packed in just one vector.</p>

<p>Note: Each vector is huge (768 dimensions in the base model)</p>
",2,1,1025,2020-03-03 11:03:09,https://stackoverflow.com/questions/60505798/why-can-berts-three-embeddings-be-added
BertForSequenceClassification vs. BertForMultipleChoice for sentence multi-class classification,"<p>I'm working on a text classification problem (e.g. sentiment analysis), where I need to classify a text string into one of five classes.</p>
<p>I just started using the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. What I need is a classifier with a softmax layer on top so that I can do 5-way classification. Confusingly, there seem to be two relevant options in the Transformer package: <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""noreferrer"">BertForSequenceClassification</a> and <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice"" rel=""noreferrer"">BertForMultipleChoice</a>.</p>
<p><strong>Which one should I use for my 5-way classification task? What are the appropriate use cases for them?</strong></p>
<p>The documentation for <strong>BertForSequenceClassification</strong> doesn't mention softmax at all, although it does mention cross-entropy. I am not sure if this class is only for 2-class classification (i.e. logistic regression).</p>
<blockquote>
<p><em>Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) – Labels for computing the sequence classification/regression loss. Indices should be in [0, ..., config.num_labels - 1]. If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-Entropy).</em></li>
</ul>
</blockquote>
<p>The documentation for <strong>BertForMultipleChoice</strong> mentions softmax, but the way the labels are described, it sound like this class is for multi-label classification (that is, a binary classification for multiple labels).</p>
<blockquote>
<p><em>Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) – Labels for computing the multiple choice classification loss. Indices should be in [0, ..., num_choices] where num_choices is the size of the second dimension of the input tensors.</em></li>
</ul>
</blockquote>
<p>Thank you for any help.</p>
","python, machine-learning, pytorch, bert-language-model, huggingface-transformers","<p>The answer to this lies in the (admittedly very brief) description of what the tasks are about:</p>

<blockquote>
  <p>[<code>BertForMultipleChoice</code>] [...], e.g. for RocStories/SWAG tasks.</p>
</blockquote>

<p>When looking at the <a href=""https://arxiv.org/pdf/1808.05326.pdf"" rel=""noreferrer"">paper for SWAG</a>, it seems that the task is actually learning to <em>choose from varying options</em>. This is in contrast to your ""classical"" classification task, in which the ""choices"" (i.e., classes) <em>do not vary</em> across your samples, which is exactly what <code>BertForSequenceClassification</code> is for.</p>

<p>Both variants can in fact be for an arbitrary number of classes (in the case of <code>BertForSequenceClassification</code>), respectively choices (for <code>BertForMultipleChoice</code>), via changing the <code>labels</code> parameter in the config. But, since it seems like you are dealing with a case of ""classical classification"", I suggest using the <code>BertForSequenceClassification</code> model.</p>

<p>Shortly addressing the missing Softmax in <code>BertForSequenceClassification</code>: Since classification tasks can compute loss across classes indipendent of the sample (unlike multiple choice, where your distribution is changing), this allows you to use Cross-Entropy Loss, which factors in Softmax in the backpropagation step for <a href=""https://medium.com/@zhang_yang/understanding-cross-entropy-implementation-in-pytorch-softmax-log-softmax-nll-cross-entropy-416a2b200e34"" rel=""noreferrer"">increased numerical stability</a>.</p>
",17,19,12982,2020-03-10 01:02:54,https://stackoverflow.com/questions/60610280/bertforsequenceclassification-vs-bertformultiplechoice-for-sentence-multi-class
Why should I call a BERT module instance rather than the forward method?,"<p>I'm trying to extract vector-representations of text using BERT in the transformers libray, and have stumbled on the following part of the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">documentation</a> for the ""BERTModel"" class:</p>

<p><a href=""https://i.sstatic.net/74fta.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/74fta.png"" alt=""enter image description here""></a></p>

<p>Can anybody explain this in more detail? A forward-pass makes intuitive sense to me (am trying to get final hidden states after all), and I can't find any additional information on what ""pre and post processing"" means in this context.</p>

<p>Thanks up front!</p>
","bert-language-model, huggingface-transformers","<p>I think this is just general advice concerning working with PyTorch <code>Module</code>'s. The <code>transformers</code> modules are <code>nn.Module</code>s, and they require a <code>forward</code> method. However, one should not call <code>model.forward()</code> manually but instead call <code>model()</code>. The reason is that PyTorch does some stuff under the hood when just calling the Module. You can find that in <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module"" rel=""nofollow noreferrer"">the source code</a>.</p>

<pre class=""lang-py prettyprint-override""><code>def __call__(self, *input, **kwargs):
    for hook in self._forward_pre_hooks.values():
        result = hook(self, input)
        if result is not None:
            if not isinstance(result, tuple):
                result = (result,)
            input = result
    if torch._C._get_tracing_state():
        result = self._slow_forward(*input, **kwargs)
    else:
        result = self.forward(*input, **kwargs)
    for hook in self._forward_hooks.values():
        hook_result = hook(self, input, result)
        if hook_result is not None:
            result = hook_result
    if len(self._backward_hooks) &gt; 0:
        var = result
        while not isinstance(var, torch.Tensor):
            if isinstance(var, dict):
                var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
            else:
                var = var[0]
        grad_fn = var.grad_fn
        if grad_fn is not None:
            for hook in self._backward_hooks.values():
                wrapper = functools.partial(hook, self)
                functools.update_wrapper(wrapper, hook)
                grad_fn.register_hook(wrapper)
    return result
</code></pre>

<p>You'll see that <code>forward</code> is called when necessary.</p>
",2,0,496,2020-03-16 17:33:30,https://stackoverflow.com/questions/60710606/why-should-i-call-a-bert-module-instance-rather-than-the-forward-method
BERT get sentence level embedding after fine tuning,"<p>I came across this <a href=""https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=KVB3eOcjxxm1"" rel=""nofollow noreferrer"">page</a></p>

<p>1) I would like to get sentence level embedding (embedding given by <code>[CLS]</code> token) after the fine tuning is done. How could I do it?</p>

<p>2) I also noticed that the code on that page takes a lot of time to return results on the test data. Why is that? When i trained the model it took less time as compared to when i tried to get test predictions. 
From the code on that page, I didnt use below blocks of the code</p>

<pre><code>test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                       text_a = x[DATA_COLUMN], 
                                                                       text_b = None, 
                                                                       label = x[LABEL_COLUMN]), axis = 1

test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

test_input_fn = run_classifier.input_fn_builder(
        features=test_features,
        seq_length=MAX_SEQ_LENGTH,
        is_training=False,
        drop_remainder=False)

estimator.evaluate(input_fn=test_input_fn, steps=None)
</code></pre>

<p>Rather I just used below function on my entire test data</p>

<pre><code>def getPrediction(in_sentences):
  labels = [""Negative"", ""Positive""]
  input_examples = [run_classifier.InputExample(guid="""", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, """" is just a dummy label
  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)
  predictions = estimator.predict(predict_input_fn)
  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]
</code></pre>

<p>3) how could i get probability of prediction. is there a way to use <code>keras predict</code> method?</p>

<h1>update1</h1>

<p>question 2 update -
could you test on 20000 training examples using <code>getPrediction</code> function?....it takes much longer time for me..even more than the time took to train model on 20000 examples.</p>
","python, tensorflow, keras, classification, bert-language-model","<p>1) From <a href=""https://aihub.cloud.google.com/p/products%2F2c1fe4d8-4ff3-4d4f-8ac4-45d445532a3b"" rel=""nofollow noreferrer"">BERT documentation</a></p>

<blockquote>
  <p>The output dictionary contains:</p>
  
  <p>pooled_output: pooled output of the entire sequence with shape
  [batch_size, hidden_size]. sequence_output: representations of every
  token in the input sequence with shape [batch_size,
  max_sequence_length, hidden_size].</p>
</blockquote>

<p>I've added <code>pooled_output</code> vector which corresponds to the CLS vector. </p>

<p>3) You receive log probabilities. Just apply <code>softmax</code> to get normal probabilities.</p>

<p>Now all it is left to do is for model to report it. I have left the log probs, but they are not necessary anymore.</p>

<p>See the code changes:</p>

<pre><code>def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,
                 num_labels):
  """"""Creates a classification model.""""""

  bert_module = hub.Module(
      BERT_MODEL_HUB,
      trainable=True)
  bert_inputs = dict(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids)
  bert_outputs = bert_module(
      inputs=bert_inputs,
      signature=""tokens"",
      as_dict=True)

  # Use ""pooled_output"" for classification tasks on an entire sentence.
  # Use ""sequence_outputs"" for token-level output.
  output_layer = bert_outputs[""pooled_output""]

  pooled_output = output_layer

  hidden_size = output_layer.shape[-1].value

  # Create our own layer to tune for politeness data.
  output_weights = tf.get_variable(
      ""output_weights"", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(
      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope(""loss""):

    # Dropout helps prevent overfitting
    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)
    probs = tf.nn.softmax(logits, axis=-1)

    # Convert labels into one-hot encoding
    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))
    # If we're predicting, we want predicted labels and the probabiltiies.
    if is_predicting:
      return (predicted_labels, log_probs, probs, pooled_output)

    # If we're train/eval, compute loss between predicted and actual label
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, predicted_labels, log_probs, probs, pooled_output)
</code></pre>

<p>Now in the <code>model_fn_builder()</code> add support for those values:</p>

<pre><code>  # this should be changed in both places
  (predicted_labels, log_probs, probs, pooled_output) = create_model(
    is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)

  # return dictionary of all the values you wanted
  predictions = {
      'log_probabilities': log_probs,
      'probabilities': probs,
      'labels': predicted_labels,
      'pooled_output': pooled_output
  }
</code></pre>

<p>Adjust <code>getPrediction()</code> accordingly and in the end your predictions will look like this:</p>

<pre><code>('That movie was absolutely awful',
  array([0.99599314, 0.00400678], dtype=float32),  &lt;= Probability
  array([-4.0148855e-03, -5.5197663e+00], dtype=float32), &lt;= Log probability, same as previously
  'Negative', &lt;= Label
  array([ 0.9181199 ,  0.7763732 ,  0.9999883 , -0.93533266, -0.9841384 ,
          0.78126144, -0.9918988 , -0.18764131,  0.9981035 ,  0.99999994,
          0.900716  , -0.99926263, -0.5078789 , -0.99417543, -0.07695035,
          0.9501321 ,  0.75836045,  0.49151263, -0.7886792 ,  0.97505844,
         -0.8931161 , -1.        ,  0.9318583 , -0.60531116, -0.8644371 ,
        ...
        and this is 768-d [CLS] vector (sentence embedding).    
</code></pre>

<p>Regarding 2): At my end training took about 5 minutes and test about 40 seconds. Very reasonable.</p>

<p><strong>UPDATE</strong></p>

<p>For 20k samples it took 12:48 to train and 2:07 minutes to test.</p>

<p>For 10k samples timings are 8:40 and 1:07 respectively.</p>
",3,4,1662,2020-03-20 00:46:09,https://stackoverflow.com/questions/60767089/bert-get-sentence-level-embedding-after-fine-tuning
Access the output of several layers of pretrained DistilBERT model,"<p>I am trying to access the output embeddings from several different layers of the pretrained ""DistilBERT"" model. (""distilbert-base-uncased"")</p>

<pre><code>bert_output = model(input_ids, attention_mask=attention_mask)
</code></pre>

<p>The bert_output seems to return only the embedding values of the last layer for the input tokens.</p>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>If you want to get the output of all the hidden layers, you need to add the <code>output_hidden_states=True</code> kwarg to your config.</p>

<p>Your code will look something like</p>

<pre><code>from transformers import DistilBertModel, DistilBertConfig

config = DistilBertConfig.from_pretrained('distilbert-base-cased', output_hidden_states=True)
model = DistilBertModel.from_pretrained('distilbert-base-cased', config=config)
</code></pre>

<p>The hidden layers will be made available as <code>bert_output[2]</code></p>
",5,1,1155,2020-03-20 19:02:08,https://stackoverflow.com/questions/60780181/access-the-output-of-several-layers-of-pretrained-distilbert-model
Confusion in understanding the output of BERTforTokenClassification class from Transformers library,"<p>It is the example given in the documentation of transformers pytorch library</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForTokenClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', 
                      output_hidden_states=True, output_attentions=True)

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", 
                         add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)

loss, scores, hidden_states,attentions = outputs
</code></pre>

<p>Here <code>hidden_states</code> is a tuple of length 13 and contains hidden-states of the model at the output of each layer plus the initial embedding outputs. I would like to know, <strong>whether hidden_states[0] or hidden_states[12] represent the final hidden state vectors</strong>?</p>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>If you check the source code, specifically <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L512"" rel=""nofollow noreferrer""><code>BertEncoder</code></a>, you can see that the returned states are initialized as an empty tuple and then simply appended per iteration of each layer.</p>
<p>The final layer is appended as the last element <em>after</em> this loop, see <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L585"" rel=""nofollow noreferrer"">here</a>, so we can safely assume that <code>hidden_states[12]</code> is the final vectors.</p>
",3,1,1006,2020-03-25 10:50:01,https://stackoverflow.com/questions/60847291/confusion-in-understanding-the-output-of-bertfortokenclassification-class-from-t
Does BertForSequenceClassification classify on the CLS vector?,"<p>I'm using the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. I'm trying to do 4-way sentiment classification and am using <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""noreferrer"">BertForSequenceClassification</a> to build a model that eventually leads to a 4-way softmax at the end.</p>

<p>My understanding from reading the BERT paper is that the final dense vector for the input <code>CLS</code> token serves as a representation of the whole text string:</p>

<blockquote>
  <p>The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</p>
</blockquote>

<p>So, does <code>BertForSequenceClassification</code> actually train and use this vector to perform the final classification?</p>

<p>The reason I ask is because when I <code>print(model)</code>, it is not obvious to me that the <code>CLS</code> vector is being used. </p>

<pre class=""lang-py prettyprint-override""><code>model = BertForSequenceClassification.from_pretrained(
    model_config,
    num_labels=num_labels,
    output_attentions=False,
    output_hidden_states=False
)

print(model)
</code></pre>

<p>Here is the bottom of the output:</p>

<pre><code>        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
</code></pre>

<p>I see that there is a pooling layer <code>BertPooler</code> leading to a <code>Dropout</code> leading to a <code>Linear</code> which presumably performs the final 4-way softmax. However, the use of the <code>BertPooler</code> is not clear to me. Is it operating on only the hidden state of <code>CLS</code>, or is it doing some kind of pooling over hidden states of all the input tokens?</p>

<p>Thanks for any help.</p>
","python, machine-learning, pytorch, bert-language-model, huggingface-transformers","<p>The short answer: <em>Yes</em>, you are correct. Indeed, they use the CLS token (and only that) for <code>BertForSequenceClassification</code>.</p>

<p>Looking at the implementation of the <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L426"" rel=""noreferrer""><code>BertPooler</code></a> reveals that it is using the first hidden state, which corresponds to the <code>[CLS]</code> token.
I briefly checked one other model (RoBERTa) to see whether this is consistent across models. Here, too,  classification only takes place based on the <code>[CLS]</code> token, albeit less obvious (check lines 539-542 <a href=""https://github.com/huggingface/transformers/blob/3ee431dd4c720e67e35a449b453d3dc2b15ccfff/src/transformers/modeling_roberta.py#L539"" rel=""noreferrer"">here</a>).</p>
",5,5,2686,2020-03-26 21:27:21,https://stackoverflow.com/questions/60876394/does-bertforsequenceclassification-classify-on-the-cls-vector
How to load BertforSequenceClassification models weights into BertforTokenClassification model?,"<p>Initially, I have a fine-tuned BERT base cased model using a text classification dataset and I have used BertforSequenceClassification class for this. </p>

<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    ""bert-base-uncased"", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>

<p>Now I want to use this fine-tuned BERT model weights for Named Entity Recognition and I have to use BertforTokenClassification class for this. I'm unable to figure out how to load the fine-tuned BERT model weights into the new model created using BertforTokenClassification.</p>

<p>Thanks in advance.......................</p>
","nlp, pytorch, named-entity-recognition, bert-language-model","<p>You can get weights from the bert inside the first model and load into the bert inside the second:</p>

<pre><code>new_model = BertForTokenClassification(config=config)
new_model.bert.load_state_dict(model.bert.state_dict())
</code></pre>
",4,2,3827,2020-03-28 05:15:26,https://stackoverflow.com/questions/60897514/how-to-load-bertforsequenceclassification-models-weights-into-bertfortokenclassi
Issue when preprocessing text with Ktrain and DistilBERT,"<p>Following the example notebook here:</p>

<p><a href=""https://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroup-distilbert.ipynb"" rel=""nofollow noreferrer"">https://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroup-distilbert.ipynb</a></p>

<p>At STEP 1: Preprocess Data, I run into the errors listed below. When I do exactly the same in a Colab notebook, it works. What am I missing on my machine? I <strong>am</strong> able to run this with BERT, DistilBERT causes problems.</p>

<pre><code>trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,
                                      x_test=x_test, y_test=y_test,
                                      class_names=class_names,
                                      preprocess_mode='distilbert',
                                      maxlen=350)
</code></pre>

<p>causes:</p>

<pre><code>    ---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-142-ff3842c91276&gt; in &lt;module&gt;
      3                                           class_names=class_names,
      4                                           preprocess_mode='distilbert',
----&gt; 5                                           maxlen=350)

/usr/local/lib/python3.7/site-packages/ktrain/text/data.py in texts_from_array(x_train, y_train, x_test, y_test, class_names, max_features, maxlen, val_pct, ngram_range, preprocess_mode, lang, random_state, verbose)
    337                            classes = class_names,
    338                            lang=lang, ngram_range=ngram_range)
--&gt; 339     trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    340     val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    341     return (trn, val, preproc)

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in preprocess_train(self, texts, y, mode, verbose)
    766                                       pad_on_left=bool(self.name in ['xlnet']),
    767                                       pad_token=self.tok.convert_tokens_to_ids([self.tok.pad_token][0]),
--&gt; 768                                       pad_token_segment_id=4 if self.name in ['xlnet'] else 0)
    769         self.set_multilabel(dataset, mode)
    770         return dataset

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in hf_convert_examples(texts, y, tokenizer, max_length, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)
    280                                           pad_token=pad_token,
    281                                           pad_token_segment_id=pad_token_segment_id,
--&gt; 282                                           mask_padding_with_zero=mask_padding_with_zero)
    283             features_list.append(features)
    284             labels.append(y[idx] if y is not None else None)

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in hf_convert_example(text, tokenizer, max_length, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)
    206         max_length=max_length,
    207     )
--&gt; 208     input_ids, token_type_ids = inputs[""input_ids""], inputs[""token_type_ids""]
    209 
    210     # The mask has 1 for real tokens and 0 for padding tokens. Only real

KeyError: 'token_type_ids'
</code></pre>

<p>Any ideas what's wrong here?</p>
","python, keras, transformer-model, bert-language-model, distilbert","<p>Indeed, as mentioned by Jindřich, it was a version issue. Updating solved the problem. The error message was somewhat misleading...</p>
",1,1,1484,2020-04-01 16:33:22,https://stackoverflow.com/questions/60975829/issue-when-preprocessing-text-with-ktrain-and-distilbert
Difficulty in understanding the tokenizer used in Roberta model,"<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoTokenizer

tokenizer1 = AutoTokenizer.from_pretrained(""roberta-base"")
tokenizer2 = AutoTokenizer.from_pretrained(""bert-base-cased"")

sequence = ""A Titan RTX has 24GB of VRAM""
print(tokenizer1.tokenize(sequence))
print(tokenizer2.tokenize(sequence))
</code></pre>

<p>Output:</p>

<p>['A', 'ĠTitan', 'ĠRTX', 'Ġhas', 'Ġ24', 'GB', 'Ġof', 'ĠVR', 'AM']</p>

<p>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</p>

<p>Bert model uses WordPiece tokenizer. Any word that does not occur in the WordPiece vocabulary is broken down into sub-words greedily. For example, 'RTX' is broken into 'R', '##T' and '##X' where ## indicates it is a subtoken. </p>

<p>Roberta uses BPE tokenizer but I'm unable to understand </p>

<p>a) how BPE tokenizer works? </p>

<p>b) what does G represents in each of tokens?</p>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>This question is extremely broad, so I'm trying to give an answer that focuses on the main problem at hand. If you feel the need to have other questions answered, please open another question focusing on <em>one question at a time</em>, see the [help/on-topic] rules for Stackoverflow.</p>

<p>Essentially, as you've correctly identified, BPE is central to any tokenization in modern deep networks. I highly recommend you to read the <a href=""https://www.aclweb.org/anthology/P16-1162/"" rel=""noreferrer"">original BPE paper by Sennrich et al.</a>, in which they also highlight a bit more of the history of BPEs. <br/>
In any case, the tokenizers for any of the huggingface models are pretrained, meaning that they are usually generated from the training set of the algorithm beforehand. Common implementations such as <a href=""https://github.com/google/sentencepiece"" rel=""noreferrer"">SentencePiece</a> also give a bit better understanding of it, but essentially the task is framed as a constrained optimization problem, where you specify a maximum number of <code>k</code> allowed vocabulary words (the constraint), and the algorithm tries to then keep as many words intact without exceeding <code>k</code>.</p>

<p>if there are not enough words to cover the whole vocabulary, smaller units are used to approximate the vocabulary, which results in the splits observed in the example you gave.
RoBERTa uses a variant called ""<em>byte-level BPE</em>"", the best explanation is probably given in <a href=""https://arxiv.org/pdf/1909.03341.pdf"" rel=""noreferrer"">this study by Wang et al.</a>. The main benefit is, that it results in a smaller vocabulary while maintaining the quality of splits, from what I understand.</p>

<p>The second part of your question is easier to explain; while BERT highlights the <em>merging</em> of two subsequent tokens (with <code>##</code>), RoBERTa's tokenizer instead highlights the <em>start of a new token</em> with a specific unicode character (in this case, <code>\u0120</code>, the G with a dot). The best reason I could find for this was <a href=""https://github.com/openai/gpt-2/issues/80"" rel=""noreferrer"">this thread</a>, which argues that it basically avoids the use of whitespaces in training.</p>
",25,18,12365,2020-04-10 04:58:26,https://stackoverflow.com/questions/61134275/difficulty-in-understanding-the-tokenizer-used-in-roberta-model
how to do avg pool on the output of bert model for each sentence?,"<p>for classification, we usually use [CLS] to predict labels. but now i have another request to do avg-pooling on the output of each sentence in bert model. it seems a little bit hard for me? 
sentence is split by [SEP] but lengh of each sentence in each sample of a batch is not equal, so tf.split is not fit for this problem?  </p>

<p>an example as follows(batch_size=2), how to get the avg-pooling of each sentences?</p>

<p>[CLS] w1 w2 w3 [sep] w4 w5 [sep]</p>

<p>[CLS] x1 x2 [sep] x3 w4 x5 [sep]</p>
","tensorflow, pooling, bert-language-model","<p>You can get the averages by masking.</p>

<p>If you call <code>encode_plus</code> on the tokenizer and set <code>return_token_type_ids</code> to <code>True</code>, you will get a dictionary that contains:</p>

<ul>
<li><code>'input_ids'</code>: token indices that you pass into your model</li>
<li><code>'token_type_ids'</code>: a list of 0s and 1s that says which token belongs to which input sentence.</li>
</ul>

<p>Assuming you batched the <code>token_type_ids</code>, such that 0s are the first sentence, 1s are the second sentence and padding is something else (like -1) in a tensor in variable <code>mask</code> with shape <em>batch</em> × <em>length</em>, and you have the BERT output in a tensor in variable <code>output</code> of shape <em>batch</em> × <em>length</em> × 768, you can do:</p>

<pre class=""lang-py prettyprint-override""><code>first_sent_mask  = tf.cast(mask == 0, tf.float32)
first_sent_lens = tf.reduce_sum(first_sent_mask, axis=1, keepdims=True)
first_sent_mean = (
    tf.reduce_sum(output * tf.expand_dims(first_sent_mask, 2)) /
    first_sent_lens)
second_sent_mask = tf.cast(mask == 1, tf.float32)
...
</code></pre>
",0,0,2161,2020-04-13 02:50:19,https://stackoverflow.com/questions/61180882/how-to-do-avg-pool-on-the-output-of-bert-model-for-each-sentence
Confusion in Pre-processing text for Roberta Model,"<p>I want to apply Roberta model for  text similarity. Given a pair of sentences,the input should be in the format <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code>. I figure out two possible ways to generate the input ids namely</p>

<p>a)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands')

list2 = tokenizer.encode('Numbness of upper limb')

sequence = list1+[2]+list2[1:]

</code></pre>

<p>In this case, sequence is <code>[0, 12178, 3814, 2400, 11, 1420, 2, 2, 234, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>b)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands', add_special_tokens=False)

list2 = tokenizer.encode('Numbness of upper limb', add_special_tokens=False)

sequence = [0]+list1+[2,2]+list2+[2]
</code></pre>

<p>In this case, sequence is <code>[0, 25101, 3814, 2400, 11, 1420, 2, 2, 487, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>Here <code>0</code> represents <code>&lt;s&gt;</code> token and 2 represents <code>&lt;/s&gt;</code> token. I'm not sure which is the correct way to encode the given two sentences for calculating sentence similarity using Roberta model.</p>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>The easiest way is probably to directly use the provided function by HuggingFace's Tokenizers themselves, namely the <code>text_pair</code> argument in the <code>encode</code> function, see <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode"" rel=""noreferrer"">here</a>. This allows you to directly feed in two sentences, which will be giving you the desired output:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')
sequence = tokenizer.encode(text='Very severe pain in hands',
                            text_pair='Numbness of upper limb',
                            add_special_tokens=True)
</code></pre>

<p>This is especially convenient if you are dealing with very long sequences, as the <code>encode</code> function automatically reduces your lengths according to the <code>truncaction_strategy</code> argument. You obviously don't have to worry about this, if it is only short sequences.</p>

<p>Alternatively, you can also make use of the more explicit <a href=""https://huggingface.co/transformers/model_doc/roberta.html#transformers.RobertaTokenizer.build_inputs_with_special_tokens"" rel=""noreferrer""><code>build_inputs_with_special_tokens()</code></a> function of the <code>RobertaTokenizer</code>, specifically, which could be added to your example like so:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands', add_special_tokens=False)
list2 = tokenizer.encode('Numbness of upper limb', add_special_tokens=False)

sequence = tokenizer.build_inputs_with_special_tokens(list1, list2)
</code></pre>

<p>Note that in that case, you have to generate the sequences <code>list1</code> and <code>list2</code> still <em>without</em> any special tokens, as you have already done correctly.</p>
",5,1,1832,2020-04-15 05:17:51,https://stackoverflow.com/questions/61221810/confusion-in-pre-processing-text-for-roberta-model
Error importing BERT: module &#39;tensorflow._api.v2.train&#39; has no attribute &#39;Optimizer&#39;,"<p>I tried to use <code>bert-tensorflow</code> in Google Colab, but I got the following error:</p>

<blockquote>
  <p>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
        1 import  bert
  ----> 2 from bert import run_classifier_with_tfhub # run_classifier
        3 from bert import optimization
        4 from bert import tokenization</p>
  
  <p>1 frames /usr/local/lib/python3.6/dist-packages/bert/optimization.py
  in ()
       85 
       86 
  ---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):
       88   """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""
       89 </p>
  
  <p>AttributeError: module 'tensorflow._api.v2.train' has no attribute
  'Optimizer'</p>
</blockquote>

<p>Here is the code I tried:</p>

<ol>
<li>Install the libraries:</li>
</ol>

<p><code>!pip install --upgrade --force-reinstall tensorflow
!pip install --upgrade --force-reinstall tensorflow-gpu
!pip install tensorflow_hub
!pip install sentencepiece
!pip install bert-tensorflow</code></p>

<ol start=""2"">
<li>Run this code:</li>
</ol>

<p><code>from sklearn.model_selection import train_test_split
import pandas as pd
from datetime import datetime
from tensorflow.keras import optimizers
import  bert
from bert import run_classifier
from bert import optimization
from bert import tokenization
</code></p>

<p>I've also tried 
<code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()</code></p>

<p>But got the same error.</p>
","python, tensorflow, classification, bert-language-model","<p>I did some experimentation in my own colab notebook (please provide a link next time) and I found that in the error message, there was </p>

<pre><code>class AdamWeightDecayOptimizer(tf.train.Optimizer):
</code></pre>

<p>this being the header of the class. But there is nothing like <code>tf.train.optimizer</code> instead it should be :</p>

<pre><code>class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer):
</code></pre>

<p>The link where there is exact issue with (lol) exact same line is <a href=""https://github.com/google-research/bert/issues/934"" rel=""noreferrer"">here</a></p>
",9,10,16504,2020-04-16 12:30:46,https://stackoverflow.com/questions/61250311/error-importing-bert-module-tensorflow-api-v2-train-has-no-attribute-optimi
Gradient of the loss of DistilBERT for measuring token importance,"<p>I am trying to access the gradient of the loss in DistilBERT with respect to each attention weight in the first layer. I could access the computed gradient value of the output weight matrix via the following code when <code>requires_grad=True</code> </p>

<pre><code>loss.backward()
for name, param in model.named_parameters():
    if name == 'transformer.layer.0.attention.out_lin.weight':
       print(param.grad)  #shape is [768,768]
</code></pre>

<p>where <code>model</code> is the loaded distilbert model.
My question is how to get the gradient with respect to [SEP] or [CLS] or other tokens' attention? I need it to reproduce the figure about the ""Gradient-based feature importance estimates for attention to [SEP]"" in the following link: 
<a href=""https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062</a></p>

<p>A similar question for the same purpose has been asked in the following, but it is not my issue:
<a href=""https://stackoverflow.com/questions/61286574/bert-token-importance-measuring-issue-grad-is-none"">BERT token importance measuring issue. Grad is none</a> </p>
","pytorch, transformer-model, attention-model, huggingface-transformers, bert-language-model","<p>By default, the gradients are retained only for parameters, basically just to save memory. If you need gradients of inner nodes of the computation graph, you need to have the respective tensor before calling <code>backward()</code> and add a hook that will be executed at the backward pass.</p>

<p>A minimum solution from <a href=""https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/3"" rel=""nofollow noreferrer"">PyTorch forum</a>:</p>

<pre class=""lang-py prettyprint-override""><code>yGrad = torch.zeros(1,1)
def extract(xVar):
    global yGrad
    yGrad = xVar    

xx = Variable(torch.randn(1,1), requires_grad=True)
yy = 3*xx
zz = yy**2

yy.register_hook(extract)

#### Run the backprop:
print (yGrad) # Shows 0.
zz.backward()
print (yGrad) # Show the correct dzdy
</code></pre>

<p>In this case, the gradients are stored in a global variable where they persist after PyTorch get rid of them in the graph itself.</p>
",1,0,1006,2020-04-20 16:04:42,https://stackoverflow.com/questions/61326892/gradient-of-the-loss-of-distilbert-for-measuring-token-importance
bert + text and structur data,"<p>For each instance, I have the text and tabular data. I was wondering if there is any way I can use Bert_classification and combine the result to classify the whole dataset without overfitting. Is there any way to make two different classifications for text and tabular data and combine them together?  </p>
","text, tabular, bert-language-model","<p>BERT can return a vector, so-called <code>[CLS]</code> vector that can be used as an input to any further model. So, the answer is yes: you can process your text with BERT and get the single-vector representation, get whatever representation/features you do for your tabular data, concatenate them and train a classifier on top of them.</p>
",0,0,557,2020-04-21 15:44:18,https://stackoverflow.com/questions/61347507/bert-text-and-structur-data
Is it possible to fine-tune BERT to do retweet prediction?,"<p>I want to build a classifier that predicts if user <code>i</code> will retweet tweet <code>j</code>. </p>

<p>The dataset is huge, it contains 160 million tweets. Each tweet comes along with some metadata(e.g. does the retweeter follow the user of the tweet).</p>

<p>the text tokens for a single tweet is an ordered list of BERT ids. To get the embedding of the tweet, you just use the ids (So it is not text)</p>

<p>Is it possible to fine-tune BERT to do the prediction? if yes, what do courses/sources do you recommend to learn how to fine-tune? (I'm a beginner)</p>

<p>I should add that the prediction should be a probability.</p>

<p>If it's not possible, I'm thinking of converting the embeddings back to text then using some arbitrary classifier that I'm going to train. </p>
","machine-learning, nlp, bert-language-model","<p>You can fine-tune BERT, and you can use BERT to do retweet prediction, but you need more architecture in order to predict if user <em>i</em> will retweet tweet <em>j</em>.</p>

<p>Here is an architecture off the top of my head.</p>

<p><a href=""https://i.sstatic.net/02ppi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/02ppi.png"" alt=""enter image description here""></a></p>

<p>At a high level:</p>

<ol>
<li>Create a dense vector representation (embedding) of user <em>i</em> (perhaps containing something about the user's interests, such as sports).</li>
<li>Create an embedding of tweet <em>j</em>.</li>
<li>Create an embedding of the combination of the first two embeddings together, such as with concatenation or hadamard product.</li>
<li>Feed this embedding through a NN that performs binary classification to predict retweet or non-retweet.</li>
</ol>

<p>Let's break this architecture down by item.</p>

<p>To create an embedding of user <em>i</em>, you will need to create some kind of neural network that accepts whatever features you have about the user and produces a dense vector. This part is the most difficult component of the architecture. This area is not in my wheelhouse, but a quick google search for ""user interest embedding"" brings up this research paper on an algorithm called <a href=""https://medium.com/deep-learning-hk/starspace-mining-and-embedding-user-interests-28081937f95"" rel=""nofollow noreferrer"">StarSpace</a>. It suggests that it can ""obtain highly informative user embeddings according to user behaviors"", which is what you want.</p>

<p>To create an embedding of tweet <em>j</em>, you can use any type of neural network that takes tokens and produces a vector. Research prior to 2018 would have suggested using an LSTM or a CNN to produce the vector. However, BERT (as you mentioned in your post) is the current state-of-the-art. It takes in text (or text indices) and produces a vector for each token; one of those tokens should have been the prepended <code>[CLS]</code> token, which commonly is taken to be the representation of the whole sentence. <a href=""https://towardsdatascience.com/bert-to-the-rescue-17671379687f"" rel=""nofollow noreferrer"">This article</a> provides a conceptual overview of the process. It is in this part of the architecture that you can fine-tune BERT. <a href=""https://mccormickml.com/2019/07/22/BERT-fine-tuning/"" rel=""nofollow noreferrer"">This webpage</a> provides concrete code using PyTorch and the Huggingface implementation of BERT to do this step (I've gone through the steps and can vouch for it). In the future, you'll want to google for ""BERT single sentence classification"".</p>

<p>To create an embedding representing the combination of user <em>i</em> and tweet <em>j</em>, you can do one of many things. You can simply concatenate them together into one vector; so if user <em>i</em> is an M-dimensional vector and tweet <em>j</em> is an N-dimensional vector, then the concatenation produces an (M+N)-dimensional vector. An alternative approach is to compute the hadamard product (element-wise multiplication); in this case, both vectors must have the same dimension.</p>

<p>To make the final classification of retweet or not-retweet, build a simple NN that takes the combination vector and produces a single value. Here, since you are doing binary classification, a NN with a logistic (sigmoid) function would be appropriate. You can interpret the output as the probability of retweeting, so a value above 0.5 would be to retweet. See <a href=""https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_binary_classification/"" rel=""nofollow noreferrer"">this webpage</a> for basic details on building a NN for binary classification.</p>

<p>In order to get this whole system to work, you need to train it all together <a href=""https://www.quora.com/What-does-it-mean-for-a-neural-network-to-be-trained-end-to-end"" rel=""nofollow noreferrer"">end-to-end</a>. That is, you have to get all the pieces hooked up first and train it rather than training the components separately.</p>

<p>Your input dataset would look something like this:</p>

<pre><code>user                          tweet                  retweet?
----                          -----                  --------
20 years old, likes sports    Great game             Y
30 years old, photographer    Teen movie was good    N 
</code></pre>

<p>If you want an easier route where there is no user personalization, then just leave out the components that create an embedding of user <em>i</em>. You can use BERT to build a model to determine if the tweet is retweeted without regard to user. You can again follow the links I mentioned above.</p>
",3,1,453,2020-04-21 18:38:58,https://stackoverflow.com/questions/61350737/is-it-possible-to-fine-tune-bert-to-do-retweet-prediction
Use Bert to predict multiple tokens,"<p>I'm looking for suggestions on using Bert and Bert's masked language model to predict multiple tokens.</p>

<p>My data looks like:</p>

<p>context: <code>some very long context paragraph</code></p>

<p>question: <code>rainy days lead to @placeholder</code> and the answer for this <code>@placeholder</code> is <code>wet weather</code>. In the model, <code>wet environment</code> is the answer to predict. </p>

<p>So at the pre-processing stage, should I change the text into <code>rainy days lead to [MASK]</code> or something like <code>rainy days lead to [MASK] [MASK]</code>? I know that the masked LM works well on the single token prediction, do you think the masked LM can work well on the multiple tokens prediction? If no, do you have any suggestions on how to pre-process and train this kind of data?</p>

<p>Thanks so much!</p>
","python, bert-language-model","<p>So there are 3 questions :</p>

<p>First,</p>

<blockquote>
  <p>So at the pre-processing stage, should I change the text into rainy
  days lead to [MASK] or something like rainy days lead to [MASK]
  [MASK]?</p>
</blockquote>

<p>In a word point of view, you should set [MASK] [MASK]. But remember that in BERT, the mask is set at a token point of view. In fact, 'wet weather' may be tokenized in something like : [wet] [weath] [##er], and in this case, you should have [MASK] [MASK] [MASK]. So one [MASK] per token.</p>

<p>Second,</p>

<blockquote>
  <p>I know that the masked LM works well on the single token prediction,
  do you think the masked LM can work well on the multiple tokens
  prediction?</p>
</blockquote>

<p>As you can read it in <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">the original paper</a>, they said :</p>

<blockquote>
  <p>The training data generator chooses 15% of the token positions at
  random for prediction. If the i-th token is chosen, we replace the
  i-th token with (1) the [MASK] token 80% of the time (2) a random
  token 10% of the time (3) the unchanged i-th token 10% of the time.</p>
</blockquote>

<p>They notice no limitation in the amount of MASKED token per sentence, you have several MASKED token during pre-training BERT.
In my own experience, I pre-trained BERT several times and I noticed that there were almost non differences between the prediction made on MASKED token if there were only one or more MASKED token in my input.</p>

<p>Third,</p>

<blockquote>
  <p>If no, do you have any suggestions on how to pre-process and train
  this kind of data?</p>
</blockquote>

<p>So the answer is yes, but if you really want to MASK elements you choose (and not randomly like in the paper), you should adapt the MASK when the data will be tokenized because the number of MASKED token will be greater (or equal) that the number of MASK in the word space you set (like the example I gave you : 1 word is not equals to 1 token, so basically, 1 MASKED word will be 1 or more MASK token). But honestly, the process of labellisation will be so huge I recommend you to increase the 15% of probability for MASK tokien or make a process that MASK the 1 or 2 next token for each MASKED token (or something like this)..</p>
",3,5,4686,2020-04-24 23:41:34,https://stackoverflow.com/questions/61419089/use-bert-to-predict-multiple-tokens
Huggingface&#39;s BERT tokenizer not adding pad token,"<p>It's not entirely clear from the documentation, but I can see that <code>BertTokenizer</code> is initialised with <code>pad_token='[PAD]'</code>, so I assume when you encode with <code>add_special_tokens=True</code> then it would automatically pad it. Given that <code>pad_token_id=0</code>, I can't see any <code>0</code>s in the <code>token_ids</code> however:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.encode(text, add_special_tokens=True, max_length=2048)

# Print the original sentence.
print('Original: ', text)

# Print the sentence split into tokens.
print('\nTokenized: ', tokens)

# Print the sentence mapped to token ids.
print('\nToken IDs: ', token_ids)
</code></pre>

<p>Output:</p>

<pre><code>Original:  Toronto's key stock index ended higher in brisk trading on Thursday, extending Wednesday's rally despite being weighed down by losses on Wall Street.
The TSE 300 Composite Index rose 29.80 points to close at 5828.62, outperforming the Dow Jones Industrial Average which slumped 21.27 points to finish at 6658.60.
Toronto added to Wednesday's 55-point rally while investors took profits in New York after the Dow's 92-point gains, said MMS International analyst Katherine Beattie.
""That shows that the markets are very fragile,"" Beattie said. ""They (investors) want to take advantage of any strength to sell,"" she said.
Toronto was also buoyed by its heavyweight gold group which jumped nearly 2.2 percent, aided by firmer COMEX gold prices. The key June contract rose $1.00 to $344.30.
Ten of Toronto's 14 sub-indices posted gains, led by golds, transportation, forestry products and consumer products.
The weak side included conglomerates, base metals and utilities.
Trading was heavy at 100 million shares worth C$1.54 billion ($1.1 billion).
Advancing stocks outnumbered declines 556 to 395, with 276 issues flat.
Among hot stocks, Bre-X Minerals Ltd. rose 0.13 to 2.30 on 5.0 million shares as investors continued to consider the viability of its Busang gold discovery in Indonesia.
Kenting Energy Services Inc. rose 0.25 to 9.05 after Precision Drilling Corp. amended its takeover offer
Bakery and foodstuffs maker George Weston Ltd. jumped 4.50 to close at 74.50, the TSE's top gainer.


Tokenized:  ['toronto', ""'"", 's', 'key', 'stock', 'index', 'ended', 'higher', 'in', 'brisk', 'trading', 'on', 'thursday', ',', 'extending', 'wednesday', ""'"", 's', 'rally', 'despite', 'being', 'weighed', 'down', 'by', 'losses', 'on', 'wall', 'street', '.', 'the', 'ts', '##e', '300', 'composite', 'index', 'rose', '29', '.', '80', 'points', 'to', 'close', 'at', '58', '##28', '.', '62', ',', 'out', '##per', '##form', '##ing', 'the', 'dow', 'jones', 'industrial', 'average', 'which', 'slumped', '21', '.', '27', 'points', 'to', 'finish', 'at', '66', '##58', '.', '60', '.', 'toronto', 'added', 'to', 'wednesday', ""'"", 's', '55', '-', 'point', 'rally', 'while', 'investors', 'took', 'profits', 'in', 'new', 'york', 'after', 'the', 'dow', ""'"", 's', '92', '-', 'point', 'gains', ',', 'said', 'mm', '##s', 'international', 'analyst', 'katherine', 'beat', '##tie', '.', '""', 'that', 'shows', 'that', 'the', 'markets', 'are', 'very', 'fragile', ',', '""', 'beat', '##tie', 'said', '.', '""', 'they', '(', 'investors', ')', 'want', 'to', 'take', 'advantage', 'of', 'any', 'strength', 'to', 'sell', ',', '""', 'she', 'said', '.', 'toronto', 'was', 'also', 'bu', '##oy', '##ed', 'by', 'its', 'heavyweight', 'gold', 'group', 'which', 'jumped', 'nearly', '2', '.', '2', 'percent', ',', 'aided', 'by', 'firm', '##er', 'come', '##x', 'gold', 'prices', '.', 'the', 'key', 'june', 'contract', 'rose', '$', '1', '.', '00', 'to', '$', '344', '.', '30', '.', 'ten', 'of', 'toronto', ""'"", 's', '14', 'sub', '-', 'indices', 'posted', 'gains', ',', 'led', 'by', 'gold', '##s', ',', 'transportation', ',', 'forestry', 'products', 'and', 'consumer', 'products', '.', 'the', 'weak', 'side', 'included', 'conglomerate', '##s', ',', 'base', 'metals', 'and', 'utilities', '.', 'trading', 'was', 'heavy', 'at', '100', 'million', 'shares', 'worth', 'c', '$', '1', '.', '54', 'billion', '(', '$', '1', '.', '1', 'billion', ')', '.', 'advancing', 'stocks', 'outnumbered', 'declines', '55', '##6', 'to', '395', ',', 'with', '276', 'issues', 'flat', '.', 'among', 'hot', 'stocks', ',', 'br', '##e', '-', 'x', 'minerals', 'ltd', '.', 'rose', '0', '.', '13', 'to', '2', '.', '30', 'on', '5', '.', '0', 'million', 'shares', 'as', 'investors', 'continued', 'to', 'consider', 'the', 'via', '##bility', 'of', 'its', 'bus', '##ang', 'gold', 'discovery', 'in', 'indonesia', '.', 'kent', '##ing', 'energy', 'services', 'inc', '.', 'rose', '0', '.', '25', 'to', '9', '.', '05', 'after', 'precision', 'drilling', 'corp', '.', 'amended', 'its', 'takeover', 'offer', 'bakery', 'and', 'foods', '##tu', '##ffs', 'maker', 'george', 'weston', 'ltd', '.', 'jumped', '4', '.', '50', 'to', 'close', 'at', '74', '.', '50', ',', 'the', 'ts', '##e', ""'"", 's', 'top', 'gain', '##er', '.']

Token IDs:  [101, 4361, 1005, 1055, 3145, 4518, 5950, 3092, 3020, 1999, 28022, 6202, 2006, 9432, 1010, 8402, 9317, 1005, 1055, 8320, 2750, 2108, 12781, 2091, 2011, 6409, 2006, 2813, 2395, 1012, 1996, 24529, 2063, 3998, 12490, 5950, 3123, 2756, 1012, 3770, 2685, 2000, 2485, 2012, 5388, 22407, 1012, 5786, 1010, 2041, 4842, 14192, 2075, 1996, 23268, 3557, 3919, 2779, 2029, 14319, 2538, 1012, 2676, 2685, 2000, 3926, 2012, 5764, 27814, 1012, 3438, 1012, 4361, 2794, 2000, 9317, 1005, 1055, 4583, 1011, 2391, 8320, 2096, 9387, 2165, 11372, 1999, 2047, 2259, 2044, 1996, 23268, 1005, 1055, 6227, 1011, 2391, 12154, 1010, 2056, 3461, 2015, 2248, 12941, 9477, 3786, 9515, 1012, 1000, 2008, 3065, 2008, 1996, 6089, 2024, 2200, 13072, 1010, 1000, 3786, 9515, 2056, 1012, 1000, 2027, 1006, 9387, 1007, 2215, 2000, 2202, 5056, 1997, 2151, 3997, 2000, 5271, 1010, 1000, 2016, 2056, 1012, 4361, 2001, 2036, 20934, 6977, 2098, 2011, 2049, 8366, 2751, 2177, 2029, 5598, 3053, 1016, 1012, 1016, 3867, 1010, 11553, 2011, 3813, 2121, 2272, 2595, 2751, 7597, 1012, 1996, 3145, 2238, 3206, 3123, 1002, 1015, 1012, 4002, 2000, 1002, 29386, 1012, 2382, 1012, 2702, 1997, 4361, 1005, 1055, 2403, 4942, 1011, 29299, 6866, 12154, 1010, 2419, 2011, 2751, 2015, 1010, 5193, 1010, 13116, 3688, 1998, 7325, 3688, 1012, 1996, 5410, 2217, 2443, 22453, 2015, 1010, 2918, 11970, 1998, 16548, 1012, 6202, 2001, 3082, 2012, 2531, 2454, 6661, 4276, 1039, 1002, 1015, 1012, 5139, 4551, 1006, 1002, 1015, 1012, 1015, 4551, 1007, 1012, 10787, 15768, 21943, 26451, 4583, 2575, 2000, 24673, 1010, 2007, 25113, 3314, 4257, 1012, 2426, 2980, 15768, 1010, 7987, 2063, 1011, 1060, 13246, 5183, 1012, 3123, 1014, 1012, 2410, 2000, 1016, 1012, 2382, 2006, 1019, 1012, 1014, 2454, 6661, 2004, 9387, 2506, 2000, 5136, 1996, 3081, 8553, 1997, 2049, 3902, 5654, 2751, 5456, 1999, 6239, 1012, 5982, 2075, 2943, 2578, 4297, 1012, 3123, 1014, 1012, 2423, 2000, 1023, 1012, 5709, 2044, 11718, 15827, 13058, 1012, 13266, 2049, 15336, 3749, 18112, 1998, 9440, 8525, 21807, 9338, 2577, 12755, 5183, 1012, 5598, 1018, 1012, 2753, 2000, 2485, 2012, 6356, 1012, 2753, 1010, 1996, 24529, 2063, 1005, 1055, 2327, 5114, 2121, 1012, 102]
</code></pre>
","tokenize, huggingface-transformers, bert-language-model","<p>No, it would not. There is a different parameter to allow padding:</p>
<ul>
<li><strong>transformers &gt;=3.0.0</strong> <a href=""https://huggingface.co/docs/transformers/master/main_classes/tokenizer#transformers.PreTrainedTokenizerBase.__call__"" rel=""nofollow noreferrer"">padding</a> (accepts <code>True</code>, <code>max_length</code> and <code>False</code> as values)</li>
<li><strong>transformers &lt; 3.0.0</strong> <a href=""https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode"" rel=""nofollow noreferrer"">pad_to_max_length</a> (accepts <code>True</code> or <code>False</code> as Values)</li>
</ul>
<p><code>add_special_tokens</code> will add the [CLS] and the [SEP] token (101 and 102 respectively).</p>
",13,3,10354,2020-04-26 15:37:47,https://stackoverflow.com/questions/61443480/huggingfaces-bert-tokenizer-not-adding-pad-token
BERT Certainty (iOS),"<p>I am currently integrating the BERT model listed on <a href=""https://developer.apple.com/machine-learning/models/#text"" rel=""nofollow noreferrer"">https://developer.apple.com/machine-learning/models/#text</a> into an iOS application and have had difficulty removing answers that have low certainty.</p>

<p>I have used the sample code found at the link above but because I wanted to answer questions based on larger volumes of text, I loop over an array of paragraphs and predict an answer for each one. However, the model does not return nil or ""No Answer"" if an answer is not found and instead returns a (seemingly) random substring. I suppose what I am trying to ask is: is it possible to access the certainty of BERT's response to filter out unlikely results? Or is there another way to get BERT to only return results above a set certainty threshold?</p>
","ios, machine-learning, bert-language-model","<p>After hours of searching, I've now found a solution. Ironically it only took three lines of code, but here it is anyway:</p>

<pre><code>if bestSum &lt; 7.5 {
    return nil
}
</code></pre>

<p>I implemented this in the <code>findBestLogitPair()</code> method in the <code>BERTOutput.swift</code> file as provided in Apple's sample code for text analysis using BERT. I have now discovered that the word logit does kind of mean probability in statistics - but being a programmer, I had no idea!</p>
",0,-1,250,2020-04-26 16:11:55,https://stackoverflow.com/questions/61444000/bert-certainty-ios
"Removing commas after processing lists of strings, when &#39; &#39;.join(x) does not work","<p>So I fed in a dataframe of sentences for token prediction in BERT, and I received as output along with the predictions, the sentences split into words. 
  Now i want to revert my dataframe of the split/tokenized sentences and predictions back to the original sentence.(of course i have the original sentence, but i need to do this process so that the predictions are in harmony with the sentence tokens)</p>

<pre><code>original sentence
You couldn't have done any better because if you could have, you would have.

Post processing
['[CLS]', 'You', 'couldn', ""'"", 't', 'have', 'done', 'any', 'better', 'because', 'if', 'you', 'could', 'have', ',', 'you', 'would', 'have', '.', '[SEP]']

</code></pre>

<p>I identified three processes necessary.
1. Remove quote marks 2. removes the CLS ,SEP and their extra quote marks and commas, 3. remove the commas separating the words and merge them.</p>

<pre><code>def fix_df(row):
    sentences = row['t_words'] 
    return remove_edges(sentences)

def remove_edges(sentences):
    x = sentences[9:-9]
    return remove_qmarks(x)

def remove_qmarks(x):
    y = x.replace(""'"", """")
    return join(y)

def join(y):
    z = ' '.join(y)
    return z


a_df['sents'] = a_df.apply(fix_df, axis=1) 

</code></pre>

<p>The first two functions largely worked correctly, but the last one did not. instead, i got a result that looked like this.</p>

<pre><code>Y o u , c o u l d n , "" "" , t , h a v e, d o n e ,...

</code></pre>

<p>The commas didnt go away, and the text got distorted instead. I am definitely missing something. what could that be?</p>
","python, nlp, spacy, bert-language-model, python-re","<p>The result string really, really looks like a string representation of an otherwise perfectly normal list, so let's have Python convert it back to a list, safely, per <a href=""https://stackoverflow.com/questions/1894269/convert-string-representation-of-list-to-list"">Convert string representation of list to list</a>:</p>

<pre class=""lang-py prettyprint-override""><code>import ast

result = """"""['[CLS]', 'You', 'couldn', ""'"", 't', 'have', 'done', 'any', 'better', 'because', 'if', 'you', 'could', 'have', ',', 'you', 'would', 'have', '.', '[SEP]']""""""

result_as_list = ast.literal_eval(result)
</code></pre>

<p>Now we have this</p>

<pre><code>['[CLS]', 'You', 'couldn', ""'"", 't', 'have', 'done', 'any', 'better', 'because', 'if', 'you', 'could', 'have', ',', 'you', 'would', 'have', '.', '[SEP]']
</code></pre>

<p>let's go over your steps again. First, ""remove the quote marks"". But there aren't any (obsolete) quote marks, because this is a list of strings; the extra quotes you see in the representation are only because that is how a <em>string</em> is represented in Python.</p>

<p>Next, ""remove the beginning and end markers"". As this is a list, they're just the first and last elements, no further counting needed:</p>

<pre class=""lang-py prettyprint-override""><code>result_as_list = result_as_list[1:-1]
</code></pre>

<p>Next, ""remove the commas"". As in the first step, there are no (obsolete) comma's; they are part of how Python <em>shows</em> a list and are not there in the actual data.</p>

<p>So we end up with</p>

<pre><code>['You', 'couldn', ""'"", 't', 'have', 'done', 'any', 'better', 'because', 'if', 'you', 'could', 'have', ',', 'you', 'would', 'have', '.']
</code></pre>

<p>which can be joined back into the original string using</p>

<pre class=""lang-py prettyprint-override""><code>result_as_string = ' '.join(result_as_list)
</code></pre>

<p>and the only problem remaining is that BERT apparently treats apostrophes, commas and full stops as separate 'words':</p>

<pre><code>You couldn ' t have done any better because if you could have , you would have .
</code></pre>

<p>which need a bit o'replacing:</p>

<pre class=""lang-py prettyprint-override""><code>result_as_string = result_as_string.replace(' ,', ',').replace(' .','.').replace("" ' "", ""'"")
</code></pre>

<p>and you have your sentence back:</p>

<pre><code>You couldn't have done any better because if you could have, you would have.
</code></pre>

<p>The only problem I see is if there are leading or closing quotes that aren't part of a contraction. If this is necessary, you can replace the space-quote-space replacement with a more focused one targeting specifically ""couldn't"", ""can't"", ""aren't"" etc.</p>
",2,1,1400,2020-04-27 15:38:50,https://stackoverflow.com/questions/61462608/removing-commas-after-processing-lists-of-strings-when-joinx-does-not-wor
How to get intermediate layers&#39; output of pre-trained BERT model in HuggingFace Transformers library?,"<p>(I'm following <a href=""https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"" rel=""noreferrer"">this</a> pytorch tutorial about BERT word embeddings, and in the tutorial the author is access the intermediate layers of the BERT model.)</p>

<p>What I want is to access the last, lets say, 4 last layers of a single input token of the BERT model in TensorFlow2 using HuggingFace's Transformers library. Because each layer outputs a vector of length 768, so the last 4 layers will have a shape of <code>4*768=3072</code> (for each token).</p>

<p>How can I implement this in TF/keras/TF2, to get the intermediate layers of pretrained model for an input token? (later I will try to get the tokens for each token in a sentence, but for now one token is enough).</p>

<p>I'm using the HuggingFace's BERT model:</p>

<pre><code>!pip install transformers
from transformers import (TFBertModel, BertTokenizer)

bert_model = TFBertModel.from_pretrained(""bert-base-uncased"")  # Automatically loads the config
bert_tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
sentence_marked = ""hello""
tokenized_text = bert_tokenizer.tokenize(sentence_marked)
indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)

print (indexed_tokens)
&gt;&gt; prints [7592]
</code></pre>

<p>The output is a token (<code>[7592]</code>), which should be the input of the for the BERT model.</p>
","tensorflow, keras, tensorflow2.0, huggingface-transformers, bert-language-model","<p>The third element of the BERT model's output is a tuple which consists of output of embedding layer as well as the intermediate layers hidden states. From <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">documentation</a>:</p>

<blockquote>
  <p><strong>hidden_states (<code>tuple(tf.Tensor)</code>, optional, returned when <code>config.output_hidden_states=True</code>):</strong>
  tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
  
  <p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</blockquote>

<p>For the <code>bert-base-uncased</code> model, the <code>config.output_hidden_states</code> is by default <code>True</code>. Therefore, to access hidden states of the 12 intermediate layers, you can do the following:</p>

<pre><code>outputs = bert_model(input_ids, attention_mask)
hidden_states = outputs[2][1:]
</code></pre>

<p>There are 12 elements in <code>hidden_states</code> tuple corresponding to all the layers from beginning to the last, and each of them is an array of shape <code>(batch_size, sequence_length, hidden_size)</code>. So, for example, to access the hidden state of third layer for the fifth token of all the samples in the batch, you can do: <code>hidden_states[2][:,4]</code>.</p>

<hr>

<p>Note that if the model you are loading does not return the hidden states by default, then you can load the config using <code>BertConfig</code> class and pass <code>output_hidden_state=True</code> argument, like this:</p>

<pre><code>config = BertConfig.from_pretrained(""name_or_path_of_model"",
                                    output_hidden_states=True)

bert_model = TFBertModel.from_pretrained(""name_or_path_of_model"",
                                         config=config)
</code></pre>
",13,11,17194,2020-04-27 17:47:33,https://stackoverflow.com/questions/61465103/how-to-get-intermediate-layers-output-of-pre-trained-bert-model-in-huggingface
How can i use BERT fo machine Translation?,"<p>I got a big problem. For my bachelor thesis I have to make a machine tranlation model with BERT. 
But I am not getting anywhere right now. 
Do you know a documentation or something that can help me here? 
I have read some papers in that direction but maybe there is a documentation or tutorial that can help me.</p>

<p>For my bachelor thesis I have to translate from a summary of a text into a title.
I hope someone can help me.</p>
","jupyter-notebook, machine-translation, bert-language-model, sequence-to-sequence","<p>BERT is not a machine translation model, BERT is designed to provide a contextual sentence representation that should be useful for various NLP tasks. Although there exist ways how BERT can be incorporated into machine translation (<a href=""https://openreview.net/forum?id=Hyl7ygStwB"" rel=""nofollow noreferrer"">https://openreview.net/forum?id=Hyl7ygStwB</a>), it is not an easy problem and there are doubts if it really pays off.</p>
<p>From your question, it seems that you are not really machine translation, but automatic summarization. Similarly to machine translation, it can be approached using sequence-to-sequence models, but we do not call it translation in NLP.
For sequence-to-sequence modeling, there are different pre-trained models, such as <a href=""https://arxiv.org/abs/1910.13461"" rel=""nofollow noreferrer"">BART</a> or <a href=""https://arxiv.org/abs/1905.02450"" rel=""nofollow noreferrer"">MASS</a>. These should be much more useful than BERT.</p>
<hr />
<p>Update in September 2022: There are multilingual BERT-like models, the most famous are <a href=""https://huggingface.co/bert-base-multilingual-cased"" rel=""nofollow noreferrer"">multilingual BERT</a> and <a href=""https://huggingface.co/xlm-roberta-base"" rel=""nofollow noreferrer"">XLM-RoBERTa</a>. When fine-tuned carefully, they can be used as a universal encoder for machine translation and enable so-called zero-shot machine translation. The model is trained to translate from several source languages into English, but in the end, it can translate from all languages covered by the multilingual BERT-like models. The method is called <a href=""https://arxiv.org/abs/2104.08757v1"" rel=""nofollow noreferrer"">SixT</a>.</p>
",11,4,12910,2020-04-30 12:51:41,https://stackoverflow.com/questions/61523829/how-can-i-use-bert-fo-machine-translation
HuggingFace BERT `inputs_embeds` giving unexpected result,"<p>The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to feed in a precomputed embedding in place of the embedding lookup that is native to BERT. This is done using the model's <code>call</code> method's optional parameter <code>inputs_embeds</code> (in place of <code>input_ids</code>). To test this out, I wanted to make sure that if I <em>did</em> feed in BERT's embedding lookup, I would get the same result as having fed in the <code>input_ids</code> themselves.</p>

<p>The result of BERT's embedding lookup can be obtained by setting the BERT configuration parameter <code>output_hidden_states</code> to <code>True</code> and extracting the first tensor from the last output of the <code>call</code> method. (The remaining 12 outputs correspond to each of the 12 hidden layers.)</p>

<p>Thus, I wrote the following code to test my hypothesis:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]
result2 = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})

print(tf.reduce_sum(tf.abs(result[0] - result2[0])))  # 458.2522, should be 0
</code></pre>

<p>Again, the output of the <code>call</code> method is a tuple. The first element of this tuple is the output of the last layer of BERT. Thus, I expected <code>result[0]</code> and <code>result2[0]</code> to match. <strong>Why is this not the case?</strong></p>

<p>I am using Python 3.6.10 with <code>tensorflow</code> version 2.1.0 and <code>transformers</code> version 2.5.1.</p>

<p><strong>EDIT</strong>: Looking at some of the <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html"" rel=""noreferrer"">HuggingFace code</a>, it seems that the raw embeddings that are looked up when <code>input_ids</code> is given or assigned when <code>inputs_embeds</code> is given are added to the positional embeddings and token type embeddings before being fed into subsequent layers. If this is the case, then it <em>may</em> be possible that what I'm getting from <code>result[-1][0]</code> is the raw embedding plus the positional and token type embeddings. This would mean that they are erroneously getting added in again when I feed <code>result[-1][0]</code> as <code>inputs_embeds</code> in order to calculate <code>result2</code>.</p>

<p><strong>Could someone please tell me if this is the case and if so, please explain how to get the positional and token type embeddings, so I can subtract them out?</strong> Below is what I came up with for positional embeddings based on the equations given <a href=""https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3"" rel=""noreferrer"">here</a> (but according to the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, the positional embeddings may actually be learned, so I'm not sure if these are valid):</p>

<pre><code>import numpy as np

positional_embeddings = np.stack([np.zeros(shape=(len(sent),768)) for sent in input_ids])
for s in range(len(positional_embeddings)):
    for i in range(len(positional_embeddings[s])):
        for j in range(len(positional_embeddings[s][i])):
            if j % 2 == 0:
                positional_embeddings[s][i][j] = np.sin(i/np.power(10000., j/768.))
            else:
                positional_embeddings[s][i][j] = np.cos(i/np.power(10000., (j-1.)/768.))
positional_embeddings = tf.constant(positional_embeddings)
inputs_embeds += positional_embeddings
</code></pre>
","python, tensorflow, nlp, huggingface-transformers, bert-language-model","<p>My intuition about positional and token type embeddings being added in turned out to be correct. After looking closely at the <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_tf_bert.html"" rel=""nofollow noreferrer"">code</a>, I replaced the line:</p>

<pre><code>inputs_embeds = result[-1][0]
</code></pre>

<p>with the lines:</p>

<pre><code>embeddings = bert_model.bert.get_input_embeddings().word_embeddings
inputs_embeds = tf.gather(embeddings, input_ids)
</code></pre>

<p>Now, the difference is 0.0, as expected.</p>
",3,8,3596,2020-05-02 23:18:17,https://stackoverflow.com/questions/61567599/huggingface-bert-inputs-embeds-giving-unexpected-result
Getting embedding lookup result from BERT,"<p>Prior to passing my tokens through BERT, I would like to perform some processing on their embeddings, (the result of the embedding lookup layer). The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to access the output of embedding lookup using:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]  # output of embedding lookup
</code></pre>

<p>Subsequently, one can process <code>inputs_embeds</code> and then send this in as an input to the same model using:</p>

<pre><code>inputs_embeds = process(inputs_embeds)  # some processing on inputs_embeds done here (dimensions kept the same)
result = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
output = result[0]
</code></pre>

<p>where <code>output</code> now contains the output of BERT for the modified input. However, this requires two full passes through BERT. Instead of running BERT all the way through just to perform embedding lookup, I would like to just get the output of the embedding lookup layer. <strong>Is this possible, and if so, how?</strong></p>
","python, tensorflow, nlp, huggingface-transformers, bert-language-model","<p>It is in fact incorrect to treat the first output <code>result[-1][0]</code> as the result of an embedding lookup. The raw embedding lookup is given by:</p>

<pre><code>embeddings = bert_model.bert.get_input_embeddings()
word_embeddings = embeddings.word_embeddings
inputs_embeds = tf.gather(word_embeddings, input_ids)
</code></pre>

<p>while <code>result[-1][0]</code> gives the embedding lookup <em>plus</em> positional embeddings and token type embeddings. The above code does not require a full pass through BERT, and the result can be processed prior to feeding into the remaining layers of BERT.</p>

<p><strong>EDIT</strong>: To get the result of addition positional and token type embeddings to an arbitrary <code>inputs_embeds</code>, one can use:</p>

<pre><code>full_embeddings = embeddings(inputs=[None, None, token_type_ids, inputs_embeds])
</code></pre>

<p>Here, the <code>call</code> method for the <code>embeddings</code> object accepts a list which is fed into the <code>_embeddings</code> method. The first value is <code>input_ids</code>, the second <code>position_ids</code>, the third <code>token_type_ids</code>, and the fourth <code>inputs_embeds</code>. (See <a href=""https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/modeling_tf_bert.py#L133"" rel=""noreferrer"">here</a> for more details.) If you have multiple sentences in one input, you may need to set <code>position_ids</code>.</p>
",6,5,2785,2020-05-03 05:10:34,https://stackoverflow.com/questions/61569900/getting-embedding-lookup-result-from-bert
Unable to load SpanBert model with transformers package,"<p>I have some questions regarding of SpanBert loading using transformers packages.</p>

<p>I downloaded the pre-trained file from <a href=""https://github.com/facebookresearch/SpanBERT"" rel=""nofollow noreferrer"">SpanBert</a> GitHub Repo and <code>vocab.txt</code> from Bert. Here is the code I used for loading:</p>

<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained(config_file=config_file,
                                  pretrained_model_name_or_path=model_file,
                                  vocab_file=vocab_file)
model.to(""cuda"")
</code></pre>

<p>where </p>

<ul>
<li><code>config_file</code> -> <code>config.json</code></li>
<li><code>model_file</code> -> <code>pytorch_model.bin</code></li>
<li><code>vocab_file</code> -> <code>vocab.txt</code></li>
</ul>

<p>But I got the <code>UnicodeDecoderError</code> with the above code saying that <code>'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</code></p>

<p>I also tried loading SpanBert with the method mentioned <a href=""https://huggingface.co/SpanBERT/spanbert-large-cased"" rel=""nofollow noreferrer"">here</a>. But it returned <code>OSError: file SpanBERT/spanbert-base-cased not found</code>.</p>

<p>Do you have any suggestions on loading the pre-trained model correctly? Any suggestions are much appreciated. Thanks!</p>
","python, huggingface-transformers, bert-language-model","<ol>
<li>Download the pre-trained weights from the Github page. </li>
</ol>

<p><a href=""https://github.com/facebookresearch/SpanBERT"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/SpanBERT</a></p>

<p><a href=""https://dl.fbaipublicfiles.com/fairseq/models/spanbert_hf_base.tar.gz"" rel=""nofollow noreferrer"">SpanBERT (base &amp; cased): 12-layer, 768-hidden, 12-heads , 110M parameters</a></p>

<p><a href=""https://dl.fbaipublicfiles.com/fairseq/models/spanbert_hf_base.tar.gz"" rel=""nofollow noreferrer"">SpanBERT (large &amp; cased): 24-layer, 1024-hidden, 16-heads, 340M parameters</a></p>

<ol start=""2"">
<li><p>Extract them to a folder, for example I extracted to spanbert_hf_base folder which contains a <code>.bin</code> file and a <code>config.json</code> file.</p></li>
<li><p>You can use <strong>AutoModel</strong> to load the model and simple bert tokenizer. From their repo:</p></li>
</ol>

<blockquote>
  <p>These models have the same format as the HuggingFace BERT models, so you can easily replace them with our SpanBET models.</p>
</blockquote>

<pre><code>import torch
from transformers import AutoModel
model = AutoModel.from_pretrained('spanbert_hf_base/') # the path to .bin and config.json

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

b = torch.tensor(tokenizer.encode('hi this is me, mr. meeseeks', add_special_tokens=True, max_length = 512)).unsqueeze(0)

out = model(b)
</code></pre>

<p>Out:</p>

<pre><code>(tensor([[[-0.1204, -0.0806, -0.0168,  ..., -0.0599, -0.1932, -0.0967],
          [-0.0851, -0.0980,  0.0039,  ..., -0.0563, -0.1655, -0.0156],
          [-0.1111, -0.0318,  0.0141,  ..., -0.0518, -0.1068, -0.1271],
          [-0.0317, -0.0441, -0.0306,  ..., -0.1049, -0.1940, -0.1919],
          [-0.1200,  0.0277, -0.0372,  ..., -0.0930, -0.0627,  0.0143],
          [-0.1204, -0.0806, -0.0168,  ..., -0.0599, -0.1932, -0.0967]]],
        grad_fn=&lt;NativeLayerNormBackward&gt;),
 tensor([[-9.7530e-02,  1.6328e-01,  9.3202e-03,  1.1010e-01,  7.3047e-02,
          -1.7635e-01,  1.0046e-01, -1.4826e-02,  9.2583e-
         ............
</code></pre>
",4,2,1514,2020-05-03 20:09:32,https://stackoverflow.com/questions/61580961/unable-to-load-spanbert-model-with-transformers-package
How much faster is training on an AWS GPU enabled instance compared to training on 8-core CPU laptop?,"<p>I'm working on fine-tuning a BERT model that takes about 1 hour 20 minutes per epoch.  I'm curious about how much more quickly I can expect this model to run after migrating to a g3s.xlarge.  Looking for a percentage time savings / very ballpark estimate of runtime. </p>
","python, amazon-web-services, amazon-ec2, deep-learning, bert-language-model","<p>if your workload is suitable and things are set up well then 4 to 5 times faster</p>

<p>see <a href=""https://ieeexplore.ieee.org/document/8751930"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/8751930</a></p>
",1,0,57,2020-05-07 13:47:22,https://stackoverflow.com/questions/61659422/how-much-faster-is-training-on-an-aws-gpu-enabled-instance-compared-to-training
Understanding the Hugging face transformers,"<p>I am new to the Transformers concept and I am going through some tutorials and writing my own code to understand the Squad 2.0 dataset Question Answering using the transformer models. In the hugging face website, I came across 2 different links</p>

<ul>
<li><a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a></li>
<li><a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/pretrained_models.html</a></li>
</ul>

<p>I want to know the difference between these 2 websites. Does one link have just a pre-trained model and the other have a pre-trained and fine-tuned model? </p>

<p>Now if I want to use, let's say an Albert Model For Question Answering and train with my Squad 2.0 training dataset on that and evaluate the model, to which of the link should I further?</p>
","pre-trained-model, huggingface-transformers, bert-language-model, nlp-question-answering, squad","<p>I would formulate it like this:
The second link basically describes ""community-accepted models"", i.e., models that serve as the basis for the implemented Huggingface classes, like BERT, RoBERTa, etc., and some related models that have a high aceptance or have been peer-reviewed.</p>

<p>This list has bin around much longer, whereas the list in the first link only recently got introduced directly on the Huggingface website, where the community can basically upload arbitrary checkpoints that are simply considered ""compatible"" with the library. Oftentimes, these are additional models trained by practitioners or other volunteers, and have a task-specific fine-tuning. Note that al models from <code>/pretrained_models.html</code> are also included in the <code>/models</code> interface as well.</p>

<p>If you have a very narrow usecase, you  might as well check and see if there was already some model that has been fine-tuned on your specific task. In the worst case, you'll simply end up with the base model anyways.</p>
",3,1,893,2020-05-13 12:40:10,https://stackoverflow.com/questions/61774933/understanding-the-hugging-face-transformers
Freezing of BERT layers while using tfhub module,"<p>In this link <a href=""https://medium.com/@prasad.pai/how-to-use-tensorflow-hub-with-code-examples-9100edec29af"" rel=""nofollow noreferrer"">click here</a> the author says that:</p>

<pre><code>import tensorflow_hub as hub
module = hub.Module(&lt;&lt;Module URL as string&gt;&gt;, trainable=True)
</code></pre>

<p>If user wishes to fine-tune/modify the weights of the model, this parameter has to be set as True.
So my doubt is if I set this to false does it mean that I am freezing all the layers of the BERT which is my intension too. I want to know if my approach is right.</p>
","tensorflow, bert-language-model, tensorflow-hub","<p>I have a multi-part answer for you.</p>

<h2>How to freeze a Module</h2>

<p>It all comes down to how your optimizer is set up. The usual approach for TF1 is to initialize it with all Variables found in the TRAINABLE_VARIABLES <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/GraphKeys"" rel=""nofollow noreferrer"">collection</a>. The <a href=""https://www.tensorflow.org/hub/api_docs/python/hub/Module"" rel=""nofollow noreferrer"">doc for hub.Module</a> says about <code>trainable</code>: ""If False, no variables are added to TRAINABLE_VARIABLES collection, ..."". So, <strong>yes</strong>, setting <code>trainable=False</code> (explicitly or by default) freezes the module in the standard usage of TF1.</p>

<h2>Why not to freeze BERT</h2>

<p>That said, BERT is meant to be fine-tuned. The <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> talks about the <em>feature-based</em> (i.e., frozen) vs <em>fine-tuning</em> approaches in more general terms, but the <a href=""https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1"" rel=""nofollow noreferrer"">module doc</a> spells it out clearly: <strong>""fine-tuning all parameters is the recommended practice.""</strong> This gives the final parts of computing the pooled output a better shot at adapting to the features that matter most for the task at hand.</p>

<p>If you intend to follow this advice, please also mind <a href=""https://www.tensorflow.org/hub/tf1_hub_module#fine-tuning"" rel=""nofollow noreferrer"">tensorflow.org/hub/tf1_hub_module#fine-tuning</a> and pick the correct graph version: BERT uses <a href=""https://arxiv.org/abs/1207.0580"" rel=""nofollow noreferrer"">dropout</a> regularization during training, and you need to set <code>hub.Module(..., tags={""train""})</code> to get that. But for inference (in evaluation and prediction), where dropout does nothing, you omit the <code>tags=</code> argument (or set it to the empty <code>set()</code> or to <code>None</code>).</p>

<h2>Outlook: TF2</h2>

<p>You asked about <code>hub.Module()</code>, which is an API for TF1, so I answered in that context. The same considerations apply for <a href=""https://tfhub.dev/google/collections/bert/1"" rel=""nofollow noreferrer"">BERT</a> in TF2 SavedModel format. There, it's all about setting <code>hub.KerasLayer(..., trainable=True)</code> or not, but the need to select a graph version has gone away (the layer picks up Keras' <code>training</code> state and applies it under the hood).</p>

<p>Happy training!</p>
",2,0,1497,2020-05-15 11:22:38,https://stackoverflow.com/questions/61818043/freezing-of-bert-layers-while-using-tfhub-module
Can you train a BERT model from scratch with task specific architecture?,"<p>BERT pre-training of the base-model is done by a language modeling approach, where we mask certain percent of tokens in a sentence, and we make the model learn those missing mask. Then, I think in order to do downstream tasks, we add a newly initialized layer and we fine-tune the model.</p>
<p>However, suppose we have a gigantic dataset for sentence classification. Theoretically, can we initialize the BERT base architecture from scratch, train both the additional downstream task specific layer + the base model weights form scratch with this sentence classification dataset only, and still achieve a good result?</p>
","machine-learning, nlp, bert-language-model","<p>BERT can be viewed as a language encoder, which is trained on a humongous amount of data to learn the language well. As we know, the original BERT model was trained on the entire English Wikipedia and Book corpus, which sums to <strong>3,300M</strong> words. BERT-base has 109M model parameters. So, if you think you have large enough data to train BERT, then the answer to your question is yes. </p>

<p>However, when you said ""still achieve a good result"", I assume you are comparing against the original BERT model. In that case, the answer lies in the size of the training data.</p>

<p>I am wondering why do you prefer to train BERT from scratch instead of fine-tuning it? Is it because you are afraid of the domain adaptation issue? If not, pre-trained BERT is perhaps a better starting point.</p>

<p>Please note, if you want to train BERT from scratch, you may consider a <strong>smaller</strong> architecture. You may find the following papers useful.</p>

<ul>
<li><a href=""https://arxiv.org/abs/1908.08962"" rel=""noreferrer"">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></li>
<li><a href=""https://arxiv.org/abs/1909.11942"" rel=""noreferrer"">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></li>
</ul>
",11,6,6800,2020-05-15 19:21:56,https://stackoverflow.com/questions/61826824/can-you-train-a-bert-model-from-scratch-with-task-specific-architecture
extract_features sentence embedding BERT,"<p>I'm using this code to get the embeddings of sentences that are in my dataset(I'm using my pretrained model). </p>

<pre><code>`python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=32`
</code></pre>

<p>But, there is a problem: there is a way to get embeddings faster? Because for 2000 sentences it took 6 hours. My dataset contains 20000 sentences; 60 hours would be too long for Colab.
Thanks.</p>
","python, google-colaboratory, embedding, bert-language-model","<p>I resolved it.
I wrote all of sentences in input.txt and after that I used this code:</p>

<pre><code>import jsonlines
df_emb=pd.DataFrame()
with jsonlines.open('/content/tmp/output.jsonl') as f:
    for line in f.iter():
        s=line['features'][0]['layers'][0]['values']
        df_tmp=pd.DataFrame(s).T
        df_emb=df_emb.append(df_tmp,ignore_index=True)
</code></pre>

<p>After that I saved the dataframe in csv file</p>
",0,-2,358,2020-05-18 08:26:09,https://stackoverflow.com/questions/61865427/extract-features-sentence-embedding-bert
"Deploying on heroku bert pytorch model using flask: ERROR: _pickle.UnpicklingError: invalid load key, &#39;v&#39;","<p>Trying to deploy bert model on Heroku. </p>

<pre><code>import torch
import transformers
import numpy as np
from flask import Flask, render_template, request
from model import DISTILBERTBaseUncased

MAX_LEN = 320
TOKENIZER = transformers.DistilBertTokenizer.from_pretrained(
    ""distilbert-base-uncased"", do_lower_case=True
)
DEVICE = ""cpu""
MODEL = DISTILBERTBaseUncased()
MODEL.load_state_dict(torch.load(""weight.bin""))
MODEL.to(DEVICE)
MODEL.eval()

app = Flask(__name__)


def sentence_prediction(sentence):
    tokenizer = TOKENIZER
    max_len = MAX_LEN
    comment = str(sentence)
    comment = "" "".join(comment.split())

    inputs = tokenizer.encode_plus(
        comment,
        None,
        add_special_tokens=True,
        max_length=max_len,
        pad_to_max_length=True,
    )

    ids = inputs[""input_ids""]
    mask = inputs[""attention_mask""]

    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)
    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)

    ids = ids.to(DEVICE, dtype=torch.long)
    mask = mask.to(DEVICE, dtype=torch.long)

    outputs = MODEL(ids=ids, mask=mask)

    outputs = torch.sigmoid(outputs).cpu().detach().numpy()
    return outputs[0][0]


@app.route(""/"")
def index_page():
    return render_template(""index.html"")


@app.route(""/model"")
def models():
    return render_template(""model.html"")


@app.route(""/predict"", methods=[""POST"", ""GET""])
def predict():
    if request.method == ""POST"":
        sentence = request.form.get(""text"")
        Toxic_prediction = sentence_prediction(sentence)
        return render_template(
            ""index.html"", prediction_text=np.round((Toxic_prediction * 100), 2)
        )
    return render_template(""index.html"", prediction_text="""")


if __name__ == ""__main__"":
    app.run(debug=True)
</code></pre>

<p><strong>ERROR</strong></p>

<p>MODEL.load_state_dict(torch.load(""weight.bin""))</p>

<p>2020-05-18T06:32:32.134536+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.7/site-packages/torch/serialization.py"", line 593, in load</p>

<p>2020-05-18T06:32:32.134536+00:00 app[web.1]: return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)</p>

<p>2020-05-18T06:32:32.134536+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.7/site-packages/torch/serialization.py"", line 763, in _legacy_load</p>

<p>2020-05-18T06:32:32.134537+00:00 app[web.1]: magic_number = pickle_module.load(f, **pickle_load_args)</p>

<p>2020-05-18T06:32:32.134537+00:00 app[web.1]: _pickle.UnpicklingError: invalid load key, 'v'.</p>

<ol>
<li>codes are working fine locally. </li>
<li>The Heroku deployment method is Github</li>
<li>weight.bin size is 255 MB</li>
<li>flask API working fine in localhost</li>
</ol>
","python, git, flask, heroku, bert-language-model","<p>Checking Error
1.MODEL.load_state_dict(torch.load(""weight.bin"")) --> You should just use below or checking letter properly. </p>

<pre><code>model.load_state_dict(torch.load(model_state_dict))
</code></pre>

<p>2._pickle.UnpicklingError: invalid load key, 'v'. 
--> I think <strong>git-lfs</strong> is not installed in your environment. after install it, just try again. </p>
",3,0,1644,2020-05-19 06:03:17,https://stackoverflow.com/questions/61884256/deploying-on-heroku-bert-pytorch-model-using-flask-error-pickle-unpicklingerr
Cannot load German BERT model in spaCy,"<p>Here is my problem: I am working on the German text classification project. I use spacy for that and decided to fine-tune its pretrained BERT model to get better results. However, when I try to load it to the code, it shows me errors.</p>

<p>Here is what I've done:</p>

<ol>
<li>Installed spacy-transformers: <code>pip install spacy-transformers</code></li>
<li>Downloaded German BERT model: <code>python -m spacy download de_trf_bertbasecased_lg</code>. It was downloaded successfully and showed me: <code>✔ Download and installation successful
You can now load the model via spacy.load('de_trf_bertbasecased_lg')</code></li>
<li>Wrote the following code:</li>
</ol>

<p><code>import spacy
 nlp = spacy.load('de_trf_bertbasecased_lg')</code></p>

<p>And the output was:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#1&gt;"", line 1, in &lt;module&gt;
nlp = spacy.load('de_trf_bertbasecased_lg')
  File ""C:\Python\Python37\lib\site-packages\spacy\__init__.py"", line 30, in load
return util.load_model(name, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 164, in load_model
return load_model_from_package(name, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 185, in load_model_from_package
return cls.load(**overrides)
  File ""C:\Python\Python37\lib\site-packages\de_trf_bertbasecased_lg\__init__.py"", line 12, in load
return load_model_from_init_py(__file__, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 228, in load_model_from_init_py
return load_model_from_path(data_path, meta, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 196, in load_model_from_path
cls = get_lang_class(lang)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 70, in get_lang_class
if lang in registry.languages:
  File ""C:\Python\Python37\lib\site-packages\catalogue.py"", line 56, in __contains__
has_entry_point = self.entry_points and self.get_entry_point(name)
  File ""C:\Python\Python37\lib\site-packages\catalogue.py"", line 140, in get_entry_point
return entry_point.load()
  File ""C:\Python\Python37\lib\site-packages\importlib_metadata\__init__.py"", line 94, in load
module = import_module(match.group('module'))
  File ""C:\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1006, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 728, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\__init__.py"", line 1, in &lt;module&gt;
from .language import TransformersLanguage
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\language.py"", line 5, in &lt;module&gt;
from .util import is_special_token, pkg_meta, ATTRS, PIPES, LANG_FACTORY
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\util.py"", line 2, in &lt;module&gt;
import transformers
  File ""C:\Python\Python37\lib\site-packages\transformers\__init__.py"", line 20, in &lt;module&gt;
from .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,
  File ""C:\Python\Python37\lib\site-packages\transformers\file_utils.py"", line 37, in &lt;module&gt;
import torch
  File ""C:\Python\Python37\lib\site-packages\torch\__init__.py"", line 81, in &lt;module&gt;
ctypes.CDLL(dll)
  File ""C:\Python\Python37\lib\ctypes\__init__.py"", line 356, in __init__
self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
</code></pre>

<p>If I run the same code in PyCharm, it also shows me these two lines before all of those above:</p>

<pre><code>2020-05-19 18:00:55.132721: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not 
load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-05-19 18:00:55.132990: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart 
dlerror if you do not have a GPU set up on your machine.
</code></pre>

<p>If I got it right, these two lines complain that I don't have a GPU. However, according to the docs, I should be able to use BERT even without GPU.</p>

<p>So I am really stuck right now and looking for your help. </p>

<p>I should also mention, that I used <code>de_core_news_sm</code> model before and it worked fine.</p>

<p>I have also already tried several solutions, but none of them worked. I tried:
<a href=""https://www.kaggle.com/questions-and-answers/103976"" rel=""nofollow noreferrer"">this</a> and <a href=""https://stackoverflow.com/questions/1940578/windowserror-error-126-the-specified-module-could-not-be-found"">this</a>. I have also tried to uninstall all <code>spacy</code>-related libraries and installed them again. Didn't help either.</p>

<p>I am working with:</p>

<blockquote>
  <p>Windows 10 Home</p>
  
  <p>Python: 3.7.2</p>
  
  <p>Spacy: 2.2.4</p>
  
  <p>Spacy-transformers: 0.5.1</p>
</blockquote>

<p>Would appreciate any help or advice!</p>
","python, spacy, transformer-model, bert-language-model","<p>It's probably a problem with your installation of <code>torch</code>. Start in a clean virtual environment and install <code>torch</code> using the instructions here with CUDA as None: <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a>. Then install <code>spacy-transformers</code> with <code>pip</code>.</p>
",2,0,835,2020-05-19 19:24:36,https://stackoverflow.com/questions/61899118/cannot-load-german-bert-model-in-spacy
Cased VS uncased BERT models in spacy and train data,"<p>I want to use <code>spacy</code>'s pretrained BERT model for text classification but I'm a little confused about <code>cased/uncased</code> models. I read somewhere that <code>cased</code> models should only be used when there is a chance that letter casing will be helpful for the task. In my specific case: I am working with German texts. And in German all nouns start with the capital letter. So, I think, (correct me if I'm wrong) that this is the exact situation where <code>cased</code> model must be used. (There is also no <code>uncased</code> model available for German in <code>spacy</code>). </p>

<p>But what must be done with data in this situation?
Should I (while preprocessing train data) leave it as it is (by that I mean not using the <code>.lower()</code> function) or it doesn't make any difference?</p>
","python, spacy, bert-language-model","<p>As a non-German-speaker, your comment about nouns being uppercase does make it seem like case is more relevant for German than it might be for English, but that doesn't obviously mean that a cased model will give better performance on all tasks.</p>

<p>For something like part-of-speech detection, case would probably be enormously helpful for the reason you describe, but for something like sentiment analysis, it's less clear whether the added complexity of having a much larger vocabulary is worth the benefits. (As a human, you could probably imagine doing sentiment analysis with all lowercase text just as easily.)</p>

<p>Given that the only model available is the cased version, I would just go with that - I'm sure it will still be one of the best pretrained German models you can get your hands on. Cased models have separate vocab entries for differently-cased words (e.g. in english <code>the</code> and <code>The</code> will be different tokens). So yes, during preprocessing you wouldn't want to remove that information by calling <code>.lower()</code>, just leave the casing as-is.</p>
",22,26,27939,2020-05-19 23:20:00,https://stackoverflow.com/questions/61902426/cased-vs-uncased-bert-models-in-spacy-and-train-data
Spacy&#39;s BERT model doesn&#39;t learn,"<p>I've been trying to use <code>spaCy</code>'s pretrained BERT model <code>de_trf_bertbasecased_lg</code> to increase accuracy in my classification project. I used to build a model from scratch using <code>de_core_news_sm</code> and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in <a href=""https://explosion.ai/blog/spacy-transformers"" rel=""nofollow noreferrer"">this article</a> as an example.</p>

<p>Here is my code:</p>

<pre><code>import spacy
from spacy.util import minibatch
from random import shuffle

spacy.require_gpu()
nlp = spacy.load('de_trf_bertbasecased_lg')

data = get_data()  # get_data() function returns a list with train data (I'll explain later how it looks)

textcat = nlp.create_pipe(""trf_textcat"", config={""exclusive_classes"": False})

for category in categories:  # categories - a list of 21 different categories used for classification
    textcat.add_label(category)
nlp.add_pipe(textcat)

num = 0  # number used for counting batches
optimizer = nlp.resume_training()
for i in range(2):
    shuffle(data)
    losses = {}
    for batch in minibatch(data):
        texts, cats = zip(*batch)
        nlp.update(texts, cats, sgd=optimizer, losses=losses)
        num += 1

        if num % 10000 == 0:  # test model's performance every 10000 batches
            acc = test(nlp)  # function test() will be explained later
            print(f'Accuracy: {acc}')

nlp.to_disk('model/')
</code></pre>

<p>Function <code>get_data()</code> opens files with different categories, creates a tuple like this one <code>(text, {'cats' : {'category1': 0, 'category2':1, ...}})</code>, gathers all these tuples into one array, which is then being returned to the main function.</p>

<p>Function <code>test(nlp)</code> opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.</p>

<p>Again, everything worked just fine with <code>de_core_news_sm</code>, so I'm pretty sure that functions <code>get_data()</code> and <code>test(nlp)</code> are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.</p>

<p>Thanks in advance for any help!</p>

<p><strong>UPDATE</strong></p>

<p>Trying to understand the above problem I decided to try the model with only a few examples (just like it is advised <a href=""https://github.com/explosion/spacy-transformers/issues/144"" rel=""nofollow noreferrer"">here</a>). Here is the code:</p>

<pre><code>import spacy
from spacy.util import minibatch
import random
import torch

train_data = [
    (""It is realy cool"", {""cats"": {""POSITIVE"": 1.0, ""NEGATIVE"": 0.0}}),
    (""I hate it"", {""cats"": {""POSITIVE"": 0.0, ""NEGATIVE"": 1.0}})
]

is_using_gpu = spacy.prefer_gpu()
if is_using_gpu:
    torch.set_default_tensor_type(""torch.cuda.FloatTensor"")

nlp = spacy.load(""en_trf_bertbaseuncased_lg"")
textcat = nlp.create_pipe(""trf_textcat"", config={""exclusive_classes"": True})
for label in (""POSITIVE"", ""NEGATIVE""):
    textcat.add_label(label)
nlp.add_pipe(textcat)

optimizer = nlp.resume_training()
for i in range(10):
    random.shuffle(train_data)
    losses = {}
    for batch in minibatch(train_data):
        texts, cats = zip(*batch)
        nlp.update(texts, cats, sgd=optimizer, losses=losses)
    print(i, losses)
print()

test_data = [
    ""It is really cool"",
    ""I hate it"",
    ""Great!"",
    ""I do not think this is cool""
]

for line in test_data:
    print(line)
    print(nlp(line).cats)
</code></pre>

<p>And the output was:</p>

<pre><code>0 {'trf_textcat': 0.125}
1 {'trf_textcat': 0.12423406541347504}
2 {'trf_textcat': 0.12188033014535904}
3 {'trf_textcat': 0.12363225221633911}
4 {'trf_textcat': 0.11996611207723618}
5 {'trf_textcat': 0.14696261286735535}
6 {'trf_textcat': 0.12320466339588165}
7 {'trf_textcat': 0.12096124142408371}
8 {'trf_textcat': 0.15916231274604797}
9 {'trf_textcat': 0.1238454058766365}

It is really cool
{'POSITIVE': 0.47827497124671936, 'NEGATIVE': 0.5217249989509583}
I hate it
{'POSITIVE': 0.47827598452568054, 'NEGATIVE': 0.5217240452766418}
Great!
{'POSITIVE': 0.4782750606536865, 'NEGATIVE': 0.5217249393463135}
I do not think this is cool
{'POSITIVE': 0.478275328874588, 'NEGATIVE': 0.5217246413230896}
</code></pre>

<p>Not only the model performs bad, the loss is not getting smaller and scores for all the test sentences are almost the same. And most importantly: it didn't even get those questions correct, that happened to be in the train data. So my question is: does the model even learn? And what am I doing wrong?</p>

<p>Any thoughts?</p>
","python, spacy, text-classification, multiclass-classification, bert-language-model","<p>Received an answer to my question on <a href=""https://github.com/explosion/spacy-transformers/issues/180"" rel=""nofollow noreferrer"">GitHub</a> and it looks like there must be some optimizer parameters specified, just like in <a href=""https://github.com/explosion/spacy-transformers/blob/v0.6.x/examples/train_textcat.py"" rel=""nofollow noreferrer"">this example</a>.</p>
",1,2,944,2020-05-21 20:35:55,https://stackoverflow.com/questions/61943409/spacys-bert-model-doesnt-learn
huggingface bert showing poor accuracy / f1 score [pytorch],"<p>I am trying <code>BertForSequenceClassification</code> for a simple article classification task.</p>

<p>No matter how I train it (freeze all layers but the classification layer, all layers trainable, last <code>k</code> layers trainable), I always get an almost randomized accuracy score. My model doesn't go above 24-26% training accuracy (I only have 5 classes in my dataset).</p>

<p>I'm not sure what did I do wrong while designing/training the model. I tried the model with multiple datasets, every time it gives the same random baseline accuracy.</p>

<p>Dataset I used: BBC Articles (5 classes) </p>

<p><a href=""https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc"" rel=""noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc</a></p>

<blockquote>
  <p>Consists of 2225 documents from the BBC news website corresponding to
  stories in five topical areas from 2004-2005. Natural Classes: 5
  (business, entertainment, politics, sport, tech)</p>
</blockquote>

<p>I added the model part and the training part which are the most important portion (to avoid any irrelevant details). I added the full source-code + data too if that's useful for reproducibility.</p>

<p>My guess is there is something wrong with the I way I designed the network or the way I'm passing the attention_masks/ labels to the model. Also, the token length 512 should not be a problem as most of the texts has length &lt; 512 (the mean length is &lt; 300).</p>

<p><strong>Model code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

class BertClassifier(nn.Module):
    def __init__(self):
        super(BertClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)
        # as we have 5 classes

        # we want our output as probability so, in the evaluation mode, we'll pass the logits to a softmax layer
        self.softmax = torch.nn.Softmax(dim = 1) # last dimension
    def forward(self, x, attn_mask = None, labels = None):

        if self.training == True:
            # print(x.shape)
            loss = self.bert(x, attention_mask = attn_mask, labels = labels)
            # print(x[0].shape)

            return loss

        if self.training == False: # in evaluation mode
            x = self.bert(x)
            x = self.softmax(x[0])

            return x
    def freeze_layers(self, last_trainable = 1): 
        # we freeze all the layers except the last classification layer + few transformer blocks
        for layer in list(self.bert.parameters())[:-last_trainable]:
            layer.requires_grad = False


# create our model

bertclassifier = BertClassifier()
</code></pre>

<p><strong>Training code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # cuda for gpu acceleration

# optimizer

optimizer = torch.optim.Adam(bertclassifier.parameters(), lr=0.001)


epochs = 15

bertclassifier.to(device) # taking the model to GPU if possible

# metrics

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

train_losses = []

train_metrics = {'acc': [], 'f1': []}
test_metrics = {'acc': [], 'f1': []}

# progress bar

from tqdm import tqdm_notebook

for e in tqdm_notebook(range(epochs)):
    train_loss = 0.0
    train_acc = 0.0
    train_f1 = 0.0
    batch_cnt = 0

    bertclassifier.train()

    print(f'epoch: {e+1}')

    for i_batch, (X, X_mask, y) in tqdm_notebook(enumerate(bbc_dataloader_train)):
        X = X.to(device)
        X_mask = X_mask.to(device)
        y = y.to(device)


        optimizer.zero_grad()

        loss, y_pred = bertclassifier(X, X_mask, y)

        train_loss += loss.item()
        loss.backward()
        optimizer.step()

        y_pred = torch.argmax(y_pred, dim = -1)

        # update metrics
        train_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
        train_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
        batch_cnt += 1

    print(f'train loss: {train_loss/batch_cnt}')
    train_losses.append(train_loss/batch_cnt)
    train_metrics['acc'].append(train_acc/batch_cnt)
    train_metrics['f1'].append(train_f1/batch_cnt)


    test_loss = 0.0
    test_acc = 0.0
    test_f1 = 0.0
    batch_cnt = 0

    bertclassifier.eval()
    with torch.no_grad():
        for i_batch, (X, y) in enumerate(bbc_dataloader_test):
            X = X.to(device)
            y = y.to(device)

            y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


            y_pred = torch.argmax(y_pred, dim = -1)

            # update metrics
            test_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
            test_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
            batch_cnt += 1

    test_metrics['acc'].append(test_acc/batch_cnt)
    test_metrics['f1'].append(test_f1/batch_cnt)
</code></pre>

<p>Full source-code with the dataset is available here: <a href=""https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb"" rel=""noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb</a></p>

<p>Update:</p>

<p>After observing the prediction, it seems model almost always predicts 0:</p>

<pre><code>bertclassifier.eval()
with torch.no_grad():
    for i_batch, (X, y) in enumerate(bbc_dataloader_test):
        X = X.to(device)
        y = y.to(device)

        y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


        y_pred = torch.argmax(y_pred, dim = -1)

        print(y)
        print(y_pred)
        print('--------------------')
</code></pre>

<pre><code>tensor([4, 2, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 0, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 1, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 1, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 4, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
...
...
</code></pre>

<p>Actually, the model is always predicting the same output <code>[0.2270, 0.1855, 0.2131, 0.1877, 0.1867]</code> for any input, it's like it didn't learn anything at all.</p>

<p>It's weird because my dataset is not imbalanced.</p>

<pre><code>Counter({'politics': 417,
         'business': 510,
         'entertainment': 386,
         'tech': 401,
         'sport': 511})
</code></pre>
","pytorch, huggingface-transformers, bert-language-model","<p>After some digging I found out, the main culprit was the learning rate, for fine-tuning bert <code>0.001</code> is extremely high. When I reduced my learning rate from <code>0.001</code> to <code>1e-5</code>, both my training and test accuracy reached 95%.</p>

<blockquote>
  <p>When BERT is fine-tuned, all layers are trained - this is quite different from fine-tuning in a lot of other ML models, but it matches what was described in the paper and works quite well (as long as you only fine-tune for a few epochs - it's very easy to overfit if you fine-tune the whole model for a long time on a small amount of data!)</p>
</blockquote>

<p>src: <a href=""https://github.com/huggingface/transformers/issues/587"" rel=""noreferrer"">https://github.com/huggingface/transformers/issues/587</a></p>

<blockquote>
  <p>Best result is found when all the layers are trained with a really small learning rate.</p>
</blockquote>

<p>src: <a href=""https://github.com/uzaymacar/comparatively-finetuning-bert"" rel=""noreferrer"">https://github.com/uzaymacar/comparatively-finetuning-bert</a></p>
",9,6,9510,2020-05-23 09:11:11,https://stackoverflow.com/questions/61969783/huggingface-bert-showing-poor-accuracy-f1-score-pytorch
Unable to pip install -U sentence-transformers,"<p>I am unable to do: pip install -U sentence-transformers. I get this message on Anaconda Prompt:
ERROR: Could not find a version that satisfies the requirement torch>=1.0.1 (from sentence-transformers) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch>=1.0.1 (from sentence-transformers)
Can someone help?</p>
","transformer-model, sentence, bert-language-model","<p>I tried to Conda Install pytorch and then installed Sentence Transformer by doing these steps: </p>

<ol>
<li><p>conda install pytorch torchvision cudatoolkit=10.0 -c pytorch</p></li>
<li><p>pip install -U sentence-transformers</p></li>
</ol>

<p>This worked. 
Thanks</p>
",12,5,45381,2020-05-25 00:14:02,https://stackoverflow.com/questions/61994001/unable-to-pip-install-u-sentence-transformers
Why we need the init_weight function in BERT pretrained model in Huggingface Transformers?,"<p>In the code by Hugginface transformers, there are many fine-tuning models have the function <code>init_weight</code>. 
For example(<a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L1073-L1082"" rel=""noreferrer"">here</a>), there is a <code>init_weight</code> function at last.</p>

<pre class=""lang-py prettyprint-override""><code>class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

</code></pre>

<p>As I know, it will call the following <a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L520-L530"" rel=""noreferrer"">code</a></p>

<pre class=""lang-py prettyprint-override""><code>def _init_weights(self, module):
    """""" Initialize the weights """"""
    if isinstance(module, (nn.Linear, nn.Embedding)):
        # Slightly different from the TF version which uses truncated_normal for initialization
        # cf https://github.com/pytorch/pytorch/pull/5617
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, BertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if isinstance(module, nn.Linear) and module.bias is not None:
        module.bias.data.zero_()
</code></pre>

<p>My question is <strong>If we are loading the pre-trained model, why do we need to initialize the weight for every module?</strong></p>

<p>I guess I must be misunderstanding something here.</p>
","python, huggingface-transformers, bert-language-model","<p>Have a look at the code for <a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac824c9027385b149f405e4f5649273f/src/transformers/modeling_utils.py#L490"" rel=""noreferrer""><code>.from_pretrained()</code></a>. What actually happens is something like this:</p>
<ul>
<li>find the correct base model class to initialise</li>
<li>initialise that class with pseudo-random initialisation (by using the <code>_init_weights</code> function that you mention)</li>
<li>find the file with the pretrained weights</li>
<li>overwrite the weights of the model that we just created with the pretrained weights where applicable</li>
</ul>
<p>This ensure that the layers that were not pretrained (e.g. in some cases the final classification layer) <em>do</em> get initialised in <code>_init_weights</code> but don't get overridden.</p>
",14,6,10848,2020-05-27 09:54:16,https://stackoverflow.com/questions/62040309/why-we-need-the-init-weight-function-in-bert-pretrained-model-in-huggingface-tra
How to do language model training on BERT,"<p>I want to train BERT on a target corpus. I am looking at this <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling#robertabertdistilbert-and-masked-language-modeling"" rel=""nofollow noreferrer"">HuggingFace implementation</a>.
They are using .raw files for the training data. If I have .txt files of my training data, how can I use their implementation?</p>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>The <code>.raw</code> only indicates that they use the raw version of the WikiText, they are regular text files containing the raw text:</p>

<blockquote>
  <p>We're using the raw WikiText-2 (no tokens were replaced before the tokenization).</p>
</blockquote>

<p>The description of the data files options also says that they are text files. From <a href=""https://github.com/huggingface/transformers/blob/5e737018e1fcb22c8b76052058279552a8d6c806/examples/language-modeling/run_language_modeling.py#L86-L88"" rel=""nofollow noreferrer"">run_language_modeling.py - L86-L88</a>:</p>

<pre class=""lang-py prettyprint-override""><code>train_data_file: Optional[str] = field(
    default=None, metadata={""help"": ""The input training data file (a text file).""}
)
</code></pre>

<p>Therefore you can just specify your text files.</p>
",1,0,438,2020-05-28 19:01:44,https://stackoverflow.com/questions/62072536/how-to-do-language-model-training-on-bert
How to get embedding from bert finetuned model?,"<p>I have finedtuned 'bert-base-uncased' model using transformer and torch which gave me pytorch_model.bin, vocab.txt and other files as output.
After loading the model how to I get embedding for complete vocab, like a matrix which maps every word to its embedding vector </p>
","pytorch, transformer-model, bert-language-model","<p>I recommend to use <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface</a>, they make it very easy to use and train all Transformer model variants.</p>

<p>To get the embedding from a BERT fine-tuned model, you could use <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.get_input_embeddings"" rel=""nofollow noreferrer"">BertModel.set_input_embeddings()</a>.</p>
",0,1,1364,2020-05-29 13:15:33,https://stackoverflow.com/questions/62086878/how-to-get-embedding-from-bert-finetuned-model
"RuntimeError: Given groups=3, weight of size 12 64 3 768, expected input[32, 12, 30, 768] to have 192 channels, but got 12 channels instead","<p>I started working with Pytorch recently so my understanding of it isn't quite strong. I previously had a 1 layer CNN but wanted to extend it to 2 layers, but the input and output channels have been throwing errors I can seem to decipher. Why does it expect 192 channels? Can someone give me a pointer to help me understand this better? I have seen several related problems on here, but I don't understand those solutions either. </p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from transformers import BertConfig, BertModel, BertTokenizer
import math
from transformers import AdamW, get_linear_schedule_with_warmup


def pad_sents(sents, pad_token):  # Pad list of sentences according to the longest sentence in the batch.
    sents_padded = []
    max_len = max(len(s) for s in sents)
    for s in sents:
        padded = [pad_token] * max_len
        padded[:len(s)] = s
        sents_padded.append(padded)
    return sents_padded


def sents_to_tensor(tokenizer, sents, device):
    tokens_list = [tokenizer.tokenize(str(sent)) for sent in sents]
    sents_lengths = [len(tokens) for tokens in tokens_list]
    tokens_list_padded = pad_sents(tokens_list, '[PAD]')
    sents_lengths = torch.tensor(sents_lengths, device=device)
    masks = []
    for tokens in tokens_list_padded:
        mask = [0 if token == '[PAD]' else 1 for token in tokens]
        masks.append(mask)
    masks_tensor = torch.tensor(masks, dtype=torch.long, device=device)
    tokens_id_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_list_padded]
    sents_tensor = torch.tensor(tokens_id_list, dtype=torch.long, device=device)

    return sents_tensor, masks_tensor, sents_lengths


class ConvModel(nn.Module):

    def __init__(self, device, dropout_rate, n_class, out_channel=16):
        super(ConvModel, self).__init__()
        self.bert_config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
        self.dropout_rate = dropout_rate
        self.n_class = n_class
        self.out_channel = out_channel
        self.bert = BertModel.from_pretrained('bert-base-uncased', config=self.bert_config)
        self.out_channels = self.bert.config.num_hidden_layers * self.out_channel
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', config=self.bert_config)
        self.conv = nn.Conv2d(in_channels=self.bert.config.num_hidden_layers,
                              out_channels=self.out_channels,
                              kernel_size=(3, self.bert.config.hidden_size),
                              groups=self.bert.config.num_hidden_layers)
        self.conv1 = nn.Conv2d(in_channels=self.out_channels,
                               out_channels=48,
                               kernel_size=(3, self.bert.config.hidden_size),
                               groups=self.bert.config.num_hidden_layers)
        self.hidden_to_softmax = nn.Linear(self.out_channels, self.n_class, bias=True)
        self.dropout = nn.Dropout(p=self.dropout_rate)
        self.device = device

    def forward(self, sents):
        sents_tensor, masks_tensor, sents_lengths = sents_to_tensor(self.tokenizer, sents, self.device)
        encoded_layers = self.bert(input_ids=sents_tensor, attention_mask=masks_tensor)
        hidden_encoded_layer = encoded_layers[2]
        hidden_encoded_layer = hidden_encoded_layer[0]
        hidden_encoded_layer = torch.unsqueeze(hidden_encoded_layer, dim=1)
        hidden_encoded_layer = hidden_encoded_layer.repeat(1, 12, 1, 1)
        conv_out = self.conv(hidden_encoded_layer)  # (batch_size, channel_out, some_length, 1)
        conv_out = self.conv1(conv_out)
        conv_out = torch.squeeze(conv_out, dim=3)  # (batch_size, channel_out, some_length)
        conv_out, _ = torch.max(conv_out, dim=2)  # (batch_size, channel_out)
        pre_softmax = self.hidden_to_softmax(conv_out)

        return pre_softmax


def batch_iter(data, batch_size, shuffle=False, bert=None):
    batch_num = math.ceil(data.shape[0] / batch_size)
    index_array = list(range(data.shape[0]))
    if shuffle:
        data = data.sample(frac=1)
    for i in range(batch_num):
        indices = index_array[i * batch_size: (i + 1) * batch_size]
        examples = data.iloc[indices]
        sents = list(examples.train_BERT_tweet)
        targets = list(examples.train_label.values)
        yield sents, targets  # list[list[str]] if not bert else list[str], list[int]


def train():
    label_name = ['Yes', 'Maybe', 'No']
    device = torch.device(""cpu"")

    df_train = pd.read_csv('trainn.csv')  # , index_col=0)
    train_label = dict(df_train.train_label.value_counts())
    label_max = float(max(train_label.values()))
    train_label_weight = torch.tensor([label_max / train_label[i] for i in range(len(train_label))], device=device)
    model = ConvModel(device=device, dropout_rate=0.2, n_class=len(label_name))
    optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=False)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=1000)  # changed the last 2 arguments to old ones
    model = model.to(device)
    model.train()
    cn_loss = torch.nn.CrossEntropyLoss(weight=train_label_weight, reduction='mean')
    train_batch_size = 16

    for epoch in range(1):

        for sents, targets in batch_iter(df_train, batch_size=train_batch_size, shuffle=True):  # for each epoch
            optimizer.zero_grad()
            pre_softmax = model(sents)
            loss = cn_loss(pre_softmax, torch.tensor(targets, dtype=torch.long, device=device))
            loss.backward()
            optimizer.step()
            scheduler.step()
TrainingModel = train()

</code></pre>

<p>Here's a snippet of data <a href=""https://github.com/Kosisochi/DataSnippet"" rel=""nofollow noreferrer"">https://github.com/Kosisochi/DataSnippet</a></p>
","python-3.x, pytorch, conv-neural-network, bert-language-model","<p>It seems that the original version of the code you had in this question behaved differently. The final version of the code you have here gives me a different error from what you posted, more specifically - this:</p>

<pre><code>RuntimeError: Calculated padded input size per channel: (20 x 1). Kernel size: (3 x 768). Kernel size can't be greater than actual input size
</code></pre>

<p>I apologize if I misunderstood the situation, but it seems to me that your understanding of what exactly nn.Conv2d layer does is not 100% clear and that is the main source of your struggle. I interpret the part ""detailed explanation on 2 layer CNN in Pytorch"" you requested as an ask to explain in detail on how that layer works and I hope that after this is done there will be no problem applying it 1 time, 2 times or more.</p>

<p>You can find all the documentation about the layer <a href=""https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">here</a>, but let me give you a recap which hopefully will help to understand more the errors you're getting.
First of all <code>nn.Conv2d</code> inputs are 4-d tensors of the shape <code>(BatchSize, ChannelsIn, Height, Width)</code> and outputs are 4-d tensors of the shape <code>(BatchSize, ChannelsOut, HeightOut, WidthOut)</code>. The simplest way to think about <code>nn.Conv2d</code> is of something applied to 2d images with pixel grid of size <code>Height x Width</code> and having <code>ChannelsIn</code> different colors or features per pixel. Even if your inputs have nothing to do with actual images the behavior of the layer is still the same. Simplest situation is when the <code>nn.Conv2d</code> is not using padding (as in your code). In that case the <code>kernel_size=(kernel_height, kernel_width)</code> argument specifies the rectangle which you can imagine sweeping through <code>Height x Width</code> rectangle of your inputs and producing one pixel for each valid position. Without padding the coordinate of the rectangle's point can be any pair of indicies <code>(x, y)</code> with x between <code>0</code> and <code>Height - kernel_height</code> and y between <code>0</code> and <code>Width - kernel_width</code>. Thus the output will look like a 2d image of size <code>(Height - kernel_height + 1) x (Width - kernel_width + 1)</code> and will have as many output channels as specified to <code>nn.Conv2d</code> constructor, so the output tensor will be of shape <code>(BatchSize, ChannelsOut, Height - kernel_height + 1, Width - kernel_width + 1)</code>.</p>

<p>The parameter <code>groups</code> is not affecting how shapes are changed by the layer - it is only controlling which input channels are used as inputs for the output channels (<code>groups=1</code> means that every input channel is used as input for every output channel, otherwise input and output channels are divided into corresponding number of groups and only input channels from group <code>i</code> are used as inputs for the output channels from group <code>i</code>).</p>

<p>Now in your current version of the code you have BatchSize = 16 and the output of pre-trained model is <code>(BatchSize, DynamicSize, 768)</code> with <code>DynamicSize</code> depending on the input, e.g. 22. You then introduce additional dimension as axis 1 with <code>unsqueeze</code> and repeat the values along that dimension transforming the tensor of shape <code>(16, 22, 768)</code> into <code>(16, 12, 22, 768)</code>. Effectively you are using the output of the pre-trained model as 12-channel (with each channel having same values as others) 2-d images here of size <code>(22, 768)</code>, where 22 is not fixed (depends on the batch). Then you apply a nn.Conv2d with kernel size <code>(3, 768)</code> - which means that there is no ""wiggle room"" for width and output 2-d images will be of size <code>(20, 1)</code> and since your layer has 192 channels final size of the output of first convolution layer has shape <code>(16, 192, 20, 1)</code>. Then you try to apply second layer of convolution on top of that with kernel size <code>(3, 768)</code> again, but since your 2-d ""image"" is now just (20 x 1) there is no valid position to fit <code>(3, 768)</code> kernel rectangle inside a rectangle <code>(20 x 1)</code> which leads to the error message <code>Kernel size can't be greater than actual input size</code>.</p>

<p>Hope this explanation helps. Now to the choices you have to avoid the issue:</p>

<ul>
<li>(a) is to add padding in such a way that the size of the output is not changing  comparing to input (I won't go into details here,
because I don't think this is what you need)</li>
<li>(b) Use smaller kernel on both first and/or second convolutions (e.g. if you don't change first convolution the only valid width for
the second kernel would be <code>1</code>).</li>
<li>(c) Looking at what you're trying to do my guess is that you actually don't want to use 2d convolution, you want 1d convolution (on the sequence) with every position described by 768 values. When you're using one convolution layer with 768 width kernel (and same 768 width input) you're effectively doing exactly same thing as 1d convolution with 768 input channels, but then if you try to apply second one you have a problem. You can specify kernel width as <code>1</code> for the next layer(s) and that will work for you, but a more correct way would be to transpose pre-trained model's output tensor by switching the last dimensions - getting shape <code>(16, 768, DynamicSize)</code> from <code>(16, DynamicSize, 768)</code> and then apply nn.Conv1d layer with 768 input channels and arbitrary <code>ChannelsOut</code> as output channels and 1d <code>kernel_size=3</code> (meaning you look at 3 consecutive elements of the sequence for convolution). If you do that than without padding input shape of <code>(16, 768, DynamicSize)</code> will become <code>(16, ChannelsOut, DynamicSize-2)</code>, and after you apply second Conv1d with e.g. the same settings as first one you'll get a tensor of shape <code>(16, ChannelsOut, DynamicSize-4)</code>, etc. (each time the 1d length will shrink by <code>kernel_size-1</code>). You can always change number of channels/kernel_size for each subsequent convolution layer too.</li>
</ul>
",3,2,1821,2020-05-30 08:05:31,https://stackoverflow.com/questions/62099558/runtimeerror-given-groups-3-weight-of-size-12-64-3-768-expected-input32-12
Why does the BERT NSP head linear layer have two outputs?,"<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
","nlp, pytorch, transformer-model, huggingface-transformers, bert-language-model","<p>The two scores are meant to represent unnormalized probabilities (<code>logits</code>) from the model. If we softmax them, we get our predictions, where index 0 indicates next sentence, and index 1 indicates random.</p>

<p>This is just a stylistic choice on the HuggingFace author's behalf, probably to keep the loss function consistent.</p>

<p>Here's the <code>forward</code> method of <code>BertForPretraining</code>, where <code>self.cls</code> is <code>BertOnlyNSPHead</code>:</p>

<pre><code>    prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)

    outputs = (prediction_scores, seq_relationship_score,) + outputs[
        2:
    ]  # add hidden states and attention if they are here

    if masked_lm_labels is not None and next_sentence_label is not None:
        loss_fct = CrossEntropyLoss()
        masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))
        next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
        total_loss = masked_lm_loss + next_sentence_loss
        outputs = (total_loss,) + outputs
</code></pre>

<p>it's convenient to use the same CrossEntropyLoss for both MLM and NSP.</p>

<p>As you describe, it would be equivalent to have NSP produce a single output, then feed that number through a sigmoid to get probability of the next sentence. We can then train with <code>BCEWithLogitsLoss</code>. (where <code>BCE</code> is just the special binary case of cross entropy loss).</p>
",3,1,569,2020-05-30 23:40:13,https://stackoverflow.com/questions/62109957/why-does-the-bert-nsp-head-linear-layer-have-two-outputs
"TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?","<p>I am pretraining BERT in 1 machine with 4 GPU.</p>

<p>The input function code:</p>

<pre><code>    def input_fn(params):
        """"""The actual input function.""""""
        batch_size = FLAGS.train_batch_size

        name_to_features = {
            ""input_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""input_mask"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""segment_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""masked_lm_positions"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_ids"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_weights"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
            ""next_sentence_labels"":
                tf.FixedLenFeature([1], tf.int64),
        }

        # For training, we want a lot of parallel reading and shuffling.
        # For eval, we want no shuffling and parallel reading doesn't matter.
        if is_training:
            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
            d = d.repeat()
            d = d.shuffle(buffer_size=len(input_files))

            # `cycle_length` is the number of parallel files that get read.
            cycle_length = min(num_cpu_threads, len(input_files))

            # `sloppy` mode means that the interleaving is not exact. This adds
            # even more randomness to the training pipeline.
            d = d.apply(
                tf.contrib.data.parallel_interleave(
                    tf.data.TFRecordDataset,
                    sloppy=is_training,
                    cycle_length=cycle_length))
            d = d.shuffle(buffer_size=100)
        else:
            d = tf.data.TFRecordDataset(input_files)
            # Since we evaluate for a fixed number of steps we don't want to encounter
            # out-of-range exceptions.
            d = d.repeat()

        # We must `drop_remainder` on training because the TPU requires fixed
        # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
        # and we *don't* want to drop the remainder, otherwise we wont cover
        # every sample.
        d = d.apply(
            tf.contrib.data.map_and_batch(
                lambda record: _decode_record(record, name_to_features),
                batch_size=batch_size,
                num_parallel_batches=num_cpu_threads,
                drop_remainder=True))
        d = d.prefetch(10)
        return d

</code></pre>

<p>The mirrow strategy code:</p>

<pre><code>    distribution = tf.contrib.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(FLAGS.n_gpus)],
        # num_gpus=4,
        cross_tower_ops=tf.distribute.HierarchicalCopyAllReduce())
    run_config = RunConfig(
        train_distribute=distribution,
        # eval_distribute=dist_strategy,
        log_step_count_steps=log_every_n_steps,
        model_dir=FLAGS.output_dir,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps)

    model_fn = model_fn_builder(
        bert_config=bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.num_train_steps,
        num_warmup_steps=FLAGS.num_warmup_steps,
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=FLAGS.use_tpu)

    # If TPU is not available, this will fall back to normal Estimator on CPU
    # or GPU.
    estimator = Estimator(
        model_fn=model_fn,
        params={},
        config=run_config)
</code></pre>

<p>The problem is that I have 4 GPU. Each GPU could run 8 batchsize at most.</p>

<p>I set <code>train_batch_size = 8</code> not 32. Is OK but I don't know each GPU get different data in one training step.</p>

<p>If I set <code>train_batch_size = 32</code>, it will out of memory (OOM).</p>

<p>Is my code right now? Will the data be distributed to 4 GPU and each GPU get different data?</p>
","tensorflow, tensorflow-datasets, tensorflow-estimator, transformer-model, bert-language-model","<p>According to <a href=""https://github.com/guotong1988/BERT-GPU/blob/master/run_pretraining_gpu_v2.py"" rel=""nofollow noreferrer"">BERT-GPU</a>.</p>
<p>No problem with the code.</p>
<p>The <code>batch_size</code> is for 1 GPU.</p>
",0,1,272,2020-06-01 02:43:13,https://stackoverflow.com/questions/62124961/tensorflow1-15-multi-gpu-1-machine-how-to-set-batch-size
how to use ktrain for NER Offline?,"<p>I have trained my English model following this notebook (<a href=""https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb"" rel=""nofollow noreferrer"">https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb</a>). I  am able to save my pretrained model and run it with no problem. </p>

<p>However, I need to run it again but OFFLINE and it is not working, I  understand that I need to download the file and do something similar to what is done here. </p>

<p><a href=""https://github.com/huggingface/transformers/issues/136"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/136</a></p>

<p>However, I  am not able to understand where do  I need to change the settings of ktrain.</p>

<p>I  run this:</p>

<pre><code>ktrain.load_predictor('Functions/my_english_nermodel')
</code></pre>

<p>and this is  the error I get: </p>

<pre><code>Traceback (most recent call last):
  File ""Z:\Functions\NER.py"", line 155, in load_bert
    reloaded_predictor= ktrain.load_predictor('Z:/Functions/my_english_nermodel')
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\core.py"", line 1316, in load_predictor
    preproc = pickle.load(f)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\ner\anago\preprocessing.py"", line 76, in __setstate__
    if self.te_model is not None: self.activate_transformer(self.te_model, layers=self.te_layers)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\ner\anago\preprocessing.py"", line 100, in activate_transformer
    self.te = TransformerEmbedding(model_name, layers=layers)
  File ""C:\Program Files\Python37\lib\site-packages\ktrain\text\preprocessor.py"", line 1095, in __init__
    self.tokenizer = self.tokenizer_type.from_pretrained(model_name)
  File ""C:\Program Files\Python37\lib\site-packages\transformers\tokenization_utils.py"", line 903, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File ""C:\Program Files\Python37\lib\site-packages\transformers\tokenization_utils.py"", line 1008, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'bert-base-uncased' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'bert-base-dutch-cased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.

Process finished with exit code 1
</code></pre>
","python, tensorflow, offline, named-entity-recognition, bert-language-model","<p>More generally, the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a>-based pretrained models are downloaded to <code>&lt;home_directory&gt;/.cache/torch/transformers</code>. For instance, on Linux, this will be <code>/home/&lt;user_name&gt;/.cache/torch/transformers</code>.  </p>

<p>As indicated in the answer above, to reload the <em>ktrain</em> <code>predictor</code> on a machine with no internet access (for <code>ktrain</code> models that utilize models from <code>transformers</code> library), you'll need copy the model files in that folder to the same location on the new machine.</p>
",1,0,552,2020-06-02 10:40:18,https://stackoverflow.com/questions/62150139/how-to-use-ktrain-for-ner-offline
BERT skipping the 1st row of test.tsv when predicting,"<p>I'm running <a href=""https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"" rel=""nofollow noreferrer"">BERT-Base, Uncased</a> pre-trained model on a news classification problem. Most of the core logic for data preparation was copied from <a href=""https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/"" rel=""nofollow noreferrer"">here</a>. I'm running it on a different dataset though, hence relevant changes have been done. I've 490 news articles, and the train, validation, test data ratios are 405 : 45 : 40. These datasets are present in <code>train.tsv</code>, <code>dev.tsv</code> and <code>test.tsv</code> files in the same dir, all without header. The command I'm using for running the classifier is something like this:</p>

<pre><code>python /Users/&lt;username&gt;/Documents/CodeBase/Projects/BERT/run_classifier.py \
--task_name=cola \
--do_train=true \
--do_eval=true \
--do_predict=true \
--data_dir=/Users/&lt;username&gt;/Desktop/NLP_Learning/Fraud\ detection/BERT \
--vocab_file=./vocab.txt \
--bert_config_file=./bert_config.json \
--init_checkpoint=./bert_model.ckpt \
--max_seq_length=128 \
--train_batch_size=32 \
--learning_rate=2e-5 \
--num_train_epochs=3.0 \
--output_dir=/Users/&lt;username&gt;/Desktop/NLP_Learning/Fraud\ detection/BERT_Model_Pretrained/output \
--do_lower_case=True
</code></pre>

<p>Now, even though the training and prediction finishes, trouble is the generated <code>test_results.tsv</code> file contains only 39 rows, which should have been 40. By the looks of it, it seems row-0 of <code>test.tsv</code> is somehow getting skipped. What am I missing here? I've checked all three input data files, and they all contain proper number of records.</p>
","python, tensorflow, bert-language-model","<p>Yes, the data formats for <code>cola</code> tasks are very specific. It requires 3 files <code>train.tsv</code>, <code>dev.tsv</code> and <code>test.tsv</code>, for training-set, development/validation set and test set respectively.</p>

<p>Coming to the data-formats in each TSV files.
<code>train.tsv</code> and <code>dev.tsv</code> have same format: </p>

<p><code>id    class_label    segment    text</code></p>

<p>and both <code>train.tsv</code> and <code>dev.tsv</code> should <strong>not</strong> have headers.</p>

<p>However, coming to the <code>test.tsv</code>, below is the format:</p>

<p><code>id    text</code>  (Note that you should not provide the labels or the segment columns).</p>

<p>More <strong>importantly</strong>: <code>test.tsv</code> <strong>should have a header</strong>.</p>
",1,1,232,2020-06-02 20:57:06,https://stackoverflow.com/questions/62161281/bert-skipping-the-1st-row-of-test-tsv-when-predicting
huggingface transformers bert model without classification layer,"<p>I want to do a joint-embedding from vgg16 and <code>bert</code> for classification.</p>

<p>The thing with <code>huggingface transformers bert</code> is that it has the classification layer which has <code>num_labels</code> dimension.</p>

<p>But, I want the output from <code>BertPooler</code> (768 dimensions) which I will use as a text-embedding for an extended model.</p>

<pre><code>from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
</code></pre>

<p>This gives the following model:</p>

<pre><code>BertForSequenceClassification(
...
...
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
</code></pre>

<p>How can I get rid of the <code>classifier</code> layer?</p>
","pytorch, huggingface-transformers, bert-language-model","<pre><code>from transformers import BertModel
model = BertModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>Output</p>
<pre><code>(11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</code></pre>
<p>Checkout the BertModel definition <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L865"" rel=""nofollow noreferrer"">here</a>.</p>
",4,3,2236,2020-06-06 17:05:56,https://stackoverflow.com/questions/62235153/huggingface-transformers-bert-model-without-classification-layer
How to perform Multi output regression using RoBERTa?,"<p>I have a problem statement where I want to predict multiple continuous outputs using a text input. I tried using 'robertaforsequenceclassification' from HuggingFace library. But the documentation states that when the number of outputs in the final layer is more than 1, a cross entropy loss is used automatically as mentioned here: <a href=""https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification</a>.
But I want to use an RMSE loss in a regression setting with two classes in the final layer. How would one go about modifying it?</p>
","pytorch, regression, huggingface-transformers, bert-language-model","<p><code>BertForSequenceClassification</code> is a small wrapper that wraps the <code>BERTModel</code>.</p>

<p>It calls the models, takes the pooled output (the second member of the output tuple), and applies a classifier over it. The code is here <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1168"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1168</a></p>

<p>The simplest solution is writing your own simple wrapper class (based on the <code>BertForSequenceClassification</code> class) hat will do the regression that will do the regression with the loss you like.</p>
",2,2,1528,2020-06-08 05:56:15,https://stackoverflow.com/questions/62255856/how-to-perform-multi-output-regression-using-roberta
"Huggingface Bert, Which Bert flavor is the fastest to train for debugging?","<p>I am working with Bert and the library <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a> hugginface.
I was wondering which of the models available you would choose for debugging?</p>

<p>In other words which models trains/loads the fast on my GPU, to get runs as fast as possible?
Albert, distillbert or?</p>
","machine-learning, nlp, huggingface-transformers, bert-language-model","<p>I think generally using a specific model for debugging can be critical, and depends entirely on the kind of debugging you want to perform.</p>

<p>Specifically, consider the aspect of tokenization: Since each model also carries their own derivation of the <code>BaseTokenizer</code> class. Therefore, any specifics of the respective model will only show up if you also use <em>this specific tokenizer</em>; say, e.g., you want to debug a (later) RoBERTa implementation by using DistilBert for debugging. Anything specific to RoBERTa's tokenization will not be the same in DistilBERT, which <a href=""https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer"" rel=""nofollow noreferrer"">uses BERT's tokenizer</a>.
Similarly, any specifics to the training process might completely screw up the training. From anecdotal evidence, I had models train to completion (and convergence) with RoBERTa but not on BERT, which makes the proposed solution of using different models for ""debugging"" a potentially dangerous substitution.
ALBERT again has properties different from any of the above mentioned models, but analogously, the mentioned aspects still hold. </p>

<p>If you want to prototype services and simply require a model for in between, I think both of the models suggested by you would do just fine, and there should be only a minor difference in loading/saving depending on the exact number of model parameters. But keep in mind that inference time for applications is also something that is worth considering. Unless you are absolutely sure that there will not be any noticeable difference in the execution time, at least make sure that you are testing with the full model as well.</p>
",1,0,324,2020-06-10 11:31:07,https://stackoverflow.com/questions/62302499/huggingface-bert-which-bert-flavor-is-the-fastest-to-train-for-debugging
"Bert + Resnet joint learning, pytorch model is empty after instantiation","<p>I'm writing a simple joint model, which has two branches, one branch is a resnet50 another one is a <code>bert</code>. I concatenate the two outputs and pass that to a simple linear layer with 2 output neurons.</p>

<p>I implemented the following model :</p>

<pre><code>import torch
from torch import nn
import torchvision.models as models
import torch.nn as nn
from collections import OrderedDict
from transformers import BertModel

class BertResNet(nn.Module):
    def __init__(self):
        super(BertResNet, self).__init__()
        # resnet
        resnet50 = models.resnet50(pretrained=True)
        n_inputs = resnet50.fc.in_features
        # compressed embedding space
        classifier = nn.Sequential(OrderedDict([
            ('fc1', nn.Linear(n_inputs, 512))
        ]))

        resnet50.fc = classifier # 512 out resnet 


        bert = BertModel.from_pretrained('bert-base-uncased')

        # final classification layer

        classification = nn.Linear(512 + 768, 2)
        #print(resnet50)
        #print(bert)

    def forward(self, img, text):
        res_emb = self.resnet50(img)
        bert_emb = self.bert(text)

        combined = torch.cat(res_emb,
                              bet_emb, dim=1)
        out = self.classification(combined)
        return out
</code></pre>

<p>But when I instantiate, I get an empty model:</p>

<pre><code>bert_resnet = BertResNet()

print(bert_resnet)
</code></pre>

<p>Out:
<code>BertResNet()</code></p>

<p><code>list(bert_resnet.parameters())</code> also returns <code>[]</code></p>
","pytorch, resnet, bert-language-model, torchvision","<p>You never assigned the models to any attribute of the object of the <code>BertResNet</code> class. There are in temporary variables in the <code>__init__</code> method, but once that finishes, these variables are discarded. They should be assigned to <code>self</code>:</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self):
    super(BertResNet, self).__init__()
    # resnet
    self.resnet50 = models.resnet50(pretrained=True)
    n_inputs = self.resnet50.fc.in_features
    # compressed embedding space
    self.classifier = nn.Sequential(OrderedDict([
        ('fc1', nn.Linear(n_inputs, 512))
    ]))

    self.resnet50.fc = classifier # 512 out resnet 


    self.bert = BertModel.from_pretrained('bert-base-uncased')

    # final classification layer

    self.classification = nn.Linear(512 + 768, 2)
</code></pre>
",1,-1,585,2020-06-10 19:26:19,https://stackoverflow.com/questions/62311593/bert-resnet-joint-learning-pytorch-model-is-empty-after-instantiation
"TensorFlow1.15, the inner logic of Estimator&#39;s input_fn? Or the inner logic of MirroredStrategy?","<p>I am pretraining BERT in 1 machine with 4 GPU, not 1 GPU.</p>

<p>For each training step, I am wondering whether the <code>input_fn</code> give 1 GPU 1 batch or give 4 GPU 1 batch.</p>

<p>The mirrow strategy code:</p>

<pre><code>    distribution = tf.contrib.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(FLAGS.n_gpus)],
        cross_tower_ops=tf.distribute.HierarchicalCopyAllReduce())

    run_config = RunConfig(
        train_distribute=distribution,
        log_step_count_steps=log_every_n_steps,
        model_dir=FLAGS.output_dir,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps)

    model_fn = model_fn_builder(
        bert_config=bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.num_train_steps,
        num_warmup_steps=FLAGS.num_warmup_steps,
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=FLAGS.use_tpu)

    estimator = Estimator(
        model_fn=model_fn,
        params={},
        config=run_config)
</code></pre>

<p>The <code>input_fn</code> code:</p>

<pre><code>  def input_fn(params):
        batch_size = FLAGS.train_batch_size

        name_to_features = {
            ""input_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""input_mask"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""segment_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""masked_lm_positions"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_ids"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_weights"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
            ""next_sentence_labels"":
                tf.FixedLenFeature([1], tf.int64),
        }

        if is_training:
            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
            d = d.repeat()
            d = d.shuffle(buffer_size=len(input_files))

            cycle_length = min(num_cpu_threads, len(input_files))

            d = d.apply(
                tf.contrib.data.parallel_interleave(
                    tf.data.TFRecordDataset,
                    sloppy=is_training,
                    cycle_length=cycle_length))
            d = d.shuffle(buffer_size=100)
        else:
            d = tf.data.TFRecordDataset(input_files)
            d = d.repeat()

        d = d.apply(
            tf.contrib.data.map_and_batch(
                lambda record: _decode_record(record, name_to_features),
                batch_size=batch_size,
                num_parallel_batches=num_cpu_threads,
                drop_remainder=True))
        d = d.prefetch(10)
        return d
</code></pre>

<p>Other code:</p>

<pre><code>estimator.train(input_fn, max_steps=FLAGS.num_train_steps)
</code></pre>

<p>If <code>input_fn</code> give 1 GPU 1 batch, then <code>train_batch_size</code> should be max_batchsize_per_gpu.</p>

<p>If <code>input_fn</code> give 4 GPU 1 batch, then <code>train_batch_size</code> should be max_batchsize_per_gpu*4.</p>
","tensorflow, deep-learning, tensorflow-datasets, tensorflow-estimator, bert-language-model","<p>According to <a href=""https://github.com/guotong1988/BERT-GPU/blob/master/run_pretraining_gpu_v2.py"" rel=""nofollow noreferrer"">BERT-GPU</a>.</p>
<p><code>input_fn</code> return 1 batch for 1 GPU.</p>
<p>The <code>batch_size</code> is for 1 GPU.</p>
",0,2,77,2020-06-11 03:48:18,https://stackoverflow.com/questions/62316736/tensorflow1-15-the-inner-logic-of-estimators-input-fn-or-the-inner-logic-of-m
Bert Text Classification Loss is Nan,"<p>I'm try to make an model that classify the text in 3 categories.(Negative,Neural,Positive)</p>

<p>I have csv file that contain comments on different apps with their rating.</p>

<p><strong>First I import all the necessary libraries</strong></p>

<pre><code>!pip install transformers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%tensorflow_version 2.x
import tensorflow as tf

from transformers import TFBertForSequenceClassification, BertTokenizer,DistilBertTokenizer,glue_convert_examples_to_features, InputExample,BertConfig,InputFeatures
from sklearn.model_selection import train_test_split
from tqdm import tqdm


%matplotlib inline
</code></pre>

<p><strong>Then i'll get my csv file</strong></p>

<pre><code>!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV
!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv
df = pd.read_csv(""reviews.csv"")
print(df[['content','score']].head())
                                         content  score
0  Update: After getting a response from the deve...      1
1  Used it for a fair amount of time without any ...      1
2  Your app sucks now!!!!! Used to be good but no...      1
3  It seems OK, but very basic. Recurring tasks n...      1
4  Absolutely worthless. This app runs a prohibit...      1
</code></pre>

<p><strong>Converting scores to sentiment</strong></p>

<pre><code>def to_sentiment(rating):
  rating = int(rating)
  if rating &lt;= 2:
    return 0
  elif rating == 3:
    return 1
  else: 
    return 2

df['sentiment'] = df.score.apply(to_sentiment)

tokenizer = BertTokenizer.from_pretrained('bert-base-cased',do_lower_case = True)
</code></pre>

<p><strong>Creating Helper Methods to fit the data into model</strong></p>

<pre><code>def convert_example_to_feature(review):
  return tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=160, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

def map_example_to_dict(input_ids,attention_mask,token_type_ids,label):
  return {
      ""input_ids"": input_ids,
      ""attention_mask"": attention_mask,
      ""token_type_ids"" : token_type_ids
  },label

def encode_examples(ds):
  # prepare list, so that we can build up final TensorFlow dataset from slices.
  input_ids_list = []
  token_type_ids_list = []
  attention_mask_list = []
  label_list = []

  for index, row in tqdm(ds.iterrows()):
    bert_input = convert_example_to_feature(row['content'])

    input_ids_list.append(bert_input['input_ids'])
    token_type_ids_list.append(bert_input['token_type_ids'])
    attention_mask_list.append(bert_input['attention_mask'])
    label_list.append([row['sentiment']])
  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)

df_train, df_test = train_test_split(df,test_size=0.1)
</code></pre>

<p><strong>Creating Model</strong></p>

<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss,metrics=metric)

history = model.fit(ds_train_encoded,epochs=1)
14/443 [..............................] - ETA: 3:58 - loss: nan - accuracy: 0.3438
</code></pre>

<p>If i change the count of the sentiment and make it just positive and negative then it works.
But with 3 or more labels creates this problem.</p>
","python, tensorflow, sentiment-analysis, text-classification, bert-language-model","<p>The label classes index should start from 0 not 1.</p>
<blockquote>
<p>TFBertForSequenceClassification requires labels in the range [0,1,...]</p>
</blockquote>
<blockquote>
<p>labels (tf.Tensor of shape (batch_size,), optional, defaults to None)
– Labels for computing the sequence classification/regression loss.
Indices should be in [0, ..., config.num_labels - 1]. If
config.num_labels == 1 a regression loss is computed (Mean-Square
loss), If config.num_labels &gt; 1 a classification loss is computed
(Cross-Entropy).</p>
</blockquote>
<p>Source: <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification</a></p>
",0,1,1606,2020-06-11 08:03:09,https://stackoverflow.com/questions/62319735/bert-text-classification-loss-is-nan
Having 6 labels instead of 2 in Hugging Face BertForSequenceClassification,"<p>I was just wondering if it is possibel to extend the HuggingFace <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> model to more than 2 labels. The docs say, we can pass positional arguments, but it seems like ""labels"" is not working. Does anybody has an idea?</p>

<h2>Model assignment</h2>

<pre class=""lang-py prettyprint-override""><code>labels = th.tensor([0,0,0,0,0,0], dtype=th.long).unsqueeze(0)
print(labels.shape)
modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    labels=labels
    )

l = [module for module in modelBERTClass.modules()]
l
</code></pre>

<h2>Console Output</h2>

<pre><code>torch.Size([1, 6])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-122-fea9a36402a6&gt; in &lt;module&gt;()
      3 modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
      4     'bert-base-uncased',
----&gt; 5     labels=labels
      6     )
      7 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    653 
    654         # Instantiate model.
--&gt; 655         model = cls(config, *model_args, **model_kwargs)
    656 
    657         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'labels'
</code></pre>
","python, transformer-model, huggingface-transformers, bert-language-model","<p>You can set the output shape of the classification layer with <code>from_pretrained</code> via the <code>num_labels</code> parameter:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)
print(model.classifier.parameters)
</code></pre>
<p>Output:</p>
<pre><code>Linear(in_features=768, out_features=6, bias=True)
</code></pre>
",7,4,7404,2020-06-11 15:23:32,https://stackoverflow.com/questions/62327803/having-6-labels-instead-of-2-in-hugging-face-bertforsequenceclassification
How to improve code to speed up word embedding with transformer models?,"<p>I need to compute words embeddings for a bunch of documents with different language models.
No problem with that, the script is doing fine, except I'm working on a notebook, without GPU and each text needs around 1.5s to be processed which is by far too long (I have thousands of texts to process).</p>

<p>Here is how I'm doing it with pytorch and transformers lib:</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import CamembertModel, CamembertTokenizer

docs = [text1, text2, ..., text20000]
tok = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base', output_hidden_states=True)
# let try with a batch size of 64 documents
docids = [tok.encode(
  doc, max_length=512, return_tensors='pt', pad_to_max_length=True) for doc in docs[:64]]
ids=torch.cat(tuple(docids))
device = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu in my case...
model = model.to(device)
ids = ids.to(device)
model.eval()
with torch.no_grad():
    out = model(input_ids=ids)
# 103s later...
</code></pre>

<p>Do someone has any idea or suggestions to improve speed?</p>
","nlp, pytorch, word-embedding, huggingface-transformers, bert-language-model","<p>I don't think that there is a trivial way to <em>significantly</em> improve the speed, without using a GPU.</p>

<p>Some of the ways I could think of include smart batching, which is used by <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">Sentence-Transformers</a>, where you basically sort inputs of similar length together, to avoid padding to the full 512 token limit. I'm not sure how much of a speedup this is going to get you, but the only way that you can improve it significantly in a short period of time.</p>

<p>Otherwise, if you have access to <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">Google colab</a>, you can also utilize their GPU environment, if the processing can be completed in reasonable time.</p>
",2,0,1851,2020-06-15 09:19:02,https://stackoverflow.com/questions/62385092/how-to-improve-code-to-speed-up-word-embedding-with-transformer-models
Cannot import BertModel from transformers,"<p>I am trying to import BertModel from transformers, but it fails. This is code I am using</p>

<pre><code>from transformers import BertModel, BertForMaskedLM
</code></pre>

<p>This is the error I get</p>

<pre><code>ImportError: cannot import name 'BertModel' from 'transformers'
</code></pre>

<p>Can anyone help me fix this?</p>
","python, nlp, pytorch, huggingface-transformers, bert-language-model","<p>Fixed the error. This is the code</p>

<pre><code>from transformers.modeling_bert import BertModel, BertForMaskedLM
</code></pre>
",6,4,22676,2020-06-15 10:47:14,https://stackoverflow.com/questions/62386631/cannot-import-bertmodel-from-transformers
BertWordPieceTokenizer vs BertTokenizer from HuggingFace,"<p>I have the following pieces of code and trying to understand the difference between BertWordPieceTokenizer and BertTokenizer.</p>
<h1><strong>BertWordPieceTokenizer (Rust based)</strong></h1>
<pre><code>from tokenizers import BertWordPieceTokenizer

sequence = &quot;Hello, y'all! How are you Tokenizer 😁 ?&quot;
tokenizer = BertWordPieceTokenizer(&quot;bert-base-uncased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
&gt;&gt;&gt;Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])

print(tokenized_sequence.tokens)
&gt;&gt;&gt;['[CLS]', 'hello', ',', 'y', &quot;'&quot;, 'all', '!', 'how', 'are', 'you', 'token', '##izer', '[UNK]', '?', '[SEP]']
</code></pre>
<h1>BertTokenizer</h1>
<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer(&quot;bert-base-cased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
#Output: [19082, 117, 194, 112, 1155, 106, 1293, 1132, 1128, 22559, 17260, 100, 136]
</code></pre>
<ol>
<li>Why is encode working differently in both ? In BertWordPieceTokenizer it gives Encoding object while in BertTokenizer it gives the ids of the vocab.</li>
<li>What is the Difference between BertWordPieceTokenizer and BertTokenizer fundamentally, because as I understand BertTokenizer also uses WordPiece under the hood.</li>
</ol>
<p>Thanks</p>
","nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>They should produce the same output when you use the same vocabulary (in your example you have used bert-base-uncased-vocab.txt and bert-base-cased-vocab.txt). The main difference is that the tokenizers from the <a href=""https://github.com/huggingface/tokenizers"" rel=""noreferrer"">tokenizers</a> package are faster as the tokenizers from <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">transformers</a> because they are implemented in Rust.</p>

<p>When you modify your example you will see that they produce the same <code>ids</code> and other attributes (encoding object) while the transformers tokenizer only have produced the a list of <code>ids</code>:</p>

<pre class=""lang-py prettyprint-override""><code>from tokenizers import BertWordPieceTokenizer

sequence = ""Hello, y'all! How are you Tokenizer 😁 ?""
tokenizerBW = BertWordPieceTokenizer(""/content/bert-base-uncased-vocab.txt"")
tokenized_sequenceBW = tokenizerBW.encode(sequence)
print(tokenized_sequenceBW)
print(type(tokenized_sequenceBW))
print(tokenized_sequenceBW.ids)
</code></pre>

<p>Output:</p>

<pre><code>Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
&lt;class 'Encoding'&gt;
[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 19204, 17629, 100, 1029, 102]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer

tokenizerBT = BertTokenizer(""/content/bert-base-uncased-vocab.txt"")
tokenized_sequenceBT = tokenizerBT.encode(sequence)
print(tokenized_sequenceBT)
print(type(tokenized_sequenceBT))
</code></pre>

<p>Output:</p>

<pre><code>[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 19204, 17629, 100, 1029, 102]
&lt;class 'list'&gt;
</code></pre>

<p>You mentioned in the comments that your questions is more about why the produced output is different. As far as I can tell this was a design decision made by the developers and there is no specific reason for that. It is also not a the case that BertWordPieceTokenizer from <a href=""https://github.com/huggingface/tokenizers"" rel=""noreferrer"">tokenizers</a> is an in-place replacement for the BertTokenizer from <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">transformers</a>. They still use a wrapper to make it compatible with with the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">transformers</a> tokenizer API. There is a <a href=""https://github.com/huggingface/transformers/blob/e4aaa4580515446cd5a2972ab42fec0b95819c84/src/transformers/tokenization_bert.py#L589"" rel=""noreferrer"">BertTokenizerFast</a> class which has a ""clean up"" method <a href=""https://github.com/huggingface/transformers/blob/e4aaa4580515446cd5a2972ab42fec0b95819c84/src/transformers/tokenization_utils_fast.py#L142"" rel=""noreferrer"">_convert_encoding</a> to make the BertWordPieceTokenizer fully compatible. Therefore you have to compare the BertTokenizer example above with the following:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizerFast

sequence = ""Hello, y'all! How are you Tokenizer 😁 ?""
tokenizerBW = BertTokenizerFast.from_pretrained(""bert-base-uncased"")
tokenized_sequenceBW = tokenizerBW.encode(sequence)
print(tokenized_sequenceBW)
print(type(tokenized_sequenceBW))
</code></pre>

<p>Output:</p>

<pre><code>[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 19204, 17629, 100, 1029, 102]
&lt;class 'list'&gt;
</code></pre>

<p>From my perspective they have build the <a href=""https://github.com/huggingface/tokenizers"" rel=""noreferrer"">tokenizers</a> library independently from the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">transformers</a> library with the objective to be fast and useful. </p>
",16,7,12461,2020-06-16 09:19:39,https://stackoverflow.com/questions/62405155/bertwordpiecetokenizer-vs-berttokenizer-from-huggingface
Error Running &quot;config = RobertaConfig.from_pretrained( &quot;/Absolute-path-to/BERTweet_base_transformers/config.json&quot;&quot;,"<p>I'm trying to run the code 'transformers' version of <a href=""https://github.com/VinAIResearch/BERTweet#transformers"" rel=""nofollow noreferrer"">this code</a> to use the new pre-trained BERTweet model and I'm getting an error. </p>

<p>The following lines of code ran successfully in my Google Colab notebook:</p>

<pre><code>
!pip install fairseq
import fairseq
!pip install fastBPE
import fastBPE

# download the pre-trained BERTweet model zipped file
!wget https://public.vinai.io/BERTweet_base_fairseq.tar.gz

# unzip the pre-trained BERTweet model files
!tar -xzvf BERTweet_base_fairseq.tar.gz

!pip install transformers
import transformers

import torch
import argparse

from transformers import RobertaConfig
from transformers import RobertaModel

from fairseq.data.encoders.fastbpe import fastBPE
from fairseq.data import Dictionary

</code></pre>

<p>Then I tried to run the following code:</p>

<pre><code># Load model
config = RobertaConfig.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/config.json""
)
BERTweet = RobertaModel.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/model.bin"",
    config=config
)
</code></pre>

<p>...and an error was displayed:</p>

<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    242             if resolved_config_file is None:
--&gt; 243                 raise EnvironmentError
    244             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
2 frames
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    250                 f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n""
    251             )
--&gt; 252             raise EnvironmentError(msg)
    253 
    254         except json.JSONDecodeError:

OSError: Can't load config for '/Absolute-path-to/BERTweet_base_transformers/config.json'. Make sure that:

- '/Absolute-path-to/BERTweet_base_transformers/config.json' is a correct model identifier listed on 'https://huggingface.co/models'

- or '/Absolute-path-to/BERTweet_base_transformers/config.json' is the correct path to a directory containing a config.json file

</code></pre>

<p>I'm guessing the issue is that I need to replace '/Absolute-path-to' with something else but if that's the case what should it be replaced with? It's likely a very simple answer and I feel stupid for asking but I need help.</p>
","nlp, google-colaboratory, bert-language-model, huggingface-transformers, roberta-language-model","<p>First of all you have to download the proper package as described in the github readme:</p>

<pre><code>!wget https://public.vinai.io/BERTweet_base_transformers.tar.gz

!tar -xzvf BERTweet_base_transformers.tar.gz
</code></pre>

<p>After that you can click on the directory icon (left side of your screen) and list the downloaded data:
<a href=""https://i.sstatic.net/jUuCu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jUuCu.png"" alt=""colab folders""></a></p>

<p>Right click on BERTweet_base_transformers, choose <code>copy path</code> and insert the content from your clipboard to your code:</p>

<pre class=""lang-py prettyprint-override""><code>config = RobertaConfig.from_pretrained(
    ""/content/BERTweet_base_transformers/config.json""
)

BERTweet = RobertaModel.from_pretrained(
    ""/content/BERTweet_base_transformers/model.bin"",
    config=config
)
</code></pre>
",3,0,1743,2020-06-16 09:59:54,https://stackoverflow.com/questions/62405867/error-running-config-robertaconfig-from-pretrained-absolute-path-to-bertwe
BERT:Question-Answering - Total number of permissible words/tokens for training,"<p>Let's say I want to train BERT with 2 sentences (query-answer) pair against a certain binary label (1,0) for the correctness of the answer,  will BERT let me use 512 words/tokens each for the query and the answer or together(query+answer combined) they should be 512? [510 upon ignoring the [start] and [sep] token]</p>

<p>Thanks in advance!</p>
","pytorch, recurrent-neural-network, language-model, bert-language-model","<p>Together, and actually it's together they should be 509 since there are two [SEP], one after question and another after answer: </p>

<pre><code>[CLS] q_word1 q_word2 ... [SEP] a_word1 a_word2 ... [SEP]
</code></pre>

<p>where <code>q_word</code> refers to words in the question and <code>a_word</code> refers to words in the answer</p>
",0,0,218,2020-06-18 20:16:21,https://stackoverflow.com/questions/62458671/bertquestion-answering-total-number-of-permissible-words-tokens-for-training
Customize the encode module in huggingface bert model,"<p>I am working on a text classification project using <a href=""https://huggingface.co/transformers/glossary.html#token-type-ids"" rel=""nofollow noreferrer"">Huggingface transformers module</a>. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:</p>

<pre><code>from transformers import BertTokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

sample_text = 'Bamboo poles, ‍installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        return_token_type_ids=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',  # Return PyTorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)
</code></pre>

<p>However, my current project requires me to generate <strong>customized ids</strong> for a given text. For instance, for a list of words <code>[HK, US, UK]</code>, I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.</p>

<p>How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~</p>
","nlp, text-classification, huggingface-transformers, bert-language-model","<p>I think you can use the <code>&lt;unusedxxx&gt;</code> tokens in the BERT vocab and add your custom tokens there. So now you can easily refer to them with a valid token ID.</p>
",2,1,745,2020-06-19 03:39:55,https://stackoverflow.com/questions/62462878/customize-the-encode-module-in-huggingface-bert-model
TFBertMainLayer gets less accuracy compared to TFBertModel,"<p>I had a problem with saving weights of <code>TFBertModel</code> wrapped in <code>Keras</code>. the problem is described <a href=""https://github.com/huggingface/transformers/issues/2733"" rel=""nofollow noreferrer"">here in GitHub issue</a> and <a href=""https://stackoverflow.com/questions/60062624/save-model-wrapped-in-keras"">here in Stack Overflow</a>.The solution proposed in both cases is to use </p>

<pre><code> config = BertConfig.from_pretrained(transformer_model_name)
 bert = TFBertMainLayer(config=config,trainable=False)
</code></pre>

<p>instead of </p>

<pre><code> bert = TFBertModel.from_pretrained(transformer_model_name, trainable=False)
</code></pre>

<p>The problem is that when I change my model to the former code, the accuracy decreases by 10 percent.While the parameters count in both cases are the same. I wonder what is the reason and how can be prevented?</p>
","keras, transformer-model, bert-language-model","<p>It seems like the performance regression in the code snippet that instantiates <code>MainLayer</code> directly occurs because the pre-trained weights are not being loaded. You can load the weights by either:</p>
<ol>
<li>Calling <code>TFBertModel.from_pretrained</code> and grabbing the <code>MainLayer</code> from the loaded <code>TFBertModel</code></li>
<li>Creating the <code>MainLayer</code> directly, then loading the weights in a similar way to <code>from_pretrained</code></li>
</ol>
<h1>Why This Happens</h1>
<p>When you call <code>TFBertModel.from_pretrained</code>, it uses the function <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_tf_utils.html"" rel=""nofollow noreferrer""><code>TFPreTrainedModel.from_pretrained</code></a> (via inheritance) which handles a few things, including downloading, caching, and loading the model weights.</p>
<pre><code>class TFPreTrainedModel(tf.keras.Model, TFModelUtilsMixin, TFGenerationMixin):
    ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        ...
        # Load model
        if pretrained_model_name_or_path is not None:
            if os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):
            # Load from a TF 2.0 checkpoint
            archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)
            ...
            resolved_archive_file = cached_path(
                    archive_file,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
            )
            ...
            model.load_weights(resolved_archive_file, by_name=True)
</code></pre>
<p>(If you read the actual code, a lot has been <code>...</code>'ed out above).</p>
<p>However, when you instantiate <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_tf_bert.html"" rel=""nofollow noreferrer""><code>TFBertMainLayer</code></a> directly, it doesn't do any of this set up work.</p>
<pre><code>@keras_serializable
class TFBertMainLayer(tf.keras.layers.Layer):
    config_class = BertConfig

    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.num_hidden_layers = config.num_hidden_layers
        self.initializer_range = config.initializer_range
        self.output_attentions = config.output_attentions
        self.output_hidden_states = config.output_hidden_states
        self.return_dict = config.use_return_dict
        self.embeddings = TFBertEmbeddings(config, name=&quot;embeddings&quot;)
        self.encoder = TFBertEncoder(config, name=&quot;encoder&quot;)
        self.pooler = TFBertPooler(config, name=&quot;pooler&quot;)
   
   ... rest of the class
</code></pre>
<p>Essentially, you need to make sure these weights are being loaded.</p>
<h1>Solutions</h1>
<h2>(1) Using TFAutoModel.from_pretrained</h2>
<p>You can rely on transformers.TFAutoModel.from_pretrained to load the model, then just grab the <code>MainLayer</code> field from the specific subclass of <code>TFPreTrainedModel</code>. For example, if you wanted to access a distilbert main layer, it would look like:</p>
<pre><code>    model = transformers.TFAutoModel.from_pretrained(`distilbert-base-uncased`)
    assert isinstance(model, TFDistilBertModel)
    main_layer = transformer_model.distilbert
</code></pre>
<p>You can see in <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_tf_distilbert.html"" rel=""nofollow noreferrer"">modeling_tf_distilbert.html
</a> that the <code>MainLayer</code> is a field of the model.
This is less code and less duplication, but has a few disadvantages. It's less easy to change the pre-trained model you're going to use, because now you're depending on the fieldname, if you change the model type, you'll have to change the field name (for example in <code>TFAlbertModel</code> the MainLayer field is called <code>albert</code>). In addition, this doesn't seem to be the intended way to use huggingface, so this could change under your nose, and your code could break with huggingface updates.</p>
<pre><code>class TFDistilBertModel(TFDistilBertPreTrainedModel):
    def __init__(self, config, *inputs, **kwargs):
        super().__init__(config, *inputs, **kwargs)
        self.distilbert = TFDistilBertMainLayer(config, name=&quot;distilbert&quot;)  # Embeddings

[DOCS]    @add_start_docstrings_to_callable(DISTILBERT_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint=&quot;distilbert-base-uncased&quot;,
        output_type=TFBaseModelOutput,
        config_class=_CONFIG_FOR_DOC,
    )
    def call(self, inputs, **kwargs):
        outputs = self.distilbert(inputs, **kwargs)
        return outputs
</code></pre>
<h2>(2) Re-implementing the weight loading logic from <code>from_pretrained</code></h2>
<p>You can do this by essentially copy/pasting the parts of <code>from_pretrained</code> that are relevant to loading weights. This also has some serious disadvantages, you'll be duplicating logic that that can fall out of sync with the huggingface libraries. Though you could likely write it in a way that is more flexible and robust to underlying model name changes.</p>
<h2>Conclusion</h2>
<p>Ideally this is something that will get fixed internally by the huggingface team, either by providing a standard function to create a MainLayer, wrapping the weight loading logic into its own function that can be called, or by supporting serialization on the model class.</p>
",3,3,1348,2020-06-20 06:37:47,https://stackoverflow.com/questions/62482511/tfbertmainlayer-gets-less-accuracy-compared-to-tfbertmodel
NLP : Get 5 best candidates from QuestionAnsweringPipeline,"<p>I am working on a French Question-Answering model using huggingface transformers library. I'm using a pre-trained CamemBERT model which is very similar to RoBERTa but is adapted to french.</p>
<p>Currently, i am able to get the best answer candidate for a question on a text of my own, using the QuestionAnsweringPipeline from the transformers library.</p>
<p>Here is an extract of my code.</p>
<pre><code>QA_model = &quot;illuin/camembert-large-fquad&quot;
CamTokQA = CamembertTokenizer.from_pretrained(QA_model)
CamQA = CamembertForQuestionAnswering.from_pretrained(QA_model)

device_pipeline = 0 if torch.cuda.is_available() else -1
q_a_pipeline = QuestionAnsweringPipeline(model=CamQA,
                                         tokenizer=CamTokQA,
                                         device=device_pipeline)

ctx = open(&quot;text/Sample.txt&quot;, &quot;r&quot;).read()
question = 'Quel est la taille de la personne ?'
res = q_a_pipeline({'question': question, 'context': ctx})
print(res)
</code></pre>
<p>I am currently getting this :<code>{'score': 0.9630325870663725, 'start': 2421, 'end': 2424, 'answer': '{21'} </code>, which is wrong.</p>
<p>Therefore, i would like to get the 5 best candidates for the answer. Does anyone have an idea how to do that ?</p>
","nlp, huggingface-transformers, bert-language-model, nlp-question-answering","<p>When calling your pipeline, you can specify the number of results via the <em>topk</em> argument. For example for the five most probable answers do:</p>
<pre><code>res = q_a_pipeline({'question': question, 'context': ctx}, topk=5)
</code></pre>
<p>This will result in a list of dictionaries: <code>[{'score': 0.0013586128421753108, 'start': 885, 'end': 896, 'answer': &quot;L'ingénieur&quot;}, {'score': 0.0011120906285982946, 'start': 200, 'end': 209, 'answer': 'français.'}, {'score': 0.00010808186718235663, 'start': 164, 'end': 209, 'answer': 'ingénieur hydraulicien et essayiste français.'}, {'score': 5.0453970530228015e-05, 'start': 153, 'end': 209, 'answer': 'urbaniste, ingénieur hydraulicien et essayiste français.'}, {'score': 4.455333667193265e-05, 'start': 190, 'end': 209, 'answer': 'essayiste français.'}]</code></p>
<p>When you look at the <a href=""https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/pipelines.py#L1172"" rel=""nofollow noreferrer"">code</a>, you can see QuestionAnsweringPipeline accepts an argument called topk.</p>
",2,3,962,2020-06-26 08:39:48,https://stackoverflow.com/questions/62591068/nlp-get-5-best-candidates-from-questionansweringpipeline
How to obtain contextual embedding for a phrase in a sentence using BERT?,"<p>I use <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a> to obtain sentence embedding from BERT. Using this I am able to obtain embedding for sentences or phrases. For example: I can get embedding of a sentence like <strong>&quot;system not working given to service center but no response on replacement&quot;</strong>. I can also get embedding of a phrase like <strong>&quot;no response&quot;</strong>.</p>
<p>However I want to get embedding of <strong>&quot;no response&quot;</strong> in the context of <strong>&quot;system not working given to service center but no response on replacement&quot;</strong>. Any pointers on how to obtain this will be helpful. Thanks in advance.</p>
<p>I am trying to do this because the phrase <strong>&quot;no response&quot;</strong> has different contexts in different sentences. For example the context of &quot;no response&quot; is different in the following two sentences:
<strong>&quot;system not working given to service center but no response on replacement&quot;
&quot;we tried recovery procedure on the patient but there was no response&quot;</strong></p>
","nlp, bert-language-model","<p>BERT returns one vector per input sub-word, so you need to get the vectors that correspond to the phrase you are interested in.</p>
<p>What is usually called a sentence embeddings is either the embedding of the technical symbol <code>[CLS]</code> that is prepended to the sentence before processing it with BERT; or an average of the contextual sub-word vectors. Because the <code>[CLS]</code> vector necessarily covers the entire sentence, you cannot get it just for a sub-phrase, but you can use the average of the sub-word embeddings of the phrase.</p>
<p>The package you are using, <code>sentence-transformers</code>, has a very simple user-friendly API, but I am afraid it is not strong enough to do this job. I'd suggest using <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface's Transormers</a>. This package allows you to view how the sentence got tokenized and thus obtain the corresponding vectors.</p>
",2,2,2405,2020-06-26 13:33:32,https://stackoverflow.com/questions/62595908/how-to-obtain-contextual-embedding-for-a-phrase-in-a-sentence-using-bert
How to store Word vector Embeddings?,"<p>I am using BERT Word Embeddings for sentence classification task with 3 labels. I am using Google Colab for coding. My problem is, since I will have to execute the embedding part every time I restart the kernel, is there any way to save these word embeddings once it is generated? Because, it takes a lot of time to generate those embeddings.</p>
<p>The code I am using to generate BERT Word Embeddings is -</p>
<pre><code>[get_features(text_list[i]) for text_list[i] in text_list]
</code></pre>
<p>Here, gen_features is a function which returns word embedding for each i in my list text_list.</p>
<p>I read that converting embeddings into bumpy tensors and then using np.save can do it. But I actually don't know how to code it.</p>
","python-3.x, keras, nlp, word-embedding, bert-language-model","<p>You can save your embeddings data to a numpy file by following these steps:</p>
<pre><code>all_embeddings = here_is_your_function_return_all_data()
all_embeddings = np.array(all_embeddings)
np.save('embeddings.npy', all_embeddings)
</code></pre>
<p>If you're saving into google colab, then you can download it to your local computer. Whenever you need it, just upload it and load it.</p>
<pre><code>all_embeddings = np.load('embeddings.npy')
</code></pre>
<p>That's it.</p>
<p>Btw, You can also directly save your file to google drive.</p>
",14,9,18076,2020-07-03 07:51:33,https://stackoverflow.com/questions/62710872/how-to-store-word-vector-embeddings
Summarization-Text rank algorithm,"<p>What are the advantages of using text rank algorithm for summarization over BERT summarization?
Even though both can be used as extractive summarization method, is there any particular advantage for text rank?</p>
","python, machine-learning, nlp, bert-language-model","<p>TextRank implementations tend to be lightweight and can run fast even with limited memory resources, while the transformer models such as <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a> tend to be rather large and require lots of memory. While the <a href=""https://www.tinyml.org/summit/"" rel=""nofollow noreferrer"">TinyML</a> community has outstanding work on techniques to make DL models run within limited resources, there may be a resource advantage for some use cases.</p>
<p>Some of the TextRank implementations can be &quot;directed&quot; by adding semantic relations, which one can consider as a priori structure to enrich the graph used -- or in some cases means of incorporating <em>human-in-the-loop</em> approaches. Those can provide advantages over supervised learning models which have been trained purely on data. Even so, there are similar efforts for DL in general (e.g., variations on the theme of <em>transfer learning</em>) from which transformers may benefit.</p>
<p>Another potential benefit is that TextRank approaches tend to be more <em>transparent</em>, while transformer models can be challenging in terms of <em>explainability</em>. There are tools that help greatly, but this concern becomes important in the context of <em>model bias and fairness</em>, <em>data ethics</em>, <em>regulatory compliance</em>, and so on.</p>
<p>Based on personal experience, while I'm the lead committer for one of the popular TextRank <a href=""https://github.com/DerwenAI/pytextrank"" rel=""nofollow noreferrer"">open source implementations</a>, I only use its <em>extractive summarization</em> features for use cases where a &quot;cheap and fast&quot; solution is needed. Otherwise I'd recommend considering more sophisticated approaches to summarization. For example, I recommend keeping watch on the ongoing research by the author of TextRank, <a href=""https://twitter.com/radamihalcea"" rel=""nofollow noreferrer"">Rada Mihalcea</a>, and her graduate students at U Michigan.</p>
<p>In terms of comparing <strong>&quot;Which text summarization methods work better?&quot;</strong> I'd point toward work on <em>abstractive summarization</em>, particularly recent work by <a href=""https://arxiv.org/pdf/1904.08455.pdf"" rel=""nofollow noreferrer"">John Bohannon, et al.</a>, at <a href=""https://primer.ai/"" rel=""nofollow noreferrer"">Primer</a>. For excellent examples, check the <a href=""https://covid19primer.com/dailybriefing"" rel=""nofollow noreferrer"">&quot;Daily Briefings&quot;</a> of CV19 research which their team generates using natural language understanding, knowledge graph, abstractive summarization, etc. Amy Heineike discusses their approach in <a href=""https://thedataexchange.media/machines-for-unlocking-the-deluge-of-covid-19-papers-articles-and-conversations/"" rel=""nofollow noreferrer"">&quot;Machines for unlocking the deluge of COVID-19 papers, articles, and conversations&quot;</a>.</p>
",4,2,1138,2020-07-04 16:15:14,https://stackoverflow.com/questions/62731497/summarization-text-rank-algorithm
Using Hugging Face Transformers library how can you POS_TAG French text,"<p>I am trying to POS_TAG French using the Hugging Face Transformers library. In English I was able to do so given a sentence like e.g:</p>
<blockquote>
<p>The weather is really great. So let us go for a walk.</p>
</blockquote>
<p>the result is:</p>
<pre><code>    token   feature
0   The     DET
1   weather NOUN
2   is      AUX
3   really  ADV
4   great   ADJ
5   .       PUNCT
6   So      ADV
7   let     VERB
8   us      PRON
9   go      VERB
10  for     ADP
11  a       DET
12  walk    NOUN
13  .       PUNCT
</code></pre>
<p>Does anyone have an idea how a similar thing could be achieved for French?</p>
<p>This is the code I used for the English version in a Jupyter notebook:</p>
<pre><code>!git clone https://github.com/bhoov/spacyface.git
!python -m spacy download en_core_web_sm

from transformers import pipeline
import numpy as np
import pandas as pd

nlp = pipeline('feature-extraction')
sequence = &quot;The weather is really great. So let us go for a walk.&quot;
result = nlp(sequence)
# Just displays the size of the embeddings. The sequence
# In this case there are 16 tokens and the embedding size is 768
np.array(result).shape

import sys
sys.path.append('spacyface')

from spacyface.aligner import BertAligner

alnr = BertAligner.from_pretrained(&quot;bert-base-cased&quot;)
tokens = alnr.meta_tokenize(sequence)
token_data = [{'token': tok.token, 'feature': tok.pos} for tok in tokens]
pd.DataFrame(token_data)
</code></pre>
<p>The output of this notebook is above.</p>
","python, nlp, huggingface-transformers, bert-language-model","<p>We have ended up training a model for POS Tagging (part of speech tagging) with the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">Hugging Face Transformers</a> library. The resulting model is available here:</p>
<p><a href=""https://huggingface.co/gilf/french-postag-model?text=En+Turquie%2C+Recep+Tayyip+Erdogan+ordonne+la+reconversion+de+Sainte-Sophie+en+mosqu%C3%A9e"" rel=""noreferrer"">https://huggingface.co/gilf/french-postag-model?text=En+Turquie%2C+Recep+Tayyip+Erdogan+ordonne+la+reconversion+de+Sainte-Sophie+en+mosqu%C3%A9e</a></p>
<p>You can basically see how it assigns POS tags on the webpage mentioned above. If you have the Hugging Face Transformers library installed you can try it out in a Jupyter notebook with this code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;gilf/french-postag-model&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;gilf/french-postag-model&quot;)

nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)
nlp_token_class('En Turquie, Recep Tayyip Erdogan ordonne la reconversion de Sainte-Sophie en mosquée')
</code></pre>
<p>This is the result on the console:</p>
<pre><code>[{'entity_group': 'PONCT', 'score': 0.11994100362062454, 'word': '[CLS]'},
{'entity_group': 'P', 'score': 0.9999570250511169, 'word': 'En'}, 
{'entity_group': 'NPP', 'score': 0.9998692870140076, 'word': 'Turquie'},
{'entity_group': 'PONCT', 'score': 0.9999769330024719, 'word': ','},
{'entity_group': 'NPP',   'score': 0.9996993020176888,  'word': 'Recep Tayyip Erdogan'},
{'entity_group': 'V', 'score': 0.9997997283935547, 'word': 'ordonne'},  
{'entity_group': 'DET', 'score': 0.9999586343765259, 'word': 'la'},
{'entity_group': 'NC', 'score': 0.9999251365661621, 'word': 'reconversion'},  
{'entity_group': 'P', 'score': 0.9999709129333496, 'word': 'de'},
{'entity_group': 'NPP', 'score': 0.9985082149505615, 'word': 'Sainte'},  
{'entity_group': 'PONCT', 'score': 0.9999614357948303, 'word': '-'},
{'entity_group': 'NPP', 'score': 0.9461128115653992, 'word': 'Sophie'},
{'entity_group': 'P', 'score': 0.9999079704284668, 'word': 'en'},
{'entity_group': 'NC', 'score': 0.8998225331306458, 'word': 'mosquée [SEP]'}]
</code></pre>
",8,2,4839,2020-07-07 18:52:36,https://stackoverflow.com/questions/62782001/using-hugging-face-transformers-library-how-can-you-pos-tag-french-text
HuggingFace for Japanese tokenizer,"<p>I recently tested on the below code based on the source:
<a href=""https://github.com/cl-tohoku/bert-japanese/blob/master/masked_lm_example.ipynb"" rel=""nofollow noreferrer"">https://github.com/cl-tohoku/bert-japanese/blob/master/masked_lm_example.ipynb</a></p>
<pre><code>import torch 
from transformers.tokenization_bert_japanese import BertJapaneseTokenizer
from transformers.modeling_bert import BertForMaskedLM

tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')

input_ids = tokenizer.encode(f'''
    青葉山で{tokenizer.mask_token}の研究をしています。
''', return_tensors='pt')
</code></pre>
<p>when i try to encode it, I received error such as:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-29-f8582275f4db&gt; in &lt;module&gt;
      1 input_ids = tokenizer.encode(f'''
      2     青葉山で{tokenizer.mask_token}の研究をしています。
----&gt; 3 ''', return_tensors='pt')

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in encode(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)
   1428             stride=stride,
   1429             return_tensors=return_tensors,
-&gt; 1430             **kwargs,
   1431         )
   1432 

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in encode_plus(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   1740             return_length=return_length,
   1741             verbose=verbose,
-&gt; 1742             **kwargs,
   1743         )
   1744 

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils.py in _encode_plus(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    452             )
    453 
--&gt; 454         first_ids = get_input_ids(text)
    455         second_ids = get_input_ids(text_pair) if text_pair is not None else None
    456 

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils.py in get_input_ids(text)
    423         def get_input_ids(text):
    424             if isinstance(text, str):
--&gt; 425                 tokens = self.tokenize(text, **kwargs)
    426                 return self.convert_tokens_to_ids(tokens)
    427             elif isinstance(text, (list, tuple)) and len(text) &gt; 0 and isinstance(text[0], str):

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils.py in tokenize(self, text, **kwargs)
    362 
    363         no_split_token = self.unique_no_split_tokens
--&gt; 364         tokenized_text = split_on_tokens(no_split_token, text)
    365         return tokenized_text
    366 

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils.py in split_on_tokens(tok_list, text)
    356                     (
    357                         self._tokenize(token) if token not in self.unique_no_split_tokens else [token]
--&gt; 358                         for token in tokenized_text
    359                     )
    360                 )

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_utils.py in &lt;genexpr&gt;(.0)
    356                     (
    357                         self._tokenize(token) if token not in self.unique_no_split_tokens else [token]
--&gt; 358                         for token in tokenized_text
    359                     )
    360                 )

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_bert_japanese.py in _tokenize(self, text)
    153     def _tokenize(self, text):
    154         if self.do_word_tokenize:
--&gt; 155             tokens = self.word_tokenizer.tokenize(text, never_split=self.all_special_tokens)
    156         else:
    157             tokens = [text]

~/.pyenv/versions/3.7.0/envs/personal/lib/python3.7/site-packages/transformers/tokenization_bert_japanese.py in tokenize(self, text, never_split, **kwargs)
    205                 break
    206 
--&gt; 207             token, _ = line.split(&quot;\t&quot;)
    208             token_start = text.index(token, cursor)
    209             token_end = token_start + len(token)

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>Does anyone experienced this before? I tried a lot of different ways and refer to many posts but all using the same methods and no explanations, I just wanted to test multiple languages, other languages seem to work fine but not with japanese and I dont know why.</p>
","python, cjk, bert-language-model","<p>NOTE: Shortly after this question I released a version of IPADic that works with the latest versions of mecab-python3. You should be able to fix things by installing <code>transformers[ja]</code>, which will install the main dictionaries used with HuggingFace models.</p>
<hr />
<p>I'm the mecab-python3 maintainer. Transformers relies on the bundled dictionary in versions prior to 1.0, which has been removed because it's old. I will be adding it as an option in a release soon, but in the meantime you can install an old version.</p>
<p>The command posted by vivasra doesn't work because it specifies a version of a different package (notice there's no &quot;3&quot; in the package name) that doesn't exist. You can use this:</p>
<pre><code>pip install mecab-python3=0.996.5
</code></pre>
<p>If you still have trouble please open an issue <a href=""https://github.com/SamuraiT/mecab-python3/issues"" rel=""nofollow noreferrer"">on Github</a>.</p>
",2,2,2160,2020-07-12 11:49:55,https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer
How does BertForSequenceClassification classify on the CLS vector?,"<p><strong>Background:</strong></p>
<p>Following along with this <a href=""https://stackoverflow.com/questions/60876394/does-bertforsequenceclassification-classify-on-the-cls-vector"">question</a> when using bert to classify sequences the model uses the &quot;[CLS]&quot; token representing the classification task. According to the paper:</p>
<blockquote>
<p>The first token of every sequence is always a special classification
token ([CLS]). The final hidden state corresponding to this token is
used as the aggregate sequence representation for classification
tasks.</p>
</blockquote>
<p>Looking at the huggingfaces repo their BertForSequenceClassification utilizes the bert pooler method:</p>
<pre><code>class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We &quot;pool&quot; the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
</code></pre>
<p>We can see they take the first token (CLS) and use this as a representation for the whole sentence. Specifically they perform <code>hidden_states[:, 0]</code> which looks a lot like its taking the first element from each state rather than taking the first tokens hidden state?</p>
<p><strong>My Question:</strong></p>
<p>What I don't understand is how do they encode the information from the entire sentence into this token? Is the CLS token a regular token which has its own embedding vector that &quot;learns&quot; the sentence level representation? Why can't we just use the average of the hidden states (the output of the encoder) and use this to classify?</p>
<p><strong>EDIT</strong>: After thinking a little about it: Because we use the CLS tokens hidden state to predict, is the CLS tokens embedding being trained on the task of classification as this is the token being used to classify (thus being the major contributor to the error which gets propagated to its weights?)</p>
","python, transformer-model, huggingface-transformers, bert-language-model","<blockquote>
<p>Is the CLS token a regular token which has its own embedding vector that &quot;learns&quot; the sentence level representation?</p>
</blockquote>
<p>Yes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

clsToken = tokenizer.convert_tokens_to_ids('[CLS]') 
print(clsToken)
#or
print(tokenizer.cls_token, tokenizer.cls_token_id)

print(model.get_input_embeddings()(torch.tensor(clsToken)))
</code></pre>
<p>Output:</p>
<pre><code>101
[CLS] 101
tensor([ 1.3630e-02, -2.6490e-02, -2.3503e-02, -7.7876e-03,  8.5892e-03,
        -7.6645e-03, -9.8808e-03,  6.0184e-03,  4.6921e-03, -3.0984e-02,
         1.8883e-02, -6.0093e-03, -1.6652e-02,  1.1684e-02, -3.6245e-02,
         ...
         5.4162e-03, -3.0037e-02,  8.6773e-03, -1.7942e-03,  6.6826e-03,
        -1.1929e-02, -1.4076e-02,  1.6709e-02,  1.6860e-03, -3.3842e-03,
         8.6805e-03,  7.1340e-03,  1.5147e-02], grad_fn=&lt;EmbeddingBackward&gt;)
</code></pre>
<p>You can get a list of all other special tokens for your model with:</p>
<pre class=""lang-py prettyprint-override""><code>print(tokenizer.all_special_tokens)
</code></pre>
<p>Output:</p>
<pre><code>['[CLS]', '[UNK]', '[PAD]', '[SEP]', '[MASK]']
</code></pre>
<blockquote>
<p>What I don't understand is how do they encode the information from the
entire sentence into this token?</p>
</blockquote>
<p>and</p>
<blockquote>
<p>Because we use the CLS tokens hidden state to predict, is the CLS
tokens embedding being trained on the task of classification as this
is the token being used to classify (thus being the major contributor
to the error which gets propagated to its weights?)</p>
</blockquote>
<p>Also yes. As you have already stated in your question <a href=""https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_bert.py#L1227"" rel=""noreferrer"">BertForSequenceClassification</a> utilizes the <a href=""https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_bert.py#L476"" rel=""noreferrer"">BertPooler</a> to train the linear layer on top of Bert:</p>
<pre class=""lang-py prettyprint-override""><code>#outputs contains the output of BertModel and the second element is the pooler output
pooled_output = outputs[1]

pooled_output = self.dropout(pooled_output)
logits = self.classifier(pooled_output)

#...loss calculation based on logits and the given labels
</code></pre>
<blockquote>
<p>Why can't we just use the average of the hidden states (the output of
the encoder) and use this to classify?</p>
</blockquote>
<p>I can't really answer this in general, but why do you think this would be easier or better as a linear layer? You also need to train the hidden layers to produce an output where the average maps to your class. Therefore you also need an &quot;average layer&quot; to be the major contributor to your loss. In general when you can show that it leads to better results instead of the current approach, nobody will reject it.</p>
",7,5,3388,2020-07-17 20:14:29,https://stackoverflow.com/questions/62961194/how-does-bertforsequenceclassification-classify-on-the-cls-vector
"OSError: Error no file named [&#39;pytorch_model.bin&#39;, &#39;tf_model.h5&#39;, &#39;model.ckpt.index&#39;]","<p>When I load the BERT pretrained model online I get this error <code>OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory uncased_L-12_H-768_A-12 or 'from_tf' set to False</code> what should I do?</p>
","python, tensorflow, pytorch, bert-language-model","<p>Here is what I found. Go to the following link, and click the circled to download, rename it to <code>pytorch_model.bin</code>, and drop it to the directory of <code>biobert-nli</code>,  then the issue is resolved. Didn't figure out how to clone from the link.</p>
<p><a href=""https://huggingface.co/gsarti/biobert-nli/tree/main"" rel=""noreferrer"">https://huggingface.co/gsarti/biobert-nli/tree/main</a></p>
<p><a href=""https://i.sstatic.net/BDvux.png?s=256"" rel=""noreferrer""><img src=""https://i.sstatic.net/BDvux.png?s=256"" alt=""enter image description here"" /></a></p>
",7,12,56310,2020-07-17 20:52:48,https://stackoverflow.com/questions/62961627/oserror-error-no-file-named-pytorch-model-bin-tf-model-h5-model-ckpt-in
Sliding window for long text in BERT for Question Answering,"<p>I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.</p>
<p>From what I understand if the input are too long, sliding window can be used to process the text.</p>
<p>Please correct me if I am wrong.
Say I have a text <em><strong>&quot;In June 2017 Kaggle announced that it passed 1 million registered users&quot;</strong></em>.</p>
<p>Given some <code>stride</code> and <code>max_len</code>, the input can be split into chunks with over lapping words (not considering padding).</p>
<pre><code>In June 2017 Kaggle announced that # chunk 1
announced that it passed 1 million # chunk 2
1 million registered users # chunk 3
</code></pre>
<p>If my questions were <em><strong>&quot;when did Kaggle make the announcement&quot;</strong></em> and <em><strong>&quot;how many registered users&quot;</strong></em> I can use <code>chunk 1</code> and <code>chunk 3</code> and <strong>not use</strong> <code>chunk 2</code> <strong>at all</strong> in the model. Not quiet sure if I should still use <code>chunk 2</code> to train the model</p>
<p>So the input will be:
<code>[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]</code>
and
<code>[CLS]how many registered users[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as <strong>-1</strong>? For example <em><strong>&quot;can pigs fly?&quot;</strong></em></p>
<p><code>[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]announced that it passed 1 million[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>As suggested in the comments, II tried to run <code>squad_convert_example_to_features</code> (<a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L134"" rel=""noreferrer"">source code</a>) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like <code>run_squad.py</code> from huggingface uses <code>squad_convert_example_to_features</code> with the <code>s</code> in <code>example</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
</code></pre>
<p>I get the error.</p>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 159.95it/s]
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 25, in &lt;module&gt;
    sub_tokens = tokenizer.tokenize(token)
NameError: name 'tokenizer' is not defined
</code></pre>
<p>The error indicates that there are no <code>tokenizers</code> but it does not allow us to pass a <code>tokenizer</code>. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the <code>squad_convert_example_to_features</code> function?</p>
","nlp, text-classification, huggingface-transformers, nlp-question-answering, bert-language-model","<p>I think there is a problem with the examples you pick. Both <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L273"" rel=""nofollow noreferrer"">squad_convert_examples_to_features</a> and <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L86"" rel=""nofollow noreferrer"">squad_convert_example_to_features</a> have a sliding window approach implemented because <code>squad_convert_examples_to_features</code> is just a parallelization wrapper for <code>squad_convert_example_to_features</code>. But let's look at the single example function. First of all you need to call <a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L268"" rel=""nofollow noreferrer"">squad_convert_example_to_features_init</a> to make the tokenizer global (this is done automatically for you in <code>squad_convert_examples_to_features</code>):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features, squad_convert_example_to_features_init
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
squad_convert_example_to_features_init(tokenizer)

processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
print(len(features))
</code></pre>
<p>Output:</p>
<pre><code>1
</code></pre>
<p>You might say that this function is not using a sliding window approach, but this is wrong because your example doesn't needed to be split:</p>
<pre class=""lang-py prettyprint-override""><code>print(len(examples[0].question_text.split()) + len(examples[0].doc_tokens))
</code></pre>
<p>Output:</p>
<pre><code>115
</code></pre>
<p>which is less as the max_seq_length which you have set to 384. Now lets try a different one:</p>
<pre><code>print(len(examples[129603].question_text.split()) + len(examples[129603].doc_tokens))

features = squad_convert_example_to_features(
    example=examples[129603],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
print(len(features))
</code></pre>
<p>Output:</p>
<pre><code>454
3
</code></pre>
<p>Which you can now compare with the original sample:</p>
<pre class=""lang-py prettyprint-override""><code>print('[CLS]' + examples[129603].question_text + '[SEP]' + ' '.join(examples[129603].doc_tokens) + '[SEP]')

for idx, f in enumerate(features):
    print('Split {}'.format(idx))
    print(' '.join(f.tokens))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]How often is hunting occurring in Delaware each year?[SEP]There is a very active tradition of hunting of small to medium-sized wild game in Trinidad and Tobago. Hunting is carried out with firearms, and aided by the use of hounds, with the illegal use of trap guns, trap cages and snare nets. With approximately 12,000 sport hunters applying for hunting licences in recent years (in a very small country of about the size of the state of Delaware at about 5128 square kilometers and 1.3 million inhabitants), there is some concern that the practice might not be sustainable. In addition there are at present no bag limits and the open season is comparatively very long (5 months - October to February inclusive). As such hunting pressure from legal hunters is very high. Added to that, there is a thriving and very lucrative black market for poached wild game (sold and enthusiastically purchased as expensive luxury delicacies) and the numbers of commercial poachers in operation is unknown but presumed to be fairly high. As a result, the populations of the five major mammalian game species (red-rumped agouti, lowland paca, nine-banded armadillo, collared peccary, and red brocket deer) are thought to be quite low (although scientifically conducted population studies are only just recently being conducted as of 2013). It appears that the red brocket deer population has been extirpated on Tobago as a result of over-hunting. Various herons, ducks, doves, the green iguana, the gold tegu, the spectacled caiman and the common opossum are also commonly hunted and poached. There is also some poaching of 'fully protected species', including red howler monkeys and capuchin monkeys, southern tamanduas, Brazilian porcupines, yellow-footed tortoises, Trinidad piping guans and even one of the national birds, the scarlet ibis. Legal hunters pay very small fees to obtain hunting licences and undergo no official basic conservation biology or hunting-ethics training. There is presumed to be relatively very little subsistence hunting in the country (with most hunting for either sport or commercial profit). The local wildlife management authority is under-staffed and under-funded, and as such very little in the way of enforcement is done to uphold existing wildlife management laws, with hunting occurring both in and out of season, and even in wildlife sanctuaries. There is some indication that the government is beginning to take the issue of wildlife management more seriously, with well drafted legislation being brought before Parliament in 2015. It remains to be seen if the drafted legislation will be fully adopted and financially supported by the current and future governments, and if the general populace will move towards a greater awareness of the importance of wildlife conservation and change the culture of wanton consumption to one of sustainable management.[SEP]
Split 0
[CLS] how often is hunting occurring in delaware each year ? [SEP] there is a very active tradition of hunting of small to medium - sized wild game in trinidad and tobago . hunting is carried out with firearms , and aided by the use of hounds , with the illegal use of trap guns , trap cages and s ##nare nets . with approximately 12 , 000 sport hunters applying for hunting licence ##s in recent years ( in a very small country of about the size of the state of delaware at about 512 ##8 square kilometers and 1 . 3 million inhabitants ) , there is some concern that the practice might not be sustainable . in addition there are at present no bag limits and the open season is comparatively very long ( 5 months - october to february inclusive ) . as such hunting pressure from legal hunters is very high . added to that , there is a thriving and very lucrative black market for po ##ache ##d wild game ( sold and enthusiastically purchased as expensive luxury del ##ica ##cies ) and the numbers of commercial po ##ache ##rs in operation is unknown but presumed to be fairly high . as a result , the populations of the five major mammalian game species ( red - rum ##ped ago ##uti , lowland pac ##a , nine - banded arm ##adi ##llo , collar ##ed pe ##cca ##ry , and red brock ##et deer ) are thought to be quite low ( although scientific ##ally conducted population studies are only just recently being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , [SEP]
Split 1
[CLS] how often is hunting occurring in delaware each year ? [SEP] october to february inclusive ) . as such hunting pressure from legal hunters is very high . added to that , there is a thriving and very lucrative black market for po ##ache ##d wild game ( sold and enthusiastically purchased as expensive luxury del ##ica ##cies ) and the numbers of commercial po ##ache ##rs in operation is unknown but presumed to be fairly high . as a result , the populations of the five major mammalian game species ( red - rum ##ped ago ##uti , lowland pac ##a , nine - banded arm ##adi ##llo , collar ##ed pe ##cca ##ry , and red brock ##et deer ) are thought to be quite low ( although scientific ##ally conducted population studies are only just recently being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , trinidad pip ##ing gu ##ans and even one of the national birds , the scarlet ib ##is . legal hunters pay very small fees to obtain hunting licence ##s and undergo no official basic conservation biology or hunting - ethics training . there is presumed to be relatively very little subsistence hunting in the country ( with most hunting for either sport or commercial profit ) . the local wildlife management authority is under - staffed and under - funded , and as such very little in the way of enforcement is done to uphold existing wildlife management laws , with hunting occurring both in and out of season , and even in wildlife san ##ct ##uaries . there is some indication that the government is beginning to [SEP]
Split 2
[CLS] how often is hunting occurring in delaware each year ? [SEP] being conducted as of 2013 ) . it appears that the red brock ##et deer population has been ex ##ti ##rp ##ated on tobago as a result of over - hunting . various heron ##s , ducks , dove ##s , the green i ##gua ##na , the gold te ##gu , the spectacle ##d cai ##man and the common op ##oss ##um are also commonly hunted and po ##ache ##d . there is also some po ##achi ##ng of ' fully protected species ' , including red howl ##er monkeys and cap ##uchi ##n monkeys , southern tam ##and ##ua ##s , brazilian por ##cup ##ines , yellow - footed tor ##to ##ises , trinidad pip ##ing gu ##ans and even one of the national birds , the scarlet ib ##is . legal hunters pay very small fees to obtain hunting licence ##s and undergo no official basic conservation biology or hunting - ethics training . there is presumed to be relatively very little subsistence hunting in the country ( with most hunting for either sport or commercial profit ) . the local wildlife management authority is under - staffed and under - funded , and as such very little in the way of enforcement is done to uphold existing wildlife management laws , with hunting occurring both in and out of season , and even in wildlife san ##ct ##uaries . there is some indication that the government is beginning to take the issue of wildlife management more seriously , with well drafted legislation being brought before parliament in 2015 . it remains to be seen if the drafted legislation will be fully adopted and financially supported by the current and future governments , and if the general populace will move towards a greater awareness of the importance of wildlife conservation and change the culture of want ##on consumption to one of sustainable management . [SEP]
</code></pre>
<hr />
<blockquote>
<p>If my questions were &quot;when did Kaggle make the announcement&quot; and &quot;how
many registered users&quot; I can use chunk 1 and chunk 3 and not use chunk
2 at all in the model. Not quiet sure if I should still use chunk 2 to
train the model</p>
</blockquote>
<p>Yes you should also use chunk 2 to train your model, because when you try to predict the same sequence you hope that your model predicts 0:0 as answer span for chunk 2 (i.e. you can easily select the chunk which contains the answer).</p>
",3,6,8557,2020-07-19 10:18:32,https://stackoverflow.com/questions/62978957/sliding-window-for-long-text-in-bert-for-question-answering
How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?,"<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForMaskedLM
# Load pre-trained model (weights)
with torch.no_grad():
    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')
    model.eval()
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')
    sentence = &quot;我不会忘记和你一起奋斗的时光。&quot;
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    sen_len = len(tokenize_input)
    sentence_loss = 0.

    for i, word in enumerate(tokenize_input):
        # add mask to i-th character of the sentence
        tokenize_input[i] = '[MASK]'
        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

        output = model(mask_input)

        prediction_scores = output[0]
        softmax = nn.Softmax(dim=0)
        ps = softmax(prediction_scores[0, i]).log()
        word_loss = ps[tensor_input[0, i]]
        sentence_loss += word_loss.item()

        tokenize_input[i] = word
    ppl = np.exp(-sentence_loss/sen_len)
    print(ppl)
</code></pre>
<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>
<pre><code>if masked_lm_labels is not None:
    loss_fct = CrossEntropyLoss()  # -100 index = padding token
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), 
    masked_lm_labels.view(-1))
    outputs = (masked_lm_loss,) + outputs
</code></pre>
","nlp, pytorch, transformer-model, huggingface-transformers, bert-language-model","<p>Yes, you can use the  parameter <code>labels</code> (or <code>masked_lm_labels</code>, I think the param name varies in versions of huggingface transformers, whatever) to specify the masked token position, and use <code>-100</code> to ignore the tokens that you dont want to include in the loss computing.
For example,</p>
<pre class=""lang-py prettyprint-override""><code>sentence='我爱你'
from transformers import BertTokenizer, BertForMaskedLM
import torch
import numpy as np

tokenizer = BertTokenizer(vocab_file='vocab.txt')
model = BertForMaskedLM.from_pretrained('bert-base-chinese')

tensor_input = tokenizer(sentence, return_tensors='pt')
# tensor([[ 101, 2769, 4263,  872,  102]])

repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
# tensor([[ 101, 2769, 4263,  872,  102],
#         [ 101, 2769, 4263,  872,  102],
#         [ 101, 2769, 4263,  872,  102]])

mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
# tensor([[0., 1., 0., 0., 0.],
#         [0., 0., 1., 0., 0.],
#         [0., 0., 0., 1., 0.]])

masked_input = repeat_input.masked_fill(mask == 1, 103)
# tensor([[ 101,  103, 4263,  872,  102],
#         [ 101, 2769,  103,  872,  102],
#         [ 101, 2769, 4263,  103,  102]])

labels = repeat_input.masked_fill( masked_input != 103, -100)
# tensor([[-100, 2769, -100, -100, -100],
#         [-100, -100, 4263, -100, -100],
#         [-100, -100, -100,  872, -100]])

loss,_ = model(masked_input, masked_lm_labels=labels)

score = np.exp(loss.item())
</code></pre>
<p>The function:</p>
<pre class=""lang-py prettyprint-override""><code>def score(model, tokenizer, sentence,  mask_token_id=103):
  tensor_input = tokenizer.encode(sentence, return_tensors='pt')
  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
  masked_input = repeat_input.masked_fill(mask == 1, 103)
  labels = repeat_input.masked_fill( masked_input != 103, -100)
  loss,_ = model(masked_input, masked_lm_labels=labels)
  result = np.exp(loss.item())
  return result

score(model, tokenizer, '我爱你') # returns 45.63794545581973
</code></pre>
",8,9,8634,2020-07-22 09:07:00,https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence
How to extract and use BERT encodings of sentences for Text similarity among sentences. (PyTorch/Tensorflow),"<p>I want to make a text similarity model which I tend to use for FAQ finding and other methods to get the most related text. I want to use the highly optimised <code>BERT</code> model for this NLP task .I tend to use the the encodings of all the sentences to get a similarity matrix using the <code>cosine_similarity</code> and return results.</p>
<p>In the hypothetical conditions, if I have two sentences as <code>hello world</code> and <code>hello hello world</code> then I am assuming the BRT would give me something like <code>[0.2,0.3,0]</code>, (0 for padding) and <code>[0.2,0.2,0.3]</code> and I can pass these two inside the <code>sklearn's</code> <code>cosine_similarity</code>.</p>
<p>How am I supposed to extract the embeddings the sentences to use them in the model? I found somewhere that it can be extracted like:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
input_ids = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
Using Tensorflow:

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')
input_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
</code></pre>
<p>Is this the right way? Because I read somewhere that there are different type of embeddings that BERT offers.</p>
<p><strong>ALSO please do suggest any other method to find the text similarity</strong></p>
","tensorflow, deep-learning, nlp, pytorch, bert-language-model","<p>When you want to compare the embeddings of sentences the recommended way to do this with BERT is to use the value of the CLS token. This corresponds to the first token of the output (after the batch dimension).</p>
<pre><code>last_hidden_states = outputs[0]
cls_embedding = last_hidden_states[0][0]
</code></pre>
<p>This will give you one embedding for the entire sentence. As you will have the same size embedding for each sentence you can then easily compute the cosine similarity.</p>
<p>If you do not get satisfactory results using the CLS token you can also try averaging the output embedding for each word in the sentence.</p>
",2,1,3212,2020-07-22 18:31:52,https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen
Where can I get the pretrained word embeddinngs for BERT?,"<p>I know that BERT has total vocabulary size of 30522 which contains some words and subwords. I want to get the initial input embeddings of BERT. So, my requirement is to get the table of size <code>[30522, 768]</code> to which I can index by token id to get its embeddings. Where can I get this table?</p>
","embedding, huggingface-transformers, bert-language-model","<p>The BertModels have <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.get_input_embeddings"" rel=""noreferrer"">get_input_embeddings()</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert = BertModel.from_pretrained('bert-base-uncased')

token_embedding = {token: bert.get_input_embeddings()(torch.tensor(id))  for token, id in tokenizer.get_vocab().items()}

print(len(token_embedding))
print(token_embedding['[CLS]'])
</code></pre>
<p>Output:</p>
<pre><code>30522
tensor([ 1.3630e-02, -2.6490e-02, -2.3503e-02, -7.7876e-03,  8.5892e-03,
        -7.6645e-03, -9.8808e-03,  6.0184e-03,  4.6921e-03, -3.0984e-02,
         1.8883e-02, -6.0093e-03, -1.6652e-02,  1.1684e-02, -3.6245e-02,
         8.3482e-03, -1.2112e-03,  1.0322e-02,  1.6692e-02, -3.0354e-02,
        -1.2372e-02, -2.5173e-02, -8.9602e-03,  8.1994e-03, -2.0011e-02,
        -1.5901e-02, -3.8394e-03,  1.4241e-03,  7.0500e-03,  1.6092e-03,
        -2.7764e-03,  9.4931e-03, -2.2768e-02,  1.9317e-02, -1.3442e-02,
        -2.3763e-02, -1.4617e-02,  9.7735e-03, -2.2428e-03,  3.0642e-02,
         6.7829e-03, -2.6471e-03, -1.8553e-02, -1.2363e-02,  7.6489e-03,
        -2.5461e-03, -3.1498e-01,  6.3761e-03,  4.8914e-02, -7.7636e-03,
         6.0919e-02,  2.1346e-02, -3.9741e-02,  2.2853e-01,  2.6502e-02,
        -1.0144e-03, -7.8480e-03, -1.9995e-03,  1.7057e-02, -3.3270e-02,
         4.5421e-03,  6.1751e-03, -1.0077e-01, -2.0973e-02, -1.4512e-04,
        -9.6657e-03,  1.0871e-02, -1.4786e-02,  2.6437e-04,  2.1166e-02,
         1.6492e-02, -5.1928e-03, -1.1857e-02, -9.9159e-03, -1.4363e-02,
        -1.2405e-02, -1.2973e-02,  2.6778e-02, -1.0986e-02,  1.0572e-02,
        -2.5566e-02,  5.2494e-03,  1.5890e-02, -5.1504e-03, -7.5859e-03,
         2.0259e-02, -7.0155e-03,  1.6359e-02,  1.7487e-02,  5.4297e-03,
        -8.6403e-03,  2.8821e-02, -7.8964e-03,  1.9259e-02,  2.3868e-02,
        -4.3472e-03,  5.5662e-02, -2.1940e-02,  4.1779e-03, -5.7216e-03,
         2.6712e-02, -5.0371e-03,  2.4923e-02, -1.3429e-02, -8.4337e-03,
         9.8188e-02, -1.2940e-03,  1.2865e-02, -1.5930e-03,  3.6437e-03,
         1.5569e-02,  1.8620e-02, -9.0643e-03, -1.9740e-02,  1.0530e-02,
        -2.7359e-03, -7.5283e-03,  1.1492e-03,  2.6162e-03, -6.2757e-03,
        -8.6096e-03,  6.6221e-01, -3.2235e-03, -4.1309e-02,  3.3047e-03,
        -2.5040e-03,  1.2838e-04, -6.8073e-03,  6.0291e-03, -9.8468e-03,
         8.0641e-03, -1.9815e-03,  2.5801e-02,  5.7429e-03, -1.0712e-02,
         2.9176e-02,  5.9414e-03,  2.4795e-02, -1.7887e-02,  7.3183e-01,
         1.0964e-02,  5.9942e-03, -4.6157e-02,  4.0131e-02, -9.7481e-03,
        -8.9496e-01,  1.6385e-02, -1.9816e-03,  1.4691e-02, -1.9837e-02,
        -1.7611e-02, -4.5263e-04, -1.8605e-02, -1.5660e-02, -1.0709e-02,
         1.8016e-02, -3.4149e-03, -1.2632e-02,  4.2877e-03, -3.9169e-01,
         1.0016e-02, -1.0955e-02,  4.5133e-03, -5.1150e-03,  4.9968e-03,
         1.7852e-02,  1.1313e-02,  2.6519e-03,  3.3658e-01, -1.8168e-02,
         1.3170e-02,  7.3927e-03,  5.2521e-03, -9.6230e-03,  1.2844e-02,
         4.1554e-01, -9.7247e-03, -4.2439e-03,  5.5287e-04,  1.8271e-02,
        -1.3889e-03, -2.0502e-03, -8.1946e-03, -6.5979e-06, -7.2764e-04,
        -1.4625e-03, -6.9872e-03, -6.9633e-03, -8.0701e-03,  1.9936e-02,
         4.8370e-03,  8.6883e-03, -4.9246e-02, -2.0028e-02,  1.4124e-03,
         1.0444e-02, -1.1236e-02, -4.4654e-03, -2.0491e-02, -2.7654e-02,
        -3.7079e-02,  1.3215e-02,  6.9498e-02, -3.1109e-02,  7.0562e-03,
         1.0887e-02, -7.8090e-03, -1.0501e-02, -4.8735e-03, -6.8399e-04,
         1.4717e-02,  4.4342e-03,  1.6012e-02, -1.0427e-02, -2.5767e-02,
        -2.2699e-01,  8.6569e-02,  2.3453e-02,  4.6362e-02,  3.5609e-03,
         2.1353e-02,  2.3703e-02, -2.0252e-02,  2.1580e-02,  7.2652e-03,
         2.0933e-01,  1.2108e-02,  1.0869e-02,  7.0568e-03, -3.1132e-02,
         2.0505e-02,  3.2248e-03, -2.2724e-03,  5.5342e-03,  3.0563e-03,
         1.9542e-02,  1.2827e-03,  1.5952e-02, -1.5458e-02, -3.8455e-03,
        -4.9417e-03, -1.0446e-02,  7.0516e-03,  2.2467e-03, -9.3643e-03,
         1.9163e-02,  1.4239e-02, -1.5816e-02,  8.7413e-03,  2.4737e-02,
        -7.3777e-03, -4.0975e-02,  9.4948e-03,  1.4700e-02,  2.6819e-02,
         1.0706e-02,  1.0621e-02, -7.1816e-03, -8.5402e-03,  1.2261e-02,
        -4.8679e-03, -9.6136e-03,  7.8765e-04,  3.8504e-02, -7.7485e-03,
        -6.5018e-03,  3.4352e-03,  2.2931e-04,  5.7456e-03, -4.8441e-03,
        -9.0898e-03,  8.6298e-03,  5.4740e-03,  2.2274e-02, -2.1218e-02,
        -2.6795e-02, -3.5337e-03,  1.0785e-02,  1.2475e-02, -6.1160e-03,
         1.0729e-02, -9.7955e-03,  1.8543e-02, -6.0488e-03, -4.5744e-03,
         2.7089e-03,  1.5632e-02, -1.2928e-02, -3.0778e-03, -1.0325e-02,
        -7.9550e-03, -6.3065e-02,  2.1062e-02, -6.6717e-03,  8.4616e-03,
         1.4475e-02,  1.1477e-01, -2.2838e-02, -3.7491e-02, -3.6218e-02,
        -3.1994e-02, -8.9252e-03,  3.1720e-02, -1.1260e-02, -1.2980e-01,
        -1.0315e-03, -4.7242e-03, -2.0092e-02, -9.4521e-01, -2.2178e-02,
        -4.4297e-04,  1.9711e-02,  3.3402e-02, -1.0513e-02,  1.4492e-02,
        -1.9697e-02, -9.8452e-03, -1.7347e-02,  2.3472e-02,  7.6570e-02,
         1.9504e-02,  9.3617e-03,  8.2672e-03, -1.0471e-02, -1.9932e-03,
         2.0000e-02,  2.0485e-02,  1.0977e-02,  1.7720e-02,  1.3532e-02,
         7.3682e-03,  3.4906e-04,  1.8772e-03,  1.9976e-02, -3.2041e-02,
        -8.9169e-03,  1.2900e-02, -1.3331e-02,  6.6207e-03, -5.7063e-03,
        -1.1482e-02,  8.3907e-03, -6.4162e-03,  1.5816e-02,  7.8921e-03,
         4.4177e-03,  2.2568e-02,  1.0239e-02, -3.0194e-04,  1.3294e-02,
        -2.1606e-02,  3.8832e-03,  2.4475e-02,  4.3808e-02, -2.1031e-03,
        -1.2163e-02, -4.0786e-02,  1.5565e-02,  1.4750e-02,  1.6645e-02,
         2.8083e-02,  1.8920e-03, -1.4733e-04, -2.6208e-02,  2.3780e-02,
         1.8657e-04, -2.2931e-03,  3.0334e-03, -1.7294e-02, -2.3001e-02,
         8.6004e-03, -3.3497e-02,  2.5660e-02, -1.9225e-02, -2.7186e-02,
        -2.1020e-02, -3.5213e-02, -1.8228e-03, -8.2840e-03,  1.1212e-02,
         1.0387e-02, -3.4194e-01, -1.9705e-03,  1.1558e-02,  5.1976e-03,
         7.4498e-03,  5.7142e-03,  2.8401e-02, -7.7551e-03,  1.0682e-02,
        -1.2657e-02, -1.8065e-02,  2.6681e-03,  3.3947e-03, -4.5565e-02,
        -2.1170e-02, -1.7830e-02,  3.4679e-03, -2.2051e-02, -5.4176e-03,
        -1.1517e-02, -3.4155e-02, -3.0335e-03, -1.3915e-02,  6.2173e-03,
        -1.1101e-02, -1.5308e-02,  9.2188e-03, -7.5665e-03,  6.5685e-03,
         8.0935e-03,  3.1139e-03, -5.5047e-03, -3.1347e-02,  2.2140e-02,
         1.0865e-02, -2.7849e-02, -4.9580e-03,  1.8804e-03,  1.0007e-01,
        -1.8013e-03, -4.8792e-03,  1.5534e-02, -2.0179e-02, -1.2351e-02,
        -1.3871e-02,  1.1439e-02, -9.0208e-03,  1.2580e-02, -2.5973e-02,
        -2.0398e-02, -1.9464e-03,  4.3189e-03,  2.0707e-02,  5.0029e-03,
        -1.0679e-02,  1.2298e-02,  1.0269e-02,  2.2228e-02,  2.9754e-02,
        -2.6392e-03,  1.9286e-02, -1.5137e-02,  2.1914e-01,  1.3030e-02,
        -7.4460e-03, -9.6818e-04,  2.9736e-02,  9.8722e-03, -5.6688e-03,
         4.2518e-03,  1.8941e-02, -6.3909e-03,  8.0590e-03, -6.7893e-03,
         6.0878e-03, -5.3970e-03,  7.5776e-04,  1.1374e-03, -5.0035e-03,
        -1.6159e-03,  1.6764e-02,  9.1251e-03,  1.3020e-02, -1.0368e-02,
         2.2141e-02, -2.5411e-03, -1.5227e-02,  2.3444e-02,  8.4076e-04,
        -1.1465e-01,  2.7017e-03, -4.4961e-03,  2.9762e-04, -3.9612e-03,
         8.9038e-05,  2.8683e-02,  5.0068e-03,  1.6509e-02,  7.8983e-04,
         5.7728e-03,  3.2685e-02, -1.0457e-01,  1.2989e-02,  1.1278e-02,
         1.1943e-02,  1.5258e-02, -6.2411e-04,  1.0682e-04,  1.2087e-02,
         7.2984e-03,  2.7758e-02,  1.7572e-02, -6.0345e-03,  1.7211e-02,
         1.4121e-02,  6.4663e-02,  9.1813e-03,  3.2555e-03, -3.2667e-02,
         2.9132e-02, -1.7770e-02,  1.5302e-03, -2.9944e-02, -2.0706e-02,
        -3.6528e-03, -1.5497e-02,  1.5223e-02, -1.4751e-02, -2.2381e-02,
         6.9636e-03, -8.0838e-03, -2.4583e-03, -2.0677e-02,  8.8132e-03,
        -6.9554e-04,  1.6965e-02,  1.8535e-01,  3.5843e-04,  1.0812e-02,
        -4.2391e-03,  8.1779e-03,  3.4144e-02, -1.8996e-03,  2.9939e-03,
         3.6898e-04, -1.0144e-02, -5.7416e-03, -5.7676e-03,  1.7565e-01,
        -1.5793e-03, -2.6617e-02, -1.2572e-02,  3.0421e-04, -1.2132e-02,
        -1.4168e-02,  1.2154e-02,  8.4700e-03, -1.6284e-02,  2.6983e-03,
        -6.8554e-03,  2.7829e-01,  2.4060e-02,  1.1130e-02,  7.6095e-04,
         3.1341e-01,  2.1668e-02,  1.0277e-02, -3.0065e-02, -8.3565e-03,
         5.2488e-03, -1.1287e-02, -1.8266e-02,  1.1814e-02,  1.2662e-02,
         2.9036e-04,  7.0254e-04, -1.4084e-02,  1.2925e-02,  3.9504e-03,
        -7.9568e-03,  3.2794e-02,  7.3839e-03,  2.4609e-02,  9.6109e-03,
        -8.7206e-03,  9.2571e-03, -3.5850e-03, -8.9996e-03,  2.3120e-03,
        -1.8475e-02, -1.9610e-02,  1.1994e-02,  6.7156e-03,  1.9903e-02,
         3.0703e-02, -4.9538e-03, -6.1673e-02, -6.4986e-03, -2.1317e-02,
        -3.3650e-03,  2.3200e-03, -6.2224e-03,  3.7458e-03,  1.1542e-02,
        -1.0181e-02, -8.4711e-03,  1.1603e-02, -5.6247e-03, -1.0220e-02,
        -8.6501e-04, -1.2285e-02, -8.7487e-03, -1.1265e-02,  1.6322e-02,
         1.5160e-02,  1.8882e-02,  5.1557e-03, -8.8616e-03,  4.2153e-03,
        -1.9450e-02, -8.7365e-03, -9.7867e-03,  1.1667e-02,  5.0613e-03,
         2.8221e-03, -7.1795e-03,  9.3306e-03, -4.9663e-02,  1.7708e-02,
        -2.0959e-02, -3.3989e-02,  2.2581e-03,  5.1748e-03, -1.0133e-01,
         2.1052e-03,  5.5644e-03,  1.3607e-03,  8.8388e-03,  1.0244e-02,
        -3.8072e-03,  5.9209e-03,  6.7993e-03,  1.1594e-02, -1.1802e-02,
        -2.4233e-03, -5.1504e-03, -1.1903e-02,  1.4075e-02, -4.0701e-03,
        -2.9465e-02, -1.7579e-03,  4.3654e-03,  1.0429e-02,  3.7096e-02,
         8.6493e-03,  1.5871e-02,  1.8034e-02, -3.2165e-03, -2.1941e-02,
         2.6274e-02, -7.6941e-03, -5.9618e-03, -1.4179e-02,  8.0281e-03,
         1.1293e-02, -6.6936e-05,  1.2899e-02,  1.0056e-02, -6.3919e-04,
         2.0299e-02,  3.1528e-03, -4.8988e-03,  3.2754e-03, -1.1003e-01,
         1.8414e-02,  2.2272e-03, -2.2185e-02, -4.8672e-03,  1.9643e-03,
         3.0928e-02, -8.9599e-03, -1.1446e-02, -1.3794e-02,  7.1943e-03,
        -5.8965e-03,  2.2605e-03, -2.6114e-02, -5.6616e-03,  6.5073e-03,
         9.2219e-02, -6.7243e-03,  4.4427e-04,  7.2846e-03, -1.1021e-02,
         7.8802e-04, -3.8878e-03,  1.0489e-02,  9.2883e-03,  1.8895e-02,
         2.1808e-02,  6.2590e-04, -2.6519e-02,  7.0343e-04, -2.9067e-02,
        -9.1515e-03,  1.0418e-03,  8.3222e-03, -8.7548e-03, -2.0637e-03,
        -1.1450e-02, -8.8985e-04, -4.4062e-03,  2.3629e-02, -2.7221e-02,
         3.2008e-02,  6.6325e-03, -1.1302e-02, -1.0138e-03, -1.6902e-01,
        -8.4473e-03,  2.8536e-02,  1.4117e-03, -1.2136e-02, -1.4781e-02,
         4.9960e-03,  3.3916e-02,  5.2710e-03,  1.7382e-02, -4.6315e-03,
         1.1680e-02, -9.1395e-03,  1.8310e-02,  1.2321e-02, -2.4871e-02,
         1.1535e-02,  5.0308e-03,  5.5028e-03, -7.2184e-03, -5.5210e-03,
         1.7085e-02,  5.7236e-03,  1.7463e-03,  1.9969e-03,  6.1670e-03,
         2.9347e-03,  1.3946e-02, -1.9984e-03,  1.0091e-02,  1.0388e-03,
        -6.1902e-03,  3.0905e-02,  6.6038e-03, -9.1223e-02, -1.8411e-02,
         5.4185e-03,  2.4396e-02,  1.5696e-02, -1.2742e-02,  1.8126e-02,
        -2.6138e-02,  1.1170e-02, -1.3058e-02, -1.9386e-02, -5.9828e-03,
         1.9176e-02,  1.9962e-03, -2.1538e-03,  3.3003e-02,  1.8407e-02,
        -5.9498e-03, -3.2533e-03, -1.8917e-02, -1.5897e-02, -4.7057e-03,
         5.4162e-03, -3.0037e-02,  8.6773e-03, -1.7942e-03,  6.6826e-03,
        -1.1929e-02, -1.4076e-02,  1.6709e-02,  1.6860e-03, -3.3842e-03,
         8.6805e-03,  7.1340e-03,  1.5147e-02], grad_fn=&lt;EmbeddingBackward&gt;)
</code></pre>
",8,3,2698,2020-07-28 02:57:50,https://stackoverflow.com/questions/63126386/where-can-i-get-the-pretrained-word-embeddinngs-for-bert
Why is there no pooler layer in huggingfaces&#39; FlauBERT model?,"<p>BERT model for Language Model and Sequence classification includes an extra projection layer between the last transformer and the classification layer (it contains a linear layer of size <code>hidden_dim x hidden_dim</code>, a dropout layer and a <code>tanh</code> activation). This was not described in the paper originally but was clarified <a href=""https://github.com/google-research/bert/issues/43"" rel=""nofollow noreferrer"">here</a>. This intermediate layer is pre-trained together with the rest of the transformers.</p>
<p>In huggingface's <code>BertModel</code>, this layer is called <code>pooler</code>.</p>
<p>According to <a href=""https://arxiv.org/pdf/1912.05372.pdf#subsection.5.1"" rel=""nofollow noreferrer"">the paper</a>, FlauBERT model (XLMModel fine-tuned on French corpus) also includes this pooler layer: &quot;The classification head is composed of the following layers, in order: dropout, linear,tanhactivation, dropout, and linear.&quot;. However, when loading a FlauBERT model with huggingface (<em>e.g</em>, with <code>FlaubertModel.from_pretrained(...)</code>, or <code>FlaubertForSequenceClassification.from_pretrained(...)</code>), the model seem to include no such layer.</p>
<p>Hence the question: why is there no pooler layer in huggingfaces' FlauBERT model ?</p>
","bert-language-model, huggingface-transformers","<p>Pooler is necessary for the next sentence classification task. This task has been removed from Flaubert training making Pooler an optional layer. HuggingFace commented that &quot;pooler's output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence&quot;. Thus I belive they decided to remove the layer.</p>
",3,3,2153,2020-08-11 13:04:45,https://stackoverflow.com/questions/63358768/why-is-there-no-pooler-layer-in-huggingfaces-flaubert-model
what is the difference between pooled output and sequence output in bert layer?,"<p>everyone! I was reading about Bert and wanted to do <strong>text classification</strong> with its word embeddings. I came across this line of code:</p>
<pre><code>pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   
</code></pre>
<p>and then:</p>
<pre><code>clf_output = sequence_output[:, 0, :]
out = Dense(1, activation='sigmoid')(clf_output)
</code></pre>
<p>But I can't understand the use of pooled output. <strong>Doesn't sequence output contain all the information including the word embedding of ['CLS']?</strong> If so, why do we have pooled output?</p>
<p><em>Thanks in advance!</em></p>
","python-3.x, tensorflow, neural-network, text-classification, bert-language-model","<p>If you have given a sequence, &quot;You are on StackOverflow&quot;. The sequence_output will give 768 embeddings of these four words. But, the pooled output will just give you one embedding of 768, it will pool the embeddings of these four words.</p>
",2,4,6213,2020-08-12 13:09:10,https://stackoverflow.com/questions/63377198/what-is-the-difference-between-pooled-output-and-sequence-output-in-bert-layer
Is there a way to get the location of the substring from which a certain token has been produced in BERT?,"<p>I am feeding sentences to a BERT model (Hugging Face library). These sentences get tokenized with a pretrained tokenizer. I know that you can use the decode function to go back from tokens to strings.</p>
<pre><code>string = tokenizer.decode(...)
</code></pre>
<p>However, the reconstruction is not perfect. If you use an uncased pretrained model, the uppercase letters get lost. Also, if the tokenizer splits a word into 2 tokens, the second token will start with '##'. For example, the word 'coronavirus' gets split into 2 tokens: 'corona' and '##virus'.</p>
<p>So my question is: is there a way to get the indices of the substring from which every token is created?
For example, take the string &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;. The 9th token is the token corresponding to 'virus'.</p>
<pre><code>['[CLS]', 'tokyo', 'to', 'report', 'nearly', '370', 'new', 'corona', '##virus', 'cases', ',', 'setting', 'new', 'single', '-', 'day', 'record', '[SEP]']
</code></pre>
<p>I want something that tells me that the token '##virus' comes from the 'virus' substring in the original string, which is located between the indices 37 and 41 of the original string.</p>
<pre><code>sentence = &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;
print(sentence[37:42]) # --&gt; outputs 'virus
</code></pre>
","tokenize, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>As far as I know their is no built-in method for that, but you can create one by yourself:</p>
<pre class=""lang-py prettyprint-override""><code>import re
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

sentence = &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;

b = []
b.append(([101],))
for m in re.finditer(r'\S+', sentence):
  w = m.group(0)
  t = (tokenizer.encode(w, add_special_tokens=False), (m.start(), m.end()-1))

  b.append(t)

b.append(([102],))

b
</code></pre>
<p>Output:</p>
<pre><code>[([101],),
 ([5522], (0, 4)),
 ([2000], (6, 7)),
 ([3189], (9, 14)),
 ([3053], (16, 21)),
 ([16444], (23, 25)),
 ([2047], (27, 29)),
 ([21887, 23350], (31, 41)),
 ([3572, 1010], (43, 48)),
 ([4292], (50, 56)),
 ([2047], (58, 60)),
 ([2309, 1011, 2154], (62, 71)),
 ([2501], (73, 78)),
 ([102],)]
</code></pre>
",2,2,3202,2020-08-14 13:09:15,https://stackoverflow.com/questions/63413414/is-there-a-way-to-get-the-location-of-the-substring-from-which-a-certain-token-h
BERT sentence embeddings from transformers,"<p>I'm trying to get sentence vectors from hidden states in a BERT model.  Looking at the huggingface BertModel instructions <a href=""https://huggingface.co/bert-base-multilingual-cased?text=This%20sentence%20etc"" rel=""noreferrer"">here</a>, which say:</p>
<pre><code>from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='pt') 
output = model(**encoded_input)
</code></pre>
<p>So first note, as it is on the website, this does /not/ run. You get:</p>
<pre><code>&gt;&gt;&gt; Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'BertTokenizer' object is not callable
</code></pre>
<p>But it looks like a minor change fixes it, in that you don't call the tokenizer directly, but ask it to encode the input:</p>
<pre><code>encoded_input = tokenizer.encode(text, return_tensors=&quot;pt&quot;)
output = model(encoded_input)
</code></pre>
<p>OK, that aside, the tensors I get, however, have a different shape than I expected:</p>
<pre><code>&gt;&gt;&gt; output[0].shape
torch.Size([1,11,768])
</code></pre>
<p>This is a lot of layers.  Which is the correct layer to use for sentence embeddings?  <code>[0]</code>?  <code>[-1]</code>?  Averaging several?  I have the goal of being able to do cosine similarity with these, so I need a proper 1xN vector rather than an NxK tensor.</p>
<p>I see that the popular <a href=""https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes"" rel=""noreferrer"">bert-as-a-service project</a> appears to use <code>[0]</code></p>
<p>Is this correct? Is there documentation for what each of the layers are?</p>
","bert-language-model, huggingface-transformers","<p>I don't think there is single authoritative documentation saying what to use and when. You need to experiment and measure what is best for your task. Recent observations about BERT are nicely summarized in this paper: <a href=""https://arxiv.org/pdf/2002.12327.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/2002.12327.pdf</a>.</p>
<p>I think the rule of thumb is:</p>
<ul>
<li><p>Use the last layer if you are going to fine-tune the model for your specific task. And finetune whenever you can, several hundred or even dozens of training examples are enough.</p>
</li>
<li><p>Use some of the middle layers (7-th or 8-th) if you cannot finetune the model. The intuition behind that is that the layers first develop a more and more abstract and general representation of the input. At some point, the representation starts to be more target to the pre-training task.</p>
</li>
</ul>
<p>Bert-as-services uses the last layer by default (but it is configurable). Here, it would be <code>[:, -1]</code>. However, it always returns a list of vectors for all input tokens. The vector corresponding to the first special (so-called <code>[CLS]</code>) token is considered to be the sentence embedding. This where the <code>[0]</code> comes from in the snipper you refer to.</p>
",18,17,26005,2020-08-18 03:00:39,https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers
Correct Way to Fine-Tune/Train HuggingFace&#39;s Model from scratch (PyTorch),"<p>For example, I want to train a BERT model from scratch but using the existing configuration. Is the following code the correct way to do so?</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased')
model.init_weights()
</code></pre>
<p>Because I think the <code>init_weights</code> method will re-initialize all the weights.</p>
<p>Second question, if I want to change a bit the configuration, such as the number of hidden layers.</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased', num_hidden_layers=10)
model.init_weights()
</code></pre>
<p>I wonder if the above is the correct way to do so. Because they don't appear to have an error when I run the above code.</p>
","python, pytorch, bert-language-model, huggingface-transformers","<p>In this way, you would unnecessarily download and load the pre-trained model weights. You can avoid that by downloading the BERT config</p>
<pre class=""lang-py prettyprint-override""><code>config = transformers.AutoConfig.from_pretrained(&quot;bert-base-cased&quot;)
model = transformers.AutoModel.from_config(config)
</code></pre>
<p>Both yours and this solution assume you want to tokenize the input in the same as the original BERT and use the same vocabulary. If you want to use a different vocabulary, you can change in the config before instantiating the model:</p>
<pre class=""lang-py prettyprint-override""><code>config.vocab_size = 123456
</code></pre>
<p>Similarly, you can change any hyperparameter that you want to have different from the original BERT.</p>
",4,4,2193,2020-08-19 01:57:17,https://stackoverflow.com/questions/63478947/correct-way-to-fine-tune-train-huggingfaces-model-from-scratch-pytorch
TensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list),"<p>I'm trying to write <a href=""https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert"" rel=""nofollow noreferrer"">this code</a> into colab. Interestingly, I was running the same code in colab a few days ago but now it won't work. the code also works in kaggle kernel. I tried changing the TensorFlow version but all of them give different errors. Why do you think I can't run this code? This is the <a href=""https://github.com/mitramir55/Kaggle_NLP_competition/blob/master/Error_in_class.ipynb"" rel=""nofollow noreferrer"">colab notebook</a> if you needed more info.
Thanks in advance!</p>
<p>class DisasterDetector:</p>
<pre><code>def __init__(self, tokenizer, bert_layer, max_len =30, lr = 0.0001,
             epochs = 15, batch_size = 32, dtype = tf.int32 ,
             activation = 'sigmoid', optimizer = 'SGD',
             beta_1=0.9, beta_2=0.999, epsilon=1e-07,
             metrics = 'accuracy', loss = 'binary_crossentropy'):
    
    self.lr = lr
    self.epochs = epochs
    self.max_len = max_len
    self.batch_size = batch_size
    self.tokenizer = tokenizer
    self.bert_layer = bert_layer
    self.models = []

    self.activation = activation
    self.optimizer = optimizer
    self.dtype = dtype
    
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon =epsilon
    
    self.metrics = metrics
    self.loss = loss
    
def encode(self, texts):
    all_tokens = []
    masks = []
    segments = []
    
    for text in texts:
        
        tokenized = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + self.tokenizer.tokenize(text) + ['[SEP]'])
        
        len_zeros = self.max_len - len(tokenized)
        
        
        padded = tokenized + [0] * len_zeros
        mask = [1] * len(tokenized) + [0] * len_zeros
        segment = [0] * self.max_len
        
        all_tokens.append(padded)
        masks.append(mask)
        segments.append(segment)
        
    print(len(all_tokens[0]))
    return np.array(all_tokens), np.array(masks), np.array(segments)
    
def make_model(self):
    

    input_word_ids = Input(shape = (self.max_len, ), dtype=tf.int32,
                        name = 'input_word_ids')
    
    input_mask = Input(shape = (self.max_len, ), dtype=tf.int32,
                       name = 'input_mask')
    
    segment_ids = Input(shape = (self.max_len, ), dtype=tf.int32,
                        name = 'segment_ids')


    #pooled output is the output of dimention and

    pooled_output, sequence_output = self.bert_layer([input_word_ids,
                                                 input_mask,
                                                 segment_ids])

    clf_output = sequence_output[:, 0, :]
    out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output)
    #out = tf.keras.layers.Dense(1, activation = 'sigmoid', input_shape =  (clf_output,) )(clf_output)
    

    model = Model(inputs = [input_word_ids, input_mask, segment_ids],
                  outputs = out)
    if self.optimizer is 'SGD':
        optimizer = SGD(learning_rate = self.lr)

    elif self.optimizer is 'Adam': 
        optimizer = Adam(learning_rate = self.lr, beta_1=self.beta_1,
                         beta_2=self.beta_2, epsilon=self.epsilon)

    model.compile(loss = self.loss, optimizer = self.optimizer,
                  metrics = [self.metrics])
    
    return model




def train(self, x, k = 3):    
    kfold = StratifiedKFold(n_splits = k, shuffle = True)


    for fold, (train_idx, val_idx) in enumerate(kfold.split(x['cleaned_text'], x['target'])):
        print('fold: ', fold)

        x_trn = self.encode(x.loc[train_idx, 'cleaned_text'])
        x_val = self.encode(x.loc[val_idx, 'cleaned_text'])
        y_trn = np.array(x.loc[train_idx, 'target'], dtype = np.uint8)
        y_val = np.array(x.loc[val_idx, 'target'], dtype = np.uint8)
        print('the data type of y train: ', type(y_trn))
        print('x_val shape', x_val[0].shape)
        print('x_trn shape', x_trn[0].shape)
        
        model = self.make_model()
        print('model made.')
        model.fit(x_trn, tf.convert_to_tensor(y_trn),
                validation_data = (x_val, tf.convert_to_tensor(y_val)),
                batch_size=self.batch_size, epochs = self.epochs)

        self.models.append(model)
</code></pre>
<p>and after calling the train function of the class I get that error.</p>
<pre><code>classifier = DisasterDetector(tokenizer = tokenizer, bert_layer = bert_layer, max_len = max_len, lr = 0.0001,
                  epochs = 10,  activation = 'sigmoid',
                batch_size = 32,optimizer = 'SGD',
                beta_1=0.9, beta_2=0.999, epsilon=1e-07)
classifier.train(train_cleaned)
</code></pre>
<p>and here is the error:</p>
<pre><code>ValueError                                Traceback (most 

recent call last)
&lt;ipython-input-10-106c756f2e47&gt; in &lt;module&gt;()
----&gt; 1 classifier.train(train_cleaned)

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 
    100 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
","python, tensorflow, valueerror, bert-language-model","<p>Well, it turns out that by not giving the appropriate maximum sequence length, TensorFlow throws this error. By changing the max_len variable to 54 I could run my program with no difficulty.  So the problem was not about the type of the input or the numpy arrays.</p>
",1,2,6563,2020-08-19 06:55:52,https://stackoverflow.com/questions/63481527/tensorflow-valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported
Does BERT and other language attention model only share cross-word information in the initial embedding stage?,"<p>I study visual attention models but have recently been reading up on BERT and other language attention models to fill a serious gap in my knowledge.</p>
<p>I am a bit confused by what I seem to be seeing in these model architectures. Given a sentence like &quot;the cat chased the dog&quot;. I would have expected cross information streams between the embeddings of each word. For example, I would have expected to see a point in the model where the embedding for &quot;cat&quot; is combined with the embedding for &quot;dog&quot;, in order to create the attention mask.</p>
<p>Instead what I seem to be seeing, (correct me if I am wrong) is that the embedding of a word like &quot;cat&quot; is initially set up to include information about the words around them. So that each embedding of each word includes all of the other words around them. Then each of these embeddings are passed through the model in parallel. This seems weird to me and redundant. Why would they set up the model in this way?</p>
<p>If we were to block out cat. &quot;the ... chased the dog.&quot; Would we then, during inference, only need to send the &quot;...&quot; embedding through the model?</p>
","deep-learning, nlp, bert-language-model","<p>The embeddings don't contain any information about the other embeddings around them. BERT and other models like OpenGPT/GPT2 don't have context dependent inputs.</p>
<p>The context related part comes later. What they do in attention based models is use these input embeddings to create other vectors which then interact with each other and using various matrix multiplications, summing, normalizing and this helps the model understand the context which in turn helps it do interesting things including language generation etc.</p>
<p>When you say ' I would have expected to see a point in the model where the embedding for &quot;cat&quot; is combined with the embedding for &quot;dog&quot;, in order to create the attention mask.', you are right. That does happen. Just not at the embedding level. We make more vectors by matrix multiplying the embeddings with learned matrices that then interact with each other.</p>
",1,0,304,2020-08-19 16:28:08,https://stackoverflow.com/questions/63491247/does-bert-and-other-language-attention-model-only-share-cross-word-information-i
Tokens returned in transformers Bert model from encode(),"<p>I have a small dataset for sentiment analysis. The classifier will be a simple KNN but I wanted to get the word embedding with the <code>Bert</code> model from the <code>transformers</code> library. Note that I just found out about this library - I am still learning.</p>
<p>So looking at online example, I am trying to understand the dimensions that are returned from the model.</p>
<p>Example:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;)
print(tokens)

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;)
print(tokens)
</code></pre>
<p>The output is the following:</p>
<pre><code>[101, 100, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]

[101, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]
</code></pre>
<p>I can't seem to find the docs for <code>encode()</code> - I have no idea why it returns different stuff when the input is passed as a list. What is this doing?</p>
<p>Additionally, is there a method to pass a word token and get the actual word back - to troubleshoot the above?</p>
<p>Thank you in advance</p>
","python, machine-learning, nlp, bert-language-model, huggingface-transformers","<p>You can call <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.convert_ids_to_tokens"" rel=""noreferrer"">tokenizer.convert_ids_to_tokens()</a> to get the actual token for an id:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = []

tokens.append(tokenizer.encode([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;]))

tokens.append(tokenizer.encode(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;))

tokens.append(tokenizer.encode([&quot;Hello, my dog is cute&quot;]))

tokens.append(tokenizer.encode(&quot;Hello, my dog is cute&quot;))

for t in tokens:
    print(tokenizer.convert_ids_to_tokens(t))
</code></pre>
<p>Output:</p>
<pre><code>['[CLS]', '[UNK]', '[UNK]', '[SEP]']
['[CLS]', 'hello', ',', 'my', 'dog', 'is', 'cute', '[SEP]', 'he', 'is', 'really', 'nice', '[SEP]']
['[CLS]', '[UNK]', '[SEP]']
['[CLS]', 'hello', ',', 'my', 'dog', 'is', 'cute', '[SEP]']
</code></pre>
<p>As you can see here, each of your inputs was tokenized and special tokens were added according your model (bert). The encode function hasn't processed your lists properly which could be a bug or intended beheaviour depending on how you define it because their is a method for batch processing <code>batch_encode_plus</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.batch_encode_plus([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;], return_token_type_ids=False, return_attention_mask=False)
</code></pre>
<p>Output:</p>
<pre><code>{'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102], [101, 2002, 2003, 2428, 3835, 102]]}
</code></pre>
<p>I'm not sure why the encode method is not documented, but it could be the case that huggingface wants us to use the <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"" rel=""noreferrer""><strong>call</strong></a> method directly:</p>
<pre class=""lang-py prettyprint-override""><code>tokens = []

tokens.append(tokenizer([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;],  return_token_type_ids=False, return_attention_mask=False))

tokens.append(tokenizer(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;,  return_token_type_ids=False, return_attention_mask=False))

tokens.append(tokenizer([&quot;Hello, my dog is cute&quot;], return_token_type_ids=False, return_attention_mask=False))

tokens.append(tokenizer(&quot;Hello, my dog is cute&quot;, return_token_type_ids=False, return_attention_mask=False))

print(tokens)
</code></pre>
<p>Output:</p>
<pre><code>[{'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102], [101, 2002, 2003, 2428, 3835, 102]]}, {'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]}, {'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]]}, {'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102]}]

</code></pre>
",8,3,6897,2020-08-27 01:38:31,https://stackoverflow.com/questions/63607919/tokens-returned-in-transformers-bert-model-from-encode
GCP AI Platform Notebook driver too old?,"<p>I am trying to run the following <a href=""https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"" rel=""nofollow noreferrer"">Hugging Face Transformers tutorial</a> on GCP's AI Platform Notebook with 32 vCPUs, 208 GB RAM, and 2 NVIDIA Tesla T4s.</p>
<p>However, when I try to run the part</p>
<p><code>model = DistillBERTClass()</code></p>
<p><code>model.to(device)</code></p>
<p>I get the following Assertion Error:</p>
<pre><code>AssertionError: The NVIDIA driver on your system is too old (found version 10010).
Please update your GPU driver by downloading and installing a new
version from the URL: http://www.nvidia.com/Download/index.aspx
Alternatively, go to: https://pytorch.org to install
a PyTorch version that has been compiled with your version
of the CUDA driver.
</code></pre>
<p>However, when I run
!nvidia-smi</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   38C    P0    22W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |
| N/A   39C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
</code></pre>
<p>The version on the NVIDIA driver is compatible with the latest PyTorch version, which I am using.
Has anyone else ran into this error, and is there a way around it?</p>
","pytorch, nvidia, bert-language-model, huggingface-transformers, gcp-ai-platform-notebook","<p>You can try a newer NVIDIA driver version,  we support latest CUDA 11 driver version, and then install Pytorch on top of it:</p>
<pre><code>gcloud beta notebooks instances create cuda11 \
--vm-image-project=deeplearning-platform-release \
--vm-image-family=common-cu110-notebooks-debian-9 \
--machine-type=n1-standard-1 \
--location=us-west1-a \
--format=json
</code></pre>
<p>Image family:</p>
<ul>
<li>common-cu110-notebooks-debian-9</li>
<li>common-cu110-notebooks-debian-10</li>
</ul>
",0,1,1056,2020-08-30 21:59:51,https://stackoverflow.com/questions/63662548/gcp-ai-platform-notebook-driver-too-old
HuggingFace Transformers model for German news classification,"<p>I've been trying to find a suitable model for my project (multiclass German text classification) but got a little confused with the models offered <a href=""https://huggingface.co/models?search=german"" rel=""nofollow noreferrer"">here</a>. There are models with <code>text-classification</code> tag, but they are for binary classification. Most of the other models are for <code>[MASK]</code> word predicting. I am not sure, which one to choose and if it will work with multiple classes at all</p>
<p>Would appreciate any advice!</p>
","python, nlp, text-classification, bert-language-model, huggingface-transformers","<p>You don't need to look for a specific text classification model when your classes are completely different because most listed models used one of the base models and finetuned the base layers and trained the output layers for their needs. In your case you will remove the output layers and their finetuning of the base layers will not benefit or hurt you much. Sometimes they have extended the vocabulary which could be beneficial for your task but you have to check description (which is often sparse :() and the vocabulary by yourself to get more details about the respective model.</p>
<p>In general I recommend you to work with one of the base models right away and only look for other models in case of insufficient results.</p>
<p>The following is an example for bert with 6 classes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertForSequenceClassification, BertTokenizer

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-german-dbmdz-uncased&quot;)
model = BertForSequenceClassification.from_pretrained(&quot;bert-base-german-dbmdz-uncased&quot;, num_labels=6)
</code></pre>
",1,0,606,2020-08-31 13:56:09,https://stackoverflow.com/questions/63672169/huggingface-transformers-model-for-german-news-classification
Hugging face - RuntimeError: Caught RuntimeError in replica 0 on device 0 on Azure Databricks,"<p>How do I run the run_language_modeling.py script from hugging face using the pretrained roberta case model to fine-tune  using my own data on the Azure databricks with a GPU cluster.</p>
<p>Using Transformer version 2.9.1 and 3.0 .
Python 3.6
Torch `1.5.0
torchvision 0.6</p>
<p>This is the script I ran below on Azure databricks</p>
<pre><code>%run '/dbfs/FileStore/tables/dev/run_language_modeling.py' \
  --output_dir='/dbfs/FileStore/tables/final_train/models/roberta_base_reduce_n' \
  --model_type=roberta \
  --model_name_or_path=roberta-base \
  --do_train \
  --num_train_epochs 5 \
  --train_data_file='/dbfs/FileStore/tables/final_train/train_data/all_data_desc_list_full.txt' \
  --mlm 
</code></pre>
<p>This is the error I get after running the above command.</p>
<pre><code>/dbfs/FileStore/tables/dev/run_language_modeling.py in &lt;module&gt;
   279 
   280 if __name__ == &quot;__main__&quot;:
--&gt; 281     main()

/dbfs/FileStore/tables/dev/run_language_modeling.py in main()
   243             else None
   244         )
--&gt; 245         trainer.train(model_path=model_path)
   246         trainer.save_model()
   247         # For convenience, we also re-save the tokenizer to the same directory,

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in train(self, model_path)
   497                     continue
   498 
--&gt; 499                 tr_loss += self._training_step(model, inputs, optimizer)
   500 
   501                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in _training_step(self, model, inputs, optimizer)
   620             inputs[&quot;mems&quot;] = self._past
   621 
--&gt; 622         outputs = model(**inputs)
   623         loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
   624 

/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
   548             result = self._slow_forward(*input, **kwargs)
   549         else:
--&gt; 550             result = self.forward(*input, **kwargs)
   551         for hook in self._forward_hooks.values():
   552             hook_result = hook(self, input, result)

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)
   153             return self.module(*inputs[0], **kwargs[0])
   154         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
--&gt; 155         outputs = self.parallel_apply(replicas, inputs, kwargs)
   156         return self.gather(outputs, self.output_device)
   157 

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in parallel_apply(self, replicas, inputs, kwargs)
   163 
   164     def parallel_apply(self, replicas, inputs, kwargs):
--&gt; 165         return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
   166 
   167     def gather(self, outputs, output_device):

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py in parallel_apply(modules, inputs, kwargs_tup, devices)
    83         output = results[i]
    84         if isinstance(output, ExceptionWrapper):
---&gt; 85             output.reraise()
    86         outputs.append(output)
    87     return outputs

/databricks/python/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
   393             # (https://bugs.python.org/issue2651), so we work around it.
   394             msg = KeyErrorMessage(msg)
--&gt; 395         raise self.exc_type(msg)

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 60, in _worker
   output = module(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_roberta.py&quot;, line 239, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 762, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 439, in forward
   output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 371, in forward
   hidden_states, attention_mask, head_mask, output_attentions=output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 315, in forward
   hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 240, in forward
   attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.68 GiB already allocated; 95.31 MiB free; 10.77 GiB reserved in total by PyTorch)```

Please how do I resolve this
</code></pre>
","pytorch, databricks, azure-databricks, bert-language-model, huggingface-transformers","<p>The out of memory error is likely caused by not cleaning up the session and or freeing up the GPU.</p>
<p>From the similar <a href=""https://github.com/pytorch/pytorch/issues/16417"" rel=""nofollow noreferrer"">Github</a> issue.</p>
<blockquote>
<p>It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.</p>
</blockquote>
",2,0,5957,2020-08-31 18:31:12,https://stackoverflow.com/questions/63676307/hugging-face-runtimeerror-caught-runtimeerror-in-replica-0-on-device-0-on-azu
How to use another pretrained BERT model with the ktrain text classifier?,"<p>How can we use a different pretrained model for the text classifier in the ktrain library? When using:</p>
<blockquote>
<p>model = text.text_classifier('bert', (x_train, y_train) ,
preproc=preproc)</p>
</blockquote>
<p><a href=""https://github.com/google-research/bert/blob/master/multilingual.md"" rel=""nofollow noreferrer"">This uses the multilangual pretrained model</a></p>
<p>However, I want to try out a monolingual model as well. Namely the Dutch one: ''wietsedv/bert-base-dutch-cased', which is also used in other k-train implementations, <a href=""https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/examples/text/CoNLL2002_Dutch-BiLSTM.ipynb"" rel=""nofollow noreferrer"">for example</a>.</p>
<p>However, when trying to use this command in the text classifier it does not work:</p>
<pre><code>model = text.text_classifier('bert', (x_train, y_train) ,
&gt; preproc=preproc, bert_model='wietsedv/bert-base-dutch-cased')
</code></pre>
<p>or</p>
<pre><code>model = text.text_classifier('wietsedv/bert-base-dutch-cased', (x_train, y_train), preproc=preproc)
</code></pre>
<p>Does anyone how to do this? Thanks!</p>
","python, bert-language-model, ktrain","<p>There are two text classification APIs in <strong>ktrain</strong>.  The first is the <code>text_classifier</code> API which can be used for a select number of both transformers and non-transformers models.  The second is the <code>Transformer</code> API which can be used with any <code>transformers</code> model including the one you listed.</p>
<p>The latter is explained in detail in <a href=""https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb"" rel=""noreferrer"">this tutorial notebook</a> and <a href=""https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed?gi=77820bfc9da6"" rel=""noreferrer"">this medium article</a>.</p>
<p>For instance, you can replace <code>MODEL_NAME</code> with any model you want in the example below:</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code># load text data
categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']
from sklearn.datasets import fetch_20newsgroups
train_b = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)
test_b = fetch_20newsgroups(subset='test',categories=categories, shuffle=True)
(x_train, y_train) = (train_b.data, train_b.target)
(x_test, y_test) = (test_b.data, test_b.target)

# build, train, and validate model (Transformer is wrapper around transformers library)
import ktrain
from ktrain import text
MODEL_NAME = 'distilbert-base-uncased'  # replace this with model of choice
t = text.Transformer(MODEL_NAME, maxlen=500, class_names=train_b.target_names)
trn = t.preprocess_train(x_train, y_train)
val = t.preprocess_test(x_test, y_test)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)
learner.fit_onecycle(5e-5, 4)
learner.validate(class_names=t.get_classes()) # class_names must be string values

# Output from learner.validate()
#                        precision    recall  f1-score   support
#
#           alt.atheism       0.92      0.93      0.93       319
#         comp.graphics       0.97      0.97      0.97       389
#               sci.med       0.97      0.95      0.96       396
#soc.religion.christian       0.96      0.96      0.96       398
#
#              accuracy                           0.96      1502
#             macro avg       0.95      0.96      0.95      1502
#          weighted avg       0.96      0.96      0.96      1502
</code></pre>
",5,1,2495,2020-09-03 17:43:15,https://stackoverflow.com/questions/63729057/how-to-use-another-pretrained-bert-model-with-the-ktrain-text-classifier
PyTorch torch.no_grad() versus requires_grad=False,"<p>I'm following a <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">PyTorch tutorial</a> which uses the BERT NLP model (feature extractor) from the Huggingface Transformers library. There are two pieces of interrelated code for gradient updates that I don't understand.</p>
<p>(1) <code>torch.no_grad()</code></p>
<p>The tutorial has a class where the <code>forward()</code> function creates a <code>torch.no_grad()</code> block around a call to the BERT feature extractor, like this:</p>
<pre class=""lang-py prettyprint-override""><code>bert = BertModel.from_pretrained('bert-base-uncased')

class BERTGRUSentiment(nn.Module):
    
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        
    def forward(self, text):
        with torch.no_grad():
            embedded = self.bert(text)[0]
</code></pre>
<p>(2) <code>param.requires_grad = False</code></p>
<p>There is another portion in the same tutorial where the BERT parameters are frozen.</p>
<pre class=""lang-py prettyprint-override""><code>for name, param in model.named_parameters():                
    if name.startswith('bert'):
        param.requires_grad = False
</code></pre>
<p><strong>When would I need (1) and/or (2)?</strong></p>
<ul>
<li>If I want to train with a frozen BERT, would I need to enable both?</li>
<li>If I want to train to let BERT be updated, would I need to disable both?</li>
</ul>
<p>Additionaly, I ran all four combinations and found:</p>
<pre><code>   with torch.no_grad   requires_grad = False  Parameters  Ran
   ------------------   ---------------------  ----------  ---
a. Yes                  Yes                      3M        Successfully
b. Yes                  No                     112M        Successfully
c. No                   Yes                      3M        Successfully
d. No                   No                     112M        CUDA out of memory
</code></pre>
<p><strong>Can someone please explain what's going on?</strong> Why am I getting <code>CUDA out of memory</code> for (d) but not (b)? Both have 112M learnable parameters.</p>
","python, machine-learning, pytorch, bert-language-model, huggingface-transformers","<p>This is an older discussion, which has changed slightly over the years (mainly due to the purpose of <code>with torch.no_grad()</code> as a pattern. An excellent answer that kind of answers your question as well can be found <a href=""https://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false"">on Stackoverflow already</a>.<br />
However, since the original question is vastly different, I'll refrain from marking as duplicate, especially due to the second part about the memory.</p>
<p>An initial explanation of <code>no_grad</code> is given <a href=""https://discuss.pytorch.org/t/no-grad-vs-requires-grad/21272"" rel=""noreferrer"">here</a>:</p>
<blockquote>
<p><code>with torch.no_grad()</code> is a context manager and is used to prevent calculating gradients [...].</p>
</blockquote>
<p><code>requires_grad</code> on the other hand is used</p>
<blockquote>
<p>to freeze part of your model and train the rest [...].</p>
</blockquote>
<p>Source again <a href=""https://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false"">the SO post</a>.</p>
<p>Essentially, with <code>requires_grad</code> you are just disabling parts of a network, whereas <code>no_grad</code> will not store <em>any</em> gradients at all, since you're likely using it for inference and not training.<br />
To analyze the behavior of your combinations of parameters, let us investigate what is happening:</p>
<ul>
<li><code>a)</code> and <code>b)</code> do not store any gradients at all, which means that you have vastly more memory available to you, no matter the number of parameters, since you're not retaining them for a potential backward pass.</li>
<li><code>c)</code> has to store the forward pass for later backpropagation, however, only a limited number of parameter (3 million) are stored, which makes this still manageable.</li>
<li><code>d)</code>, however, needs to store the forward pass <em>for all 112 million</em> parameters, which causes you to run out of memory.</li>
</ul>
",13,14,17901,2020-09-07 23:23:32,https://stackoverflow.com/questions/63785319/pytorch-torch-no-grad-versus-requires-grad-false
Why is there is a change in data items during learner.autofit using BERT?,"<p>I am trying to fit BERT text classifier. My training and test data looks as follows.</p>
<pre><code>x_train = data[&quot;TEXT&quot;].head(4500).tolist()
y_train= [label2id[label] for label in data[&quot;EMOTION&quot;].head(4500).values.tolist()]
x_test = data[&quot;TEXT&quot;].tail(500).tolist()
y_test = [label2id[label] for label in data[&quot;EMOTION&quot;].tail(500).values.tolist()]
</code></pre>
<p>Then, I download the pretrained BERT model (uncased_L-12_H-768_A-12.zip)...</p>
<pre><code>(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=x_train, y_train=y_train,
                                                                       x_test=x_test, y_test=y_test,
                                                                       class_names=data['EMOTION'].unique().tolist(),
                                                                       preprocess_mode='bert',
                                                                       ngram_range=1, 
                                                                       maxlen=350, 
                                                                       max_features=35000)
</code></pre>
<p>For classification, we set the bert model as</p>
<pre><code>model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)
learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=6)
</code></pre>
<p>Finally, I try fit to the model using 1cycle policy rate</p>
<pre><code>hist = learner.fit_onecycle(2e-5, 1)
</code></pre>
<p>I get the result with 750 samples rather than 4500 samples. I also tested this with various data. So there is always variations in data items. Can you give an idea what is behind it?</p>
<pre><code>begin training using onecycle policy with max lr of 2e-05...
750/750 [==============================] - 875s 1s/step - loss: 0.3740 - accuracy: 0.8544
</code></pre>
<p>Thank you for your response in Advance.</p>
","python-3.x, bert-language-model","<p>My personal idea is that when you instantiate the learner with <code>ktrain.get_learner</code> you give it a <code>batch size = 6</code> as input parameter.</p>
<p>So when you try to train the learner by simply doing <code>learner.fit_onecycle (2e-5, 1)</code>, it takes exactly one batch for training, in fact <code>4500 training data / batch size (6) = 750</code> data to train on.</p>
<p>At this point either try to change the batch size, or do a for loop like this:</p>
<pre><code>for epoch in range(X):
    ....
    for batch in chunker(train, batch_size):
    ....
</code></pre>
<p>where <code>chunker()</code> could be something like:</p>
<pre><code>def chunker(sequence, size):
    &quot;&quot;&quot;useful for splitting a sequence into minibatches&quot;&quot;&quot;
    for i in range(0, len(sequence), size):
        chunk = sequence[i:i+size]
        # sort sentences in batch by length in descending order
        chunk.sort(key=lambda x: len(x), reverse=True)
        yield chunk
</code></pre>
<p>In a nutshell the idea is that you have to do a loop in which you go to select each time a set of data (batch) that you want to use to train your model.</p>
",1,0,104,2020-09-08 13:32:37,https://stackoverflow.com/questions/63795023/why-is-there-is-a-change-in-data-items-during-learner-autofit-using-bert
"Why do we need the custom dataset class and use of _getitem_ method in NLP, BERT fine tuning etc","<p>I am a newbie in NLP and has been studying the usage of BERT for NLP tasks. In many notebooks, I See that a custom dataset class is defined and <em>getitem</em> method is defined (along with len).</p>
<p>Tweetdataset class in this notebook - <a href=""https://www.kaggle.com/abhishek/roberta-inference-5-folds"" rel=""nofollow noreferrer"">https://www.kaggle.com/abhishek/roberta-inference-5-folds</a></p>
<p>and text_Dataset class in this notebook - <a href=""https://engineering.wootric.com/when-bert-meets-pytorch"" rel=""nofollow noreferrer"">https://engineering.wootric.com/when-bert-meets-pytorch</a></p>
<p>Can some one please explain the reason, need for defining the custom dataset class and the <em>getitem</em> (and len) method. thank you</p>
","nlp, pytorch, bert-language-model","<p>It is a recommended abstraction in pytorch to define <code>datasets</code> by inheriting <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" rel=""nofollow noreferrer""><code>torch.utils.data.Dataset</code></a>. Those objects define how many elements are there (<code>__len__</code> method) and how to get a single item via specified index (<code>__getitem__(index)</code>).</p>
<p>Its <a href=""https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset"" rel=""nofollow noreferrer"">source code</a>:</p>
<pre><code>class Dataset(object):   
    def __getitem__(self, index):
        raise NotImplementedError

    def __add__(self, other):
        return ConcatDataset([self, other])
</code></pre>
<p>So it's basically a thin wrapper which adds possibility to concatenate two <code>Dataset</code> objects. For readability and API compatibility you should inherit from it (unlike the one provided in <code>kaggle</code>).</p>
<p>You can read more about PyTorch's data functionality <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" rel=""nofollow noreferrer""><strong>here</strong></a></p>
",1,1,663,2020-09-10 04:15:15,https://stackoverflow.com/questions/63822730/why-do-we-need-the-custom-dataset-class-and-use-of-getitem-method-in-nlp-bert
Bert sentence-transformers stops/quits during fine tuning,"<p>I am following BERT instructions to fine tune as described <a href=""https://www.sbert.net/docs/training/overview.html"" rel=""nofollow noreferrer"">here</a></p>
<p>Here is my code:</p>
<pre><code>from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, evaluation
from torch.utils.data import DataLoader

# load model
embedder = SentenceTransformer('bert-large-nli-mean-tokens')
print(&quot;embedder loaded...&quot;)

# define your train dataset, the dataloader, and the train loss
train_dataset = SentencesDataset(x_sample[&quot;input&quot;].tolist(), embedder)
train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=16)
train_loss = losses.CosineSimilarityLoss(embedder)

sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']
sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']
scores = [0.3, 0.6, 0.2]
evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)

# tune the model
embedder.fit(train_objectives=[(train_dataloader, train_loss)], 
    epochs=1, 
    warmup_steps=100, 
    evaluator=evaluator, 
    evaluation_steps=1)
</code></pre>
<p>At 4% the training stops and the programs exists with no warnings or errors. There is no output.</p>
<p>I have no idea how to troubleshoot - any help would be great.</p>
<p>Edit: Changed the title from fails to stops/quits because I don't know if its failing</p>
<p>Here is what I see on my terminal:
Epoch:  0%|
<strong>Killed</strong>tion:   0%|</p>
<p>The word &quot;Killed&quot; overlaps the word iteration... memory problem perhaps? FYI: I am running it from the terminal of vscode with wsl on ubuntu vm in windows</p>
<p>Found the issue on github:
<a href=""https://github.com/ElderResearch/gpu_docker/issues/38"" rel=""nofollow noreferrer"">https://github.com/ElderResearch/gpu_docker/issues/38</a></p>
","python, machine-learning, bert-language-model, sentence-similarity","<p>My solution was to set batch and worker to one and its very slow</p>
<pre><code>train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=1, num_workers=1)
</code></pre>
",1,2,1812,2020-09-18 16:15:30,https://stackoverflow.com/questions/63959319/bert-sentence-transformers-stops-quits-during-fine-tuning
Why BERT model have to keep 10% MASK token unchanged?,"<p>I am reading BERT model paper. In Masked Language Model task during pre-training BERT model, the paper said the model will choose 15% token ramdomly. In the chose token (Ti), 80% it will be replaced with [MASK] token, 10% Ti is unchanged and 10% Ti replaced with another word. I think the model just need to replace with [MASK] or another word is enough. Why does the model have to choose randomly a word and keep it unchanged? Does pre-training process predict only [MASK] token or it predict 15% a whole random token?</p>
","deep-learning, nlp, bert-language-model","<p>This is done because they want to pre-train a bidirectional model. Most of the time the network will see a sentence with a [MASK] token, and its trained to predict the word that is supposed to be there. But in fine-tuning, which is done after pre-training (fine-tuning is the training done by everyone who wants to use BERT on their task), there are no [MASK] tokens! (unless you specifically do masked LM).</p>
<p>This mismatch between pre-training and training (sudden disappearence of the [MASK] token) is softened by them, with a probability of 15% the word is not replaced by [MASK]. The task is still there, the network has to predict the token, but it actually gets the answer already as input. This might seem counterintuitive but makes sense when combined with the [MASK] training.</p>
",8,9,4499,2020-09-22 16:20:30,https://stackoverflow.com/questions/64013808/why-bert-model-have-to-keep-10-mask-token-unchanged
Any reason to save a pretrained BERT tokenizer?,"<p>Say I am using <code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)</code>, and all I am doing with that tokenizer during fine-tuning of a new model is the standard <code>tokenizer.encode()</code></p>
<p>I have seen in most places that people save that tokenizer at the same time that they save their model, but I am unclear on why it's necessary to save since it seems like an out-of-the-box tokenizer that does not get modified in any way during training.</p>
","save, pytorch, bert-language-model, huggingface-tokenizers","<p>In your case, if you are using tokenizer only to tokenize the text (<code>encode()</code>), then you need not have to save the tokenizer. You can always load the tokenizer of the pretrained model.</p>
<p>However, sometimes you may want to use the tokenizer of the pretrained model, then add new tokens to it's vocabulary, or redefine the special symbols such as '[CLS]', '[MASK]', '[SEP]', '[PAD]' or any such special tokens. In this case, since you have made the changes to the tokenizer, it will be useful to save the tokenizer for the future use.</p>
",3,2,2495,2020-09-22 22:54:11,https://stackoverflow.com/questions/64018723/any-reason-to-save-a-pretrained-bert-tokenizer
Inconsistent vector representation using transformers BertModel and BertTokenizer,"<p>I have a <code>BertTokenizer</code> (<code>tokenizer</code>) and a <code>BertModel</code> (<code>model</code>) from the <code>transformers</code> library.
<strong>I have pre-trained the model from scratch</strong> with a few wikipedia articles, just to test how it works.</p>
<p>Once the model is pre-trained, <strong>I want to extract a layer vector representation for a given sentence</strong>. For that, I calculate the average of the 11 hidden (768-sized) vectors. I do this as follows (<code>line</code> is a single <code>String</code>):</p>
<pre><code>padded_sequence = tokenizer(line, padding=True)
        
indexed_tokens = padded_sequence['input_ids']
attention_mask = padded_sequence[&quot;attention_mask&quot;]

tokens_tensor = torch.tensor([indexed_tokens])
attention_mask_tensor = torch.tensor([attention_mask])

outputs = model(tokens_tensor, attention_mask_tensor)
hidden_states = outputs[0]

line_vectorized = hidden_states[0].data.numpy().mean(axis=0)
</code></pre>
<p>So far so good. <strong>I can do this for every sentence individually. But now I want to do it in batch</strong>, ie. I have a bunch of sentences and instead of iterating each sentence I send the appropiate tensor representations to get all vectors at once. I do this as follows (<code>lines</code> is a <code>list of Strings</code>):</p>
<pre><code>padded_sequences = self.tokenizer_PYTORCH(lines, padding=True)
        
indexed_tokens_list = padded_sequences['input_ids']
attention_mask_list = padded_sequences[&quot;attention_mask&quot;]
        
tokens_tensors_list = [torch.tensor([indexed_tokens]) for indexed_tokens in indexed_tokens_list]
attention_mask_tensors_list = [torch.tensor([attention_mask ]) for attention_mask in attention_mask_list ]
        
tokens_tensors = torch.cat((tokens_tensors_list), 0)
attention_mask_tensors = torch.cat((attention_mask_tensors_list ), 0)

outputs = model(tokens_tensors, attention_mask_tensors)
hidden_states = outputs[0]

lines_vectorized = [hidden_states[i].data.numpy().mean(axis=0) for i in range(0, len(hidden_states))]
</code></pre>
<p>The problem is the following: <strong>I have to use padding so that I can appropiately concatenate the token tensors</strong>. That means that the indexed tokens and the attention masks can be larger than in the previous case where the sentences were evaluated individually. <strong>But when I use padding, I get different results for the sentences which have been padded</strong>.</p>
<p><em>EXAMPLE</em>:
I have two sentences (in French but it doesn't matter):</p>
<p><code>sentence_A</code> = &quot;appareil digestif un article de wikipedia l encyclopedie libre&quot;</p>
<p><code>sentence_B</code> = &quot;sauter a la navigation sauter a la recherche cet article est une ebauche concernant la biologie&quot;</p>
<p>When I evaluate the two sentences <strong>individually</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.9304411   0.53798294 -1.6231083 ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>But when I evaluate the two sentences <strong>in batch</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000, 10004, 10004, 10004, 10004, 10004, 10004, 10004]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
line_vectorized =  [-1.0473819   0.6090186  -1.727466  ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>That is, <strong>since <code>sentence_B</code> is larger than <code>sentence_A</code>, <code>sentence_A</code> has been padded and the attention mask has been padded with zeros as well</strong>. The indexed tokens contain now extra tokens (<code>10004</code> which I assume <code>empty</code>).
The vector representation of <code>sentence_B</code> has NOT changed. But <strong>the vector representation of <code>sentence_A</code> HAS CHANGED</strong>.</p>
<p>I would like to know if this is working as intended or not (I assume not).
And I guess I am doing something wrong but I can't figure out what.</p>
<p>Any ideas?</p>
","python, nlp, bert-language-model, huggingface-transformers","<p>When you do it in single sentence per batch, the maximum length of the sentence is maximum number of tokens, however, when you do it in batch the maximum length of the sentences remains the same across the batch, which defaults to the maximum number of tokens in the longest sentence. The max values of <code>1</code> in this case indicates its not a <code>&lt;PAD&gt;</code> token, and <code>0</code> indicates a <code>&lt;PAD&gt;</code> token. The best way to control this is to define the maximum sequence length and truncate the sentences longer than the maximum sequence length.</p>
<p>This can be done using an alternative method to tokenize the text in batches (a single sentence can be considered as batch-size of 1):</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;&lt;your bert model&gt;&quot;, do_lower_case=True)
encoding = tokenizer.batch_encode_plus(lines, return_tensors='pt',padding=True, truncation=True, max_length=50, add_special_tokens = True) ## Change the max_length to the required max length
indexed_tokens = encoding['input_ids']
attention_mask = encoding['attention_mask']
</code></pre>
",1,1,625,2020-09-23 08:04:00,https://stackoverflow.com/questions/64023547/inconsistent-vector-representation-using-transformers-bertmodel-and-berttokenize
Are special tokens [CLS] [SEP] absolutely necessary while fine tuning BERT?,"<p>I am following the tutorial <a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"" rel=""nofollow noreferrer"">https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/</a> to do Named Entity Recognition with BERT.</p>
<p>While fine-tuning, before feeding the tokens to the model, the author does:</p>
<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=MAX_LEN, dtype=&quot;long&quot;, value=0.0,
                          truncating=&quot;post&quot;, padding=&quot;post&quot;)
</code></pre>
<p>According to my tests, this doesn't add special tokens to the ids. So am I missing something or i it not always necessary to include [CLS] (101) [SEP] (102)?</p>
","bert-language-model, named-entity-recognition, cls","<p>I'm also following this tutorial. It worked for me without adding these tokens, however, I found in another tutorial (<a href=""https://vamvas.ch/bert-for-ner"" rel=""nofollow noreferrer"">https://vamvas.ch/bert-for-ner</a>) that it is better to add them, because the model was trained in this format.</p>
<p>[Update]
Actually just checked it, it turned out that the accuracy improved by 20% after adding the tokens. But note that I am using it on a different dataset</p>
",1,2,2650,2020-09-29 23:13:27,https://stackoverflow.com/questions/64128864/are-special-tokens-cls-sep-absolutely-necessary-while-fine-tuning-bert
what&#39;s the difference between &quot;self-attention mechanism&quot; and &quot;full-connection&quot; layer?,"<p>I am confused with these two structures. In theory, the output of them are all connected to their input. what magic make 'self-attention mechanism' is more powerful than the full-connection layer?</p>
","pytorch, bert-language-model, transformer-model","<p>Ignoring details like normalization, biases, and such, fully connected networks are fixed-weights:</p>
<pre><code>f(x) = (Wx)
</code></pre>
<p>where <code>W</code> is learned in training, and fixed in inference.</p>
<p>Self-attention layers are dynamic, changing the weight as it goes:</p>
<pre><code>attn(x) = (Wx)
f(x) = (attn(x) * x)
</code></pre>
<p>Again this is ignoring a lot of details but there are many different implementations for different applications and you should really check a paper for that.</p>
",18,11,2871,2020-10-06 02:50:36,https://stackoverflow.com/questions/64218678/whats-the-difference-between-self-attention-mechanism-and-full-connection-l
My checkpoint albert files does not change when training,"<p>I train Albert model for question answering task. I have 200 thousand question-answer pairs and I use a saved checkpoint file with 2gb. I trained it on my GPU GeForce 2070 RTX with 1000 steps each time to save checkpoint, during training the checkpoint <code>model.ckpt-96000.data-00000-of-00001</code> files just keep the size of <code>135MB</code> and don't increase. Is this a problem?</p>
<p>I can't see why with a much smaller dataset like 1500 question-answer pairs, it also produces 135 MB checkpoint file. It hasn't stopped training yet but is it possible that the model will improve with this training?</p>
","nlp, training-data, bert-language-model, checkpoint, nlp-question-answering","<p>While training your model you can store the weights in a collection of files formatted as <code>checkpoints</code> that contain only the weights trained in a binary format.</p>
<p>In particular, the checkpoints contain:</p>
<ul>
<li>one or more blocks that contain the weights of our model</li>
<li>an index file indicating which weights are stored in a particular block</li>
</ul>
<p>So the fact that the size of the checkpoint file is always the same depends on the fact that the model used is always the same. So the number of model parameters is always the same so the size of the weights you are going to save is always the same. While the suffix <code>data-00000-of-00001</code> indicates that you are training the model on a single machine.</p>
<p>The size of the dataset, in my opinion, has nothing to do with it.</p>
",1,0,109,2020-10-07 03:42:15,https://stackoverflow.com/questions/64236859/my-checkpoint-albert-files-does-not-change-when-training
AttributeError: &#39;Tensor&#39; object has no attribute &#39;size&#39; pretrained bert,"<p>This is the model that I have defined:</p>
<pre><code>def build_model():
  input_layer = keras.layers.Input(name=&quot;Input&quot;, shape=(MAX_LEN), dtype='int64')
  bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')(input_layer)
  bert = bert[0][:,0,:]
  x = keras.layers.Bidirectional(keras.layers.LSTM(256, name=&quot;LSTM&quot;, activation='tanh', dropout=0.3), name=&quot;Bidirectional_LSTM&quot;)(bert)
  x = keras.layers.Dense(64, 'relu')(x)
  output_layer = keras.layers.Dense(1, 'sigmoid', name=&quot;Output&quot;)(x)

  model = keras.Model(inputs=input_layer, outputs=output_layer)

  model.compile(loss=loss,
                optimizer=optimizer)
  return model
</code></pre>
<p>On running
<code>model = build_model()</code></p>
<p>This is the error I am getting.</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-57-671884cecb64&gt; in &lt;module&gt;()
----&gt; 1 model = build_model()

4 frames
&lt;ipython-input-56-ef0d67347557&gt; in build_model()
      1 def build_model():
      2   input_layer = keras.layers.Input(name=&quot;Input&quot;, shape=(MAX_LEN), dtype='int64')
----&gt; 3   bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')(input_layer)
      4   bert = bert[0][:,0,:]
      5   x = keras.layers.Bidirectional(keras.layers.LSTM(256, name=&quot;LSTM&quot;, activation='tanh', dropout=0.3), name=&quot;Bidirectional_LSTM&quot;)(bert)

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    720             result = self._slow_forward(*input, **kwargs)
    721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
    723         for hook in itertools.chain(
    724                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, next_sentence_label, output_attentions, output_hidden_states, return_dict, **kwargs)
    938             output_attentions=output_attentions,
    939             output_hidden_states=output_hidden_states,
--&gt; 940             return_dict=return_dict,
    941         )
    942 

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    720             result = self._slow_forward(*input, **kwargs)
    721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
    723         for hook in itertools.chain(
    724                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)
    793             raise ValueError(&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;)
    794         elif input_ids is not None:
--&gt; 795             input_shape = input_ids.size()
    796         elif inputs_embeds is not None:
    797             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'Tensor' object has no attribute 'size'
</code></pre>
","python, tensorflow, keras, bert-language-model, pre-trained-model","<p>The error is caused because, you have created the Keras or Tensorflow based model. However, <code>BertForPreTraining</code> is a pytorch model.
Hence, you have to call the Tensorflow based BERT model as:</p>
<pre><code>from transformers import TFBertForPreTraining

###other lines of code###

bert = TFBertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')(input_layer)

###other lines of code###
</code></pre>
",0,0,2149,2020-10-10 07:41:22,https://stackoverflow.com/questions/64291153/attributeerror-tensor-object-has-no-attribute-size-pretrained-bert
Extracting word features from BERT model,"<p>So as you know, we can extract BERT features of word in a sentence. My question is, can we also extract word features that are not included in a sentence? For example, bert features of single words such as &quot;dog&quot;, &quot;human&quot;, etc.</p>
","word-embedding, bert-language-model, latent-semantic-analysis","<p>The very first layer of BERT is a static embeddings table, so you can use it as any other embeddings table and embeddings for words (or more frequently subwords) that BERT uses input to the first self-attentive layer. The static embeddings are only comparable with each other, not with the standard contextual embeddings. If need them comparable embeddings, you can try passing single-word sentences to BERT, but note that this will be an embeddings of a single-word sentenece, not the  word in general.</p>
<p>However, BERT is a sentence-level model that is supposed to get embeddings of words in context. It is not designed for static word embeddings, and methods specifically designed for static word embeddings (such as FastText) would certainly get better results.</p>
",2,0,909,2020-10-15 05:55:08,https://stackoverflow.com/questions/64365639/extracting-word-features-from-bert-model
How can I apply pruning on a BERT model?,"<p>I have trained a <a href=""https://en.wikipedia.org/wiki/BERT_(language_model)"" rel=""nofollow noreferrer"">BERT</a> model using ktrain (TensorFlow wrapper) to recognize emotion on text. It works, but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research, and it seems pruning could help.</p>
<p>TensorFlow provides some options for pruning, e.g., <em>tf.contrib.model_pruning</em>. The problem is that it is not a not a widely used technique. What would be a simple enough example that could help me to understand how to use it?</p>
<p>I provide my working code below for reference.</p>
<pre><code>import pandas as pd
import numpy as np
import preprocessor as p
import emoji
import re
import ktrain
from ktrain import text
from unidecode import unidecode
import nltk

# Text preprocessing class
class TextPreprocessing:
    def __init__(self):
        p.set_options(p.OPT.MENTION, p.OPT.URL)

    def _punctuation(self, val):
        val = re.sub(r'[^\w\s]', ' ', val)
        val = re.sub('_', ' ', val)
        return val

    def _whitespace(self, val):
        return &quot; &quot;.join(val.split())

    def _removenumbers(self, val):
        val = re.sub('[0-9] + ', '', val)
        return val

    def _remove_unicode(self, text):
        text = unidecode(text).encode(&quot;ascii&quot;)
        text = str(text, &quot;ascii&quot;)
        return text

    def _split_to_sentences(self, body_text):
        sentences = re.split(r&quot;(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s&quot;, body_text)
        return sentences

    def _clean_text(self, val):
        val = val.lower()
        val = self._removenumbers(val)
        val = p.clean(val)
        val = ' '.join(self._punctuation(emoji.demojize(val)).split())
        val = self._remove_unicode(val)
        val = self._whitespace(val)
        return val

    def text_preprocessor(self, body_text):

        body_text_df = pd.DataFrame({&quot;body_text&quot;: body_text}, index=[1])

        sentence_split_df = body_text_df.copy()

        sentence_split_df[&quot;body_text&quot;] = sentence_split_df[&quot;body_text&quot;].apply(
            self._split_to_sentences)

        lst_col = &quot;body_text&quot;
        sentence_split_df = pd.DataFrame(
            {
                col: np.repeat(
                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(
                    )
                )
                for col in sentence_split_df.columns.drop(lst_col)
            }
        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[
            sentence_split_df.columns
        ]

        body_text_df[&quot;body_text&quot;] = body_text_df[&quot;body_text&quot;].apply(self._clean_text)

        final_df = (
            pd.concat([sentence_split_df, body_text_df])
            .reset_index()
            .drop(columns=[&quot;index&quot;])
        )

        return final_df[&quot;body_text&quot;]

# Instantiate data preprocessing object
text1 = TextPreprocessing()

# Import data
data_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')
data_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')

# Clean the data
data_train['Text'] = data_train['Text'].apply(text1._clean_text)
data_test['Text'] = data_test['Text'].apply(text1._clean_text)

X_train = data_train.Text.tolist()
X_test = data_test.Text.tolist()

y_train = data_train.Emotion.tolist()
y_test = data_test.Emotion.tolist()

data = data_train.append(data_test, ignore_index=True)

class_names = ['joy', 'sadness', 'fear', 'anger', 'neutral']

encoding = {
    'joy': 0,
    'sadness': 1,
    'fear': 2,
    'anger': 3,
    'neutral': 4
}

# Integer values for each class
y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]

trn, val, preproc = text.texts_from_array(x_train=X_train, y_train=y_train,
                                          x_test=X_test, y_test=y_test,
                                          class_names=class_names,
                                          preprocess_mode='distilbert',
                                          maxlen=350)

model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)

learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)

predictor = ktrain.get_predictor(learner.model, preproc)

# Save the model on a file for later use
predictor.save(&quot;models/bert_model&quot;)

message = &quot;This is a happy message&quot;

# Cleaning - takes 5 ms to run
clean = text1._clean_text(message)

# Prediction - takes 325 ms to run
predictor.predict_proba(clean)
</code></pre>
","python, tensorflow, nlp, bert-language-model, huggingface-transformers","<p>The <code>distilbert</code> model in <strong>ktrain</strong> is created using Hugging Face <strong>transformers</strong>, which means you can use that library to prune the model.  See <a href=""https://huggingface.co/transformers/bertology.html"" rel=""nofollow noreferrer"">this link</a> for more information and <a href=""https://github.com/huggingface/transformers/blob/master/examples/research_projects/bertology/run_bertology.py"" rel=""nofollow noreferrer"">the example script</a>. You may need to convert the model to PyTorch before using the script (in addition to making some modifications to the script itself). The approach is based on the paper <a href=""https://arxiv.org/abs/1905.10650"" rel=""nofollow noreferrer"">Are Sixteen Heads Really Better Than One?</a>.</p>
",3,4,1703,2020-10-20 13:04:26,https://stackoverflow.com/questions/64445784/how-can-i-apply-pruning-on-a-bert-model
huggingface - save fine tuned model locally - and tokenizer too?,"<p>I just wonder if the tokenizer is somehow affected or changed if fine tune a BERT model and save it. Do I need to save the tokenizer locally too to reload it when using the saved BERT model later?</p>
<p>I just do:</p>
<pre><code>bert_model.save_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>then later</p>
<pre><code>bert_model = TFBertModel.from_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>But do i need to saver the tokenizer too? Or could I just use it in the normal way like:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
","bert-language-model, huggingface-transformers","<p>In your case, the tokenizer need not be saved as it you have not changed the tokenizer or added new tokens. Huggingface tokenizer provides an option of adding new tokens or redefining the special tokens such as <code>[MASK]</code>, <code>[CLS]</code>, etc. If you do such modifications, then you may have to save the tokenizer to reuse it later.</p>
",8,4,4762,2020-10-20 13:37:34,https://stackoverflow.com/questions/64446355/huggingface-save-fine-tuned-model-locally-and-tokenizer-too
How is the number of parameters be calculated in BERT model?,"<p>The paper &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&quot; by Devlin &amp; Co. calculated for the base model size 110M parameters (i.e. L=12, H=768, A=12) where L = number of layers, H = hidden size and A = number of self-attention operations. As far as I know parameters in a neural network are usually the count of &quot;weights and biases&quot; between the layers. So how is this calculated based on the given information? 12<em>768</em>768*12?</p>
","neural-network, nlp, bert-language-model","<p><a href=""https://i.sstatic.net/BhVnx.png"" rel=""noreferrer"">Transformer Encoder-Decoder Architecture</a>
The BERT model contains only the encoder block of the transformer architecture. Let's look at individual elements of an encoder block for BERT to visualize the number weight matrices as well as the bias vectors. The given configuration L = 12 means there will be 12 layers of self attention, H = 768 means that the embedding dimension of individual tokens will be of 768 dimensions, A = 12 means there will be 12 attention heads in one layer of self attention. The encoder block performs the following sequence of operations:</p>
<ol>
<li><p>The input will be the sequence of tokens as a matrix of S * d dimension. Where s is the sequence length and d is the embedding dimension. The resultant input sequence will be the sum of token embeddings, token type embeddings as well as position embedding as a d-dimensional vector for each token. In the BERT model, the first set of parameters is the vocabulary embeddings. BERT uses WordPiece[<a href=""https://arxiv.org/abs/1609.08144"" rel=""noreferrer"">2</a>] embeddings that has 30522 tokens. Each token is of 768 dimensions.</p>
</li>
<li><p>Embedding layer normalization. One weight matrix and one bias vector.</p>
</li>
<li><p>Multi-head self attention. There will be h number of heads, and for each head there will be three matrices which will correspond to query matrix, key matrix and the value matrix. The first dimension of these matrices will be the embedding dimension and the second dimension will be the embedding dimension divided by the number of attention heads. Apart from this, there will be one more matrix to transform the concatenated values generated by attention heads to the final token representation.</p>
</li>
<li><p>Residual connection and layer normalization. One weight matrix and one bias vector.</p>
</li>
<li><p>Position-wise feedforward network will have one hidden layer, that will correspond to two weight matrices and two bias vectors. In the paper, it is mentioned that the number of units in the hidden layer will be four times the embedding dimension.</p>
</li>
<li><p>Residual connection and layer normalization. One weight matrix and one bias vector.</p>
</li>
</ol>
<p>Let's calculate the actual number of parameters by associating the right dimensions to the weight matrices and bias vectors for the BERT base model.</p>
<p><strong>Embedding Matrices:</strong></p>
<ul>
<li>Word Embedding Matrix size [Vocabulary size, embedding dimension] = [30522, 768] = 23440896</li>
<li>Position embedding matrix size, [Maximum sequence length, embedding dimension] = [512, 768] = 393216</li>
<li>Token Type Embedding matrix size [2, 768] = 1536</li>
<li>Embedding Layer Normalization, weight and Bias [768] + [768] = 1536</li>
<li>Total Embedding parameters = <strong>𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 ≈ 𝟐𝟒𝑴</strong></li>
</ul>
<p><strong>Attention Head:</strong></p>
<ul>
<li><p>Query Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li><p>Key Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li><p>Value Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li><p>Total parameters for one layer attention with 12 heads = 12∗(3 ∗(49152+768)) = 1797120</p>
</li>
<li><p>Dense weight for projection after concatenation of heads [768, 768] = 589824 and Bias [768] = 768, (589824+768 = 590592)</p>
</li>
<li><p>Layer Normalization weight and Bias [768], [768] = 1536</p>
</li>
<li><p>Position wise feedforward network weight matrices and bias [3072, 768] = 2359296, [3072] = 3072 and [768, 3072 ] = 2359296, [768] = 768, (2359296+3072+ 2359296+768 = 4722432)</p>
</li>
<li><p>Layer Normalization weight and Bias [768], [768] = 1536</p>
</li>
<li><p>Total parameters for one complete attention layer (1797120 + 590592 + 1536 + 4722432 + 1536 = <strong>7113216 ≈ 7𝑀</strong>)</p>
</li>
<li><p>Total parameters for 12 layers of attention (𝟏𝟐 ∗ 𝟕𝟏𝟏𝟑𝟐𝟏𝟔 = <strong>𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 ≈ 𝟖𝟓𝑴</strong>)</p>
</li>
</ul>
<p><strong>Output layer of BERT Encoder:</strong></p>
<ul>
<li>Dense Weight Matrix and Bias [768, 768] = 589824, [768] = 768, (589824 + 768 = 590592)</li>
</ul>
<p><em>Total Parameters in 𝑩𝑬𝑹𝑻 𝑩ase = 𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 + 𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 + 𝟓𝟗𝟎𝟓𝟗𝟐 = <strong>𝟏𝟎𝟗𝟕𝟖𝟔𝟑𝟔𝟖 ≈ 𝟏𝟏𝟎𝑴</strong></em></p>
",27,10,22206,2020-10-22 15:41:18,https://stackoverflow.com/questions/64485777/how-is-the-number-of-parameters-be-calculated-in-bert-model
Training on deeppavlov for NER keeps failing,"<p>I have been trying to train a deeppavlov model for NER based on the train syntax given on their docs and it keeps failing with below error message:</p>
<pre><code>/opt/anaconda3/envs/py36/lib/python3.6/site-packages/deeppavlov/dataset_readers/conll2003_reader.py in parse_ner_file(self, file_name)
    104                     items = line.split()
    105                     if len(items) &lt; expected_items:
--&gt; 106                         raise Exception(f&quot;Input is not valid {line}&quot;)
    107                     tokens.append(items[0])
    108                     tags.append(items[-1])

Exception: Input is not valid aio-pika==6.4.1

</code></pre>
<p>Used the following code to train the deeppavlov model, it seems to be working on their sample dataset, but when I created my own dataset as per their training sample guide, I keep getting above error message.
Training ner code:</p>
<pre><code>from deeppavlov import configs, train_model, build_model
from deeppavlov.core.commands.utils import parse_config
import json


with configs.ner.ner_ontonotes_bert_mult.open(encoding='utf8') as f:
    ner_config = json.load(f)

ner_config['dataset_reader']['data_path'] = '/Users/smankari001/deeppavlov'  # directory with train.txt, valid.txt and test.txt files
ner_config['metadata']['variables']['NER_PATH'] = '/Users/smankari001/deeppavlov'
ner_config['metadata']['download'] = [ner_config['metadata']['download'][-1]]  # do not download the pretrained ontonotes model

ner_model = train_model(ner_config, download=True)
</code></pre>
<p>input train.txt file:</p>
<pre><code>What    O
kind    O
of  O
memory  O
?   O

We  O
respectfully    O
invite  O
you O
to  O
watch   O
a   O
special O
edition O
of  O
Across  B-ORG
China   I-ORG
.   O

WW  B-WORK_OF_ART
II  I-WORK_OF_ART
Landmarks   I-WORK_OF_ART
on  I-WORK_OF_ART
the I-WORK_OF_ART
Great   I-WORK_OF_ART
Earth   I-WORK_OF_ART
of  I-WORK_OF_ART
China   I-WORK_OF_ART
:   I-WORK_OF_ART
Eternal I-WORK_OF_ART
Memories    I-WORK_OF_ART
of  I-WORK_OF_ART
Taihang I-WORK_OF_ART
Mountain    I-WORK_OF_ART

Standing    O
tall    O
on  O
Taihang B-LOC
Mountain    I-LOC
is  O
the B-WORK_OF_ART
Monument    I-WORK_OF_ART
to  I-WORK_OF_ART
the I-WORK_OF_ART
Hundred I-WORK_OF_ART
Regiments   I-WORK_OF_ART
Offensive   I-WORK_OF_ART
.   O

It  O
is  O
composed    O
of  O
a   O
primary O
stele   O
,   O
secondary   O
steles  O
,   O
a   O
huge    O
round   O
sculpture   O
and O
beacon  O
tower   O
,   O
and O
the B-WORK_OF_ART
Great   I-WORK_OF_ART
Wall    I-WORK_OF_ART
,   O
among   O
other   O
things  O
.   O

A   O
primary O
stele   O
,   O
three   B-CARDINAL
secondary   O
steles  O
,   O
and O
two B-CARDINAL
inscribed   O
steles  O
.   O

The B-EVENT
Hundred I-EVENT
Regiments   I-EVENT
Offensive   I-EVENT
was O
the O
campaign    O
of  O
the O
largest O
scale   O
launched    O
by  O
the B-ORG
Eighth  I-ORG
Route   I-ORG
Army    I-ORG
during  O
the B-EVENT
War I-EVENT
of  I-EVENT
Resistance  I-EVENT
against I-EVENT
Japan   I-EVENT
.   O

This    O
campaign    O
broke   O
through O
the O
Japanese    B-NORP
army    O
's  O
blockade    O
to  O
reach   O
base    O
areas   O
behind  O
enemy   O
lines   O
,   O
stirring    O
up  O
anti-Japanese   B-NORP
spirit  O
throughout  O
the O
nation  O
and O
influencing O
the O
situation   O
of  O
the O
anti-fascist    O
war O
of  O
the O
people  O
worldwide   O
.   O

</code></pre>
","bert-language-model, named-entity-recognition, deeppavlov","<p>As <code>ner_config['dataset_reader']['data_path']</code> you need to specify path to folder with only dataset files (train/valid/test).</p>
<p>This error:</p>
<pre><code>Exception: Input is not valid aio-pika==6.4.1
</code></pre>
<p>says that DatasetReader started to read lines from <code>requirements.txt</code> file.</p>
",0,0,291,2020-10-23 12:30:21,https://stackoverflow.com/questions/64500038/training-on-deeppavlov-for-ner-keeps-failing
BERT tokenize URLs,"<p>I want to classify a bunch of tweets and therefore I'm using the huggingface implementation of BERT. However I noticed that the deafult BertTokenizer does not use special tokens for urls.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from transformers import BertTokenizer
&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
&gt;&gt;&gt; tokenizer.tokenize(&quot;https://stackoverflow.com/questions/ask&quot;)
['https', ':', '/', '/', 'stack', '##over', '##flow', '.', 'com', '/', 'questions', '/', 'ask']
</code></pre>
<p>This seems quite inefficent to me. What would be the best way, to encode URLs?</p>
","python, machine-learning, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>Well, it depends. If the URL contains information that is relevant for the classification, then the best thing you can do is keeping it as it is. There certainly were some URLs in the pre-training data and BERT learned how to handle them properly.</p>
<p>If you are sure, the URLs are irrelevant for the classificaion, you can replace them by a special token, which is a very common thing to do in NLP in general. But in that case, you need to fine-tune BERT, so it knows what the special token mean. If you do not fine-tune BERT and only train a classifier on top of it, then again, the best thing you can do is keeping the URLs as they are.</p>
",3,1,2435,2020-10-27 23:51:42,https://stackoverflow.com/questions/64564545/bert-tokenize-urls
BERT embeddings in SPARKNLP or BERT for token classification in huggingface,"<p>Currently I am working on productionize a NER model on Spark. I have a current implementation that is using Huggingface DISTILBERT with the TokenClassification head, but as the performance is a bit slow and costly, I am trying to find ways to optimize.</p>
<p>I have checked SPARKNLP implementation, which lacks a pretrained DISTILBERT and has I think a different approach, so some questions regarding this arose:</p>
<ol>
<li>Huggingface uses the entire BERT model and adds a head for token classification. Is this the same as obtaining the BERT embeddings and just feeding them to another NN?</li>
<li>I ask this because this is the SPARKNLP approach, a class that helps obtaim those embeddings and use it as a feature for another complex NN. Doesnt this lose some of the knowledge inside BERT?</li>
<li>Does SPARKNLP have any optimization regarding SPARK that helps in inference time or is it just another BERT implementation.</li>
</ol>
","nlp, bert-language-model, huggingface-transformers, johnsnowlabs-spark-nlp","<p>To answer your Question no. 1:</p>
<p>Hugging face uses different head for different tasks, this is almost the same as what the authors of BERT did with their model. They added task-specific layer on top of the existing model to fine-tune for a particular task. One thing that must be noted here is that when you add task specific layer (a new layer), you jointly learn the new layer and update the existing learnt weights of the BERT model. So, basically your BERT model is part of gradient updates. This is quite different from obtaining the embeddings and then using it as input to Neural Nets.</p>
<p>Question 2:
When you obtain the embeddings and use it for another complex model, I am not sure how to quantify in terms of loosing the information, because you are still using the information obtained using BERT, from your data to build another model. So, we cannot attribute to loosing the information, but the performance need not be the best when compared with learning another model on top of BERT (and along with BERT).</p>
<p>Often, people would obtain the embeddings and then as input to another the classifier due to resource constraint, where it may not be feasible to train or fine-tune BERT.</p>
",1,2,791,2020-10-30 10:09:40,https://stackoverflow.com/questions/64606333/bert-embeddings-in-sparknlp-or-bert-for-token-classification-in-huggingface
BERT-based NER model giving inconsistent prediction when deserialized,"<p>I am trying to train an NER model using the HuggingFace transformers library on Colab cloud GPUs, pickle it and load the model on my own CPU to make predictions.</p>
<p><strong>Code</strong></p>
<p>The model is the following:</p>
<pre><code>from transformers import BertForTokenClassification

model = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=NUM_LABELS,
    output_attentions = False,
    output_hidden_states = False
)
</code></pre>
<p>I am using this snippet to save the model on Colab</p>
<pre><code>import torch

torch.save(model.state_dict(), FILENAME)
</code></pre>
<p>Then load it on my local CPU using</p>
<pre><code># Initiating an instance of the model type

model_reload = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)

# Loading the model
model_reload.load_state_dict(torch.load(FILENAME, map_location='cpu'))
model_reload.eval()

</code></pre>
<p>The code snippet used to tokenize the text and make actual predictions is the same both on the Colab GPU notebook instance and my CPU notebook instance.</p>
<p><strong>Expected Behavior</strong></p>
<p>The GPU-trained model behaves correctly and classifies the following tokens perfectly:</p>
<pre><code>O       [CLS]
O       Good
O       morning
O       ,
O       my
O       name
O       is
B-per   John
I-per   Kennedy
O       and
O       I
O       am
O       working
O       at
B-org   Apple
O       in
O       the
O       headquarters
O       of
B-geo   Cupertino
O       [SEP]
</code></pre>
<p><strong>Actual Behavior</strong></p>
<p>When loading the model and use it to make predictions on my CPU, the predictions are totally wrong:</p>
<pre><code>I-eve   [CLS]
I-eve   Good
I-eve   morning
I-eve   ,
I-eve   my
I-eve   name
I-eve   is
I-geo   John
B-eve   Kennedy
I-eve   and
I-eve   I
I-eve   am
I-eve   working
I-eve   at
I-gpe   Apple
I-eve   in
I-eve   the
I-eve   headquarters
I-eve   of
B-org   Cupertino
I-eve   [SEP]
</code></pre>
<p>Does anyone have ideas why it doesn't work? Did I miss something?</p>
","python, pytorch, bert-language-model, huggingface-transformers","<p>I fixed it, there were two problems:</p>
<ol>
<li><p>The index-label mapping for tokens was wrong, for some reason the <code>list()</code> function worked differently on Colab GPU than my CPU (??)</p>
</li>
<li><p>The snippet used to save the model was not correct, for models based on the huggingface-transformers library you can't use <code>model.save_dict()</code> and load it later, you need to use the <code>save_pretrained()</code> method of your model class, and load it later using <code>from_pretrained()</code>.</p>
</li>
</ol>
",4,5,2113,2020-10-30 15:00:15,https://stackoverflow.com/questions/64610841/bert-based-ner-model-giving-inconsistent-prediction-when-deserialized
word synonym / antonym detection,"<p>I need to create a classifier that takes 2 words and determines if they are synonyms or antonyms. I tried nltk's antsyn-net but it doesn't have enough data.</p>
<p>example:</p>
<ul>
<li>capitalism &lt;-[antonym]-&gt; socialism</li>
<li>capitalism =[synonym]= free market</li>
<li>god &lt;-[antonym]-&gt; atheism</li>
<li>political correctness &lt;-[antonym]-&gt; free speach</li>
<li>advertising =[synonym]= marketing</li>
</ul>
<p>I was thinking about taking a BERT model, because may be some of the relations would be embedded  in it and transfer-learn on a data-set that I found.</p>
","deep-learning, nlp, pytorch, data-science, bert-language-model","<p>I would suggest a following pipeline:</p>
<ol>
<li>Construct a training set from existing dataset of synonyms and antonyms (taken e.g. from the Wordnet thesaurus). You'll need to craft negative examples carefully.</li>
<li>Take a pretrained model such as BERT and fine-tune it on your tasks. If you choose BERT, it should be probably <code>BertForNextSentencePrediction</code> where you use your words/prhases instead of sentences, and predict 1 if they are synonyms and 0 if they are not; same for antonyms.</li>
</ol>
",1,0,913,2020-11-01 20:42:51,https://stackoverflow.com/questions/64636782/word-synonym-antonym-detection
Train BERT with CLI commands,"<p>I have downloaded the HuggingFace BERT model from the transformer repository found <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">here</a> and would like to train the model on custom NER labels by using the run_ner.py script as it is referenced <a href=""https://huggingface.co/transformers/task_summary.html"" rel=""nofollow noreferrer"">here</a> in the section &quot;Named Entity Recognition&quot;.</p>
<p>I define model (&quot;bert-base-german-cased&quot;), data_dir (&quot;Data/sentence_data.txt&quot;) and labels (&quot;Data/labels.txt)&quot; as defaults in the code.</p>
<p>Now I'm using this input for the command line:</p>
<pre><code>python run_ner.py --output_dir=&quot;Models&quot; --num_train_epochs=3 --logging_steps=100 --do_train --do_eval --do_predict
</code></pre>
<p>But all it does is telling me:</p>
<pre><code>Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.w
eight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>After that it just stops, not ending the script, but simply waiting.</p>
<p>Does anyone know what could be the problem here? Am I missing a parameter?</p>
<p>My sentence_data.txt in CoNLL format looks like this (small snippet):</p>
<pre><code>Strafverfahren O
gegen O
; O
wegen O
Diebstahls O
hat O
das O
Amtsgericht Ort
Leipzig Ort
- O
Strafrichter O
</code></pre>
<p>And that's how I defined my labels in labels.txt:</p>
<pre><code>&quot;Date&quot;, &quot;Delikt&quot;, &quot;Strafe_Tatbestand&quot;, &quot;Schadensbetrag&quot;, &quot;Geständnis_ja&quot;, &quot;Vorstrafe_ja&quot;, &quot;Vorstrafe_nein&quot;, &quot;Ort&quot;,
&quot;Strafe_Gesamtfreiheitsstrafe_Dauer&quot;, &quot;Strafe_Gesamtsatz_Dauer&quot;, &quot;Strafe_Gesamtsatz_Betrag&quot;
</code></pre>
","python, machine-learning, nlp, bert-language-model, huggingface-transformers","<p>Found out the problem. It had to do with the CUDA driver not being compatible with the installed version of pytorch.</p>
<p>For anyone with Nvidia GPU encountering the same problem: going to the Nvidia control panel -&gt; Help -&gt; System Information -&gt; Components, there is a setting called &quot;NVCUDA.DLL&quot; with a driver number in the names column. Choosing the corresponding CUDA version in the installation builder on pytorch.org should do the trick.</p>
<p>Also, there is a good Readme in the transformers repository explaining all steps to train the BERT model with CLI commands <a href=""https://github.com/huggingface/transformers/blob/master/examples/token-classification/README.md"" rel=""nofollow noreferrer"">here</a>.</p>
",0,0,461,2020-11-02 13:53:54,https://stackoverflow.com/questions/64646890/train-bert-with-cli-commands
How do I import BERTforsequencclassification in same way importing bert?,"<p>I have a full code work on bert model using this code segment</p>
<pre><code>import bert #BertForSequenceClassification
from bert import run_classifier
from bert import optimization
from bert import tokenization
</code></pre>
<p>How I can import BertForSequenceClassification instead of bert and work on the same code, given that I am working using transformers?
thanks</p>
","import, bert-language-model","<p>If you are using huggingface <code>transformers</code> library, then you can use it as follows:</p>
<p><code>from transformers import BertForSequenceClassification</code></p>
<p><code>BertForSequenceClassification</code> is a class available in transformers library of hugging-face. It is not available in the BERT provided by the authors of BERT</p>
",1,1,856,2020-11-02 20:46:55,https://stackoverflow.com/questions/64653106/how-do-i-import-bertforsequencclassification-in-same-way-importing-bert
Fine-tune Bert for specific domain (unsupervised),"<p>I want to fine-tune BERT on texts that are related to a specific domain (in my case related to engineering). The training should be unsupervised since I don't have any labels or anything. Is this possible?</p>
","python, deep-learning, neural-network, nlp, bert-language-model","<p>What you in fact want to is continue pre-training BERT on text from your specific domain. What you do in this case is to continue training the model as masked language model, but on your domain-specific data.</p>
<p>You can use the <a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py"" rel=""nofollow noreferrer""><code>run_mlm.py</code></a> script from the Huggingface's Transformers.</p>
",9,7,2193,2020-11-06 09:54:40,https://stackoverflow.com/questions/64712375/fine-tune-bert-for-specific-domain-unsupervised
How to parse or clean my corpus in Python,"<p>So I have this corpus with Dutch chat messages, but I want to remove the usernames within the &lt; &gt; brackets. I am not really familiar with parsing in python. Also, I'm not sure if parsing is the right way to remove the usernames. I am actually looking for advice. How do I remove the usernames in python.</p>
<p>This is what the .txt file looks like:</p>
<pre><code>&lt;Chickaaa&gt; Heeerlijk zo'n kopje warme chocolademelk
&lt;ilmas-nador&gt; 3ndak  chi  khtk
&lt;Chickaaa&gt; met een sultana derbij
&lt;bellamafia&gt; hahah
&lt;bellamafia&gt; welkom terug chika
&lt;Chickaaa&gt; dankjee
&lt;bellamafia&gt; ga je nog naar school
&lt;Chickaaa&gt; jazeker
&lt;bellamafia&gt; ok
&lt;Chickaaa&gt; ben op stage nu
&lt;Chickaaa&gt; nog 7 uurtjes
&lt;Chickaaa&gt; pff
&lt;bellamafia&gt; wat doe je dan
&lt;Chickaaa&gt; management assistent
&lt;bellamafia&gt; ok
&lt;Chickaaa&gt; jij?
</code></pre>
<p>I need to put the sentences between a [CLS] and [SEP] if I want to tokenize them. The reason for this is to use the word embedding model BERT. I am reading the .txt as followed:</p>
<pre><code>df = pd.read_fwf('moroccorp.txt')
</code></pre>
<p>After that I want to mark the sentences like this:</p>
<pre><code>marked_text = &quot;[CLS] &quot; + df + &quot; [SEP]&quot;
</code></pre>
<p>and tokenize it in this way:</p>
<pre><code># Tokenize our sentence with the BERT tokenizer.
tokenized_text = tokenizer.tokenize(marked_text)
</code></pre>
","python, parsing, bert-language-model","<p>If your sample is representative, simply remove the <code>&lt;...&gt;</code> from each beginning of line.</p>
<pre class=""lang-py prettyprint-override""><code>import re

user = re.compile(r'^&lt;[^&lt;&gt;]+&gt;\s+')
with open(filename) as corpus:
  text = [user.sub('', line) for line in corpus]
</code></pre>
<p>If you want to do this in Pandas, it should not be hard to find a similar recipe for doing this transformation as part of your current code.</p>
<p><em>Parsing</em> generally refers to picking apart a structure of some sort (like dividing a sentence into subject, verb, and object), whereas this is a simple mechanical transformation.</p>
",1,0,123,2020-11-06 10:21:05,https://stackoverflow.com/questions/64712760/how-to-parse-or-clean-my-corpus-in-python
download the following model: distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es,"<p>I have two PCs: one of them has an internet connection and the other PC does not have an internet connection, I need to download the following model: <code>distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es</code>, but not I find the link.</p>
<p>I do not have and i cannot install python on the pc with internet access, but I can use wget.</p>
<p>Where i can download the following model: <code>distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es</code>?</p>
","bert-language-model, transformer-model","<p>You can get and use the model directly from Transformers library at this <a href=""https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"" rel=""nofollow noreferrer"">link</a>.</p>
<p>You can also copy the example code from the same page and include it in your project using your pc with internet.</p>
",0,0,488,2020-11-06 18:06:19,https://stackoverflow.com/questions/64719591/download-the-following-model-distill-bert-base-spanish-wwm-cased-finetuned-spa
Error implementing pretrained BETO model (spanish version of bert) using pytorch,"<p>I am trying to test this model called BETO (the model is an implementation of Bert in Spanish):</p>
<pre><code>!pip install transformers
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz 
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt 
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json 
!tar -xzvf pytorch_weights.tar.gz
!mv config.json pytorch/.
!mv vocab.txt pytorch/.

import torch
from transformers import BertForMaskedLM, BertTokenizer
tokenizer = BertTokenizer.from_pretrained(&quot;pytorch/&quot;, do_lower_case=False)
model = BertForMaskedLM.from_pretrained(&quot;pytorch/&quot;)
model.eval()

</code></pre>
<p>the enviroment is as follows:</p>
<pre><code>platform            debian 10
transformers            3.4.0
python                  3.7.3
torch                   1.7.0
tensorflow              2.3.1
</code></pre>
<p>but in the following line:</p>
<pre><code>model = BertForMaskedLM.from_pretrained(&quot;pytorch/&quot;)
</code></pre>
<p>I get this error:</p>
<pre><code>Exception has occurred: OSError
Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.
</code></pre>
<p>Thanks in advance</p>
","python, torch, bert-language-model","<p>I've tried the exact same code in Ubuntu with no problems.</p>
<p><strong>OSError</strong> is a built-in exception in Python and serves as the error class for the os module, which is raised when an os specific system function returns a system-related error, including I/O failures such as “file not found” or “disk full”.</p>
<p>Maybe you can dig deeper into your OSError?</p>
",0,1,351,2020-11-07 22:37:51,https://stackoverflow.com/questions/64733098/error-implementing-pretrained-beto-model-spanish-version-of-bert-using-pytorch
How to generate a list of tokens that are most likely to occupy the place of a missing token in a given sentence?,"<p>How to generate a list of tokens that are most likely to occupy the place of a missing token in a given sentence?</p>
<p>I've found this <a href=""https://stackoverflow.com/questions/56822991/an-nlp-model-that-suggest-a-list-of-words-in-an-incomplete-sentence?rq=1"">StackOverflow answer</a>, however, this only generates <strong>a</strong> possible word, and <strong>not a list</strong> of words that fits the sentence. I tried printing out every variable to see if he might have generated all the possible words, but no luck.</p>
<p>For example,</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; sentence = 'Cristiano Ronaldo dos Santos Aveiro GOIH ComM is a Portuguese professional [].' # [] is missing word
&gt;&gt;&gt; generate(sentence)
['soccer', 'basketball', 'tennis', 'rugby']
</code></pre>
","python, nlp, nltk, spacy, bert-language-model","<p>You can essentially do the same as in <a href=""https://stackoverflow.com/questions/56822991/an-nlp-model-that-suggest-a-list-of-words-in-an-incomplete-sentence?rq=1"">this answer</a>, but instead of adding just the best fitting token, take for example the five most fitting tokens:</p>
<pre><code>def fill_the_gaps(text):
    text = '[CLS] ' + text + ' [SEP]'
    tokenized_text = tokenizer.tokenize(text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [0] * len(tokenized_text)
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])
    with torch.no_grad():
        predictions = model(tokens_tensor, segments_tensors)
    results = []
    for i, t in enumerate(tokenized_text):
        if t == '[MASK]':
            #instead of argmax, we use argsort to sort the tokens which best fit
            predicted_index = torch.argsort(predictions[0, i], descending=True)
            tokens = []
            #the the 5 best fitting tokens and add the to the list
            for k in range(5):
                 predicted_token = tokenizer.convert_ids_to_tokens([predicted_index[k].item()])[0]
                tokens.append(predicted_token)
            results.append(tokens)
    return results
</code></pre>
<p>For you sentence, this results in : <code>[['footballer', 'golfer', 'football', 'cyclist', 'boxer']]</code></p>
",1,1,333,2020-11-19 04:16:01,https://stackoverflow.com/questions/64905346/how-to-generate-a-list-of-tokens-that-are-most-likely-to-occupy-the-place-of-a-m
Error &quot;version&quot; not found after adding bert as a submodule to my git repo,"<p>After adding BERT as a submodule, cannot use it, the version info is missing in the config file. These are the main steps:</p>
<p>1- I've used the <code>git submodule add https://huggingface.co/bert-base-multilingual-uncased</code> command to add it as a submodule to my repos
2- I've put it in a directory whose name is: <code>pretrained/mbert/</code>
3- I've used the following code to use it:</p>
<pre><code>from sentence_transformers import SentenceTransformer


def embed_text(sentences, pretrained=&quot;../pretrained/mbert/bert-base-multilingual-cased&quot;): 
    &quot;&quot;&quot;
    Computes the embeddings of the different sentences in input.
    :param sentences: list, of sentences
    :param pretrained: str, the pretrained bert model
    :return: list, of list
    &quot;&quot;&quot;

    model = SentenceTransformer(pretrained) 
    sentence_embeddings = model.encode(sentences)

    return [arr.tolist() for arr in sentence_embeddings]
</code></pre>
<p>I've got the following error:</p>
<pre><code>model = SentenceTransformer(pretrained)  
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sentence_transformers\SentenceTransformer.py&quot;, line 104, in __init__
    if config['__version__'] &gt; __version__:
KeyError: '__version__'
</code></pre>
","git, git-submodules, bert-language-model","<p>That cannot directly be used, the model download from huggingface.co. See this <a href=""https://github.com/UKPLab/sentence-transformers/issues/184"" rel=""nofollow noreferrer"">issue</a>, the model folder frameworks are different between the trained PTM using transformer and trained ones using sentence-transformer.</p>
<p>For PTM trained using sentence-transformer,</p>
<blockquote>
<p>The folder should consist these files:<br />
0_Transformer/<br />
1_Pooling/<br />
config.json<br />
modules.json</p>
</blockquote>
",2,3,1704,2020-11-19 16:14:46,https://stackoverflow.com/questions/64915612/error-version-not-found-after-adding-bert-as-a-submodule-to-my-git-repo
RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1,"<p>I'm trying to build a model for document classification. I'm using <code>BERT</code> with <code>PyTorch</code>.</p>
<p>I got the bert model with below code.</p>
<pre><code>bert = AutoModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>This is the code for training.</p>
<pre><code>for epoch in range(epochs):
 
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    #train model
    train_loss, _ = modhelper.train(proc.train_dataloader)

    #evaluate model
    valid_loss, _ = modhelper.evaluate()

    #save the best model
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(modhelper.model.state_dict(), 'saved_weights.pt')

    # append training and validation loss
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
<p>this is my train method, accessible with the object <code>modhelper</code>.</p>
<pre><code>def train(self, train_dataloader):
    self.model.train()
    total_loss, total_accuracy = 0, 0
    
    # empty list to save model predictions
    total_preds=[]
    
        # iterate over batches
    for step, batch in enumerate(train_dataloader):
        
        # progress update after every 50 batches.
        if step % 50 == 0 and not step == 0:
            print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
        
        # push the batch to gpu
        #batch = [r.to(device) for r in batch]
        
        sent_id, mask, labels = batch
        
        # clear previously calculated gradients 
        self.model.zero_grad()        

        print(sent_id.size(), mask.size())
        # get model predictions for the current batch
        preds = self.model(sent_id, mask) #This line throws the error
        
        # compute the loss between actual and predicted values
        self.loss = self.cross_entropy(preds, labels)
        
        # add on to the total loss
        total_loss = total_loss + self.loss.item()
        
        # backward pass to calculate the gradients
        self.loss.backward()
        
        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # update parameters
        self.optimizer.step()
        
        # model predictions are stored on GPU. So, push it to CPU
        #preds=preds.detach().cpu().numpy()
        
        # append the model predictions
        total_preds.append(preds)
      
    # compute the training loss of the epoch
    avg_loss = total_loss / len(train_dataloader)
    
    # predictions are in the form of (no. of batches, size of batch, no. of classes).
    # reshape the predictions in form of (number of samples, no. of classes)
    total_preds  = np.concatenate(total_preds, axis=0)
      
    #returns the loss and predictions
    return avg_loss, total_preds
</code></pre>
<p><code>preds = self.model(sent_id, mask)</code> this line throws the following error(including full traceback).</p>
<pre><code> Epoch 1 / 1
torch.Size([32, 4000]) torch.Size([32, 4000])
Traceback (most recent call last):

File &quot;&lt;ipython-input-39-17211d5a107c&gt;&quot;, line 8, in &lt;module&gt;
train_loss, _ = modhelper.train(proc.train_dataloader)

File &quot;E:\BertTorch\model.py&quot;, line 71, in train
preds = self.model(sent_id, mask)

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\model.py&quot;, line 181, in forward
#pass the inputs to the model

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 837, in forward
embedding_output = self.embeddings(

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 201, in forward
embeddings = inputs_embeds + position_embeddings + token_type_embeddings

RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>If you observe I've printed the torch size in the code.
<code>print(sent_id.size(), mask.size())</code></p>
<p>The output of that line of code is <code>torch.Size([32, 4000]) torch.Size([32, 4000])</code>.</p>
<p>as we can see that size is the same but it throws the error. Please put your thoughts. Really appreciate it.</p>
<p>please comment if you need further information. I'll be quick to add whatever is required.</p>
","python, deep-learning, pytorch, bert-language-model, huggingface-transformers","<p>The issue is regarding the BERT's limitation with the word count. I've passed the word count as 4000 where the maximum supported is 512(have to give up 2 more for '[cls]' &amp; '[Sep]' at the beginning and the end of the string, so it is 510 only). Reduce the word count or use some other model for your promlem. something like <a href=""https://medium.com/dair-ai/longformer-what-bert-should-have-been-78f4cd595be9"" rel=""noreferrer"">Longformers</a> as suggested by @cronoik in the comments above.</p>
<p>Thanks.</p>
",20,9,23663,2020-11-26 14:01:21,https://stackoverflow.com/questions/65023526/runtimeerror-the-size-of-tensor-a-4000-must-match-the-size-of-tensor-b-512
how to handle BERT &quot;UNK&quot; Token in the output prediction,"<p>I train a pre trained BERT model on my data.<br />
I try to make a Json containing two list:<br />
first: a list conclude prediction of model (desire value)<br />
second: a list of true value</p>
<p>but the first list has many ['UNK'] token in it<br />
some thing like this:<br />
<a href=""https://i.sstatic.net/6bheR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6bheR.png"" alt=""enter image description here"" /></a><br />
why this happen? and how can I solve it?</p>
<p>this UNK tag make the prediction result near to zero:(
because the accuracy rate is base on <strong>exact match</strong> of <em>true</em> and <em>desire</em> and this UNKs make <em>desire</em> differ...</p>
<p>what can I do for it?</p>
","json, bert-language-model","<p>ultimately, I found the problem... the Version of Bert I have used was adapted to Persian language and I was not passed the Persian normalizing process completely:)
after completing that phase and some debugging into Bert configuration, it solved:)</p>
",1,1,765,2020-11-28 16:15:19,https://stackoverflow.com/questions/65051744/how-to-handle-bert-unk-token-in-the-output-prediction
Make sure BERT model does not load pretrained weights?,"<p>I want to make sure my BertModel does not loads pre-trained weights. I am using auto class (hugging face) which loads model automatically.</p>
<p>My question is how do I load bert model without pretrained weights?</p>
","pytorch, bert-language-model, huggingface-transformers","<p>Use AutoConfig instead of AutoModel:</p>
<pre><code>from transformers import AutoConfig, AutoModel
config = AutoConfig.from_pretrained('bert-base-uncased')
model =  AutoModel.from_config(config)
</code></pre>
<p>this should set up the model without loading the weights.</p>
<p><a href=""https://huggingface.co/transformers/model_doc/auto.html?highlight=from_pretrained#transformers.AutoConfig.from_pretrained"" rel=""nofollow noreferrer"">Documentation here</a> <a href=""https://huggingface.co/transformers/_modules/transformers/models/auto/modeling_auto.html#AutoModel.from_config"" rel=""nofollow noreferrer"">and here</a></p>
",6,2,4931,2020-11-30 11:27:58,https://stackoverflow.com/questions/65072694/make-sure-bert-model-does-not-load-pretrained-weights
How to compute mean/max of HuggingFace Transformers BERT token embeddings with attention mask?,"<p>I'm using the HuggingFace Transformers BERT model, and I want to compute a summary vector (a.k.a. embedding) over the tokens in a sentence, using either the <code>mean</code> or <code>max</code> function. The complication is that some tokens are <code>[PAD]</code>, so I want to ignore the vectors for those tokens when computing the average or max.</p>
<p>Here's an example. I initially instantiate a <code>BertTokenizer</code> and a <code>BertModel</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import transformers
from transformers import AutoTokenizer, AutoModel

transformer_name = 'bert-base-uncased'

tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)

model = AutoModel.from_pretrained(transformer_name)
</code></pre>
<p>I then input some sentences into the tokenizer and get out <code>input_ids</code> and <code>attention_mask</code>. Notably, an <code>attention_mask</code> value of 0 means that the token was a <code>[PAD]</code> that I can ignore.</p>
<pre class=""lang-py prettyprint-override""><code>sentences = ['Deep learning is difficult yet very rewarding.',
             'Deep learning is not easy.',
             'But is rewarding if done right.']
tokenizer_result = tokenizer(sentences, max_length=32, padding=True, return_attention_mask=True, return_tensors='pt')

input_ids = tokenizer_result.input_ids
attention_mask = tokenizer_result.attention_mask

print(input_ids.shape) # torch.Size([3, 11])

print(input_ids)
# tensor([[  101,  2784,  4083,  2003,  3697,  2664,  2200, 10377,  2075,  1012,  102],
#         [  101,  2784,  4083,  2003,  2025,  3733,  1012,   102,     0,     0,    0],
#         [  101,  2021,  2003, 10377,  2075,  2065,  2589,  2157,  1012,   102,   0]])

print(attention_mask.shape) # torch.Size([3, 11])

print(attention_mask)
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])
</code></pre>
<p>Now, I call the BERT model to get the 768-D token embeddings (the top-layer hidden states).</p>
<pre class=""lang-py prettyprint-override""><code>model_result = model(input_ids, attention_mask=attention_mask, return_dict=True)

token_embeddings = model_result.last_hidden_state
print(token_embeddings.shape) # torch.Size([3, 11, 768])
</code></pre>
<p>So at this point, I have:</p>
<ol>
<li>token embeddings in a [3, 11, 768] matrix: 3 sentences, 11 tokens, 768-D vector for each token.</li>
<li>attention mask in a [3, 11] matrix: 3 sentences, 11 tokens. A 1 value indicates non-<code>[PAD]</code>.</li>
</ol>
<p>How do I compute the <code>mean</code> / <code>max</code> over the vectors for the valid, non-<code>[PAD]</code> tokens?</p>
<p>I tried using the attention mask as a mask and then called <code>torch.max()</code>, but I don't get the right dimensions:</p>
<pre class=""lang-py prettyprint-override""><code>masked_token_embeddings = token_embeddings[attention_mask==1]
print(masked_token_embeddings.shape) # torch.Size([29, 768] &lt;-- WRONG. SHOULD BE [3, 11, 768]

pooled = torch.max(masked_token_embeddings, 1)
print(pooled.values.shape) # torch.Size([29]) &lt;-- WRONG. SHOULD BE [3, 768]
</code></pre>
<p>What I really want is a tensor of shape [3, 768]. That is, a 768-D vector for each of the 3 sentences.</p>
","machine-learning, pytorch, bert-language-model, huggingface-transformers","<p>For <code>max</code>, you can multiply with <code>attention_mask</code>:</p>
<pre><code>pooled = torch.max((token_embeddings * attention_mask.unsqueeze(-1)), axis=1)
</code></pre>
<p>For <code>mean</code>, you can sum along the axis and divide by <code>attention_mask</code> along that axis:</p>
<pre><code>mean_pooled = token_embeddings.sum(axis=1) / attention_mask.sum(axis=-1).unsqueeze(-1)
</code></pre>
",8,8,6214,2020-12-01 01:38:17,https://stackoverflow.com/questions/65083581/how-to-compute-mean-max-of-huggingface-transformers-bert-token-embeddings-with-a
"Codes worked fine one week ago, but keep getting error since yesterday: Fine-tuning Bert model training via PyTorch on Colab","<p>I am new to Bert. Two weeks ago I successfully ran a fine-tuning Bert model on a nlp classification task though the outcome was not brilliant. Yesterday, however, when I tried to run the same code and data, an AttributeError was always there, which says: 'str' object has no attribute 'dim'. Please know everything is on Colab and via PyTorch Transformers.
What should I do to fix it?</p>
<p>Here is one thing I tried when I installed transformers but turned out it did not work:
instead of
!pip install transformers ,
I tried to use previous transformers version:
!pip install --target lib --upgrade transformers==3.5.0</p>
<p>Any feedback will be greatly appreciated!</p>
<p>Please see the code and the error message as below:</p>
<p>Code:</p>
<ol>
<li>train definition</li>
</ol>
<pre><code># function to train the model
def train():
  
  model.train()

  total_loss, total_accuracy = 0, 0
  
  # empty list to save model predictions
  total_preds=[]
  
  # iterate over batches
  for step,batch in enumerate(train_dataloader):
    
    # progress update after every 50 batches.
    if step % 200 == 0 and not step == 0:
      print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))

    # push the batch to gpu
    batch = [r.to(device) for r in batch]
 
    sent_id, mask, labels = batch

    # clear previously calculated gradients 
    model.zero_grad()        

    # get model predictions for the current batch
    preds = model(sent_id, mask)

    # compute the loss between actual and predicted values
    loss = cross_entropy(preds, labels)

    # add on to the total loss
    total_loss = total_loss + loss.item()

    # backward pass to calculate the gradients
    loss.backward()

    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    # update parameters
    optimizer.step()

    # update learning rate schedule
    # scheduler.step()  

    # model predictions are stored on GPU. So, push it to CPU
    preds=preds.detach().cpu().numpy()

    # append the model predictions
    total_preds.append(preds)

  # compute the training loss of the epoch
  avg_loss = total_loss / len(train_dataloader)
  
  # predictions are in the form of (no. of batches, size of batch, no. of classes).
  # reshape the predictions in form of (number of samples, no. of classes)
  total_preds  = np.concatenate(total_preds, axis=0)

  #returns the loss and predictions
  return avg_loss, total_preds
</code></pre>
<ol start=""2"">
<li>training process</li>
</ol>
<pre><code># set initial loss to infinite
best_valid_loss = float('inf')

# empty lists to store training and validation loss of each epoch
train_losses=[]
valid_losses=[]

#for each epoch
for epoch in range(epochs):
     
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))
    
    #train model
    train_loss, _ = train()
    
    #evaluate model
    valid_loss, _ = evaluate()
    
    #save the best model
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'saved_weights.pt')
    
    # append training and validation loss
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)
    
    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
<ol start=""3"">
<li>Error message:</li>
</ol>
<pre><code> Epoch 1 / 10
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-41-c5138ddf6b25&gt; in &lt;module&gt;()
     12 
     13     #train model
---&gt; 14     train_loss, _ = train()
     15 
     16     #evaluate model

5 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1686         if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
   1687             return handle_torch_function(linear, tens_ops, input, weight, bias=bias)
-&gt; 1688     if input.dim() == 2 and bias is not None:
   1689         # fused op is marginally faster
   1690         ret = torch.addmm(bias, input, weight.t())

AttributeError: 'str' object has no attribute 'dim'
</code></pre>
","tensorflow, pytorch, bert-language-model, google-colaboratory","<p>As far as I remember - there was an old transformer version in colab. Something like 2.11.0. Try:</p>
<pre><code>!pip install transformers~=2.11.0
</code></pre>
<p>Change the version number until it works.</p>
",3,1,1170,2020-12-01 22:51:17,https://stackoverflow.com/questions/65099753/codes-worked-fine-one-week-ago-but-keep-getting-error-since-yesterday-fine-tun
BertModel transformers outputs string instead of tensor,"<p>I'm following <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""noreferrer"">this</a> tutorial that codes a sentiment analysis classifier using BERT with the <a href=""https://huggingface.co/"" rel=""noreferrer"">huggingface</a> library and I'm having a very odd behavior. When trying the BERT model with a sample text I get a string instead of the hidden state. This is the code I'm using:</p>
<pre><code>import transformers
from transformers import BertModel, BertTokenizer

print(transformers.__version__)

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
PATH_OF_CACHE = &quot;/home/mwon/data-mwon/paperChega/src_classificador/data/hugingface&quot;

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)

sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'

encoding_sample = tokenizer.encode_plus(
  sample_txt,
  max_length=32,
  add_special_tokens=True, # Add '[CLS]' and '[SEP]'
  return_token_type_ids=False,
  padding=True,
  truncation = True,
  return_attention_mask=True,
  return_tensors='pt',  # Return PyTorch tensors
)

bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)


last_hidden_state, pooled_output = bert_model(
  encoding_sample['input_ids'],
  encoding_sample['attention_mask']
)

print([last_hidden_state,pooled_output])
</code></pre>
<p>that outputs:</p>
<pre><code>4.0.0
['last_hidden_state', 'pooler_output']
 
</code></pre>
","bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>While the answer from <a href=""https://stackoverflow.com/a/65137768/6664872"">Aakash</a> provides a solution to the problem, it does not explain the issue. Since one of the 3.X releases of the transformers library, the models do not return tuples anymore but specific output objects:</p>
<pre><code>o = bert_model(
    encoding_sample['input_ids'],
    encoding_sample['attention_mask']
)
print(type(o))
print(o.keys())
</code></pre>
<p>Output:</p>
<pre><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions
odict_keys(['last_hidden_state', 'pooler_output'])
</code></pre>
<p>You can return to the previous behavior by adding <code>return_dict=False</code> to get a tuple:</p>
<pre class=""lang-py prettyprint-override""><code>o = bert_model(
   encoding_sample['input_ids'],
   encoding_sample['attention_mask'],
   return_dict=False
)

print(type(o))
</code></pre>
<p>Output:</p>
<pre><code>&lt;class 'tuple'&gt;
</code></pre>
<p>I do not recommend that, because it is now unambiguous to select a specific part of the output without turning to the documentation as shown in the example below:</p>
<pre class=""lang-py prettyprint-override""><code>o = bert_model(encoding_sample['input_ids'],  encoding_sample['attention_mask'], return_dict=False, output_attentions=True, output_hidden_states=True)
print('I am a tuple with {} elements. You do not know what each element presents without checking the documentation'.format(len(o)))

o = bert_model(encoding_sample['input_ids'],  encoding_sample['attention_mask'], output_attentions=True, output_hidden_states=True)
print('I am a cool object and you can acces my elements with o.last_hidden_state, o[&quot;last_hidden_state&quot;] or even o[0]. My keys are; {} '.format(o.keys()))
</code></pre>
<p>Output:</p>
<pre><code>I am a tuple with 4 elements. You do not know what each element presents without checking the documentation
I am a cool object and you can acces my elements with o.last_hidden_state,  o[&quot;last_hidden_state&quot;] or even o[0]. My keys are; odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions']) 
</code></pre>
",15,15,10983,2020-12-03 18:42:50,https://stackoverflow.com/questions/65132144/bertmodel-transformers-outputs-string-instead-of-tensor
generating segment labels for a Tensor given a value indicating segment boundaries,"<p>Does anyone know of a way to generate a 'segment label' for a Tensor, given a unique value that represents segment boundaries within the Tensor?</p>
<p>For example, given a 1D input tensor where the value <code>1</code> represents a segment boundary,</p>
<p><code>x = torch.Tensor([5, 4, 1, 3, 6, 2])</code></p>
<p>the resulting segment label Tensor should have the same shape with values representing the two segments:</p>
<p><code>segment_label = torch.Tensor([1, 1, 1, 2, 2, 2])</code></p>
<p>Likewise, for a batch of inputs, e.g. batch size = 3,</p>
<pre><code>x = torch.Tensor([
    [5, 4, 1, 3, 6, 2],
    [9, 4, 5, 1, 8, 10],
    [10, 1, 5, 4, 8, 9]
    ])
</code></pre>
<p>the resulting segment label Tensor (using <code>1</code> as the segment separator) should look something like this:</p>
<pre><code>segment_label = torch.Tensor([
    [1, 1, 1, 2, 2, 2],
    [1, 1, 1, 1, 2, 2],
    [1, 1, 2, 2, 2, 2]
    ])
</code></pre>
<p><strong>Context</strong>: I'm currently working with Fairseq's Transformer implementation in PyTorch for a seq2seq NLP task. I am looking for a way to incorporate BERT-like segment embeddings in Transformer during the encoder's forward pass, rather than modifying an exisiting dataset used for translation tasks such as <code>language_pair_dataset</code>.</p>
<p>Thanks in advance!</p>
","python, pytorch, tensor, bert-language-model, fairseq","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"" rel=""nofollow noreferrer""><code>torch.cumsum</code></a> to pull the trick:</p>
<pre class=""lang-py prettyprint-override""><code>mask = (x == 1).to(x)  # mask with only the boundaries
segment_label = mask.cumsum(dim=-1) - mask + 1
</code></pre>
<p>Results with the desired <code>segment_label</code>.</p>
",2,2,130,2020-12-07 03:54:10,https://stackoverflow.com/questions/65175941/generating-segment-labels-for-a-tensor-given-a-value-indicating-segment-boundari
"How does max_length, padding and truncation arguments work in HuggingFace&#39; BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;)?","<p>I am working with Text Classification problem where I want to use the BERT model as the base followed by Dense layers. I want to know how does the 3 arguments work? For example, if I have 3 sentences as:</p>
<pre><code>'My name is slim shade and I am an aspiring AI Engineer',
'I am an aspiring AI Engineer',
'My name is Slim'
</code></pre>
<p>SO what will these 3 arguments do? What I think is as follows:</p>
<ol>
<li><code>max_length=5</code> will keep all the sentences as of length 5 strictly</li>
<li><code>padding=max_length</code> will add a padding of 1 to the third sentence</li>
<li><code>truncate=True</code> will truncate the first and second sentence so that their length will be strictly 5.</li>
</ol>
<p>Please correct me if I am wrong.</p>
<p>Below is my code which I have used.</p>
<pre><code>! pip install transformers==3.5.1

from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

tokens = tokenizer.batch_encode_plus(text,max_length=5,padding='max_length', truncation=True)
  
text_seq = torch.tensor(tokens['input_ids'])
text_mask = torch.tensor(tokens['attention_mask'])
</code></pre>
","python, deep-learning, pytorch, bert-language-model, huggingface-tokenizers","<p>What you have assumed is almost correct, however, there are few differences.</p>
<p><code>max_length=5</code>, the <code>max_length</code> <strong>specifies</strong> the length of the <strong>tokenized text</strong>. By default, BERT performs word-piece tokenization. For example the word &quot;playing&quot; can be split into &quot;play&quot; and &quot;##ing&quot; (This may not be very precise, but just to help you understand about word-piece tokenization), followed by adding <code>[CLS]</code> token at the beginning of the sentence, and <code>[SEP]</code> token at the end of sentence. Thus, it first tokenizes the sentence, truncates it to <code>max_length-2</code> (if <code>truncation=True</code>), then prepend <code>[CLS]</code> at the beginning and <code>[SEP]</code> token at the end.(So a total length of <code>max_length</code>)</p>
<p><code>padding='max_length'</code>, In this example it is not very evident that the 3rd example will be padded, as the length exceeds <code>5</code> after appending <code>[CLS]</code> and <code>[SEP]</code> tokens. However, if you have a <code>max_length</code> of 10. The tokenized text corresponds to <code>[101, 2026, 2171, 2003, 11754, 102, 0, 0, 0, 0]</code>, where 101 is id of <code>[CLS]</code> and 102 is id of <code>[SEP]</code> tokens. Thus, padded by zeros to make all the text to the length of <code>max_length</code></p>
<p>Likewise, <code>truncate=True</code> will ensure that the max_length is strictly adhered, i.e, longer sentences are truncated to <code>max_length</code> only if <code>truncate=True</code></p>
",36,27,60594,2020-12-11 06:26:28,https://stackoverflow.com/questions/65246703/how-does-max-length-padding-and-truncation-arguments-work-in-huggingface-bertt
How to add a multiclass multilabel layer on top of pretrained BERT model?,"<p>I am trying to do a multitask multiclass sentence classification task using the pretrained BERT model from the huggingface transformers library . I have tried to use the BERTForSequenceClassification model from there but the issue I am having is that I am not able to extend it for multiple tasks . I will try to make it more informative through this example.</p>
<p>Suppose we have four different tasks and for each sentence and for each task we have labels like this as follows in the examples:</p>
<ol>
<li>A :[ 'a' , 'b' , 'c' , 'd' ]</li>
<li>B :[ 'e' , 'f' , 'g' , 'h' ]</li>
<li>C :[ 'i' , 'j' , 'k' , 'l' ]</li>
<li>D :[ 'm' , 'n' , 'o' , 'p' ]</li>
</ol>
<p>Now , if I have a sentence for this model , I want the output to give me output for all the four different tasks (A,B,C,D).</p>
<p>This is what I was doing earlier</p>
<pre><code>   model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 4, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>
<p>Then I tried to implement a CustomBERT model like this :</p>
<pre><code>class CustomBERTModel(nn.Module):
    def __init__(self):
          super(CustomBERTModel, self).__init__()
          self.bert = BertModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
          ### New layers:
          self.linear1 = nn.Linear(768, 256)
          self.linear2 = nn.Linear(256, num_classes) ## num_classes is the number of classes in this example

    def forward(self, ids, mask):
          sequence_output, pooled_output = self.bert(
               ids, 
               attention_mask=mask)

          # sequence_output has the following shape: (batch_size, sequence_length, 768)
          linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) 
          linear2_output = self.linear2(linear2_output)

          return linear2_output
</code></pre>
<p>I have went through the answers to questions similar to it available earlier but none of them appeared to
answer my question . I have tried to get through all the points which I think can be helpful for the understanding of my problem and would try to clear further more in case of any descrepancies made by me in the explaination of the question . Any answers related to this will be very much helpful .</p>
","deep-learning, pytorch, bert-language-model, huggingface-transformers, transfer-learning","<p>You should use <code>BertModel</code> and not <code>BertModelForSequenceClassification</code>, as <code>BertModelForSequenceClassification</code> adds a linear layer for classification on top of BERT model and uses <code>CrossEntropyLoss</code>, which is meant for multiclass classification.</p>
<p>Hence, first use <code>BertModel</code> instead of <code>BertModelForSequenceClassification</code>:</p>
<pre><code>class CustomBERTModel(nn.Module):
    def __init__(self):
          super(CustomBERTModel, self).__init__()
          self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
          ### New layers:
          self.linear1 = nn.Linear(768, 256)
          self.linear2 = nn.Linear(256, 4) ## as you have 4 classes in the output
          self.sig = nn.functional.sigmoid()

    def forward(self, ids, mask):
          sequence_output, pooled_output = self.bert(
               ids, 
               attention_mask=mask)

          # sequence_output has the following shape: (batch_size, sequence_length, 768)
          linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) 
          linear2_output = self.linear2(linear2_output)
          linear2_output = self.sig(linear2_output)

          return linear2_output

</code></pre>
<p>Next, multilabel classification uses 'Sigmoid' activation instead of 'Softmax' (Here, the sigmoid layer is added in the above code)</p>
<p>Further, for multilabel classification, you need to use <code>BCELoss</code> instead of <code>CrossEntropyLoss</code>.</p>
",3,0,2501,2020-12-14 07:22:16,https://stackoverflow.com/questions/65285054/how-to-add-a-multiclass-multilabel-layer-on-top-of-pretrained-bert-model
How to use my own corpus on word embedding model BERT,"<p>I am trying to create a question-answering model with the word embedding model BERT from google. I am new to this and would really want to use my own corpus for the training. At first I used an example from the <a href=""https://huggingface.co/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2"" rel=""nofollow noreferrer"">huggingface site</a> and that worked fine:</p>
<pre><code>from transformers import pipeline

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': &quot;Amsterdam is de hoofdstad en de dichtstbevolkte stad van Nederland.&quot;,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>output</p>
<pre><code>&gt; {'answer': 'Amsterdam', 'end': 9, 'score': 0.825619101524353, 'start': 0}
</code></pre>
<p>So, I tried creating a .txt file to test if it was possible to interchange the sentence in the context parameter with the exact same sentence but in a .txt file.</p>
<pre><code>with open('test.txt') as f:
    lines = f.readlines()

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': lines,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>But this gave me the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-2bae0ecad43e&gt; in &lt;module&gt;()
     10 qa_pipeline({
     11     'context': lines,
---&gt; 12     'question': &quot;Wat is de hoofdstad van Nederland?&quot;})

5 frames
/usr/local/lib/python3.6/dist-packages/transformers/data/processors/squad.py in _is_whitespace(c)
     84 
     85 def _is_whitespace(c):
---&gt; 86     if c == &quot; &quot; or c == &quot;\t&quot; or c == &quot;\r&quot; or c == &quot;\n&quot; or ord(c) == 0x202F:
     87         return True
     88     return False

TypeError: ord() expected a character, but string of length 66 found
</code></pre>
<p>I was just experimenting with ways to read and use a .txt file, but I don't seem to find a different solution. I did some research on the huggingface pipeline() function and this is what was written about the question and context parameters:</p>
<p><a href=""https://i.sstatic.net/Yk4bM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Yk4bM.png"" alt=""enter image description here"" /></a></p>
","word-embedding, bert-language-model, huggingface-transformers","<p>Got it! The solution was really easy. I assumed that the variable 'lines' was already a str but that wasn't the case. Just by casting to a string the question-answering model accepted my test.txt file.</p>
<p>so from:</p>
<pre><code>with open('test.txt') as f:
    lines = f.readlines()
</code></pre>
<p>to:</p>
<pre><code>with open('test.txt') as f:
    lines = str(f.readlines())
</code></pre>
",1,1,301,2020-12-15 10:29:14,https://stackoverflow.com/questions/65304058/how-to-use-my-own-corpus-on-word-embedding-model-bert
calculate two losses in a model and backpropagate twice,"<p>I'm creating a model using BertModel to identify answer span (without using BertForQA).</p>
<p>I have an indepent linear layer for determining start and end token respectively. In <strong>init</strong>():</p>
<pre><code>self.start_linear = nn.Linear(h, output_dim)

self.end_linear = nn.Linear(h, output_dim)
</code></pre>
<p>In forward(), I output a predicted start layer and predicted end layer:</p>
<pre><code> def forward(self, input_ids, attention_mask):

 outputs = self.bert(input_ids, attention_mask) # input = bert tokenizer encoding

 lhs = outputs.last_hidden_state # (batch_size, sequence_length, hidden_size)

 out = lhs[:, -1, :] # (batch_size, hidden_dim)

 st = self.start_linear(out)

 end = self.end_linear(out) 



 predict_start = self.softmax(st)

 predict_end = self.softmax(end)

 return predict_start, predict_end
</code></pre>
<p>Then in train_epoch(), I tried to backpropagate the losses separately:</p>
<pre><code>def train_epoch(model, train_loader, optimizer):

 model.train()

 total = 0

 st_loss, st_correct, st_total_loss = 0, 0, 0

 end_loss, end_correct, end_total_loss = 0, 0, 0

 for batch in train_loader:

   optimizer.zero_grad()

   input_ids = batch['input_ids'].to(device)

   attention_mask = batch['attention_mask'].to(device)

   start_idx = batch['start'].to(device)

   end_idx = batch['end'].to(device)

   start, end = model(input_ids=input_ids, attention_mask=attention_mask)


   st_loss = model.compute_loss(start, start_idx)

   end_loss = model.compute_loss(end, end_idx)

   st_total_loss += st_loss.item()

   end_total_loss += end_loss.item()

 # perform backward propagation to compute the gradients

   st_loss.backward()

   end_loss.backward()

 # update the weights

   optimizer.step() 
</code></pre>
<p>But then I got on the line of <code>end_loss.backward()</code>:</p>
<pre><code>Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre>
<p>Am I supposed to do the backward pass separately? Or should I do it in another way? Thank you!</p>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>The standard procedure is just to sum both losses and backpropagate on the sum.</p>
<p>It can be important to make sure both losses you want to sum have values that are on average approximately as big, or at least proportional to the importance you want each to have relative to one another(otherwise, the model is going to optimize for the bigger loss more than for the smaller one). In the span detection case, I'm guessing this won't be necessary however due to the apparent symmetry of the problem.</p>
",1,0,523,2020-12-17 09:21:05,https://stackoverflow.com/questions/65337804/calculate-two-losses-in-a-model-and-backpropagate-twice
"Multi Head Attention: Correct implementation of Linear Transformations of Q, K, V","<p>I am implementing the <strong>Multi-Head Self-Attention</strong> in Pytorch now. I looked at a couple of implementations and they seem a bit wrong, or at least I am not sure why it is done the way it is. They would often apply the <strong>linear projection just once</strong>:</p>
<pre><code>    self.query_projection = nn.Linear(input_dim, output_dim)
    self.key_projection = nn.Linear(input_dim, output_dim)
    self.value_projection = nn.Linear(input_dim, output_dim)
</code></pre>
<p>and then they would often reshape the projection as</p>
<pre><code>    query_heads = query_projected.view(batch_size, query_lenght, head_count, head_dimension).transpose(1,2)
    key_heads = key_projected.view(batch_size, key_len, head_count, head_dimension).transpose(1, 2)  # (batch_size, heads_count, key_len, d_head)
    value_heads = value_projected.view(batch_size, value_len, head_count, head_dimension).transpose(1, 2)  # (batch_size, heads_count, value_len, d_head)

    attention_weights = scaled_dot_product(query_heads, key_heads) 
</code></pre>
<p>According to this code, each head will <em>work</em> on a piece of a projected query. However, the initial paper says that we need to have a different Linear projection for each head in the encoder.</p>
<p>Is this displayed implementation correct?</p>
","neural-network, nlp, pytorch, bert-language-model, attention-model","<p>They are equivalent.</p>
<p>Theoretically (and in paper writing), it is easier to consider them as separate linear projections. Say if you have 8 heads, and each head has a <code>M-&gt;N</code> projection, then one would have <code>8</code> <code>N by M</code> matrix.</p>
<p>In implementation though, it is faster to have a <code>M-&gt;8N</code> transformation by having a <code>8N by M</code> matrix.</p>
<p>One can concatenate the matrices in the first formulation to obtain the matrix in the second formulation.</p>
",5,3,754,2020-12-17 11:45:06,https://stackoverflow.com/questions/65340088/multi-head-attention-correct-implementation-of-linear-transformations-of-q-k
max_length doesn&#39;t fix the question-answering model,"<p><strong>My Question</strong>:
How to make my 'question-answering' model run, given a big (&gt;512b) .txt file?</p>
<p><strong>Context</strong>:
I am creating a question answering model with the word embedding model BERT from google. The model works fine when I import a .txt file with a few sentences, but when the .txt file exceeds the limit of 512b words as context for the model to learn, the model won't answer my questions.</p>
<p><strong>My Attempt to resolve issue</strong>:
I set a max_length at the encoding part, but that does not seem to solve the problem (my attempt code is below).</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

max_seq_length = 512


tokenizer = AutoTokenizer.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)

f = open(&quot;test.txt&quot;, &quot;r&quot;)

text = str(f.read())

questions = [
    &quot;Wat is de hoofdstad van Nederland?&quot;,
    &quot;Van welk automerk is een Cayenne?&quot;,
    &quot;In welk jaar is pindakaas geproduceerd?&quot;,
]

for question in questions:
    inputs = tokenizer.encode_plus(question, 
                                   text, 
                                   add_special_tokens=True, 
                                   max_length=max_seq_length,
                                   truncation=True,
                                   return_tensors=&quot;pt&quot;)
    input_ids = inputs[&quot;input_ids&quot;].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    print(f&quot;Question: {question}&quot;)
    print(f&quot;Answer: {answer}\n&quot;)
</code></pre>
<p>Code-result:</p>
<pre><code>&gt; Question: Wat is de hoofdstad van Nederland?
&gt; Answer: [CLS]
&gt;
&gt; Question: Van welk automerk is een Cayenne?
&gt; Answer: [CLS]
&gt;
&gt; Question: In welk jaar is pindakaas geproduceerd?
&gt; Answer: [CLS]
</code></pre>
<p>As one can see, the model only returns the [CLS]-token which happens at the tokenizer encoding part.</p>
<p><strong>EDIT: I figured out that the way to solve this, is to iterate through the .txt file, so the model can find the answer through the iteration.</strong></p>
","python, machine-learning, text, bert-language-model, maxlength","<p>EDIT: I figured out that the way to solve this, is to iterate through the .txt file, so the model can find the answer through the iteration. The reason for the model to answer with a [CLS] is because it could not find the answer in the 512b context, it has to look more further into the context.</p>
<p>By creating a loop like this:</p>
<pre><code>with open(&quot;sample.txt&quot;, &quot;r&quot;) as a_file:
  for line in a_file:
    text = line.strip()
    print(text)
</code></pre>
<p>it is possible to apply the iterated text into the encode_plus.</p>
",0,2,664,2020-12-19 13:12:59,https://stackoverflow.com/questions/65370200/max-length-doesnt-fix-the-question-answering-model
Lines in text file won&#39;t iterate through for loop Python,"<p>I am trying to iterate through my questions and lines in my .txt file. Now this question may have been asked before, but I am really having trouble with this.</p>
<p>this is what I have right now:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

max_seq_length = 512

tokenizer = AutoTokenizer.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)

f = open(&quot;glad.txt&quot;, &quot;r&quot;)

questions = [
    &quot;Welke soorten gladiatoren waren er?&quot;,
    &quot;Wat is een provocator?&quot;,
    &quot;Wat voor helm droeg een retiarius?&quot;,
]
for question in questions:
    print(f&quot;Question: {question}&quot;)
    for _ in range(len(question)):
        for line in f:
            text = str(line.split(&quot;.&quot;))
            inputs = tokenizer.encode_plus(question,
                                           text,
                                           add_special_tokens=True,
                                           max_length=100,
                                           truncation=True,
                                           return_tensors=&quot;pt&quot;)
            input_ids = inputs[&quot;input_ids&quot;].tolist()[0]

            text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
            answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)

            answer_start = torch.argmax(
                answer_start_scores
            )  # Get the most likely beginning of answer with the argmax of the score
            answer_end = torch.argmax(
                answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

            answer = tokenizer.convert_tokens_to_string(
                tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

            print(text)
            # if answer == '[CLS]':
            #   continue
            # elif answer == '':
            #   continue
            # else:
            #   print(f&quot;Answer: {answer}&quot;)
            #   print(f&quot;Answer start: {answer_start}&quot;)
            #   print(f&quot;Answer end: {answer_end}&quot;)
            #   break
</code></pre>
<p>and this is the output:</p>
<pre><code>&gt; Question: Welke soorten gladiatoren waren er?
&gt; ['Er waren vele soorten gladiatoren, maar het meest kwamen de thraex, de retiarius en de murmillo voor', ' De oudste soorten gladiatoren droegen de naam van een volk: de Samniet en de Galliër', '\n']
&gt; ['Hun uitrusting bestond uit dezelfde wapens als die waarmee de Samnieten en Galliërs in hun oorlogen met de Romeinen gewoonlijk vochten', '\n']
&gt; ['De Thraciër (thraex) verscheen vanaf de tweede eeuw voor Chr', ' Hij had een vrij klein  kromzwaard (sica), een klein rond (soms vierkant) schild, een helm en lage beenplaten', &quot; De retiarius ('netvechter') had een groot net (rete) met een doorsnee van 3 m, een drietand en soms  ook een dolk&quot;, '\n']
&gt; ['Hij had alleen bescherming om zijn linkerarm en -schouder', ' Vaak droeg hij ook een bronzen beschermingsplaat (galerus) van zijn nek tot linkerelleboog', ' Vaak vocht de retiarius tegen de secutor die om die reden ook wel contraretiarius werd genoemd', '\n']
&gt; ['Hij had een langwerpig schild en een steekzwaard', ' Opvallend was zijn eivormige helm zonder rand en met een metalen kam, waarschijnlijk zo ontworpen om minder makkelijk in het net van de retiarius vast te haken', ' Een provocator (‘uitdager’) vocht doorgaans tegen een andere provocator', '\n']
&gt; ['Hij viel zijn tegenstander uit een onverwachte hoek plotseling aan', ' Hij had een lang rechthoekig schild, een borstpantser, een beenplaat alleen over het linkerbeen, een helm en een kort zwaard', '']
&gt; Question: Wat is een provocator?
&gt; Question: Wat voor helm droeg een retiarius?
</code></pre>
<p>But the sentences are supposed to repeat in the other questions too.</p>
<p>Does anyone know what I am doing wrong here? It is probably something really easy, but I really don't seem the find the mistake.</p>
","python, loops, bert-language-model","<p>You would need to add <code>f.seek(0)</code> after your first parse through the file. This is because when you read the file once, the cursor is at the end of the file, after which <code>for line in f</code> does not read the file from the beginning again. Please refer to <a href=""https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file"">Tim and Nunser's answer here</a> which explains it well.</p>
<p>Something like this:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

max_seq_length = 512

tokenizer = AutoTokenizer.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)

f = open(&quot;glad.txt&quot;, &quot;r&quot;)

questions = [
    &quot;Welke soorten gladiatoren waren er?&quot;,
    &quot;Wat is een provocator?&quot;,
    &quot;Wat voor helm droeg een retiarius?&quot;,
]
for question in questions:
    print(f&quot;Question: {question}&quot;)
    for _ in range(len(question)):
        for line in f:
            text = str(line.split(&quot;.&quot;))
            inputs = tokenizer.encode_plus(question,
                                           text,
                                           add_special_tokens=True,
                                           max_length=100,
                                           truncation=True,
                                           return_tensors=&quot;pt&quot;)
            input_ids = inputs[&quot;input_ids&quot;].tolist()[0]

            text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
            answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)

            answer_start = torch.argmax(
                answer_start_scores
            )  # Get the most likely beginning of answer with the argmax of the score
            answer_end = torch.argmax(
                answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

            answer = tokenizer.convert_tokens_to_string(
                tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

            print(text)
        f.seek(0) # reset cursor to beginning of the file
</code></pre>
",1,0,303,2020-12-19 18:27:26,https://stackoverflow.com/questions/65373128/lines-in-text-file-wont-iterate-through-for-loop-python
BERT DataLoader: Difference between shuffle=True vs Sampler?,"<p>I trained a DistilBERT model with DistilBertForTokenClassification on ConLL data fro predicting NER. Training seem to have completed with no problems but I have 2 problems during evaluation phase.</p>
<ol>
<li><p>I'm getting negative loss value</p>
</li>
<li><p>During training, I used shuffle=True for DataLoader. But during evaluation, when I do shuffle=True for DataLoader, I get very poor metric results(f_1, accuracy, recall etc). But if I do shuffle = False or use a Sampler instead of shuffling I get pretty good metric results. I'm wondering if there is anything wrong with my code.</p>
</li>
</ol>
<p>Here is the evaluation code:</p>
<hr />
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:
  b_input_ids = batch['input_ids'].to(device)
  b_input_mask = batch['attention_mask'].to(device)
  b_labels = batch['labels'].to(device)

  with torch.no_grad():
      outputs = model(b_input_ids, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.detach().cpu().numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

  eval_loss += outputs[0].mean().item()


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>out:</p>
<pre><code>Prediction started on test data
Prediction completed
Validation loss: -0.2584906197858579
</code></pre>
<p>I believe I'm calculating the loss wrong here. Is it possible to get negative loss values with BERT?</p>
<p>For DataLoader, if I use the code snippet below, I have no problems with the metric results.</p>
<pre><code>val_sampler = SequentialSampler(val_dataset)
val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=128)
</code></pre>
<p>Bu if I do this one I get very poor metric results</p>
<pre><code>val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)
</code></pre>
<p>Is it normal that I'm getting vastly different results with shuffle=True vs shuffle=False ?</p>
<p>code for the metric calculation:</p>
<pre><code>metric = load_metric(&quot;seqeval&quot;)
results = metric.compute(predictions=true_predictions, references=true_labels)
results
</code></pre>
<p>out:</p>
<pre><code>{'LOCATION': {'f1': 0.9588207767898924,
  'number': 2134,
  'precision': 0.9574766355140187,
  'recall': 0.9601686972820993},
 'MISC': {'f1': 0.8658965344048217,
  'number': 995,
  'precision': 0.8654618473895582,
  'recall': 0.8663316582914573},
 'ORGANIZATION': {'f1': 0.9066332916145182,
  'number': 1971,
  'precision': 0.8947628458498024,
  'recall': 0.9188229325215627},
 'PERSON': {'f1': 0.9632426988922457,
  'number': 2015,
  'precision': 0.9775166070516096,
  'recall': 0.9493796526054591},
 'overall_accuracy': 0.988255561629313,
 'overall_f1': 0.9324058459808882,
 'overall_precision': 0.9322748349023465,
 'overall_recall': 0.932536893886156}
</code></pre>
<p>The above metrics are printed when I use Sampler or shuffle=False. If I use shuffle=True, I get:</p>
<pre><code>{'LOCATION': {'f1': 0.03902284263959391,
  'number': 2134,
  'precision': 0.029496402877697843,
  'recall': 0.057638238050609185},
 'MISC': {'f1': 0.010318142734307824,
  'number': 995,
  'precision': 0.009015777610818933,
  'recall': 0.012060301507537688},
 'ORGANIZATION': {'f1': 0.027420984269014285,
  'number': 1971,
  'precision': 0.019160951996772892,
  'recall': 0.04819888381532217},
 'PERSON': {'f1': 0.02119907254057635,
  'number': 2015,
  'precision': 0.01590852597564007,
  'recall': 0.03176178660049628},
 'overall_accuracy': 0.5651741788003777,
 'overall_f1': 0.02722600361161272,
 'overall_precision': 0.020301063389034663,
 'overall_recall': 0.041321152494729445}
</code></pre>
<p>UPDATE: I modified loss code for evaluation. There seems to be no problem with this code. You can see the new code below:</p>
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:

  b_labels = batch['labels'].to(device)

  batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}
  
  with torch.no_grad():
      outputs = model(**batch)

      loss, logits = outputs[0:2]
      logits = logits.detach().cpu().numpy()
      label_ids = b_labels.detach().cpu().numpy()
  
      predictions.append(logits)
      true_labels.append(label_ids)

      eval_loss += loss


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>Though I still haven't got an asnwer to the DataLoader question.
Also I jsut realised when I do <code>print(model.eval())</code> I still get dropouts from the model in evaluation mode.</p>
","python, pytorch, bert-language-model, huggingface-transformers","<p>As far as I understand, the answer is pretty simple:</p>
<p>&quot;I saw my father do it this way, and his father was also doing it this way, so I'm also doing it this way&quot;.</p>
<p>I've looked around a lot of notebooks to see how people were loading the data for validation and in every notebook I saw that people were using the Sequential Sampler for validation. Nobody uses Shuffling or Random Sampling during validation. I don't exactly know why, but this is the case. So if anyone visiting this post was wondering the same thing, the answer is basically what I quoted above.</p>
<p>Also, I edited the original post for the loss problem I was having. I was calculating it wrong. Apperantly Bert reutrns loss at index 0 of the output (outputs[0]) if you also feed the model the original labels. In the first code snippet, when I was getting the outputs from the model, I was not feeding the model with the original labels, so it was not returning the loss value at index 0, but returning only the logits.</p>
<p>Basically what you need to do is:</p>
<pre><code>outputs = model(input_ids, mask, label=label)
loss = outputs[0]
logits = outputs[1]
</code></pre>
",0,1,665,2020-12-21 16:41:47,https://stackoverflow.com/questions/65396650/bert-dataloader-difference-between-shuffle-true-vs-sampler
How do I interpret my BERT output from Huggingface Transformers for Sequence Classification and tensorflow?,"<p>I am using bert for a sequence classification task with 3 labels. To do this, I am using huggingface transformers with tensorflow, more specifically the TFBertForSequenceClassification class with the bert-base-german-cased model (yes, using german sentences).</p>
<p>I am by no means an expert in NLP, which is why I pretty much followed this approch here: <a href=""https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"" rel=""nofollow noreferrer"">https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333</a> (with some tweaks of course)</p>
<p>Everything seems to be working fine, but the output I receive from my model is what throws me off.
Here's just some of the output along the way for context.</p>
<p>The main difference I have to the example from the article is the number of labels. I have 3 while the article only featured 2.</p>
<p>I use a LabelEncoder from sklearn.preprocessing to process my labels</p>
<pre><code>label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(Y)
</code></pre>
<p>*Y here is a list of labels as strings, so something like this</p>
<pre><code>['e_3', 'e_1', 'e_2',]
</code></pre>
<p>then turns into this:</p>
<pre><code>array([0, 1, 2], dtype=int64)
</code></pre>
<p>I then use the BertTokenizer to process my text and create the input datasets (training and testing).
These are the shapes of those:</p>
<pre><code> &lt;TensorSliceDataset shapes: ({input_ids: (99,), token_type_ids: (99,), attention_mask: (99,)}, ()), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I then train the model as per Huggingface docs.</p>
<p>The last epoch while training the model looks like this:</p>
<pre><code>Epoch 3/3
108/108 [==============================] - 24s 223ms/step - loss: 25.8196 - accuracy: 0.7963 - val_loss: 24.5137 - val_accuracy: 0.7243
</code></pre>
<p>Then I run model.predict on an example sentence and get this output (yes I tokenized the sentence accordingly just like the other article does). The output looks like this:</p>
<pre><code>array([ 3.1293588, -5.280143 ,  2.4700692], dtype=float32)
</code></pre>
<p>And lastly that's the softmax function I apply in the end and it's output:</p>
<pre><code>tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()[0]

output: 0.6590041
</code></pre>
<p>So here's my question:
I don't quite understand that output. With an accuracy of ~70% (validation accuracy), my model should be okay in predicting the labels. Yet only the logits from the direct output don't mean much to me tbh and the output after the softmax function seems to be on a linear scale, as if it came from a sigmoid function. How do I interpret this and translate it to the label I am trying to predict?</p>
<p>And also: shouldn't I feed one hot encoded labels into my bert model for it to work? I always thought Bert needs that but it seems like it doesn't.</p>
","python, tensorflow, bert-language-model, huggingface-transformers","<p>Your output means that probability of the first class is 65.9%.</p>
<p>You can feed your labels either as integers or as one-hot vectors. You have to use an appropriate loss function (categorical_crossentropy with one-hot or sparse_categorical_crossentropy with integers).</p>
",2,5,6401,2020-12-21 17:04:06,https://stackoverflow.com/questions/65396968/how-do-i-interpret-my-bert-output-from-huggingface-transformers-for-sequence-cla
Unable to find the word that I added to the Huggingface Bert tokenizer vocabulary,"<p>I tried to add new words to the <code>Bert tokenizer vocab</code>. I see that the length of the vocab is increasing, however I can't find the newly added word in the vocab.</p>
<pre><code>tokenizer.add_tokens(['covid', 'wuhan'])

v = tokenizer.get_vocab()

print(len(v))
'covid' in tokenizer.vocab
</code></pre>
<p>Output:</p>
<pre><code>30524

False
</code></pre>
","bert-language-model, huggingface-transformers, nltokenizer","<p>You are calling two different things with <code>tokenizer.vocab</code> and <code>tokenizer.get_vocab()</code>. The first one contains the base vocabulary without the added tokens, while the other one contains the base vocabulary with the added tokens.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer

t = BertTokenizer.from_pretrained('bert-base-uncased')

print(len(t.vocab))
print(len(t.get_vocab()))
print(t.get_added_vocab())
t.add_tokens(['covid'])
print(len(t.vocab))
print(len(t.get_vocab()))
print(t.get_added_vocab())
</code></pre>
<p>Output:</p>
<pre><code>30522
30522
{}
30522
30523
{'covid': 30522}
</code></pre>
",3,3,3349,2020-12-24 15:03:02,https://stackoverflow.com/questions/65440010/unable-to-find-the-word-that-i-added-to-the-huggingface-bert-tokenizer-vocabular
"BigBird, or Sparse self-attention: How to implement a sparse matrix?","<p>This question is related to the new paper: <a href=""https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf"" rel=""nofollow noreferrer"">Big Bird: Transformers for Longer Sequences</a>. Mainly, about the implementation of the Sparse Attention (that is specified in the <a href=""https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Supplemental.pdf"" rel=""nofollow noreferrer"">Supplemental material, part D</a>). Currently, I am trying to implement it in PyTorch.</p>
<p>They suggest a new way to speed up the computation by blocking the original query and key matrices (see, below) <a href=""https://i.sstatic.net/cnNXN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cnNXN.png"" alt=""attention with block-matrices"" /></a></p>
<p>When you do the matrix multiplaciton in the step (b), you end up with something like that:
<a href=""https://i.sstatic.net/TklKk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TklKk.png"" alt=""diagonal attention"" /></a>.</p>
<p>So I was wondering: <strong>how would you go from that representation (image above) to a sparse matrix (using PyTorch, see below)?</strong> In the paper, they just say: &quot;<em>simply reshape the result</em>&quot;, and I do not know any easy ways to do so (especially, when I have multiple blocks in different positions (see step (c) on the first image).</p>
<p><a href=""https://i.sstatic.net/VAIcK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VAIcK.png"" alt=""matrix with dioganal attention"" /></a></p>
<p><strong>RESOLUTION</strong>:
Huggingface has an implementation of BigBird in pytorch.</p>
","neural-network, pytorch, tensor, bert-language-model, attention-model","<p>I end up following the guidelines in the paper. When it comes to the unpacking of the result I use: <code>torch.sparse_coo_tensor</code></p>
<p>EDIT: Sparse tensors are still memory-hungry! The <a href=""https://stackoverflow.com/questions/65571114/add-blocks-of-values-to-a-tensor-at-specific-locations-in-pytorch"">more efficient solution</a> is described here</p>
",2,2,2055,2020-12-25 17:22:09,https://stackoverflow.com/questions/65450215/bigbird-or-sparse-self-attention-how-to-implement-a-sparse-matrix
"An error occurs when predict with the same data as when performing train (expects 3 input(s), but it received 75 input tensors.)","<p>After training the model, I tried to make predictions, but an error occurred and I don't know how to fix it.</p>
<p>The model was constructed using electra.</p>
<p><strong>here is my model</strong></p>
<pre><code>electra = TFElectraModel.from_pretrained(&quot;monologg/koelectra-base-v3-discriminator&quot;, from_pt=True)
input_ids = tf.keras.Input(shape=(MAX_LEN,), name='input_ids', dtype=tf.int32)
mask = tf.keras.Input(shape=(MAX_LEN,), name='attention_mask', dtype=tf.int32)
token = tf.keras.Input(shape=(MAX_LEN,), name='token_type_ids', dtype=tf.int32)
embeddings = electra(input_ids, attention_mask = mask, token_type_ids= token)[0]
X = tf.keras.layers.GlobalMaxPool1D()(embeddings)
X = tf.keras.layers.BatchNormalization()(X)
X = tf.keras.layers.Dense(128, activation='relu')(X)
X = tf.keras.layers.Dropout(0.1)(X)
y = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)
model = tf.keras.Model(inputs=[input_ids, mask, token], outputs=y)
model.layers[2].trainable=False
model.summary()
</code></pre>
<p><strong>and here is summary</strong></p>
<pre><code>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 25)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
tf_electra_model_4 (TFElectraMo TFBaseModelOutput(la 112330752   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 768)          0           tf_electra_model_4[3][0]         
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 768)          3072        global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 128)          98432       batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dropout_390 (Dropout)           (None, 128)          0           dense_18[0][0]                   
__________________________________________________________________________________________________
outputs (Dense)                 (None, 3)            387         dropout_390[0][0]                
==================================================================================================
Total params: 112,432,643
Trainable params: 112,431,107
Non-trainable params: 1,536
__________________________________________________________________________________________________
</code></pre>
<p><strong>This is the code to make train data set.</strong></p>
<pre><code>input_ids = []
attention_masks = []
token_type_ids = []
train_data_labels = []

for train_sent, train_label in tqdm(zip(train_data[&quot;content&quot;], train_data[&quot;label&quot;]), total=len(train_data)):
    try:
        input_id, attention_mask, token_type_id = Electra_tokenizer(train_sent, MAX_LEN)
        input_ids.append(input_id)
        attention_masks.append(attention_mask)
        token_type_ids.append(token_type_id)
        train_data_labels.append(train_label)

    except Exception as e:
        print(e)
        print(train_sent)
        pass

train_input_ids = np.array(input_ids, dtype=int)
train_attention_masks = np.array(attention_masks, dtype=int)
train_type_ids = np.array(token_type_ids, dtype=int)
intent_train_inputs = (train_input_ids, train_attention_masks, train_type_ids)
intent_train_data_labels = np.asarray(train_data_labels, dtype=np.int32)
</code></pre>
<p><strong>this is train data set shape</strong></p>
<pre><code>tf.Tensor([ 3 75 25], shape=(3,), dtype=int32)
</code></pre>
<p><strong>With this train data, the model train works fine but execute the following code to predict, an error occurs.</strong></p>
<pre><code>sample_text = 'this is sample text'
input_id, attention_mask, token_type_id = Electra_tokenizer(sample_text, MAX_LEN)
sample_text = (input_id, attention_mask, token_type_id)
model(sample_text) #or model.predict(sample_text)
</code></pre>
<p><strong>here is error</strong></p>
<pre><code>Layer model_15 expects 3 input(s), but it received 75 input tensors. Inputs received: [&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;, &lt;tf.Tensor: ....
</code></pre>
<p>It's the same shape as when i train, but why do i get an error and ask for help on how to fix it.</p>
<p>hope you have a great year ahead. Happy New Year.</p>
","python, tensorflow, machine-learning, bert-language-model, huggingface-transformers","<p>It was a tensor dimension problem.</p>
<pre><code>test_input_ids = np.array(test_input_ids, dtype=np.int32)
test_attention_mask = np.array(test_attention_mask, dtype=np.int32)
test_token_type_id = np.array(test_token_type_id, dtype=np.int32)
ids = np.expand_dims(test_input_ids, axis=0)
atm = np.expand_dims(test_attention_mask, axis=0)
tok = np.expand_dims(test_token_type_id, axis=0)
model(ids,atm.tok) works fine
</code></pre>
",0,0,197,2020-12-31 06:01:53,https://stackoverflow.com/questions/65517232/an-error-occurs-when-predict-with-the-same-data-as-when-performing-train-expect
What do you need for plotting the outcome of a question-answering model,"<p>I have been working on a question answering model, where I receive answers on my questions by my word embedding model BERT. But I really want to plot something like this:
<a href=""https://i.sstatic.net/8yyvQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8yyvQ.png"" alt=""enter image description here"" /></a></p>
<p>But the problem is, I don't really know how. I am really stuck at this quest. I don't know how to represent a part of the context in a plot. I do have two variables, named answer_start and answer_end which indicates in what part in the context the model got its answers from. Can someone please help me out with this and tell me what variables I need to put in my pyplot?</p>
<p>Below my code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch
import numpy as np
import pandas as pd

max_seq_length = 512

tokenizer = AutoTokenizer.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;)

questions = [
    &quot;Welke soorten gladiatoren waren er?&quot;,
    &quot;Wat is een provocator?&quot;
]
for question in questions: # voor elke question moet er door alle lines geiterate worden
    print(f&quot;Question: {question}&quot;)
    f = open(&quot;test.txt&quot;, &quot;r&quot;)
    for line in f:
      text = str(line) #het antwoord moet een string zijn
      #encoding met tokenizen van de zinnen
      inputs = tokenizer.encode_plus(question,
                                     text,
                                     add_special_tokens=True,
                                     max_length=max_seq_length,
                                     truncation=True,
                                     return_tensors=&quot;pt&quot;)
      input_ids = inputs[&quot;input_ids&quot;].tolist()[0]

  

      #ff uitzoeken wat die ** deed
      answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)

      answer_start = torch.argmax(
          answer_start_scores
          )  # Het antwoord met de hoogste argmax accuracy vanaf het begin woord
      answer_end = torch.argmax(
          answer_end_scores) + 1  # Zelfde maar dan eind woord
      answer = tokenizer.convert_tokens_to_string(
          tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

      #om het antwoorden [cls] en NaN te voorkomen    
      if answer == '[CLS]':
        continue
      elif answer == '':
        continue
      else:
        print(f&quot;Answer: {answer}&quot;)
        print(f&quot;Answer start: {answer_start}&quot;)
        print(f&quot;Answer end: {answer_end}&quot;) 
      f.seek(0)
      break          
    # f.seek(0)
    # break
  
f.close()
</code></pre>
<p>Also the output:</p>
<pre><code>&gt; Question: Welke soorten gladiatoren waren er?
&gt; Answer: de thraex, de retiarius en de murmillo
&gt; Answer start: 24
&gt; Answer end: 37
&gt; Question: Wat is een provocator?
&gt; Answer: telemachus
&gt; Answer start: 87
&gt; Answer end: 90
</code></pre>
","python, matplotlib, gradient, bert-language-model","<p>I don't know if I understand what your problem is. But to make a plot similar to that of the figure, I would do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt

sentence = ('list' 'of' 'words' 'that' 'make' 'up' 'the' 'sentence' 'in' 'which' 'the' 'answer' 'is' 'found')
y_pos = np.arange(len(sentence))
probability = [0.1, 0.2, 0.1, 0.8, 0.6] 

plt.bar(y_pos, probability, align='center', alpha=0.5)
plt.xticks(y_pos, sentence)
plt.ylabel('Answer probability')
plt.title('Words of the sentence')

plt.show()

</code></pre>
<p>So assuming that the answer lies within a larger sentence/paragraph, what I would do is insert all the words of the sentence/paragraph into the x axis of a bar plot (variable <code>sentence</code> - text.txt I suppose), while on the y axis the percentage indicating the probability that a particular word is the beginning or ending word of the answer (variable <code>probability</code>). Obviously the two variables <code>sentence</code> and <code>probability</code> will have the same length, where the first sentence variable corresponds to the first probability value and so on.</p>
<p>For instance <code>answer_start_scores</code> and <code>answer_end_scores</code> will be the words with the highest score, therefore their &quot;bar&quot; of the bar plot will be the highest (highest value in the list of probability).</p>
<p>Finally in <code>answer_start_scores</code> and <code>answer_end_scores</code> you should have all the scores for which the starting and ending word is most likely.</p>
<p>EDIT:
Maybe, you could also make two separate bar plots for the initial word of the answer and the final word and then join them together by adding the percentages.</p>
",1,0,126,2020-12-31 13:33:53,https://stackoverflow.com/questions/65521591/what-do-you-need-for-plotting-the-outcome-of-a-question-answering-model
Pytorch Loss Function for making embeddings similar,"<p>I am working on an embedding model, where there is a BERT model, which takes in text inputs and output a multidimensional vector. The goal of the model is to find similar embeddings (high cosine similarity) for texts which are similar and different embeddings (low cosine similarity) for texts that are dissimilar.</p>
<p>When training in mini-batch mode, the BERT model gives a <code>N*D</code> dimensional output where <code>N</code> is the batch size and <code>D</code> is the output dimension of the BERT model.</p>
<p>Also, I have a target matrix of dimension <code>N*N</code>, which contains <code>1</code> in the <code>[i, j]</code> th position if the <code>sentence[i]</code> and <code>sentence[j]</code> are similar in sense and <code>-1</code> if not.</p>
<p>What I want to do is find the loss/error for the entire batch by finding the cosine similarity of all embeddings in the BERT output and comparing it to the target matrix.</p>
<p>What I did was simply multiply the tensor with its transpose and then take elementwise sigmoid.</p>
<pre><code>scores = torch.matmul(document_embedding, torch.transpose(document_embedding, 0, 1))
scores = torch.sigmoid(scores)

loss = self.bceloss(scores, targets)
</code></pre>
<p>But this does not seem to work.</p>
<p>Is there any other way to do this?</p>
<p>P.S. What I want to do is similar to the method described in <a href=""https://arxiv.org/pdf/1811.08008.pdf"" rel=""nofollow noreferrer"">this paper</a>.</p>
","pytorch, tensor, embedding, bert-language-model, loss","<p>To calculate the cosine similarity between two vectors you would have used <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html"" rel=""nofollow noreferrer""><code>nn.CosineSimilarity</code></a>. However, I don't think this allows you to get the pair-similarity from a set of <code>n</code> vectors. Fortunately enough, you can implement it yourself with some tensor manipulation.</p>
<p>Let us call <code>x</code> your document_embedding of shape <code>(n, d)</code> where <code>d</code> is the embedding size. We'll take <code>n=3</code> and <code>d=5</code>. So <code>x</code> is made up of <code>[x1, x2, x3].T</code>.</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(n, d)
tensor([[0.8620, 0.9322, 0.4220, 0.0280, 0.3789],
        [0.2747, 0.4047, 0.6418, 0.7147, 0.3409],
        [0.6573, 0.3432, 0.5663, 0.2512, 0.0582]])
</code></pre>
<p>The cosine similarity is a normalized dot product. The <code>x@x.T</code> <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""nofollow noreferrer"">matrix multiplication</a> will give you the pairwise dot product: which contains: <code>||x1||²</code>, <code>&lt;x1/x2&gt;</code>, <code>&lt;x1/x3&gt;</code>, <code>&lt;x2/x1&gt;</code>, <code>||x2||²</code>, etc...</p>
<pre><code>&gt;&gt;&gt; sim = x@x.T
tensor([[1.9343, 1.0340, 1.1545],
        [1.0340, 1.2782, 0.8822],
        [1.1545, 0.8822, 0.9370]])
</code></pre>
<p>To normalize take the vector of all norms: <code>||x1||</code>, <code>||x2||</code>, and <code>||x3||</code>:</p>
<pre><code>&gt;&gt;&gt; norm = x.norm(dim=1)
tensor([1.3908, 1.1306, 0.9680])
</code></pre>
<p>Construct the matrix containing the normalization factors: <code>||x1||²</code>, <code>||x1||.||x2||</code>, <code>||x1||.||x3||</code>, <code>||x2||.||x1||</code>, <code>||x2||²</code>, etc...</p>
<pre><code>&gt;&gt;&gt; factor = norm*norm.unsqueeze(1)
tensor([[1.9343, 1.5724, 1.3462],
        [1.5724, 1.2782, 1.0944],
        [1.3462, 1.0944, 0.9370]])
</code></pre>
<p>Then normalize:</p>
<pre><code>&gt;&gt;&gt; sim /= factor
tensor([[1.0000, 0.6576, 0.8576],
        [0.6576, 1.0000, 0.8062],
        [0.8576, 0.8062, 1.0000]])
</code></pre>
<p>Alternatively, <strong>a quicker way</strong> which avoids having to create the norm matrix, is to normalize before multiplying:</p>
<pre><code>&gt;&gt;&gt; x /= x.norm(dim=1, keepdim=True)
&gt;&gt;&gt; sim = x@x.T
tensor([[1.0000, 0.6576, 0.8576],
        [0.6576, 1.0000, 0.8062],
        [0.8576, 0.8062, 1.0000]])
</code></pre>
<p>For the loss function I would apply <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer""><code>nn.CrossEntropyLoss</code></a> straight away between the predicted similarity matrix and the target matrix, instead of applying sigmoid + BCE. Note: <code>nn.CrossEntropyLoss</code> includes <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html?highlight=logsoftmax#torch.nn.LogSoftmax"" rel=""nofollow noreferrer""><code>nn.LogSoftmax</code></a>.</p>
",2,0,4575,2020-12-31 13:59:40,https://stackoverflow.com/questions/65521840/pytorch-loss-function-for-making-embeddings-similar
zsh: no matches found: bertopic[visualization],"<p>I am trying to install bertopic[visualization] in my macbook pro using</p>
<pre><code>pip3 install bertopic[visualization]
</code></pre>
<p>but I am getting an error whenever I am running the above command. The error is as given below:</p>
<pre><code>zsh: no matches found: bertopic[visualization]
</code></pre>
<p>Is there any way to install bert's visualization option?</p>
","python-3.x, pip, bert-language-model","<p><code>zsh</code> uses square brackets for <a href=""http://zsh.sourceforge.net/Guide/zshguide05.html#l137"" rel=""noreferrer"">pattern matching</a> which means that if you need to pass literal square brackets as an argument to a command, you either need to escape them or quote the argument like this:</p>
<p>so try using:</p>
<p><code>pip3 install 'bertopic[visualization]'</code></p>
",6,2,2091,2021-01-08 04:32:40,https://stackoverflow.com/questions/65623487/zsh-no-matches-found-bertopicvisualization
How to find the (Most important) responsible Words/ Tokens/ embeddings responsible for the label result of a text classification model in PyTorch,"<p>Let us suppose I have a model like:</p>
<pre><code>class BERT_Subject_Classifier(nn.Module):

    def __init__(self,out_classes,hidden1=128,hidden2=32,dropout_val=0.2):
      super(BERT_Subject_Classifier, self).__init__()

      self.hidden1 = hidden1
      self.hidden2 = hidden2
      self.dropout_val = dropout_val
      self.logits = logit
      self.bert = AutoModel.from_pretrained('bert-base-uncased')
      self.out_classes = out_classes
      self.unfreeze_n = unfreeze_n # make the last n layers trainable
      
      self.dropout = nn.Dropout(self.dropout_val)
      self.relu =  nn.ReLU()
      self.fc1 = nn.Linear(768,self.hidden1)
      self.fc2 = nn.Linear(self.hidden1,self.hidden2)
      self.fc3 = nn.Linear(self.hidden2,self.out_classes)

    def forward(self, sent_id, mask):
      _, cls_hs = self.bert(sent_id, attention_mask=mask)
      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)
      x = self.fc2(x)
      x = self.dropout(x)
      return self.fc3(x)
</code></pre>
<p>I train my model and for a new data point <code>x = ['My Name is Slim Shady']</code>, I get my label result as  <code>3</code>.</p>
<p>My Question is that how can I check which of the words in the sentence were responsible for the the classification? I mean it could be any collection of words. Is there a library or way to check the functionality? Just like shown in the paper and <a href=""https://www.tensorflow.org/tutorials/text/image_captioning"" rel=""nofollow noreferrer"">Tensorflow Implementation</a> of <code>show Attend and Tell</code>, you can get the areas of images where the model is paying attention to. How can I do it for the Text?</p>
","python, deep-learning, pytorch, bert-language-model, huggingface-transformers","<p>Absolutely. One way to demonstrate which words have the greatest impact is through integrated gradients methods. For PyTorch, one package you can use is Captum. I would check out this page for a good example: <a href=""https://captum.ai/tutorials/IMDB_TorchText_Interpret"" rel=""nofollow noreferrer"">https://captum.ai/tutorials/IMDB_TorchText_Interpret</a></p>
<p>For Tensorflow, one package that you can use is Seldon. I would check out this page for a good example:
<a href=""https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html"" rel=""nofollow noreferrer"">https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html</a></p>
",2,3,1362,2021-01-08 07:44:43,https://stackoverflow.com/questions/65625130/how-to-find-the-most-important-responsible-words-tokens-embeddings-responsib
How to train BERT from scratch on a new domain for both MLM and NSP?,"<p>I’m trying to train BERT model from scratch using my own dataset using HuggingFace library. I would like to train the model in a way that it has the exact architecture of the original BERT model.</p>
<p>In the original paper, it stated that: <em>“BERT is trained on two tasks: predicting randomly masked tokens (MLM) and predicting whether two sentences follow each other (NSP). SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text.”</em></p>
<p>I’m trying to understand how to train the model on two tasks as above. At the moment, I initialised the model as below:</p>
<pre><code>from transformers import BertForMaskedLM
model = BertForMaskedLM(config=config)
</code></pre>
<p>However, it would just be for MLM and not NSP. How can I initialize and train the model with NSP as well or maybe my original approach was fine as it is?</p>
<p>My assumptions would be either</p>
<ol>
<li><p>Initialize with <code>BertForPreTraining</code> (for both MLM and NSP), OR</p>
</li>
<li><p>After finish training with <code>BertForMaskedLM</code>,
initalize the same model and train again with
<code>BertForNextSentencePrediction</code> (but this approach’s computation and
resources would cost twice…)</p>
</li>
</ol>
<p>I’m not sure which one is the correct way. Any insights or advice would be greatly appreciated.</p>
","deep-learning, nlp, bert-language-model, huggingface-transformers, transformer-model","<p>I would suggest doing the following:</p>
<ol>
<li><p>First pre-train BERT on the MLM objective. HuggingFace provides a script especially for training BERT on the MLM objective on your own data. You can find it <a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling"" rel=""noreferrer"">here</a>. As you can see in the <code>run_mlm.py</code> script, they use <code>AutoModelForMaskedLM</code>, and you can specify any architecture you want.</p>
</li>
<li><p>Second, if want to train on the next sentence prediction task, you can define a <code>BertForPretraining</code> model (which has both the MLM and NSP heads on top), then load in the weights from the model you trained in step 1, and then further pre-train it on a next sentence prediction task.</p>
</li>
</ol>
<p>UPDATE: apparently the next sentence prediction task did help improve performance of BERT on some GLUE tasks. See <a href=""https://youtu.be/knTc-NQSjKA?t=1610"" rel=""noreferrer"">this talk</a> by the author of BERT.</p>
",11,14,13257,2021-01-09 19:46:24,https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp
"&quot;for tokens_tensor, segments_tensors, att_mask, pos_id, trg in data_loader: NameError: name &#39;data_loader&#39; is not defined&quot;","<p>I am trying to implement question answering model with a BERT transformer implemented by jugapuff.
Link to the code: <a href=""https://github.com/jugapuff/BERT-for-bAbi-task"" rel=""nofollow noreferrer"">https://github.com/jugapuff/BERT-for-bAbi-task</a></p>
<p>After executing the main.py file which is written below as well, I m getting this error: &quot;for tokens_tensor, segments_tensors, att_mask, pos_id, trg in data_loader: <strong>NameError: name 'data_loader' is not defined</strong>&quot;</p>
<pre><code>from dataloader import bAbi_Dataset
import torch
import torch.nn as nn
from model import model
from pytorch_transformers import AdamW

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

if torch.cuda.is_available():
    print(&quot;GPU:&quot; + str(torch.cuda.get_device_name(0)))
    
my_model = model()
my_model.to(device)

optimizer = AdamW(my_model.parameters())
criterion = nn.NLLLoss()


EPOCHS = 10
for epoch in range(1, EPOCHS+1):
    
    my_model.train()
    
    train_loss = 0
    length = 0
    for tokens_tensor, segments_tensors, att_mask, pos_id, trg in data_loader:
        output = my_model(tokens_tensor.to(device), segments_tensors.to(device), att_mask.to(device), pos_id.to(device))
        loss = criterion(output, trg.to(device))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        length+=1
        train_loss += loss.item()
        if length % 10 == 0:
          print(&quot;\t\t{:3}/25000 : {}&quot;.format(length, train_loss / length))
        
    epoch_loss = train_loss / length
    print(&quot;##################&quot;)
    print(&quot;{} epoch Loss : {:.4f}&quot;.format(epoch, epoch_loss))
  
</code></pre>
<p>and data_loader.py is as</p>
<pre><code>import os
import torch
import torch.utils.data as data
from pytorch_transformers import BertTokenizer

def _parse( file, only_supporting=False):
        data, story = [], []
        for line in file:
            tid, text = line.rstrip('\n').split(' ', 1)
            if tid == '1':
                story = []
            if text.endswith('.'):
                story.append(text[:])
            else:
                query, answer, supporting = (x.strip() for x in text.split('\t'))
                if only_supporting:
                    substory = [story[int(i) - 1] for i in supporting.split()]
                else:
                    substory = [x for x in story if x]
                data.append((substory, query[:-1], answer))
                story.append(&quot;&quot;)
        return data
    
def build_trg_dics(tenK=True, path=&quot;tasks_1-20_v1-2&quot;, train=True):
    
    if tenK:
        dirname = os.path.join(path, 'en-10k')
    else:
        dirname = os.path.join(path, 'en')

    for (dirpath, dirnames, filenames) in os.walk(dirname):
        filenames = filenames

    if train:
        filenames = [filename for filename in filenames if  &quot;train.txt&quot; in filename]
    else:
        filenames = [filename for filename in filenames if  &quot;test.txt&quot; in filename]

    temp = []
    for filename in filenames:
        f = open(os.path.join(dirname, filename), 'r')
        parsed =_parse(f)
        temp.extend([d[2] for d in parsed])
    temp = set(temp)
    
    trg_word2id = {word:i for i, word in enumerate(temp)}
    trg_id2word = {i:word for i, word in enumerate(temp)}
    return trg_word2id, trg_id2word


class bAbi_Dataset(data.Dataset):
    
    def __init__(self, trg_word2id, tenK=True, path = &quot;tasks_1-20_v1-2&quot;, train=True):
        # joint is Default
        
        
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        
        if tenK:
            dirname = os.path.join(path, 'en-10k')
        else:
            dirname = os.path.join(path, 'en')
            
        for (dirpath, dirnames, filenames) in os.walk(dirname):
            filenames = filenames
         
        if train:
            filenames = [filename for filename in filenames if  &quot;train.txt&quot; in filename]
        else:
            filenames = [filename for filename in filenames if  &quot;test.txt&quot; in filename]
        
        self.src = []
        self.trg = []
        
        for filename in filenames:
            f = open(os.path.join(dirname, filename), 'r')
            parsed = _parse(f)
            self.src.extend([d[:2] for d in parsed])
            self.trg.extend([trg_word2id[d[2]] for d in parsed])
        self.trg = torch.tensor(self.trg)
            
            
    def __getitem__(self, index):
        src_seq = self.src[index]
        trg = self.trg[index]
        src_seq, seg_seq, att_mask, pos_id = self.preprocess_sequence(src_seq)
        
        return src_seq, seg_seq, att_mask, pos_id, trg

    def __len__(self):
        return len(self.trg)
        
    def preprocess_sequence(self, seq):

        text =  [&quot;[CLS]&quot;] + list(seq[0]) + [&quot;[SEP]&quot;] + [seq[1]] + [&quot;[SEP]&quot;]

        tokenized_text = self.tokenizer.tokenize(&quot; &quot;.join(text))
        indexed_text = self.tokenizer.convert_tokens_to_ids(tokenized_text)
        where_is_sep = indexed_text.index(102) + 1
        segment_ids = [0 ]* (where_is_sep) + [1] * (len(indexed_text)- where_is_sep)
        attention_mask = [1] *len(indexed_text)
        pos_id = [i for i in range(len(indexed_text))]
        
        return torch.tensor(indexed_text), torch.tensor(segment_ids), torch.tensor(attention_mask), torch.tensor(pos_id)
    
    

def collate_fn(data):
    def merge(sequences):
        lengths = [len(seq) for seq in sequences]
        padded_seqs = torch.zeros(len(sequences), 512).long()
        for i, seq in enumerate(sequences):
            
            
            end = lengths[i]
            if end &lt;= 512:
                padded_seqs[i, :end] = seq[:end]
            else:
                padded_seqs[i] = seq[-512:]

        return padded_seqs
      
    def pos_merge(sequences):
        
        lengths = [len(seq) for seq in sequences]
        padded_seqs = torch.zeros(len(sequences), 512).long()
        for i, seq in enumerate(sequences):
            
            padded_seqs[i] = torch.tensor([i for i in range(512)])

        return padded_seqs
    
    src_seqs, seg_seqs, att_mask, pos_id, trgs = zip(*data)
    src_seqs = merge(src_seqs)
    seg_seqs = merge(seg_seqs)
    att_mask = merge(att_mask)
    pos_id = pos_merge(pos_id)
    trgs = torch.tensor(trgs)
    return src_seqs, seg_seqs, att_mask, pos_id, trgs
</code></pre>
<p>data_loader variable declaration in main.py is missing. So I tried to load data_loader as</p>
<p>for tokens_tensor, segments_tensors, att_mask, pos_id, trg in <strong>dataloader.collate_fn(bAbi_Dataset)</strong>:</p>
<p>use collate_fn() function in data_loader.py, but it did not work. When I change it as above, it gives the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 27, in &lt;module&gt;
  File &quot;/content/BERT-for-bAbi-task/dataloader.py&quot;, line 133, in collate_fn
    src_seqs, seg_seqs, att_mask, pos_id, trgs = zip(*data)
  File &quot;/usr/lib/python3.6/typing.py&quot;, line 682, in inner
    return func(*args, **kwds)
  File &quot;/usr/lib/python3.6/typing.py&quot;, line 1107, in __getitem__
    params = tuple(_type_check(p, msg) for p in params)
  File &quot;/usr/lib/python3.6/typing.py&quot;, line 1107, in &lt;genexpr&gt;
    params = tuple(_type_check(p, msg) for p in params)
  File &quot;/usr/lib/python3.6/typing.py&quot;, line 374, in _type_check
    raise TypeError(msg + &quot; Got %.100r.&quot; % (arg,))
TypeError: Parameters to generic types must be types. Got 0.
</code></pre>
<p>Could anyone please help me how to correct the error?</p>
","pytorch, bert-language-model","<p>I will just give you some pointers:</p>
<ul>
<li><p><code>collate_fn</code> is not meant to be called with a dataset as argument. It is a special callback function given to a dataloader and used to collate batch elements into a batch.</p>
</li>
<li><p>Since <code>bAbi_Dataset</code> in <code>/dataloader.py</code> is defined as a <code>torch.utils.data.Dataset</code> I would guess you are meant to initialize it instead. It is defined <a href=""https://github.com/jugapuff/BERT-for-bAbi-task/blob/60e89eeec0af88517ebbfe769019f5986d538db0/dataloader.py#L51"" rel=""nofollow noreferrer"">here</a> as:</p>
<pre><code>def __init__(self, trg_word2id, tenK=True, path = &quot;tasks_1-20_v1-2&quot;, train=True)
</code></pre>
<p>There is another function <code>build_trg_dics</code> in <code>/dataloader.py</code> which is used to create the parse the content from files. You should take a look at them before setting the right arguments for <code>bAbi_Dataset</code>.</p>
</li>
<li><p>Lastly, when you have your dataset initialized, you can attach a dataloader on it using <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer""><code>torch.utils.data.DataLoader</code></a>. This would look like:</p>
<pre><code>data_loader = DataLoader(dataset, batch_size=16)
</code></pre>
<p>At this point, you might even need to plug in the collate function provided in <code>/dataloader.py</code>.</p>
</li>
</ul>
<p>If you don't really know what you are doing, I would suggest you start with a working repository and work your way from there. Good luck!</p>
",0,0,427,2021-01-11 22:29:40,https://stackoverflow.com/questions/65675445/for-tokens-tensor-segments-tensors-att-mask-pos-id-trg-in-data-loader-name
Huggingface TFBertForSequenceClassification always predicts the same label,"<p>TL;DR:
My model always predicts the same labels and I don't know why. Below is my entire code for fine-tuning in the hopes that someone can point out to me where I am going wrong.</p>
<p>I am using Huggingface's TFBertForSequenceClassification for sequence classification task to predict 4 labels of sentences in German text.</p>
<p>I use the bert-base-german-cased model since I don't use only lower case text (since German is more case sensitive than English).</p>
<p>I get my input from a csv file that I construct from an annotated corpus I received. Here's a sample of that:</p>
<pre><code>0       Hier kommen wir ins Spiel Die App Cognitive At...
1       Doch wenn Athlet Lebron James jede einzelne Mu...
2       Wie kann ein Gehirn auf Hochleistung getrimmt ...
3       Wie schafft es Warren Buffett knapp 1000 Wörte...
4       Entfalte dein mentales Potenzial und werde ein...
Name: sentence_clean, Length: 3094, dtype: object
</code></pre>
<p>And those are my labels, from the same csv file:</p>
<pre><code>0       e_1
1       e_4
2       e_4
3       e_4
4       e_4
</code></pre>
<p>The distinct labels are: e_1, e_2, e_3, and e_4</p>
<p>This is the code I am using to fine tune my model:</p>
<pre><code>import pandas as pd
import numpy as np
import os
    
# read in data
# sentences_df = pd.read_csv('path/file.csv')


X = sentences_df.sentence_clean
Y = sentences_df.classId

# =============================================================================
# One hot encode labels
# =============================================================================

# integer encode labels
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(list(Y))


# one hot encode labels
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
Y_integer_encoded_reshaped = Y_integer_encoded.reshape(len(Y_integer_encoded), 1)
Y_one_hot_encoded = onehot_encoder.fit_transform(Y_integer_encoded_reshaped)

# train test split
from sklearn.model_selection import train_test_split


X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y_one_hot_encoded, test_size=0.20, random_state=42)


# =============================================================================
# Perpare datasets for finetuning
# =============================================================================
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU') 
tf.config.experimental.set_memory_growth(physical_devices[0], True)

from transformers import BertTokenizer, TFBertForSequenceClassification


tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased') # initialize tokenizer


# tokenize trai and test sets
max_seq_length = 128

X_train_tokens = tokenizer(list(X_train_raw),
                            truncation=True,
                            padding=True)

X_test_tokens = tokenizer(list(X_test_raw),
                            truncation=True,
                            padding=True)


# create TF datasets as input for BERT model
bert_train_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_train_tokens),
    y_train
))

bert_test_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_test_tokens),
    y_test
))

# =============================================================================
# setup model and finetune
# =============================================================================

# define hyperparams
num_labels = 4
learninge_rate = 2e-5
epochs = 3
batch_size = 16

# create BERT model
bert_categorical_partial = TFBertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=num_labels)

optimizer = tf.keras.optimizers.Adam(learning_rate=learninge_rate)
bert_categorical_partial.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

history = bert_categorical_partial.fit(bert_train_ds.shuffle(100).batch(batch_size),
          epochs=epochs,
          # batch_size=batch_size,
          validation_data=bert_test_ds.shuffle(100).batch(batch_size))
</code></pre>
<p>And here is the output from fine-tuning:</p>
<pre><code>Epoch 1/3
155/155 [==============================] - 31s 198ms/step - loss: 8.3038 - accuracy: 0.2990 - val_loss: 8.7751 - val_accuracy: 0.2811
Epoch 2/3
155/155 [==============================] - 30s 196ms/step - loss: 8.2451 - accuracy: 0.2913 - val_loss: 8.9314 - val_accuracy: 0.2779
Epoch 3/3
155/155 [==============================] - 30s 196ms/step - loss: 8.3101 - accuracy: 0.2913 - val_loss: 9.0355 - val_accuracy: 0.2746
</code></pre>
<p>Lastly, I try to predict the labels of the test set and validate the results with a confusion matrix:</p>
<pre><code>X_test_tokens_new = {'input_ids': np.asarray(X_test_tokens['input_ids']),
                     'token_type_ids': np.asarray(X_test_tokens['token_type_ids']),
                     'attention_mask': np.asarray(X_test_tokens['attention_mask']),
                     }

pred_raw = bert_categorical_partial.predict(X_test_tokens_new)
pred_proba = tf.nn.softmax(pred_raw).numpy()
pred = pred_proba[0].argmax(axis = 1)
y_true = y_test.argmax(axis = 1)

cm = confusion_matrix(y_true, pred)
</code></pre>
<p>Output of print(cm):</p>
<pre><code>array([[  0,   0,   0,  41],
       [  2,   0,   0, 253],
       [  2,   0,   0, 219],
       [  6,   0,   0,  96]], dtype=int64)
</code></pre>
<p>As you can see, my accuracy is really bad, and when I look at the cm, I can see that my model pretty much just predicts one single label.
I've tried everything and ran the model multiple times, but I always get the same results.
I do know that the data I am working with isn't great and I am only training on abour 2k sentences with labels. But I have a feeling the accuracy should still be higher and, more importantly, the model shouldn't just predict one single label 98% of the time, right?</p>
<p>I posted everything I am using to run the model in the hopes someone can point me to where I am going wrong.
Thank very much in advance for your help!</p>
","python, tensorflow, bert-language-model, huggingface-transformers","<p>You trained for a couple of minutes. It is not enough even for pretrained BERT.</p>
<p>Try to decrease learning rate to get your accuracy increasing after every epoch (for the first 10 epochs). And train for more epochs (until you see the validation accuracy decreasing for 10 epochs).</p>
",5,4,4350,2021-01-12 00:24:13,https://stackoverflow.com/questions/65676389/huggingface-tfbertforsequenceclassification-always-predicts-the-same-label
IndexError: index out of range in self while try to fine tune Roberta model after adding special tokens,"<p>I am trying to fine tune a Roberta model after adding some special tokens to its tokenizer:</p>
<pre><code>    special_tokens_dict = {'additional_special_tokens': ['[Tok1]','[Tok2]']}

    tokenizer.add_special_tokens(special_tokens_dict)
</code></pre>
<p>I get this error when i try to train the model (on cpu):</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-75-d63f8d3c6c67&gt; in &lt;module&gt;()
     50         l = model(b_input_ids, 
     51                      attention_mask=b_input_mask,
---&gt; 52                     labels=b_labels)
     53         loss,logits = l
     54         total_train_loss += l[0].item()

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 

IndexError: index out of range in self
</code></pre>
<p>p.s. If I comment <code>add_special_tokens</code> the code works.</p>
","bert-language-model, huggingface-transformers, roberta-language-model","<p>You also need to tell your model that it needs to learn the vector representations of two new tokens:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaForQuestionAnswering
t = RobertaTokenizer.from_pretrained('roberta-base')
m = RobertaForQuestionAnswering.from_pretrained('roberta-base')
#roberta-base 'knows' 50265 tokens
print(m.roberta.embeddings.word_embeddings)

special_tokens_dict = {'additional_special_tokens': ['[Tok1]','[Tok2]']}
t.add_special_tokens(special_tokens_dict)
#we now tell the model that it needs to learn new tokens:
m.resize_token_embeddings(len(t))
m.roberta.embeddings.word_embeddings.padding_idx=1
print(m.roberta.embeddings.word_embeddings)
</code></pre>
<p>Output:</p>
<pre><code>Embedding(50265, 768, padding_idx=1)
Embedding(50267, 768, padding_idx=1)
</code></pre>
",2,1,4867,2021-01-12 11:27:42,https://stackoverflow.com/questions/65683013/indexerror-index-out-of-range-in-self-while-try-to-fine-tune-roberta-model-afte
Pytorch - Caught StopIteration in replica 1 on device 1 error while Training on GPU,"<p>I am trying to train a BertPunc model on the train2012 data used in the git link: <a href=""https://github.com/nkrnrnk/BertPunc"" rel=""nofollow noreferrer"">https://github.com/nkrnrnk/BertPunc</a>.
While running on the server, with 4 GPUs enabled, below is the error I get:</p>
<pre><code>StopIteration: Caught StopIteration in replica 1 on device 1.
Original Traceback (most recent call last):
  File &quot;/home/stenoaimladmin/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/home/stenoaimladmin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/stenoaimladmin/notebooks/model_BertPunc.py&quot;, line 16, in forward
    x = self.bert(x)
  File &quot;/home/stenoaimladmin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/stenoaimladmin/anaconda3/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py&quot;, line 861, in forward
    sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,
  File &quot;/home/stenoaimladmin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/stenoaimladmin/anaconda3/lib/python3.8/site-packages/pytorch_pretrained_bert/modeling.py&quot;, line 727, in forward
    extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility
StopIteration
</code></pre>
<p>From the link: <a href=""https://github.com/huggingface/transformers/issues/8145"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/8145</a>, this appears to be happening when the data gets moved back and forth between multiple GPUs.</p>
<p>As per the git link: <a href=""https://github.com/interpretml/interpret-text/issues/117"" rel=""nofollow noreferrer"">https://github.com/interpretml/interpret-text/issues/117</a>, we need to downgrade PyTorch version to 1.4 from 1.7 which I use currently. For me downgrading the version isnt an option as I have other scripts that use Torch 1.7 version. What should I do to overcome this error?</p>
<p>I cant put the whole code here as there are too many lines, but here is the snippet that gives me the error:</p>
<pre><code>bert_punc, optimizer, best_val_loss = train(bert_punc, optimizer, criterion, epochs_top, 
        data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations_top, best_val_loss=1e9)
</code></pre>
<p>Here is my DataParallel code:</p>
<pre><code>   bert_punc = nn.DataParallel(BertPunc(segment_size, output_size, dropout)).cuda()
</code></pre>
<p>I tried changing the Dataparallel line to divert the training to only 1 GPU , out of 4 present. But that gave me a space issue, and hence had to revert the code back to default.</p>
<p>Here is the link to all scripts that I am using: <a href=""https://github.com/nkrnrnk/BertPunc"" rel=""nofollow noreferrer"">https://github.com/nkrnrnk/BertPunc</a>
Please advice.</p>
","python-3.x, pytorch, bert-language-model","<p>change</p>
<p><code>extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility</code></p>
<p>to</p>
<p><code>extended_attention_mask = extended_attention_mask.to(dtype=torch.float32) # fp16 compatibility</code></p>
<p>For more details, see <a href=""https://github.com/vid-koci/bert-commonsense/issues/6"" rel=""noreferrer"">https://github.com/vid-koci/bert-commonsense/issues/6</a></p>
",8,1,5704,2021-01-16 14:28:54,https://stackoverflow.com/questions/65750762/pytorch-caught-stopiteration-in-replica-1-on-device-1-error-while-training-on
HuggingFace Bert Sentiment analysis,"<p>I am getting the following error :</p>
<p><code>AssertionError: text input must of type str (single example), List[str] (batch or single pretokenized example) or List[List[str]] (batch of pretokenized examples).</code>, when I run <code>classifier(encoded)</code>. My text type is <code>str</code> so I am not sure what I am doing wrong. Any help is very appreciated.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForMaskedLM, AutoModelForSequenceClassification, pipeline

# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows
import logging
logging.basicConfig(level=logging.INFO)

# Load pre-trained model tokenizer (vocabulary)
# used the cased instead of uncased to account for cases like BAD.
tokenizer = BertTokenizer.from_pretrained('bert-base-cased') 


# alternative? what is the difference between these two tokenizers? 
#tokenizer = AutoTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)

model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)


# feed the model and the tokenizer into the pipeline
classifier = pipeline('sentiment-analysis', model=model, tokenizer= tokenizer)


#---------------sample raw input passage--------

text = &quot;Who was Jim Henson ? Jim Henson was a puppeteer. He is simply awful.&quot;
# tokenized_text = tokenizer.tokenize(text)

#----------Tokenization and Padding---------
# Encode the sentences to get tokenized and add padding stuff
encoded = tokenizer.encode_plus(
    text=text,  # the sentences to be encoded
    add_special_tokens=True,  # Add [CLS] and [SEP] !!!
    max_length = 64,  # maximum length of a sentence  (TODO Figure the longest passage length)
    pad_to_max_length=True,  # Add [PAD]s
    return_attention_mask = True,  # Generate the attention mask
    truncation=True,  #explicitly truncate examples to max length
    return_tensors = 'pt',  # ask the function to return PyTorch tensors
)

#-------------------------------------------
# view the IDs
for key, value in encoded.items():
    print(f&quot;{key}: {value.numpy().tolist()}&quot;)
    
#-------------------------------------------


classifier(encoded)

</code></pre>
","python, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>The pipeline already includes the encoder.
Instead of</p>
<pre><code>classifier(encoded)
</code></pre>
<p>do</p>
<pre><code>classifier(text)
</code></pre>
",1,4,11907,2021-01-25 09:13:28,https://stackoverflow.com/questions/65881820/huggingface-bert-sentiment-analysis
"About Bert embedding (input_ids, input_mask)","<p>As far as I understand it, in Bert's operating logic, he changes 50% of his sentences that he takes as input. It doesn't touch the rest.</p>
<p>1-) Is the changed part the transaction made with tokenizer.encoder? And is this equal to input_ids?</p>
<p>Then padding is done. Creating a matrix according to the specified Max_len. the empty part is filled with 0.</p>
<p>After these, cls tokens are placed per sentence. Sep token is placed at the end of the sentence.</p>
<p>2-) Is input_mask happening in this process?</p>
<p>3 -) In addition, where do we use input_segment?</p>
","python, cpu-word, embedding, bert-language-model","<ol>
<li>The <code>input_mask</code> obtained by encoding the sentences does not show the presence of <code>[MASK]</code> tokens. Instead, when the batch of sentences are tokenized, prepended with <code>[CLS]</code>, and appended with <code>[SEP]</code> tokens, it obtains an arbitrary length.</li>
</ol>
<p>To make all the sentences in the batch has fixed number of tokens, zero padding is performed. The <code>input_ids</code> variable shows whether a given token position contians actual token or if it a zero padded position.</p>
<ol start=""2"">
<li><p>Using <code>[MASK]</code> token is using only if you want to train on Masked Language Model(MLM) objective.</p>
</li>
<li><p>BERT is trained on two objectives, MLM and Next Sentence Prediction(NSP). In NSP, you pass two sentences and try to predict if the second sentence is the following sentence of first sentence or not. <code>segment_id</code> holds the information if of which sentence a particular token belongs to.</p>
</li>
</ol>
",1,0,2145,2021-01-26 12:34:38,https://stackoverflow.com/questions/65901473/about-bert-embedding-input-ids-input-mask
SimpleTransformers Error: VersionConflict: tokenizers==0.9.4? How do I fix this?,"<p>I'm trying to execute the simpletransformers example from their site on google colab.</p>
<p>Example:</p>
<pre><code>from simpletransformers.classification import ClassificationModel, ClassificationArgs
import pandas as pd
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(&quot;transformers&quot;)
transformers_logger.setLevel(logging.WARNING)

# Preparing train data
train_data = [
    [&quot;Aragorn was the heir of Isildur&quot;, 1],
    [&quot;Frodo was the heir of Isildur&quot;, 0],
]
train_df = pd.DataFrame(train_data)
train_df.columns = [&quot;text&quot;, &quot;labels&quot;]

# Preparing eval data
eval_data = [
    [&quot;Theoden was the king of Rohan&quot;, 1],
    [&quot;Merry was the king of Rohan&quot;, 0],
]
eval_df = pd.DataFrame(eval_data)
eval_df.columns = [&quot;text&quot;, &quot;labels&quot;]

# Optional model configuration
model_args = ClassificationArgs(num_train_epochs=1)

# Create a ClassificationModel
model = ClassificationModel(
    &quot;roberta&quot;, &quot;roberta-base&quot;, args=model_args
)

# Train the model
model.train_model(train_df)

# Evaluate the model
result, model_outputs, wrong_predictions = model.eval_model(eval_df)

# Make predictions with the model
predictions, raw_outputs = model.predict([&quot;Sam was a Wizard&quot;])
</code></pre>
<p>But it gives me the following error:</p>
<blockquote>
<p>VersionConflict: tokenizers==0.9.4 is required for a normal
functioning of this module, but found tokenizers==0.10.0. Try: pip
install transformers -U or pip install -e '.[dev]' if you're working
with git master</p>
</blockquote>
<p>I've tried <code>!pip install transformers -U</code>and even <code>!pip install tokenizers==0.9.4</code>  but keeps giving the same error.
I have executed this code before and it worked just fun, but now it's giving the mentioned error.</p>
","tensorflow, nlp, bert-language-model, simpletransformers, sentence-transformers","<p>I am putting this here incase someone faces the same problem.
I was helped by the creator himself.</p>
<blockquote>
<pre><code>Workaround:
Install tokenizers==0.9.4 before install simpletransformers

In Colab for example;

!pip install tokenizers==0.9.4
!pip install simpletransformers
</code></pre>
<p><a href=""https://github.com/ThilinaRajapakse/simpletransformers/issues/950"" rel=""nofollow noreferrer"">https://github.com/ThilinaRajapakse/simpletransformers/issues/950</a></p>
</blockquote>
",3,2,2007,2021-01-27 17:16:45,https://stackoverflow.com/questions/65924090/simpletransformers-error-versionconflict-tokenizers-0-9-4-how-do-i-fix-this
Assigning weights during testing the bert model,"<p>I have a basic conceptual doubt. When i train a bert model on sentence say:</p>
<pre><code>Train: &quot;went to get loan from bank&quot; 
Test :&quot;received education loan from bank&quot;
</code></pre>
<p>How does the test sentence assigns the weights for each token because i however dont pass exact sentence for testing and there is a slight addition  of words like &quot;education&quot; which change the context slightly</p>
<p>Assuming such context is not trained in my model how the weights are assigned for each token in my bert before i fine tune further</p>
<p>If i confuse with my question, simply put i am trying to understand how the weights get assigned during testing if a slight variation in context occurs that was not trained on.</p>
","bert-language-model, huggingface-transformers, transformer-model, language-model","<p>The vector representation of a token (keep in mind that token != word) is stored in an embedding layer. When we load the 'bert-base-uncased' model, we can see that it &quot;knows&quot; 30522 tokens and that the vector representation of each token consists of 768 elements:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertModel
bert= BertModel.from_pretrained('bert-base-uncased')
print(bert.embeddings.word_embeddings)
</code></pre>
<p>Output:</p>
<pre><code>Embedding(30522, 768, padding_idx=0)
</code></pre>
<p>This embedding layer is not aware of any strings but of ids. For example, the vector representation of the id <code>101</code> is:</p>
<pre class=""lang-py prettyprint-override""><code>print(bert.embeddings.word_embeddings.weight[101])
</code></pre>
<p>Output:</p>
<pre><code>tensor([ 1.3630e-02, -2.6490e-02, -2.3503e-02, -7.7876e-03,  8.5892e-03,
        -7.6645e-03, -9.8808e-03,  6.0184e-03,  4.6921e-03, -3.0984e-02,
         1.8883e-02, -6.0093e-03, -1.6652e-02,  1.1684e-02, -3.6245e-02,
         8.3482e-03, -1.2112e-03,  1.0322e-02,  1.6692e-02, -3.0354e-02,
        ...
         5.4162e-03, -3.0037e-02,  8.6773e-03, -1.7942e-03,  6.6826e-03,
        -1.1929e-02, -1.4076e-02,  1.6709e-02,  1.6860e-03, -3.3842e-03,
         8.6805e-03,  7.1340e-03,  1.5147e-02], grad_fn=&lt;SelectBackward&gt;)
</code></pre>
<p>Everything that is outside of the &quot;known&quot; ids is not processable by BERT. To answer your question we need to look at the component that maps a string to the ids. This component is called a tokenizer. There are different tokenization <a href=""https://huggingface.co/transformers/tokenizer_summary.html"" rel=""nofollow noreferrer"">approaches</a>. BERT uses a WordPiece tokenizer which is a subword algorithm. This algorithm replaces everything <strong>that can not be created</strong> from its vocabulary with an unknown token <strong>that is part</strong> of the vocabulary (<code>[UNK]</code> in the original implementation, id: 100).</p>
<p>Please have a look at the following small example in which a WordPiece tokenizer is trained from scratch to confirm that beheaviour:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers import BertWordPieceTokenizer
path ='file_with_your_trainings_sentence.txt'
tokenizer = BertWordPieceTokenizer()
tokenizer.train(files=path, vocab_size=30000, special_tokens=['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'])
otrain = tokenizer.encode(&quot;went to get loan from bank&quot;)
otest =  tokenizer.encode(&quot;received education loan from bank&quot;)

print('Vocabulary size: {}'.format(tokenizer.get_vocab_size()))
print('Train tokens: {}'.format(otrain.tokens))
print('Test tokens: {}'.format(otest.tokens))
</code></pre>
<p>Output:</p>
<pre><code>Vocabulary size: 27
Train tokens: ['w', '##e', '##n', '##t', 't', '##o', 'g', '##e', '##t', 'l', '##o', '##an', 'f', '##r', '##o', '##m', 'b', '##an', '##k']
Test tokens: ['[UNK]', '[UNK]', 'l', '##o', '##an', 'f', '##r', '##o', '##m', 'b', '##an', '##k']
</code></pre>
",0,0,1709,2021-01-27 18:58:04,https://stackoverflow.com/questions/65925640/assigning-weights-during-testing-the-bert-model
Backpropagation in bert,"<p>i would like to know when people say pretrained bert model, is it only the final classification neural network is trained</p>
<p>Or</p>
<p>Is there any update inside transformer through back propagation along with classification neural network</p>
","nlp, bert-language-model, transformer-model","<p>During pre-training, there is a complete training if the model (updation of weights). Moreover, BERT is trained on Masked Language Model objective and not classification objective.</p>
<p>In pre-training, you usually train a model with huge amount of generic data. Thus, it has to be fine-tuned with the task-specific data and task-specific objective.</p>
<p>So, if your task is classification on a dataset X. You fine-tune BERT accordingly. And now, you will be adding a task-specific layer (classification layer, in BERT they have used dense layer over <code>[CLS]</code> token). While fine-tuning, you update the pre-trained model weights as well as the new task-specific layer.</p>
",3,2,1344,2021-02-03 17:15:55,https://stackoverflow.com/questions/66032426/backpropagation-in-bert
BERT Convert &#39;SpanAnnotation&#39; to answers using scores from hugging face models,"<p>I'm following along with the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering"" rel=""nofollow noreferrer"">documentation</a> for importing a pretrained model question and answer model from huggingface</p>
<pre><code>from transformers import BertTokenizer, BertForQuestionAnswering
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits
</code></pre>
<p>this returns start and end scores, but how can I get a meaningful text answer from here?</p>
","python, pytorch, bert-language-model, huggingface-transformers","<p>So I did a little digging around and it looks like scores can be converted to tokens which can be used to build the answer.  Here is a short example:</p>
<pre><code>answer_start = torch.argmax(start_scores) 
answer_end = torch.argmax(end_scores) + 1

tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0][answer_start:answer_end]))
</code></pre>
",3,2,331,2021-02-06 03:50:23,https://stackoverflow.com/questions/66073395/bert-convert-spanannotation-to-answers-using-scores-from-hugging-face-models
Fine-Tuned ALBERT Question and Answering with HuggingFace,"<p>I'm trying to create a question and answering AI, I would like it to be as accurate as possible without having to train the model myself.</p>
<p>I can create a simple AI using the existing base models like so via their documentation:</p>
<pre><code>from transformers import AlbertTokenizer, AlbertForQuestionAnswering
import torch
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')
question, text = &quot;What does He like?&quot;, &quot;He likes bears&quot;
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits

answer_start = torch.argmax(start_scores)  # get the most likely beginning of answer with the argmax of the score
answer_end = torch.argmax(end_scores) + 1
tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0][answer_start:answer_end]))
</code></pre>
<p>However this model doesn't answer questions as accurate as others.  On the HuggingFace site I've found an example that I'd like to use of a <a href=""https://huggingface.co/ktrapeznikov/albert-xlarge-v2-squad-v2"" rel=""nofollow noreferrer"">fine-tuned model</a></p>
<p>However the instructions show how to train a model like so.  The example works on the page so clearly a pretrained model of the exists.</p>
<p>Does anyone know how I can reuse the existing models so I don't have to train one from scratch?</p>
","python, nlp, artificial-intelligence, torch, bert-language-model","<p>Turns out I just needed to grab an additional identifier when trying to request the model:</p>
<pre><code>from transformers import AlbertTokenizer, AlbertForQuestionAnswering
import torch

MODEL_PATH = 'ktrapeznikov/albert-xlarge-v2-squad-v2';

tokenizer = AlbertTokenizer.from_pretrained(MODEL_PATH)
model = AlbertForQuestionAnswering.from_pretrained(MODEL_PATH)
</code></pre>
<p>For future reference this information can be grabbed from the transformers use button.  Seem in the image below.</p>
<p><a href=""https://i.sstatic.net/xX2EX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xX2EX.png"" alt=""enter image description here"" /></a></p>
",2,0,996,2021-02-08 23:32:23,https://stackoverflow.com/questions/66110901/fine-tuned-albert-question-and-answering-with-huggingface
How can i detect if a callback is triggered in pytorch?,"<p>I am fine-tuning a BERT model. First, I want to freeze layers and train a bit. When a certain callback is triggered (let's say <code>ReduceLROnPlateau</code>) I want to unfreeze layers. How can I do it?</p>
","python, nlp, pytorch, bert-language-model","<p>I'm afraid learning rate schedulers in PyTorch don't provide hooks. Looking at the implementation of <a href=""https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau"" rel=""nofollow noreferrer""><code>ReduceLROnPlateau</code></a> <a href=""https://github.com/pytorch/pytorch/blob/d5a2429c241555ec70c1de522b74b9d173f97691/torch/optim/lr_scheduler.py#L641"" rel=""nofollow noreferrer"">here</a>, two properties are reset when the scheduler is triggered (<code>i.e.</code> when it identifies a plateau and reduces the learning rate):</p>
<pre><code>    if self.num_bad_epochs &gt; self.patience:
        self._reduce_lr(epoch)
        self.cooldown_counter = self.cooldown
        self.num_bad_epochs = 0
</code></pre>
<p>Based on that, you could wrap your scheduler step call and find out if <code>_reduce_lr</code> was triggered by checking that both <code>scheduler.cooldown_counter == scheduler.cooldown</code> and <code>scheduler.num_bad_epochs == 0</code> are true.</p>
",1,0,97,2021-02-09 14:36:38,https://stackoverflow.com/questions/66121082/how-can-i-detect-if-a-callback-is-triggered-in-pytorch
Cannot register text_classifier as Model; name already in use for TextClassifier,"<p>Trying to use text classifier model shared by <a href=""https://github.com/allenai/scibert/blob/master/scibert/models/text_classifier.py"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert/blob/master/scibert/models/text_classifier.py</a></p>
<p>Everything used to work and suddenly I keep getting this error: Cannot register text_classifier as Model; name already in use for TextClassifier</p>
<p>What might be the reason? any suggestion?</p>
<pre><code>    from typing import Dict, Optional, List, Any
    
    import torch
    import torch.nn.functional as F
    from allennlp.data import Vocabulary
    from allennlp.models.model import Model
    from allennlp.modules import FeedForward, TextFieldEmbedder, Seq2SeqEncoder
    from allennlp.nn import InitializerApplicator, RegularizerApplicator
    from allennlp.nn import util
    from allennlp.training.metrics import CategoricalAccuracy, F1Measure
    from overrides import overrides
    
    
    @Model.register(&quot;text_classifier&quot;)
    class TextClassifier(Model):
        &quot;&quot;&quot;
        Implements a basic text classifier:
        1) Embed tokens using `text_field_embedder`
        2) Seq2SeqEncoder, e.g. BiLSTM
        3) Append the first and last encoder states
        4) Final feedforward layer
        Optimized with CrossEntropyLoss.  Evaluated with CategoricalAccuracy &amp; F1.
        &quot;&quot;&quot;
    def __init__(self, vocab: Vocabulary,
                 text_field_embedder: TextFieldEmbedder,
                 text_encoder: Seq2SeqEncoder,
                 classifier_feedforward: FeedForward,
                 verbose_metrics: False,
                 initializer: InitializerApplicator = InitializerApplicator(),
                 regularizer: Optional[RegularizerApplicator] = None,
                 ) -&gt; None:
        super(TextClassifier, self).__init__(vocab, regularizer)

        self.text_field_embedder = text_field_embedder
        self.num_classes = self.vocab.get_vocab_size(&quot;labels&quot;)
        self.text_encoder = text_encoder
        self.classifier_feedforward = classifier_feedforward
        self.prediction_layer = torch.nn.Linear(self.classifier_feedforward.get_output_dim()  , self.num_classes)

        self.label_accuracy = CategoricalAccuracy()
        self.label_f1_metrics = {}

        self.verbose_metrics = verbose_metrics

        for i in range(self.num_classes):
            self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=&quot;labels&quot;)] = F1Measure(positive_label=i)
        self.loss = torch.nn.CrossEntropyLoss()

        self.pool = lambda text, mask: util.get_final_encoder_states(text, mask, bidirectional=True)

        initializer(self)

    @overrides
    def forward(self,
                text: Dict[str, torch.LongTensor],
                label: torch.IntTensor = None,
                metadata:  List[Dict[str, Any]] = None) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;
        Parameters
        ----------
        text : Dict[str, torch.LongTensor]
            From a ``TextField``
        label : torch.IntTensor, optional (default = None)
            From a ``LabelField``
        metadata : ``List[Dict[str, Any]]``, optional, (default = None)
            Metadata containing the original tokenization of the premise and
            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.
        Returns
        -------
        An output dictionary consisting of:
        label_logits : torch.FloatTensor
            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log probabilities of the label.
        label_probs : torch.FloatTensor
            A tensor of shape ``(batch_size, num_labels)`` representing probabilities of the label.
        loss : torch.FloatTensor, optional
            A scalar loss to be optimised.
        &quot;&quot;&quot;
        embedded_text = self.text_field_embedder(text)

        mask = util.get_text_field_mask(text)
        encoded_text = self.text_encoder(embedded_text, mask)
        pooled = self.pool(encoded_text, mask)
        ff_hidden = self.classifier_feedforward(pooled)
        logits = self.prediction_layer(ff_hidden)
        class_probs = F.softmax(logits, dim=1)

        output_dict = {&quot;logits&quot;: logits}
        if label is not None:
            loss = self.loss(logits, label)
            output_dict[&quot;loss&quot;] = loss

            # compute F1 per label
            for i in range(self.num_classes):
                metric = self.label_f1_metrics[self.vocab.get_token_from_index(index=i, namespace=&quot;labels&quot;)]
                metric(class_probs, label)
            self.label_accuracy(logits, label)
        return output_dict

   #@overrides
    def decode(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
        class_probabilities = F.softmax(output_dict['logits'], dim=-1)
        output_dict['class_probs'] = class_probabilities
        return output_dict

    def get_metrics(self, reset: bool = False) -&gt; Dict[str, float]:
        metric_dict = {}

        sum_f1 = 0.0
        for name, metric in self.label_f1_metrics.items():
            metric_val = metric.get_metric(reset)
            if self.verbose_metrics:
                metric_dict[name + '_P'] = metric_val[0]
                metric_dict[name + '_R'] = metric_val[1]
                metric_dict[name + '_F1'] = metric_val[2]
            sum_f1 += metric_val[2]

        names = list(self.label_f1_metrics.keys())
        total_len = len(names)
        average_f1 = sum_f1 / total_len
        metric_dict['average_F1'] = average_f1
        metric_dict['accuracy'] = self.label_accuracy.get_metric(reset)
        return metric_dict
</code></pre>
","python, bert-language-model, allennlp","<p>The name is already taken. <a href=""https://github.com/allenai/allennlp/blob/main/allennlp/predictors/text_classifier.py"" rel=""nofollow noreferrer"">Something that’s already a part of AllenNLP</a> uses that name already, so you need to pick a different one.</p>
<p>For the curious, AllenNLP creates a registry of models, so that you can select a model at the command line. (That’s what the decorator is doing.) This requires the names to be unique.</p>
<p>The name <code>text_classifier</code> was used by AllenNLP only after the external package you’re using used it. It worked in May 2019, when that file was last updated. But 17 months ago, AllenNLP started using it. So it’s not your fault; it’s a mismatch between those two packages (at least, in their current versions).</p>
",0,0,130,2021-02-17 13:27:23,https://stackoverflow.com/questions/66242860/cannot-register-text-classifier-as-model-name-already-in-use-for-textclassifier
How to get BioBERT embeddings,"<p>I have field within a pandas dataframe with a text field for which I want to generate BioBERT embeddings. Is there a simple way with which I can generate the vector embeddings? I want to use them within another model.</p>
<p>here is a hypothetical sample of the data frame</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Visit Code</th>
<th>Problem Assessment</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>ge reflux working diagnosis well</td>
</tr>
<tr>
<td>4567</td>
<td>medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried this package, but receive an error upon installation
<a href=""https://pypi.org/project/biobert-embedding"" rel=""nofollow noreferrer"">https://pypi.org/project/biobert-embedding</a></p>
<p>Error:</p>
<pre><code>Collecting biobert-embedding
  Using cached biobert-embedding-0.1.2.tar.gz (4.8 kB)
ERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from biobert-embedding) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.1)
ERROR: No matching distribution found for torch==1.2.0 (from biobert-embedding)
</code></pre>
<p>Any help is GREATLY appreciated!</p>
","python, nlp, data-science, biopython, bert-language-model","<p>Try to install it as follows:</p>
<pre><code>pip install biobert-embedding==0.1.2 torch==1.2.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>I extended your sample dataframe to illustrate how you can now calculate the sentence vectors for your <code>problem assessments</code> and use these to calculate for example the cosine similarity between similar <code>visit codes</code>.</p>
<pre><code>&gt;&gt;&gt; from biobert_embedding.embedding import BiobertEmbedding
&gt;&gt;&gt; from scipy.spatial import distance
&gt;&gt;&gt; import pandas as pd

&gt;&gt;&gt; data = {'Visit Code': [1234, 1235, 4567, 4568], 
        'Problem Assessment': ['ge reflux working diagnosis well', 
                               'other reflux diagnosis poor', 
                               'medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill',
                               'medication must be refilled diagnosis note called in brand Olmesartan 10mg qd 40 prn refill']}

&gt;&gt;&gt; df = pd.DataFrame(data)
&gt;&gt;&gt; df
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;"">Visit Code</th>
<th style=""text-align: left;"">Problem Assessment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1234</td>
<td style=""text-align: left;"">ge reflux working diagnosis well</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1234</td>
<td style=""text-align: left;"">other reflux diagnosis poor</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">4567</td>
<td style=""text-align: left;"">medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">4567</td>
<td style=""text-align: left;"">medication must be refilled diagnosis note called in brand Olmesartan 10mg qd 40 prn refill</td>
</tr>
</tbody>
</table>
</div>
<pre><code>&gt;&gt;&gt; biobert = BiobertEmbedding()
&gt;&gt;&gt; df['sentence embedding'] = df['Problem Assessment'].apply(lambda sentence: biobert.sentence_vector(sentence))
&gt;&gt;&gt; df
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;"">Visit Code</th>
<th style=""text-align: left;"">Problem Assessment</th>
<th style=""text-align: left;"">sentence embedding</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1234</td>
<td style=""text-align: left;"">ge reflux working diagnosis well</td>
<td style=""text-align: left;"">tensor([ 2.7189e-01, -1.6195e-01,  5.8270e-02, -3.2730e-01,  7.5583e-02, ...</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1234</td>
<td style=""text-align: left;"">other reflux diagnosis poor</td>
<td style=""text-align: left;"">tensor([ 1.6971e-01, -2.1405e-01,  3.4427e-02, -2.3090e-01,  1.6007e-02, ...</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">4567</td>
<td style=""text-align: left;"">medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill</td>
<td style=""text-align: left;"">tensor([ 1.5370e-01, -3.9875e-01,  2.0089e-01,  4.1506e-02, 6.9854e-02,  ...</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">4567</td>
<td style=""text-align: left;"">medication must be refilled diagnosis note called in brand Olmesartan 10mg qd 40 prn refill</td>
<td style=""text-align: left;"">tensor([ 2.2128e-01, -2.0283e-01,  2.2194e-01,  9.1156e-02,  1.1620e-01, ...</td>
</tr>
</tbody>
</table>
</div>
<pre><code>&gt;&gt;&gt; df.groupby('Visit Code')['sentence embedding'].apply(lambda sentences: 1 - distance.cosine(sentences.values) )


Visit Code
1234    0.950492
4567    0.969715
Name: sentence embedding, dtype: float64
</code></pre>
<p>We can see that, as expected, the similar sentences lie very close together</p>
",2,2,4065,2021-02-19 20:09:03,https://stackoverflow.com/questions/66284360/how-to-get-biobert-embeddings
BERT embeddings for entire sentences vs. verbs,"<p>First off, I am drawing upon assumption that majority of the semantic <em>value</em> of the sentence is mediated by verbs that connect the subject and the object of said verb. I am aware that I simplify a bit here as there can be multiple verbs and such. But abstracting from that, I'd be curious whether it applies that a BERT generated embedding vector for entire sentence, say <strong>&quot;Two halves make a whole&quot;</strong> would be measurably more similar to an embedding vector for a singular verb like <strong>&quot;make&quot;</strong> as compared to say a vector for verb <strong>&quot;eat&quot;</strong>.</p>
","nlp, word-embedding, bert-language-model","<p>To answer your &quot;question&quot;, if I were you I would try to do a practical test. For an easy way to use bert for sentence embeddings, check this <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">repo</a>: it is summarily simple to use.</p>
<p>Once you have the embedding vectors, you can use any <a href=""https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity"" rel=""nofollow noreferrer"">similarity function</a> to validate your hypothesis.</p>
<p>However for what is my (limited) experience, I think that the vector of &quot;make&quot; is more similar than that of &quot;eat&quot; also only because &quot;make&quot; is present in the other sentence and therefore contributes to the ambedding of the sentence.</p>
",1,-1,370,2021-02-20 17:46:14,https://stackoverflow.com/questions/66294710/bert-embeddings-for-entire-sentences-vs-verbs
Split a sentence by words just as BERT Tokenizer would do?,"<p>I'm trying to localize all the [UNK] tokens of BERT tokenizer on my text. Once I have the position of the UNK token, I need to identify what word it belongs to. For that, I tried to get the position of the word using words_ids() or token_to_words() methods (the result is the same, I think) which give me the id word of this token.</p>
<p>The problem is, for a large text, there are many ways to split the text by words, and the ways I tried don't match with the position I get from token_to_words method. How I can split my text in the same way Bert tokenizer do?</p>
<p>I saw BERT use WordPiece for tokenize in sub-words, but nothing for complete words.</p>
<p>I'm at this point:</p>
<pre><code>  tokenized_text = tokenizer.tokenize(texto) # Tokens
  encoding_text = tokenizer(texto) # Esto es de tipo batchEncoding, como una instancia del tokenizer
  tpos = [i for i, element in enumerate(tokenized_text) if element == &quot;[UNK]&quot;]  # Posicion en la lista de tokens

  word_list = texto.split(&quot; &quot;)
  for x in tpos:
    wpos = encoding_text.token_to_word(x) # Posicion en la lista de palabras
    print(&quot;La palabra:  &quot;, word_list[wpos], &quot;    contiene un token desconocido: &quot;, tokenizer.tokenize(word_list[wpos]))
</code></pre>
<p>but it fails because the index &quot;wpos&quot; doesn't fit properly with my word_list.</p>
","python, nlp, tokenize, bert-language-model, huggingface-transformers","<p>The problem is solved with <code>token_to_chars()</code> method as @cronoik proposed in comments. It gives me the exact position (and it is universal, not like words I used before which depends on how is splited) of any token, even UNK.</p>
",1,0,1966,2021-02-22 12:41:49,https://stackoverflow.com/questions/66315926/split-a-sentence-by-words-just-as-bert-tokenizer-would-do
Iterating through multiple files with BERT for QA returns nothing,"<p>I am trying to ease my job. I need to do some analysis on the answers BERT gives me for thousands of files. My main objective is to iterate through every file and ask A question.</p>
<p>I have been trying to automate it with the following code</p>
<pre><code>import os

directory = '/content/dva/'

for filename in os.listdir(directory):
with open(directory + filename) as infile:
    try:

      nlp({
    'question': 'How is artificial intelligence being used in real time health delivery?',
    'context': data
})
    except:
        print(filename + ' is throwing an error')
</code></pre>
<p>The above code returns nothing. Yet, if I do them one by one. It works fine. So I tried changing it.</p>
<pre><code>x = [&quot;How is artificial intelligence being used in real time health delivery?&quot;,\
     &quot;What adjunctive or supportive methods can help patients?&quot;,\
     &quot;How does hypertension affect patients?&quot;,\
      &quot;What does the computer do?&quot;]

y = [item.strip() for item in x]

def testing(theList):
  nlp = pipeline('question-answering')

  for each_element in theList:
    nlp({'question': each_element,'context': data})


  

testing(y) # returns nothing
print(testing(y)) # returns None
</code></pre>
<p>Does anyone have any insights? The above code works perfectly for Allen's ELMo.</p>
","python, nlp, bert-language-model, elmo","<p>For some reason, when looping through all files, print() actually does return the answer. It is weird, because usually you do not need to call print to make it work.</p>
<p>Working code:</p>
<pre><code>import os

directory = '/content/dva/'

for filename in os.listdir(directory):
with open(directory + filename) as infile:
try:

  print(nlp({
'question': 'How is artificial intelligence being used in real time health delivery?',
'context': data
}))
    except:
        print(filename + ' is throwing an error')
</code></pre>
",0,0,139,2021-03-01 11:25:29,https://stackoverflow.com/questions/66421258/iterating-through-multiple-files-with-bert-for-qa-returns-nothing
Calculating Probability of a Classification Model Prediction,"<p>I have a classification task. The training data has 50 different labels. The customer wants to differentiate the low probability predictions, meaning that, I have to classify some test data as <code>Unclassified / Other</code> depending on the probability (certainty?) of the model.</p>
<p>When I test my code, the prediction result is a numpy array (I'm using different models, this is one is pre-trained BertTransformer). The prediction array doesn't contain probabilities such as in Keras <code>predict_proba()</code> method. These are numbers generated by prediction method of pretrained <code>BertTransformer</code> model.</p>
<pre><code>[[-1.7862008  -0.7037363   0.09885322  1.5318055   2.1137428  -0.2216074
   0.18905772 -0.32575375  1.0748093  -0.06001111  0.01083148  0.47495762
   0.27160102  0.13852511 -0.68440574  0.6773654  -2.2712054  -0.2864312
  -0.8428862  -2.1132915  -1.0157436  -1.0340284  -0.35126117 -1.0333195
   9.149789   -0.21288703  0.11455813 -0.32903734  0.10503325 -0.3004114
  -1.3854568  -0.01692022 -0.4388664  -0.42163098 -0.09182278 -0.28269592
  -0.33082992 -1.147654   -0.6703184   0.33038092 -0.50087476  1.1643585
   0.96983343  1.3400391   1.0692116  -0.7623776  -0.6083422  -0.91371405
   0.10002492]]
</code></pre>
<p>I'm using <code>numpy.argmax()</code> to identify the correct label. The prediction works just fine. However, since these are not probabilities, I cannot compare the best result with a threshold value.</p>
<p>My question is, how can I define a threshold (say, 0.6), and then compare the probability of the argmax() element of the BertTransformer prediction array so that I can classify the prediction as &quot;Other&quot; if the probability is less than the threshold value?</p>
<p><strong>Edit 1:</strong></p>
<p>We are using 2 different models. One is Keras, and the other is BertTransformer. We have no problem in Keras since it gives the probabilities so I'm skipping Keras model.</p>
<p>The Bert model is pretrained. Here is how it is generated:</p>
<pre><code>def model(self, data):
        number_of_categories = len(data['encoded_categories'].unique())
        model = BertForSequenceClassification.from_pretrained(
            &quot;dbmdz/bert-base-turkish-128k-uncased&quot;,
            num_labels=number_of_categories,
            output_attentions=False,
            output_hidden_states=False,
        )

        # model.cuda()

        return model
</code></pre>
<p>The output given above is the result of <code>model.predict()</code> method. We compare both models, Bert is slightly ahead, therefore we know that the prediction works just fine. However, we are not sure what those numbers signify or represent.</p>
<p>Here is the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">Bert documentation</a>.</p>
","machine-learning, deep-learning, classification, bert-language-model, multiclass-classification","<p><code>BertForSequenceClassification</code> returns logits, i.e., the classification scores before normalization. You can normalize the scores by calling <a href=""https://pytorch.org/docs/stable/nn.functional.html#softmax"" rel=""nofollow noreferrer""><code>F.softmax(output, dim=-1)</code></a> where <code>torch.nn.functional</code> was imported as <code>F</code>.</p>
<p>With thousands of labels, the normalization can be costly and you do not need it when you are only interested in argmax. This is probably why the models return the raw scores only.</p>
",3,1,1472,2021-03-02 11:30:50,https://stackoverflow.com/questions/66438514/calculating-probability-of-a-classification-model-prediction
Dimension does not match when using `keras.Model.fit` in `BERT` of tensorflow,"<p>I follow the instruction of <a href=""https://www.tensorflow.org/official_models/fine_tuning_bert?hl=en"" rel=""nofollow noreferrer"">Fine-tuning BERT</a> to build a model with my own dataset(It is kind of large, and greater than 20G),  then take steps to re-cdoe my data and load them from <code>tf_record</code> files.
The <code>training_dataset</code> I create has the same signature as that in the instruction</p>
<pre><code>training_dataset.element_spec

({'input_word_ids': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 
'input_mask': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 
'input_type_ids': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None)}, 
TensorSpec(shape=(32,), dtype=tf.int32, name=None))
</code></pre>
<p>where <code>batch_size</code> is 32, <code>max_seq_length</code> is 1024.
As the instruction suggestes,</p>
<pre><code>The resulting tf.data.Datasets return (features, labels) pairs, as expected by keras.Model.fit
</code></pre>
<p>It semms that everything works as expected,(the instruction does not show how to use <code>training_dataset</code> though ) However, the following code</p>
<pre><code>bert_classifier.fit(
    x = training_dataset, 
    validation_data=test_dataset, # has the same signature just as training_dataset
    batch_size=32,
    epochs=epochs,
    verbose=1,
)
</code></pre>
<p>encouters an error that seems weird to me,</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/home/captain/project/dataload/train.py&quot;, line 81, in &lt;module&gt;
    verbose=1,
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 828, in __call__
    result = self._call(*args, **kwds)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 726, in _initialize
    *args, **kwds))
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /home/captain/.local/lib/python3.7/site-packages/official/nlp/keras_nlp/layers/position_embedding.py:88 call  *
        return tf.broadcast_to(position_embeddings, input_shape)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:845 broadcast_to  **
        &quot;BroadcastTo&quot;, input=input, shape=shape, name=name)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal
        compute_device)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal
        op_def=op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2016 __init__
        control_input_ops, op_def)
    /home/captain/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1856 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimensions must be equal, but are 512 and 1024 for '{{node bert_classifier/bert_encoder_1/position_embedding/BroadcastTo}} = 
BroadcastTo[T=DT_FLOAT, Tidx=DT_INT32](bert_classifier/bert_encoder_1/position_embedding/strided_slice_1, bert_classifier/bert_encoder_1/position_embedding/Shape)' 
with input shapes: [512,768], [3] and with input tensors computed as partial shapes: input[1] = [32,1024,768].
</code></pre>
<p>There is nothing to do with 512, and I didn't use 512 thorough my code. So where is wrong with my code and how to fix that?</p>
","tensorflow, nlp, tensorflow-datasets, bert-language-model, tensorflow-data-validation","<p>They created the <code>bert_classifier</code> based on <code>bert_config_file</code> loaded from <code>bert_config.json</code></p>
<pre><code>bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=2)
</code></pre>
<p>bert_config.json</p>
<pre><code>{
'attention_probs_dropout_prob': 0.1,
 'hidden_act': 'gelu',
 'hidden_dropout_prob': 0.1,
 'hidden_size': 768,
 'initializer_range': 0.02,
 'intermediate_size': 3072,
 'max_position_embeddings': 512,
 'num_attention_heads': 12,
 'num_hidden_layers': 12,
 'type_vocab_size': 2,
 'vocab_size': 30522
}
</code></pre>
<p>According to this config, <code>hidden_size</code> is 768 and <code>max_position_embeddings</code> is 512 so your input data used to feed to BERT model must be the same shape as described. It explains the reason why you are getting the shape-mismatched issue.</p>
<p>Therefore, to make it work, you have to change all lines for creating tensor inputs from <code>1024</code> to <code>512</code>.</p>
",2,0,510,2021-03-04 17:30:31,https://stackoverflow.com/questions/66480091/dimension-does-not-match-when-using-keras-model-fit-in-bert-of-tensorflow
Weights of pre-trained BERT model not initialized,"<p>I am using the <a href=""https://github.com/pair-code/lit"" rel=""nofollow noreferrer"">Language Interpretability Toolkit</a> (LIT) to load and analyze a BERT model that I pre-trained on an NER task.</p>
<p>However, when I'm starting the LIT script with the path to my pre-trained model passed to it, it fails to initialize the weights and tells me:</p>
<pre><code>    modeling_utils.py:648] loading weights file bert_remote/examples/token-classification/Data/Models/results_21_03_04_cleaned_annotations/04.03._8_16_5e-5_cleaned_annotations/04-03-2021 (15.22.23)/pytorch_model.bin
    modeling_utils.py:739] Weights of BertForTokenClassification not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
    modeling_utils.py:745] Weights from pretrained model not used in BertForTokenClassification: ['bert.embeddings.position_ids']

</code></pre>
<p>It then simply uses the <code>bert-base-german-cased</code> version of BERT, which of course doesn't have my custom labels and thus fails to predict anything. I think it might have to do with PyTorch, but I can't find the error.</p>
<p>If relevant, here is how I load my dataset into CoNLL 2003 format (modification of the dataloader scripts found <a href=""https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples/datasets"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self):

        # Read ConLL Test Files

        self._examples = []

        data_path = &quot;lit_remote/lit_nlp/examples/datasets/NER_Data&quot;
        with open(os.path.join(data_path, &quot;test.txt&quot;), &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
            lines = f.readlines()

        for line in lines[:2000]:
            if line != &quot;\n&quot;:
                token, label = line.split(&quot; &quot;)
                self._examples.append({
                    'token': token,
                    'label': label,
                })
            else:
                self._examples.append({
                    'token': &quot;\n&quot;,
                    'label': &quot;O&quot;
                })

    def spec(self):
        return {
            'token': lit_types.Tokens(),
            'label': lit_types.SequenceTags(align=&quot;token&quot;),
        }
</code></pre>
<p>And this is how I initialize the model and start the LIT server (modification of the <code>simple_pytorch_demo.py</code> script found <a href=""https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/simple_pytorch_demo.py"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self, model_name_or_path):
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name_or_path)
        model_config = transformers.AutoConfig.from_pretrained(
            model_name_or_path,
            num_labels=15,  # FIXME CHANGE
            output_hidden_states=True,
            output_attentions=True,
        )
        # This is a just a regular PyTorch model.
        self.model = _from_pretrained(
            transformers.AutoModelForTokenClassification,
            model_name_or_path,
            config=model_config)
        self.model.eval()

## Some omitted snippets here

    def input_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;token&quot;: lit_types.Tokens(),
            &quot;label&quot;: lit_types.SequenceTags(align=&quot;token&quot;)
        }

    def output_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;tokens&quot;: lit_types.Tokens(),
            &quot;probas&quot;: lit_types.MulticlassPreds(parent=&quot;label&quot;, vocab=self.LABELS),
            &quot;cls_emb&quot;: lit_types.Embeddings()
</code></pre>
","tensorflow, nlp, pytorch, bert-language-model, huggingface-transformers","<p>This actually seems to be expected behaviour. In the <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">documentation of the GPT models</a> the HuggingFace team writes:</p>
<blockquote>
<p>This will issue a warning about some of the pretrained weights not being used and some weights being randomly initialized. That’s because we are throwing away the pretraining head of the BERT model to replace it with a classification head which is randomly initialized.</p>
</blockquote>
<p>So it seems to not be a problem for the fine-tuning. In my use case described above it worked despite the warning as well.</p>
",0,0,1361,2021-03-10 09:30:45,https://stackoverflow.com/questions/66561880/weights-of-pre-trained-bert-model-not-initialized
BERT Encoder layer is non-trainable,"<p>I am trying to fine-tune a BERT model from TensorFlow hub. I loaded the preprocessing layer and the encoder as follow :</p>
<pre><code>bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3')
bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')

</code></pre>
<p>And this is my model definition :</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(bert_preprocess_model, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(bert_model, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)
  return tf.keras.Model(text_input, net)

classifier_model = build_classifier_model()
</code></pre>
<p>But I get the following error : ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.
In the official website, the model is fine-tunable.</p>
","python, tensorflow, keras, bert-language-model","<p>I found the solution, simply add trainable = True :</p>
<pre><code>bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',trainable=True)
</code></pre>
",2,0,1156,2021-03-10 13:37:48,https://stackoverflow.com/questions/66565894/bert-encoder-layer-is-non-trainable
BertModel or BertForPreTraining,"<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>
<p>I am not sure if I want to do finetuning for the model.</p>
<p>I think the relevant classes are BertModel or BertForPreTraining.</p>
<p><a href=""https://dejanbatanjac.github.io/bert-word-predicting/"" rel=""nofollow noreferrer"">BertForPreTraining</a>  head contains two &quot;actions&quot;:
self.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>
<pre><code>class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)
</code></pre>
<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.
what does the MLM do and is it relevant for my goal or should I use the BertModel?</p>
","deep-learning, nlp, bert-language-model, huggingface-transformers, transformer-model","<p>You should be using <code>BertModel</code> instead of <code>BertForPreTraining</code>.</p>
<p><code>BertForPreTraining</code> is used to train bert on Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. They are not meant for classification.</p>
<p>BERT model simply gives the output of the BERT model, you can then finetune the BERT model along with the classifier that you build on top of it. For classification, if its just a single layer on top of BERT model, you can directly go with <code>BertForSequenceClassification</code>.</p>
<p>In anycase, if you just want to take the output of BERT model and learn your classifier (without fine-tuning BERT model), then you can freeze the Bert model weights using:</p>
<pre><code>model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

for param in model.bert.bert.parameters():
    param.requires_grad = False
</code></pre>
<p>The above code is borrowed from <a href=""https://github.com/huggingface/transformers/issues/400"" rel=""noreferrer"">here</a></p>
",6,4,5281,2021-03-12 07:53:12,https://stackoverflow.com/questions/66596142/bertmodel-or-bertforpretraining
Python ImportError: cannot import name &#39;version&#39; from &#39;packaging&#39; (transformers),"<p>when I'm trying to simply <code>import transformers</code> I receive this error:</p>
<p>ImportError: cannot import name 'version' from 'packaging' (C:\Users\miria\packaging.py)</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.sstatic.net/RHMPL.png"" rel=""nofollow noreferrer"">traceback</a></p>
","python, bert-language-model, huggingface-transformers, transformer-model","<p>I think this is one of those cases where you have a bad naming. Maybe your file (or something inside your code) has a name that is overlapping one of the references you are trying to get. For example, if you are importing a certain module named &quot;kivy&quot; and your file is named &quot;kivy&quot;, then the code will go after your file instead of the actual package you are trying to import.</p>
<p>if that's the case, try changing the name and the problem will be solved.</p>
",1,1,4835,2021-03-16 13:54:25,https://stackoverflow.com/questions/66656622/python-importerror-cannot-import-name-version-from-packaging-transformers
Pytorch Siamese NN with BERT for sentence matching,"<p>I'm trying to build a Siamese neural network using pytorch in which I feed BERT word embeddings and trying to find whether two sentences are similar or not (imagine duplicate posts matching, product matching etc). Here's the model:</p>
<pre><code>class SiameseNetwork(torch.nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        self.brothers = torch.nn.Sequential(
            torch.nn.Linear(512 * 768, 512),
            torch.nn.BatchNorm1d(512),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(512, 256),
            torch.nn.BatchNorm1d(256),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(256, 32),
        )
        
        self.final = torch.nn.Sequential(
            torch.nn.Linear(32, 16),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(16, 2),
        )
    
    def forward(self, left, right):
        outputLeft = self.brothers(left)
        outputRight = self.brothers(right)
        output = self.final((outputLeft - outputRight) ** 2)
        return output

bros = SiameseNetwork()
bros = bros.to(device)
</code></pre>
<p>Criterion and optimizer:</p>
<pre><code>criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=bros.parameters(), lr=0.001)
</code></pre>
<p>The training loop:</p>
<pre><code>for batch in tqdm(tLoader, desc=f&quot;Train epoch: {epoch+1}&quot;):
        a = batch[0].to(device)
        b = batch[1].to(device)
        y = torch.unsqueeze(batch[2].type(torch.FloatTensor), 1).to(device)
        
        optimizer.zero_grad()
        
        output = bros(a,b)
        loss = criterion(output, y)
        loss.backward()
        
        trainingLoss += loss.item()

        optimizer.step()
</code></pre>
<p>Now, this seems to be working, as it produces results that are reasonable, but the validation error stops dropping at 0.13 after just a of epochs. Can't find a lot of things on this kind of NNs using Pytorch. Are there ways to optimize it? Am I doing something wrong?</p>
","python, neural-network, pytorch, bert-language-model, siamese-network","<p>Your first layer is severely overparameterized and prone to overfitting (counts a total of 201 million parameters). I assume the shape <code>512 * 768</code> reflects the number of tokens times their dimensionality; if that's the case, you need to rethink your architecture. You need some sort of weight sharing or pooling strategy to reduce the <code>num_words * dim</code> input to a fixed representation (that's exactly why recurrent networks replaced the fully-connected varieties for sentence encoding). Specifically in transformer-based architectures, the <code>[CLS]</code> token (token number 0, prefixing the input) is typically used as the &quot;summary&quot; token for sequence- and bisequence-level tasks.</p>
",2,0,1389,2021-03-17 17:36:15,https://stackoverflow.com/questions/66678360/pytorch-siamese-nn-with-bert-for-sentence-matching
How to use fine-tuned BERT model for sentence encoding?,"<p>I fine-tuned the BERT base model on my own dataset following the script here:</p>
<p><a href=""https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning"" rel=""nofollow noreferrer"">https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning</a></p>
<p>I saved the model as a <code>.pt</code> file and I want to use it now for a sentence similarity task. Unfortunately, it is not clear to me, how to load the fine-tuned model. I tried the following:</p>
<pre><code>model = BertModel.from_pretrained('trained_model.pt')
model.eval()
</code></pre>
<p>This doesn't work. It says:</p>
<pre><code>ReadError: not a gzip file
</code></pre>
<p>So apparently, loading a <code>.pt</code> file with the <code>from_pretrained</code> method is not possible. Can anyone help me out here? Thank's a lot!! :)</p>
<p>Edit: I saved the model in a s3 bucket as follows:</p>
<pre><code># Convert model to buffer
buffer = io.BytesIO()
torch.save(model, buffer)
# Save in s3 bucket
output_model_file = output_folder + &quot;trained_model.pt&quot;
s3_.put_object(Bucket=&quot;power-plant-embeddings&quot;, Key=output_model_file, Body=buffer.getvalue())
</code></pre>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>To load a model with <code>BertModel.from_pretrained()</code> you need to have saved it using <code>save_pretrained()</code> <a href=""https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.save_pretrained"" rel=""nofollow noreferrer"">(link)</a>.</p>
<p>Any other storage method would require the corresponding load. I am not familiar with S3, but I assume you can use <code>get_object</code> <a href=""https://docs.aws.amazon.com/cli/latest/reference/s3api/get-object.html"" rel=""nofollow noreferrer"">(link)</a> to retrieve the model, and then save it using the huggingface api. From then on, you should be able to use <code>from_pretrained()</code> normally.</p>
",1,0,831,2021-03-19 11:58:10,https://stackoverflow.com/questions/66707770/how-to-use-fine-tuned-bert-model-for-sentence-encoding
Internal error: Tried to take gradients (or similar) of a variable without handle data on running TF HUB BERT inside tf.GradientTape,"<p>I am trying to train <a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"" rel=""nofollow noreferrer"">bert_en_uncased_L-12_H-768_A-12</a> TF HUB model inside a persistent gradient tape in Tensorflow 2.4. The following is a simplified version of my code.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_hub as hub

input_mask = tf.keras.layers.Input(shape=4, dtype=tf.int32)
input_word_ids = tf.keras.layers.Input(shape=4, dtype=tf.int32)
input_type_ids = tf.keras.layers.Input(shape=4, dtype=tf.int32)

bert = hub.KerasLayer(
    &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3&quot;,
    trainable=True)({&quot;input_mask&quot;: input_mask, 'input_word_ids': input_type_ids, &quot;input_type_ids&quot;: input_type_ids})

dense = tf.keras.layers.Dense(units=1)(bert['pooled_output'])

encode = tf.keras.models.Model([input_mask, input_word_ids, input_type_ids], dense)
import numpy as np

data = np.zeros((1, 4))


@tf.function
def run():
    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:
        tape.watch(encode.trainable_weights)
        encode([data, data, data], training=True)


run()
</code></pre>
<p><strong>Error</strong></p>
<pre><code>  raise ValueError(&quot;Internal error: Tried to take gradients (or similar) &quot;

    ValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data:
    Tensor(&quot;StatefulPartitionedCall:1079&quot;, dtype=resource)
</code></pre>
<p>This error is raised only when</p>
<ul>
<li>TF HUB trainable=True option is used</li>
<li>persistent gradient tape is used.
Is this a bug in TensorFlow or am I trying something which is not supported?</li>
</ul>
","python, tensorflow, keras, bert-language-model, tensorflow-hub","<p>Using persistent GradientTapes for SavedModels requires TensorFlow 2.5+ both when saving and loading the SavedModel. Please follow along at <a href=""https://github.com/tensorflow/hub/issues/622"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/issues/622</a> for updates on the release of TF2.5 and updated SavedModels for BERT etc.</p>
<p>The answer by M. Innat explains how to avoid the problem by using the standard, non-persistent GradientTapes.</p>
",2,0,652,2021-03-24 07:14:42,https://stackoverflow.com/questions/66776275/internal-error-tried-to-take-gradients-or-similar-of-a-variable-without-handl
BERT: Weights of input embeddings as part of the Masked Language Model,"<p>I looked through different implementations of BERT's Masked Language Model.
For pre-training there are <strong>two</strong> common versions:</p>
<ol>
<li>Decoder would simply take the final embedding of the [MASK]ed token and pass it throught a linear layer (without any modifications):</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<ol start=""2"">
<li>Some implementations would use the weights of the input embeddings as weights of the decoder-linear-layer:</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size, embeddings):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.weight = embeddings.weight ## &lt;- THIS LINE
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<p>Which one is correct? Mostly, I see the first implementation. However, the second one makes sense as well - but I cannot find it mentioned in any papers (I would like to see if the second version is somehow superior to the first one)</p>
","nlp, pytorch, bert-language-model, transformer-model, language-model","<p>For those who are interested, it is called <strong>weight tying</strong> or <strong>joint input-output embedding</strong>. There are two papers that argue for the benefit of this approach:</p>
<ul>
<li><a href=""https://www.aclweb.org/anthology/W18-6308/"" rel=""nofollow noreferrer"">Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation</a></li>
<li><a href=""https://paperswithcode.com/paper/using-the-output-embedding-to-improve"" rel=""nofollow noreferrer"">Using the Output Embedding to Improve Language Models</a></li>
</ul>
",3,1,1505,2021-03-26 17:03:32,https://stackoverflow.com/questions/66821321/bert-weights-of-input-embeddings-as-part-of-the-masked-language-model
Extracting Features from BertForSequenceClassification,"<p>Hello together currently I´m trying to develop a model for contradicition detection. Using and fine-tuning a BERT Model I already got quite statisfactionary result but I think with with some other features I could get a better accuracy. I oriented myself on this <a href=""https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db"" rel=""nofollow noreferrer"">Tutorial</a>. After fine-tuning, my model looks like this:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30000, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
</code></pre>
<p>My next step would be to get the [CLS] token from this model, combine it with a few hand crafted features and feed them into a different model (MLP) for classfification. Any hints how to do this?</p>
","python, nlp, bert-language-model, huggingface-transformers","<p>You can use the pooling output (contextualized embedding of the [CLS] token fed to the pooling layers) of the BERT model:</p>
<pre><code>from transformers import BertModel, BertTokenizer

#replace bert-base-uncased with the path to your saved model
t = BertTokenizer.from_pretrained('bert-base-uncased')
m = BertModel.from_pretrained('bert-base-uncased')


i = t.batch_encode_plus(['this is a sample', 'different sample'], padding=True,return_tensors='pt')
o = m(**i)

print(o.keys())
#shape [batch_size, 768]
print(o.pooler_output.shape)
useMe = o.pooler_output
</code></pre>
",1,1,2579,2021-03-26 17:16:42,https://stackoverflow.com/questions/66821505/extracting-features-from-bertforsequenceclassification
No module named &#39;transformers.models&#39; while trying to import BertTokenizer,"<p>I am trying to import BertTokenizer from the transformers library as follows:</p>
<pre><code>import transformers
from transformers import BertTokenizer
from transformers.modeling_bert import BertModel, BertForMaskedLM
</code></pre>
<p>However, I get the following error:</p>
<p><a href=""https://i.sstatic.net/rge7T.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/rge7T.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/kfrff.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/kfrff.png"" alt=""enter image description here"" /></a></p>
<p>I am using transformers version 3.5.1 because I had a problem with the updated version which can be found <a href=""https://datascience.stackexchange.com/questions/87186/bert-dropout-argument-input-position-1-must-be-tensor-not-str"">here</a>.</p>
<p>Does anyone know how to fix this? Apart from updating the transformers library to its latest version (that will unfortunately cause more errors). <br></p>
<p>Any help is appreciated!</p>
","python, importerror, bert-language-model, huggingface-transformers","<p>you can change your code from</p>
<p><code>transformers.modeling_bert import BertModel, BertForMaskedLM</code></p>
<p>to</p>
<pre><code>from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM
</code></pre>
",8,5,43386,2021-03-26 18:31:04,https://stackoverflow.com/questions/66822496/no-module-named-transformers-models-while-trying-to-import-berttokenizer
Interpreting the output tokenization of BERT for a given word,"<pre class=""lang-py prettyprint-override""><code>from bert_embedding import BertEmbedding
bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')
output = bert_embedding(&quot;any&quot;)
</code></pre>
<p>I need clarification on the output of mBERT embeddings. I'm aware that WordPiece tokenization is used to break up the input text. Also I observed that on providing a single word (say &quot;any&quot;) as input, the output has length equal to the number of characters in the input (in our case, 3). <code>output[i]</code> is a tuple of lists where the first list contains the character at i<sup>th</sup>  position with the 'unknown' token preceding and following it as different elements in the array. Following this are three (= length of the input word) arrays (embeddings) of size 768 each. Why does the output seem to be tokenized character-wise (rather than wordpiece tokenized)?</p>
<p>Also found out the output form changes when the input is given in a list as:<code>bert_embedding([&quot;any&quot;])</code>. The output now is a single tuple with ['[UNK]', 'state', '[UNK]'] as the first element followed by three different embeddings conceivably corresponding to the three tokens listed above.</p>
<p>If I need the embedding of the last subword (not simply of the last character or the whole word) for a given input word, how do I access it?</p>
","python, nlp, pytorch, bert-language-model","<p>Checked their <a href=""https://github.com/imgarylai/bert-embedding"" rel=""nofollow noreferrer"">github page</a>. About the input format: YES it is expected as a list (of strings). Also this particular implementation provides token ( = word ) level embeddings; so subword level embedings can't be retrieved directly although it provides a choice on how the word embeddings should be derived from their subword components ( by taking avg which is default or taking sum or just the last subword embedding). Refer to the Hugggingface interface for BERT for a finer control over how the embeddings are taken e.g. from the different layers and using which operations.</p>
",0,0,640,2021-03-29 06:11:13,https://stackoverflow.com/questions/66849433/interpreting-the-output-tokenization-of-bert-for-a-given-word
Swift Metal Compiler failed with XPC_ERROR_CONNECTION_INTERRUPTED BERT Model,"<p>I've recently been working on an app where a user can ask questions about a text and the app will print out the answer. For that, I used the Apple <a href=""https://developer.apple.com/documentation/coreml/finding_answers_to_questions_in_a_text_document"" rel=""nofollow noreferrer"">Finding Answers to Questions</a> template, which is powered by BERT. This template worked just fine, until yesterday when it always printed out the error message:</p>
<pre><code>Compiler failed with XPC_ERROR_CONNECTION_INTERRUPTED
</code></pre>
<p>when executing the BERT inference code:</p>
<pre><code>let answer = self.bert.findAnswer(for: searchText, in: detail.body)
</code></pre>
<p>I haven't changed anything of the code, so this seems like a bug, does anyone know how to solve it or did anyone have similar problems?</p>
","swift, machine-learning, bert-language-model","<p>I was able to resolve the issue by cleaning my build folder, therefore this issue seemed to be caused by corrupted cache files.</p>
",0,0,210,2021-04-03 12:59:39,https://stackoverflow.com/questions/66931415/swift-metal-compiler-failed-with-xpc-error-connection-interrupted-bert-model
Python: How to pass Dataframe Columns as parameters to a function?,"<p>I have a dataframe <code>df</code> with 2 columns of text embeddings namely <code>embedding_1</code> and <code>embedding_2</code>. I want to create a third column in <code>df</code> named <code>distances</code> which should contain the cosine_similarity between every row of <code>embedding_1</code> and <code>embedding_2</code>.</p>
<p>But when I try to implement this using the following code I get a <code>ValueError</code>.</p>
<p>How to fix it?</p>
<p><strong>Dataframe <code>df</code></strong></p>
<pre><code>           embedding_1              |            embedding_2                                 
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]
 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]
</code></pre>
<p><strong>Code to Calculate Cosine Similarity</strong></p>
<pre><code>df['distances'] = cosine_similarity(df['embeddings_1'], df['embeddings_2'])
</code></pre>
<p><strong>Error</strong></p>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
<p><strong>Required Dataframe</strong></p>
<pre><code>       embedding_1              |            embedding_2                 |  distances                        
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882
 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665
 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312
</code></pre>
","python, pandas, dataframe, nlp, bert-language-model","<p>You can use <code>apply()</code> to use <code>cosine_similarity()</code> on each row:</p>
<pre class=""lang-py prettyprint-override""><code>def cal_cosine_similarity(row):
    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])

df['distances'] = df.apply(cal_cosine_similarity, axis=1)
</code></pre>
<p>or one liner</p>
<pre class=""lang-py prettyprint-override""><code>df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)
</code></pre>
",2,0,1338,2021-04-04 11:32:44,https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function
Getting predict.proba from BERT classififer,"<p>I have a classifier on top of BERT, and I would like to see the predict probability for creating the ROC curve. How do I get the predict proba?. The predicted probas will be used to calculate the TPR FPR and threshold for ROC curve. <br>
here is the code</p>
<pre><code>class BertBinaryClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(BertBinaryClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 1)
        self.sigmoid = nn.Sigmoid()
        
    
    def forward(self, tokens, masks=None):
        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        prediction = self.sigmoid(linear_output)
        return prediction
# Config setting
BATCH_SIZE = 4
EPOCHS = 5
# Making dataloaders
train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)
train_sampler =  torch.utils.data.RandomSampler(train_dataset)
train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)
test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)
test_sampler =  torch.utils.data.SequentialSampler(test_dataset)
test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)

bert_clf = BertBinaryClassifier()
bert_clf = bert_clf.cuda()
#wandb.watch(bert_clf)
optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)

# training 
for epoch_num in range(EPOCHS):
    bert_clf.train()
    train_loss = 0
    for step_num, batch_data in enumerate(train_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        preds = bert_clf(token_ids, masks)
        loss_func = nn.BCELoss()
        batch_loss = loss_func(preds, labels)
        train_loss += batch_loss.item()
        bert_clf.zero_grad()
        batch_loss.backward()
        optimizer.step()
        #wandb.log({&quot;Training loss&quot;: train_loss})
        print('Epoch: ', epoch_num + 1)
        print(&quot;\r&quot; + &quot;{0}/{1} loss: {2} &quot;.format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))

# evaluating on test
bert_clf.eval()
bert_predicted = []
all_logits = []
probs=[]
with torch.no_grad():
    test_loss = 0
    for step_num, batch_data in enumerate(test_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        logits = bert_clf(token_ids, masks)
        pr=logits.ravel()
        probs+=pr
        loss_func = nn.BCELoss()
        loss = loss_func(logits, labels)
        test_loss += loss.item()
        numpy_logits = logits.cpu().detach().numpy()
        #print(numpy_logits)
        #wandb.log({&quot;Testing loss&quot;: test_loss})
        bert_predicted += list(numpy_logits[:, 0] &gt; 0.5)
        all_logits += list(numpy_logits[:, 0])
</code></pre>
<p>I am able to get the prediction score to calculate the accuracy or f1 score. But not the probability for creating ROC curve.
Thanks</p>
","python, python-3.x, pytorch, bert-language-model, huggingface-transformers","<p>In your <code>forward</code>, you:</p>
<pre><code>def forward(self, tokens, masks=None):
    _, pooled_output = self.bert(...)             # Get output of BERT
    dropout_output = self.dropout(pooled_output)  
    linear_output = self.linear(dropout_output)   # Take linear combination of outputs
                                                  # (unconstrained score - &quot;logits&quot;)

    prediction = self.sigmoid(linear_output)      # Normalise scores 
                                                  # (constrained between [0,1] - &quot;probabilities&quot;)
    return prediction
</code></pre>
<p>Hence the result of calling your model can be directly supplied to calculate the False Positive and True Positive rates e.g:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn import metrics

...

test_probs = bert_clf(token_ids, masks)
fpr, tpr, thresholds = metrics.roc_curve(labels, test_probs)
roc_auc = metrics.auc(fpr, tpr)
</code></pre>
",0,1,1050,2021-04-05 08:24:24,https://stackoverflow.com/questions/66950157/getting-predict-proba-from-bert-classififer
How to store Bert embeddings in cassandra,"<p>I want to use Cassandra as feature store to store precomputed Bert embedding,
Each row would consist of roughly 800 integers (ex. <code>-0.18294132</code>) Should I store all 800 in one large string column or 800 separate columns?</p>
<p>Simple read pattern, On read we would want to read every value in a row. Not sure which would be better for serialization speed.</p>
","cassandra, embedding, bert-language-model","<p>Having everything as a separate column will be quite inefficient - each value will have its own metadata (writetime, for example) that will add significant overhead (at least 8 bytes per every value).  Storing data as string will be also not very efficient, and will add the complexity on the application side.</p>
<p>I would suggest to store data as <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/refCollectionType.html"" rel=""nofollow noreferrer"">fronzen list</a> of integers/longs or doubles/floats, depending on your requirements.  Something like:</p>
<pre><code>create table ks.bert(
  rowid int primary key,
  data frozen&lt;list&lt;int&gt;&gt;
);
</code></pre>
<p>In this case, the whole list will be effectively serialized as binary blob, occupying just one cell.</p>
",1,1,336,2021-04-06 00:52:47,https://stackoverflow.com/questions/66961429/how-to-store-bert-embeddings-in-cassandra
val_accuracy does not increase,"<p>Currently I'm trying to train a Keras Sequential Network with pooled output from BERT. The fine tuned BertForSequence Classification yields good results, but using the pooled_output in a Neural Network does not work as intented. As Input data I got 10.000 Values, each consisting of the 768 floats that my BERT-Model provides. I'm trying to do a simple binary classification, so I also got the labels with 1 and 0's.</p>
<p><a href=""https://i.sstatic.net/tzdqz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzdqz.png"" alt=""enter image description here"" /></a></p>
<p>As you can see my data has a good number of examples for both classes. After shuffling them, I do a normal train test split and create/fit my model with:</p>
<pre><code>model = Sequential()
model.add(Dense(1536, input_shape=(768,), activation='relu'))
model.add(Dense(1536, activation='relu'))
model.add(Dense(1536, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

opt = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

#Normally with early stopping so quite a few epochs
history = model.fit(train_features, train_labels, epochs=800, batch_size=68, verbose=1, 
validation_split=0.2, callbacks=[])
</code></pre>
<p>During training the loss decreases and my accuracy increases as expected. BUT the val_loss increases and the val_accuracy stays the same! Sure I'm overfitting, but I would expect that the val_accuracy increases, at least for a few epochs and then decreaes when I'm overfitting.</p>
<p><a href=""https://i.sstatic.net/tP9Ch.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tP9Ch.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/8ASuZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8ASuZ.png"" alt=""enter image description here"" /></a></p>
<p>Has anyone an Idea what I'm doing wrong? Perhaps 10.000 values aren't enough to generalize?</p>
","python, tensorflow, keras, nlp, bert-language-model","<p>It was not just a mislabeling in my validation set, but in my whole data.</p>
<p>I take a sample of 100000 entries</p>
<pre><code>train_df = train_df.sample(frac=1).reset_index(drop=True)
train_df = train_df.iloc[0:100000]
</code></pre>
<p>and delete some values</p>
<pre><code>train_df = train_df[train_df['label'] != '-']
</code></pre>
<p>after that i set a few values using <code>train_df.at</code> in a loop, but some indices don't exist because i deleted them. train_df.at only throws warnings so I did not see this. Also I mixed .loc and .iloc so in my case i selected .iloc[2:3] but the index 2 does not exist, so it return index 3 wich is on position 2. After that I make my changes and <code>train_df.at</code> fails at inserting on position 2, but my loop goes on. The next iteration .iloc returns index 4 on position 3. My loop then puts the data on index 3 - from now on all my labels are one position off.</p>
",0,0,3179,2021-04-08 19:24:58,https://stackoverflow.com/questions/67010607/val-accuracy-does-not-increase
UnparsedFlagAccessError: Trying to access flag --preserve_unused_tokens before flags were parsed. BERT,"<p>I want to use Bert language model for training a multi class text classification task.
Previously I trained using LSTM without any Error but Bert gives me this Error.
I get the following Error and I really don't know how to solve it, can anyone help me please?</p>
<p>Unfortunately there is very little documentation using BERT in keras library.</p>
<pre><code>!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py

import tensorflow_hub as hub
from bert import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)





vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = [&quot;[CLS]&quot;] + text + [&quot;[SEP]&quot;]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)



def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;segment_ids&quot;)

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='softmax')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='softmax')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(3, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model



max_len = 150
train_input = bert_encode(data.text_cleaned, tokenizer, max_len=max_len)

</code></pre>
<p>Error as following :</p>
<pre><code>
UnparsedFlagAccessError                   Traceback (most recent call last)
&lt;ipython-input-175-fd64df42591d&gt; in &lt;module&gt;()
      1 import sys
      2 max_len = 150
----&gt; 3 train_input = bert_encode(o.text_cleaned, tokenizer, max_len=max_len)

4 frames
/usr/local/lib/python3.7/dist-packages/absl/flags/_flagvalues.py in __getattr__(self, name)
    496         # get too much noise.
    497         logging.error(error_message)
--&gt; 498       raise _exceptions.UnparsedFlagAccessError(error_message)
    499 
    500   def __setattr__(self, name, value):

UnparsedFlagAccessError: Trying to access flag --preserve_unused_tokens before flags were parsed.

</code></pre>
","python, nlp, bert-language-model","<p>Based on this <a href=""https://github.com/google-research/bert/issues/1133"" rel=""nofollow noreferrer"">issue</a> you have to downgrade bert-tensorflow to 1.0.1. Check <a href=""https://github.com/google-research/bert/issues/1133#issuecomment-703818257"" rel=""nofollow noreferrer"">this answer</a> to find a solution. If you are following <a href=""https://www.analyticsvidhya.com/blog/2020/10/simple-text-multi-classification-task-using-keras-bert/"" rel=""nofollow noreferrer"">this tutorial</a> downgrade bert-tensorflow and use the <code>!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py</code> as suggested because inside the python code the author has made the change from <code>tf.gfile.GFile(vocab_file, &quot;r&quot;)</code> to <code>tf.io.gfile.Gfile(vocab_file, &quot;r&quot;)</code>. After that code compiles successfully. Ping me if you want anything else.</p>
",4,4,4581,2021-04-11 09:39:38,https://stackoverflow.com/questions/67043468/unparsedflagaccesserror-trying-to-access-flag-preserve-unused-tokens-before-f
sparse_categorical_crossentropy() missing 2 required positional arguments: &#39;y_true&#39; and &#39;y_pred&#39;,"<p>I want to use Bert language model for training a multi class text classification task. Previously I trained using LSTM without any Error but Bert gives me this Error. I get this Error as follwoing and I really don't know how to solve it, can any one help me please?</p>
<p>Unfortunately there is very few documentation using Bert in keras library.</p>
<p>The Error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-177-7b203e5e7f55&gt; in &lt;module&gt;()
      3 
      4 model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 2e-5),
----&gt; 5               loss = tf.keras.losses.sparse_categorical_crossentropy(from_logits=True),
      6               metrics = [tf.keras.metrics.categorical_accuracy()])
      7 model.summary

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;
    200     try:
--&gt; 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

TypeError: sparse_categorical_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'
SEARCH
</code></pre>
<p>Create tf.data.Datasets for Training and Evaluation</p>
<pre><code>X = data.text_cleaned
Y = data.label

train_df, remaining = train_test_split(data, train_size=0.8, random_state=42)
valid_df, _ = train_test_split(remaining, random_state=42)

print(train_df.shape)
print(valid_df.shape)


with tf.device('/cpu:0'):
  train_data = tf.data.Dataset.from_tensor_slices((train_df[&quot;text_cleaned&quot;].values,
                                            train_df[&quot;label&quot;].values ))
  valid_data = tf.data.Dataset.from_tensor_slices((valid_df[&quot;text_cleaned&quot;].values,
                                            valid_df[&quot;label&quot;].values ))
  for text, label in train_data.take(2):
    print(text)
    print(label)

</code></pre>
<p>Download a Pre-trained BERT Model from TensorFlow Hub</p>
<pre><code>
label_list = [0.0, 1.0, 2.0] # Label categories
max_seq_length = 64  # maximum length of (token) input sequences
train_batch_size = 32



# Get BERT layer and tokenizer:
# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2

bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;,
                            trainable= True)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

tokenizer.wordpiece_tokenizer.tokenize('Hi, how are you doing?')

</code></pre>
<p>Tokenize and Preprocess Text for BERT</p>
<pre><code># This provides a function to convert row to input features and label

def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):
  example = classifier_data_lib.InputExample(quit = None,
                                             text_a= text.numpy(),
                                             text_b= None,
                                             label= label.numpy())
  feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length,tokenizer)
  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id )
</code></pre>
<p>Wrap a Python Function into a TensorFlow op for Eager Execution</p>
<pre><code>def to_feature_map(text, label):
  input_ids, input_mask, input_type_ids, label_id = tf.py_function(to_feature,
                                                                inp = [text, label],
                                                                Tout = [tf.int32,
                                                                        tf.int32,
                                                                        tf.int32,
                                                                        tf.int32])
  input_ids.set_shape([max_seq_length])
  input_mask.set_shape([max_seq_length])
  input_type_ids.set_shape([max_seq_length])
  label_id.set_shape([])

  x= {
      'input_word_ids':input_ids,
      'input_mask' : input_mask,
      'input_type_ids' : input_type_ids,

  }
  return (x, label_id)
</code></pre>
<p>Create a TensorFlow Input Pipeline with tf.data</p>
<pre><code>with tf.device('/cpu:0'):
  train_data = (train_data.map(to_feature_map,
                               num_parallel_calls = tf.data.experimental.AUTOTUNE)
  .shuffle(1000)
  .batch(32, drop_remainder = True)
  .prefetch(tf.data.experimental.AUTOTUNE))
 

 
  valid_data = (valid_data.map(to_feature_map,
                               num_parallel_calls = tf.data.experimental.AUTOTUNE)
  .shuffle(1000)
  .batch(32, drop_remainder = True)
  .prefetch(tf.data.experimental.AUTOTUNE))
  

</code></pre>
<p>Add a Classification Head to the BERT Layer</p>
<pre><code>def create_model():
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                       name=&quot;input_word_ids&quot;)
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                   name=&quot;input_mask&quot;)
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                    name=&quot;input_type_ids&quot;)
  
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

  drop = tf.keras.layers.Dropout(0.5)(pooled_output)
  output = tf.keras.layers.Dense(3, activation='softmax',name = &quot;output&quot;)(drop)

  model = tf.keras.Model(
      inputs = {
          'input_word_ids':input_word_ids,
          'input_mask' : input_mask,
          'input_type_ids' : input_type_ids},
           outputs = output)
  return model
</code></pre>
<p>Fine-Tune BERT for Text Classification</p>
<pre><code>

model = create_model()

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 2e-5),
              loss = tf.keras.losses.sparse_categorical_crossentropy(),
              metrics = [tf.keras.metrics.categorical_accuracy()])
model.summary
</code></pre>
","python, deep-learning, nlp, lstm, bert-language-model","<p>It is solved by using :</p>
<pre><code>model = create_model()

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 2e-5),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = [tf.keras.metrics.Accuracy()])
model.summary()

</code></pre>
",1,0,1148,2021-04-11 14:43:02,https://stackoverflow.com/questions/67046302/sparse-categorical-crossentropy-missing-2-required-positional-arguments-y-tr
BERT - Is that needed to add new tokens to be trained in a domain specific environment?,"<p>My question here is no how to add new tokens, or how to train using a domain-specific corpus, I'm already doing that.</p>
<p>The thing is, am I supposed to add the domain-specific tokens before the MLM training, or I just let Bert figure out the context? If I choose to not include the tokens, am I going to get a poor task-specific model like NER?</p>
<p>To give you more background of my situation, I'm training a Bert model on medical text using Portuguese language, so, deceased names, drug names, and other stuff are present in my corpus, but I'm not sure I have to add those tokens before the training.</p>
<p>I saw this one: <a href=""https://stackoverflow.com/questions/64816669/using-pretrained-bert-model-to-add-additional-words-that-are-not-recognized-by-t"">Using Pretrained BERT model to add additional words that are not recognized by the model</a></p>
<p>But the doubts remain, as other sources say otherwise.</p>
<p>Thanks in advance.</p>
","nlp, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>Yes, you have to add them to the models vocabulary.</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(model_name)
tokenizer.add_tokens(['new', 'rdemorais', 'blabla'])
model = Bert.from_pretrained(model_name, return_dict=False)
     
model.resize_token_embeddings(len(tokenizer))
</code></pre>
<p>The last line is important and needed since you change the numbers of tokens in the model's vocabulary, you also need to update the model correspondingly.</p>
",3,3,1529,2021-04-12 12:51:18,https://stackoverflow.com/questions/67058709/bert-is-that-needed-to-add-new-tokens-to-be-trained-in-a-domain-specific-envir
Weights &amp; Biases - How can I interpret the graphs when training BERT,"<p>Can someone help me to understand the amazing graphs generated by Weights &amp; Biases tools when you are training a BERT model?</p>
<p><a href=""https://i.sstatic.net/eB0SJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eB0SJ.png"" alt=""enter image description here"" /></a></p>
<p>How can I interpret the above image? I don't know what the dispersion grey means, nor if the concentration in the blue region is good or bad.</p>
<p>Thanks in advance.</p>
",bert-language-model,"<p>So those charts show the histograms of the gradients, per time step.</p>
<p>Take the leftmost chart, layer.10 weights. In very first slice at Step 0, the grey shading tells you that the gradients for that layer had values between ~ -40 and +40. The blue parts however tell you that most of those gradients were between -2 and +2 (roughly).</p>
<p>So the shading represents the count of gradients in that particular histogram bin, for that particular time step.</p>
<p>Now interpreting gradients can be tricky sometimes, but generally I find these plots useful to check that your gradients haven't exploded (big values on the y-axis) or collapsed (concentrated blue around 0 with little to no deviation). For example if you try train with a very high learning rate you should see the values on the y-axis go into the 100s or 1000s, indicating that your gradients are huge.</p>
<p>One final tip would be to focus more on the gradients from the weights as opposed to the biases as this can be more informative about what your model is doing.</p>
",3,1,417,2021-04-12 15:38:19,https://stackoverflow.com/questions/67061350/weights-biases-how-can-i-interpret-the-graphs-when-training-bert
What is &quot;language modeling head&quot; in BertForMaskedLM,"<p>I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
","nlp, bert-language-model, huggingface-transformers, language-model","<p>The BertForMaskedLM, as you have understood correctly uses a Language Modeling(LM) head .</p>
<p>Generally, as well as in this case, LM head is a linear layer having input dimension of hidden state (for BERT-base it will be 768) and output dimension of vocabulary size. Thus, it maps to hidden state output of BERT model to a specific token in the vocabulary. The loss is calculated based on the scores obtained of a given token with respect to the target token.</p>
",3,5,6893,2021-04-14 18:48:10,https://stackoverflow.com/questions/67097467/what-is-language-modeling-head-in-bertformaskedlm
How to use BERT and Elmo embedding with sklearn,"<p>I created a text classifier that uses Tf-Idf using sklearn, and I want to use BERT and Elmo embedding instead of Tf-Idf.</p>
<p>How would one do that ?</p>
<p>I'm getting Bert embedding using the code below:</p>
<pre class=""lang-py prettyprint-override""><code>from flair.data import Sentence
from flair.embeddings import TransformerWordEmbeddings

# init embedding
embedding = TransformerWordEmbeddings('bert-base-uncased')

# create a sentence
sentence = Sentence('The grass is green .')

# embed words in sentence
embedding.embed(sentence)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

column_trans = ColumnTransformer([
    ('tfidf', TfidfVectorizer(), 'text'),
    ('number_scaler', MinMaxScaler(), ['number'])
])

# Initialize data
data = [
    ['This process, however, afforded me no means of.', 20, 1],
    ['another long description', 21, 1],
    ['It never once occurred to me that the fumbling', 19, 0],
    ['How lovely is spring As we looked from Windsor', 18, 0]
]

# Create DataFrame
df = pd.DataFrame(data, columns=['text', 'number', 'target'])

X = column_trans.fit_transform(df)
X = X.toarray()
y = df.loc[:, &quot;target&quot;].values

# Perform classification

classifier = LogisticRegression(random_state=0)
classifier.fit(X, y)
</code></pre>
","python, machine-learning, nlp, bert-language-model, elmo","<p>Sklearn offers the possibility to make custom <a href=""https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers"" rel=""noreferrer"">data transformer</a> (unrelated to the machine learning model &quot;transformers&quot;).</p>
<p>I implemented a custom sklearn data transformer that uses the <code>flair</code> library that you use. Please note that I used <code>TransformerDocumentEmbeddings</code> instead of <code>TransformerWordEmbeddings</code>. And one that works with the <code>transformers</code> library.</p>
<p>I'm adding a SO question that discuss which transformer layer is interesting to use <a href=""https://stackoverflow.com/a/63464865/14476967"">here</a>.</p>
<p>I'm not familiar with Elmo, though I found <a href=""https://tfhub.dev/google/elmo/3"" rel=""noreferrer"">this</a> that uses tensorflow. You may be able to modify the code I shared to make Elmo work.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import numpy as np
from flair.data import Sentence
from flair.embeddings import TransformerDocumentEmbeddings
from sklearn.base import BaseEstimator, TransformerMixin


class FlairTransformerEmbedding(TransformerMixin, BaseEstimator):

    def __init__(self, model_name='bert-base-uncased', batch_size=None, layers=None):
        # From https://lvngd.com/blog/spacy-word-vectors-as-features-in-scikit-learn/
        # For pickling reason you should not load models in __init__
        self.model_name = model_name
        self.model_kw_args = {'batch_size': batch_size, 'layers': layers}
        self.model_kw_args = {k: v for k, v in self.model_kw_args.items()
                              if v is not None}
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        model = TransformerDocumentEmbeddings(
                self.model_name, fine_tune=False,
                **self.model_kw_args)

        sentences = [Sentence(text) for text in X]
        embedded = model.embed(sentences)
        embedded = [e.get_embedding().reshape(1, -1) for e in embedded]
        return np.array(torch.cat(embedded).cpu())

import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from transformers import AutoTokenizer, AutoModel
from more_itertools import chunked

class TransformerEmbedding(TransformerMixin, BaseEstimator):

    def __init__(self, model_name='bert-base-uncased', batch_size=1, layer=-1):
        # From https://lvngd.com/blog/spacy-word-vectors-as-features-in-scikit-learn/
        # For pickling reason you should not load models in __init__
        self.model_name = model_name
        self.layer = layer
        self.batch_size = batch_size
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModel.from_pretrained(self.model_name)

        res = []
        for batch in chunked(X, self.batch_size):
            encoded_input = tokenizer.batch_encode_plus(
                batch, return_tensors='pt', padding=True, truncation=True)
            output = model(**encoded_input)
            embed = output.last_hidden_state[:,self.layer].detach().numpy()
            res.append(embed)

        return np.concatenate(res)
</code></pre>
<p>In your case replace your column transformer by this:</p>
<pre class=""lang-py prettyprint-override""><code>column_trans = ColumnTransformer([
    ('embedding', FlairTransformerEmbedding(), 'text'),
    ('number_scaler', MinMaxScaler(), ['number'])
])
</code></pre>
",5,2,3436,2021-04-15 09:39:19,https://stackoverflow.com/questions/67105996/how-to-use-bert-and-elmo-embedding-with-sklearn
Fine-tune a BERT model for context specific embeddigns,"<p>I'm trying to find information on how to train a BERT model, possibly from the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformers</a> library, so that the embedding it outputs are more closely related to the context o the text I'm using.</p>
<p>However, all the examples that I'm able to find, are about fine-tuning the model for another task, such as <a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">classification</a>.</p>
<p>Would anyone happen to have an example of a BERT fine-tuning model for masked tokens or next sentence prediction, that outputs another raw BERT model that is fine-tuned to the context?</p>
<p>Thanks!</p>
","python, nlp, bert-language-model","<p>Here is an example from the Transformers library on <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=a3KD3WXU3l-O"" rel=""nofollow noreferrer"">Fine tuning a language model for masked token prediction</a>.</p>
<p>The model that is used is one of the BERTForLM familly. The idea is to create a dataset using the <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/datasets/language_modeling.py"" rel=""nofollow noreferrer"">TextDataset</a> that tokenizes and breaks the text into chunks. Then use a <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=a3KD3WXU3l-O"" rel=""nofollow noreferrer"">DataCollatorForLanguageModeling</a> to randomly mask tokens in the chunks when traing, and pass the model, the data and the collator to the <a href=""https://huggingface.co/transformers/training.html#trainer"" rel=""nofollow noreferrer"">Trainer</a> to train and evaluate the results.</p>
",2,5,2208,2021-04-17 09:41:54,https://stackoverflow.com/questions/67136740/fine-tune-a-bert-model-for-context-specific-embeddigns
BERT Text Classification,"<p>I am new to BERT and try to learn BERT Fine-Tuning for Text Classification via a coursera course <a href=""https://www.coursera.org/projects/fine-tune-bert-tensorflow/"" rel=""nofollow noreferrer"">https://www.coursera.org/projects/fine-tune-bert-tensorflow/</a></p>
<p>Based on the course, I would like to compare the text classification performance between BERT-12 and BERT-24 using 'SGD' and 'ADAM' optimizer respectively.</p>
<p>I found that when I use BERT-12, the result is normal. However, when switching to BERT-24, though the accuracy is good (9X%), the recall and precision value are extremely low (even close to zero).</p>
<p>May I know if there are anything wrong with my code?</p>
<p>Also, in order to improve the precision and recall, should I add more dense layers and change the activation functions? And what are the optimal learning rate values that I should use?</p>
<pre><code>import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import sys
sys.path.append('models')
from official.nlp.data import classifier_data_lib
from official.nlp.bert import tokenization
from official.nlp import optimization

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df= pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip', compression='zip', low_memory=False)

train_data_ratio = 0.1
val_data_ratio = 0.1
rand_seed = 42

train_df, remaining = train_test_split(df, random_state=rand_seed, train_size=train_data_ratio, stratify=df.target.values)
valid_df, _ = train_test_split (remaining , random_state=rand_seed, train_size=val_data_ratio, stratify=remaining.target.values)

#load data from main memory to cpu
with tf.device('/cpu:0'):
  train_data = tf.data.Dataset.from_tensor_slices ((train_df['question_text'].values, train_df['target'].values))
  valid_data = tf.data.Dataset.from_tensor_slices ((valid_df.question_text.values, valid_df.target.values))

&quot;&quot;&quot;
Each line of the dataset is composed of the review text and its label
- Data preprocessing consists of transforming text to BERT input features:
input_word_ids, input_mask, segment_ids
- In the process, tokenizing the text is done with the provided BERT model tokenizer
&quot;&quot;&quot;

label_list = [0,1] # Label categories
max_seq_length = 128 # maximum length of (token) input sequences
train_batch_size= 32
learning_rate = 0.001 
num_layer = 24 # change between bert-12 and bert-24 to compare the diff
epochs = 4
optimizer = 'SGD'

assert num_layer in [12, 24] 
if num_layer == 12:
    train_batch_size = 32
elif num_layer == 24:
    train_batch_size = 4 

assert optimizer in ['SGD', 'Adam'] 
if optimizer == 'Adam':
    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
elif optimizer == 'SGD':
    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)


# Get BERT layer and tokenizer:
https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2
bert_12 = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;
bert_24 = &quot;https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/2&quot;

if num_layer == 12:
    bert_layer = hub.KerasLayer(bert_12, trainable=True)
elif num_layer == 24:
    bert_layer = hub.KerasLayer(bert_24, trainable=True)
    
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() #from tensor to numpy
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() #check if it is lower case (no conversion. to check better)
tokenizer = tokenization.FullTokenizer (vocab_file, do_lower_case)


# from data to features that can be understood by bert

def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):
  example = classifier_data_lib.InputExample(guid=None,
                                             text_a=text.numpy(),
                                             text_b=None,
                                             label=label.numpy())
  feature=classifier_data_lib.convert_single_example(0,example,label_list,max_seq_length, tokenizer)

  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)
  
def to_feature_map(text, label):
  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text, label], 
                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])


  input_ids.set_shape([max_seq_length])
  input_mask.set_shape([max_seq_length])
  segment_ids.set_shape([max_seq_length])
  label_id.set_shape([])

  x = {
        'input_word_ids': input_ids,
        'input_mask': input_mask,
        'input_type_ids': segment_ids
    }
  return (x, label_id)
  
with tf.device('/cpu:0'):
  # train
  train_data = (train_data.map(to_feature_map,
                              num_parallel_calls=tf.data.experimental.AUTOTUNE)
                          #.cache()
                          .shuffle(1000)
                          .batch(train_batch_size, drop_remainder=True)
                          .prefetch(tf.data.experimental.AUTOTUNE))

  # valid
  valid_data = (valid_data.map(to_feature_map,
                            num_parallel_calls=tf.data.experimental.AUTOTUNE)
                          .batch(train_batch_size, drop_remainder=True)
                          .prefetch(tf.data.experimental.AUTOTUNE)) 
  

# Building the model
def create_model():
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                      name=&quot;input_word_ids&quot;)
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                  name=&quot;input_mask&quot;)
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                  name=&quot;input_type_ids&quot;)

  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

  drop = tf.keras.layers.Dropout(0.4)(pooled_output)
  output = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;output&quot;)(drop)

  model = tf.keras.Model(
    inputs={
        'input_word_ids': input_word_ids,
        'input_mask': input_mask,
        'input_type_ids': input_type_ids
    },
    outputs=output)
  return model
  
  
model = create_model()
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.BinaryCrossentropy(),
              #metrics=[tf.keras.metrics.BinaryAccuracy()])
              metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])

epochs = epochs
history = model.fit(train_data,
                    validation_data=valid_data,
                    epochs=epochs,
                    verbose=1)
                    
import matplotlib.pyplot as plt

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel(&quot;Epochs&quot;)
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()
  


</code></pre>
<p>Thank you very much !</p>
","tensorflow, keras, deep-learning, bert-language-model","<p>Maybe try adding precision and recall to a custom callback function so you can inspect what's going on. I've added a debug point in (<code>pdb.set_trace()</code>) so the process will pause once the first epoch has ended and you can step through each point to investigate the data.</p>
<pre><code>from sklearn.metrics import precision_score, recall_score
import pdb


class Callbacks(tf.keras.callbacks.Callback):
    def __init__(self, valid_data):
        super(myCallback, self).__init__()
        self.valid_data = valid_data
        

    def on_epoch_end(self, epoch, logs={}):

        pdb.set_trace()

        val_x = valid_data[:-1] # Get bert inputs
        val_y = valid_data[-1] # Get labels

        # Get predictions for the filtered val data
        val_scores = self.model.predict(val_x)

        # Get indices of best predictions - you might need to alter this
        val_y_pred = tf.argmax(val_scores, axis=1)
        val_y_true = tf.argmax(val_y, axis=1)
        
        # Calculate precision and recall
        precision = precision_score(val_y_true, val_y_pred, average='weighted')
        recall = recall_score(val_y_true, val_y_pred, average='weighted')
        
        # Add scores to logs to see in training output
        logs['precision'] = precision
        logs['recall'] = recall

</code></pre>
<p>To pass the validation data to the callback you'll need to add something like the below to your fit function:</p>
<pre><code>cbs = Callbacks(valid_data)

model.fit(...., callbacks=[cbs])
</code></pre>
",1,2,797,2021-04-17 16:46:45,https://stackoverflow.com/questions/67140627/bert-text-classification
Cannot parse file error while .pb file to tflite conversion,"<p>Hi I was making tflite for custom albert model with pb file in tf1.15 but raised error of</p>
<pre><code>raise IOError(&quot;Cannot parse file %s: %s.&quot; % (path_to_pb, str(e)))
OSError: Cannot parse file b'/home/choss/test2/freeze2/saved_model.pb': Error parsing message.
</code></pre>
<p>Code below is How I made .pb file</p>
<pre><code>meta_path = 'model.ckpt-400.meta'  # Your .meta file
output_node_names = ['loss/Softmax']    

with tf.Session() as sess:

    # Restore the graph
    saver = tf.train.import_meta_graph(meta_path)

    # Load weights
    ckpt ='/home/choss/test2/freeze2/model.ckpt-400'
    print(ckpt) 
    saver.restore(sess, ckpt)

    output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]

    # Freeze the graph
    frozen_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,
        sess.graph_def,
        output_node_names)

    # Save the frozen graph
    with open('saved_model.pb', 'wb') as f:
        f.write(frozen_graph_def.SerializeToString())
</code></pre>
<p>And I tried to make tflite file with code below</p>
<pre><code>saved_model_dir = &quot;/home/choss/test2/freeze2&quot;
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
open(&quot;converted_model.tflite&quot;, &quot;wb&quot;).write(tflite_model)
</code></pre>
<p>I used f.graph_util.convert_variables_to_constants because of freeze_graph because</p>
<pre><code>freeze_graph.freeze_graph('./graph.pbtxt', saver, False, 'model.ckpt-400', 'loss/ArgMax', &quot;&quot;, &quot;&quot;, 'frozen.pb', True, &quot;&quot;)
</code></pre>
<p>gave me an error message</p>
<pre><code>File &quot;/home/pgb/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py&quot;, line 2154, in __getitem__
    return self._inputs[i]
IndexError: list index out of range
</code></pre>
<p>Is it because I did not use freeze_graph?
If so is there any other way aside from freeze_graph?</p>
","tensorflow, tensorflow-lite, bert-language-model","<p>Instead of freezing the graph by yourself, I recommend exporting as a TF saved model and using the saved model converter with the recent TF version. You can decouple the TensorFlow versions for training and converting. For example, training can be done in the TF 1.15 and the saved model can be exported from it. And then, it is possible to bring the saved model to the TFLite converter API in TensorFlow 2.4.1 version or beyonds.</p>
",0,0,840,2021-04-18 14:46:11,https://stackoverflow.com/questions/67149857/cannot-parse-file-error-while-pb-file-to-tflite-conversion
Cannot load BERT from local disk,"<p>I am trying to use Huggingface transformer api to load a locally downloaded M-BERT model but it is throwing an exception.
I clone this repo: <a href=""https://huggingface.co/bert-base-multilingual-cased"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-multilingual-cased</a></p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
</code></pre>
<p>The directory structure is:</p>
<p><a href=""https://i.sstatic.net/rDj4T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rDj4T.png"" alt=""Directory structure"" /></a></p>
<p>But I am getting this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1277, in from_pretrained
    missing_keys, unexpected_keys = load_tf_weights(model, resolved_archive_file, load_weight_prefix)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 467, in load_tf_weights
    with h5py.File(resolved_archive_file, &quot;r&quot;) as f:
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 408, in __init__
    swmr=swmr)
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File &quot;h5py/_objects.pyx&quot;, line 54, in h5py._objects.with_phil.wrapper
  File &quot;h5py/_objects.pyx&quot;, line 55, in h5py._objects.with_phil.wrapper
  File &quot;h5py/h5f.pyx&quot;, line 88, in h5py.h5f.open
OSError: Unable to open file (file signature not found)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;train.py&quot;, line 81, in &lt;module&gt;
    __main__()
  File &quot;train.py&quot;, line 59, in __main__
    model = create_model(num_classes)
  File &quot;/content/drive/My Drive/msc-project/code/model.py&quot;, line 26, in create_model
    bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1280, in from_pretrained
    &quot;Unable to load weights from h5 file. &quot;
OSError: Unable to load weights from h5 file. If you tried to load a TF 2.0 model from a PyTorch checkpoint, please set from_pt=True. 
</code></pre>
<p>Where am I going wrong?
Need help!
Thanks in advance.</p>
","python, tensorflow, bert-language-model, huggingface-transformers","<p>As it was already pointed in the comments - your <code>from_pretrained</code> param should be either id of a model hosted on huggingface.co or a local path:</p>
<blockquote>
<p>A path to a directory containing model weights saved using
save_pretrained(), e.g., ./my_model_directory/.</p>
</blockquote>
<p>See <a href=""https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained"" rel=""nofollow noreferrer"">documentation</a></p>
<p>Looking at your stacktrace it seems like your code is run inside:</p>
<p><code>/content/drive/My Drive/msc-project/code/model.py</code> so unless your model is in:
<code>/content/drive/My Drive/msc-project/code/input/bert-base-multilingual-cased/</code> it won't load.</p>
<p>I would also set the path to be similar to documentation example ie:</p>
<p><code>bert = TFBertModel.from_pretrained(&quot;./input/bert-base-multilingual-cased/&quot;)</code></p>
",1,1,5679,2021-04-18 20:24:00,https://stackoverflow.com/questions/67153058/cannot-load-bert-from-local-disk
What does &#39;output_dir&#39; mean in transformers.TrainingArguments?,"<p>On the huggingface site documentation, it says 'The output directory where the model predictions and checkpoints will be written'. I don't quite understand what it means. Do I have to create any file for that?</p>
","python, bert-language-model, huggingface-transformers, ray-tune","<p>The trainer of the Huggingface models can save many things. Most importantly:</p>
<ul>
<li><p>Vocabulary of the tokenizer that is used (as a JSON file)</p>
</li>
<li><p>Model configuration: a JSON file saying how to instantiate the model object, i.e., architecture and hyperparameters</p>
</li>
<li><p>Model checkpoints: trainable parameters of the model saved during training</p>
</li>
<li><p>Further it can save the values of metrics used during training and the state of the training (so the training can be restored from the same place)</p>
</li>
</ul>
<p>All these are stored in files in the <code>output_dir</code> directory. You do not have to create the directory in advance, but the path to the directory at least should exist.</p>
",4,4,6240,2021-04-19 06:49:03,https://stackoverflow.com/questions/67157185/what-does-output-dir-mean-in-transformers-trainingarguments
How does the BERT model select the label ordering?,"<p>I'm training BertForSequenceClassification for a classification task. My dataset consists of 'contains adverse effect' (1) and 'does not contain adverse effect' (0). The dataset contains all of the 1s and then the 0s after (the data isn't shuffled). For training I've shuffled my data and get the logits. From what I've understood, the logits are the probability distributions before softmax. An example logit is [-4.673831, 4.7095485]. Does the first value correspond to the label 1 (contains AE) because it appears first in the dataset, or label 0. Any help would be appreciated thanks.</p>
","pytorch, bert-language-model, huggingface-transformers, logits","<p>The first value corresponds to label 0 and the second value corresponds to label 1. What <a href=""https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> does is feeding the output of the pooler to a linear layer (after a dropout which I will ignore in this answer). Let's look at the following example:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn
from transformers import BertModel, BertTokenizer
t = BertTokenizer.from_pretrained('bert-base-uncased')
m = BertModel.from_pretrained('bert-base-uncased')
i = t.encode_plus('This is an example.', return_tensors='pt')
o = m(**i)
print(o.pooler_output.shape)
</code></pre>
<p>Output:</p>
<pre><code>torch.Size([1, 768])
</code></pre>
<p>The pooled_output is a tensor of shape [batch_size,hidden_size] and represents the contextualized (i.e. attention was applied) <code>[CLS]</code> token of your input sequences. This tensor is feed to a linear layer to calculate the <a href=""https://developers.google.com/machine-learning/glossary/#logits"" rel=""nofollow noreferrer"">logits</a> of your sequence:</p>
<pre class=""lang-py prettyprint-override""><code>classificationLayer = nn.Linear(768,2)
logits = classificationLayer(o.pooler_output)
</code></pre>
<p>When we normalize these logits we can see that the linear layer predicts that our input should belong to label 1:</p>
<pre class=""lang-py prettyprint-override""><code>print(nn.functional.softmax(logits,dim=-1))
</code></pre>
<p>Output (will differ since the linear layer is initialed randomly):</p>
<pre><code>tensor([[0.1679, 0.8321]], grad_fn=&lt;SoftmaxBackward&gt;)
</code></pre>
<p>The linear layer applies a linear transformation: <code>y=xA^T+b</code> and you can already see that the linear layer is not aware of your labels. It 'only' has a weights matrix of size [2,768] to produce logits of size [1,2] (i.e.: first row corresponds to the first value and second row to the second):</p>
<pre class=""lang-py prettyprint-override""><code>import torch:

logitsOwnCalculation = torch.matmul(o.pooler_output,  classificationLayer.weight.transpose(0,1))+classificationLayer.bias
print(nn.functional.softmax(logitsOwnCalculation,dim=-1))
</code></pre>
<p>Output:</p>
<pre><code>tensor([[0.1679, 0.8321]], grad_fn=&lt;SoftmaxBackward&gt;)
</code></pre>
<p>The <a href=""https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> model learns by applying a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">CrossEntropyLoss</a>. This loss function produces a small loss when the logits for a certain class (label in your case) deviate only slightly from the expectation. That means the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">CrossEntropyLoss</a> is the one that lets your model learn that the first logit should be high when the input <code>does not contain adverse effect</code> or small when it <code>contains adverse effect</code>. You can check this for our example with the following:</p>
<pre class=""lang-py prettyprint-override""><code>loss_fct = nn.CrossEntropyLoss()
label0 = torch.tensor([0]) #does not contain adverse effect
label1 = torch.tensor([1]) #contains adverse effect
print(loss_fct(logits, label0))
print(loss_fct(logits, label1))
</code></pre>
<p>Output:</p>
<pre><code>tensor(1.7845, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.1838, grad_fn=&lt;NllLossBackward&gt;)
</code></pre>
",3,1,1245,2021-04-21 06:15:39,https://stackoverflow.com/questions/67190212/how-does-the-bert-model-select-the-label-ordering
tflite converter error operation not supported,"<p>I was trying to convert .pb model of albert to tflite</p>
<p>I made .pb model using <a href=""https://github.com/google-research/albert"" rel=""nofollow noreferrer"">https://github.com/google-research/albert</a> in tf 1.15</p>
<p>And I used
<code>tconverter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory</code>
to make tflite file(in tf 2.4.1)</p>
<p>but</p>
<pre><code>Traceback (most recent call last):
  File &quot;convert.py&quot;, line 7, in &lt;module&gt;
    tflite_model = converter.convert()
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py&quot;, line 983, in convert
    **converter_kwargs)
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py&quot;, line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py&quot;, line 200, in toco_convert_protos
    raise ConverterError(&quot;See console for info.\n%s\n%s\n&quot; % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2021-04-25 17:30:33.543663: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ParseExample
2021-04-25 17:30:33.546255: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 163 operators, 308 arrays (0 quantized)
2021-04-25 17:30:33.547201: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 162 operators, 301 arrays (0 quantized)
2021-04-25 17:30:33.548519: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 162 operators, 301 arrays (0 quantized)
2021-04-25 17:30:33.550930: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 134 operators, 264 arrays (0 quantized)
2021-04-25 17:30:33.577037: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 127 operators, 257 arrays (0 quantized)
2021-04-25 17:30:33.578278: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 127 operators, 257 arrays (0 quantized)
2021-04-25 17:30:33.579051: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 127 operators, 257 arrays (0 quantized)
2021-04-25 17:30:33.580196: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2021-04-25 17:30:33.580514: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 11640702
2021-04-25 17:30:33.580862: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SHAPE, SOFTMAX, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, ParseExample.
Traceback (most recent call last):
  File &quot;/home/pgb/anaconda3/envs/test2/bin/toco_from_protos&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py&quot;, line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py&quot;, line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/absl/app.py&quot;, line 300, in run
    _run_main(main, args)
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/absl/app.py&quot;, line 251, in _run_main
    sys.exit(main(argv))
  File &quot;/home/pgb/anaconda3/envs/test2/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py&quot;, line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SHAPE, SOFTMAX, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, ParseExample.
</code></pre>
<p>So I used</p>
<pre><code>converter.allow_custom_ops = True
</code></pre>
<p>And it worked but when I tried to measure the runtime in android device with method <a href=""https://www.tensorflow.org/lite/performance/measurement"" rel=""nofollow noreferrer"">https://www.tensorflow.org/lite/performance/measurement</a></p>
<p>nothing comes out(And cpu goes to Idel).</p>
<ol>
<li><p>In albert github code I cannot find BatchMatMul, ParseExample where did it came from?</p>
</li>
<li><p>Is there any way beside  converter.allow_custom_ops = True?</p>
</li>
<li><p>Could the reason failure of running model in adb might be due to converter.allow_custom_ops = True?</p>
</li>
</ol>
","tensorflow, adb, tensorflow-lite, bert-language-model","<p>Please consider using the Select TF option in order to fall back to the TF ops when TFLite builtin op coverage does not fit your case.</p>
<p>For the conversion procedure, you can enable the Select TF option as follows:</p>
<pre><code>converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
</code></pre>
<p>Allowing custom ops requires users to write down the TFLite custom ops for the ops, that are not covered by TFLite builtin op set. For example, BatchMatMul and ParseExample ops are needed to be implemented by yourself. In most of cases, using the existing TF op implementations is much eaiser than implementing custom ops.</p>
<p>Please refer to this <a href=""https://www.tensorflow.org/lite/guide/ops_select"" rel=""noreferrer"">link</a>.</p>
",7,2,8659,2021-04-25 08:42:27,https://stackoverflow.com/questions/67251401/tflite-converter-error-operation-not-supported
why input_mask is all the same number in BERT language model?,"<p>For a text classification task I applied Bert(fine tune) and the output that I got is as below:
Why input_mask is all 1 ?</p>
<pre><code>#to_feature_map is a function.
to_feature_map(&quot;hi how are you doing&quot;,0)
</code></pre>
<pre><code>({'input_mask': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        dtype=int32)&gt;,
  'input_type_ids': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        dtype=int32)&gt;,
  'input_word_ids': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([ 101, 7632, 2129, 2024, 2017, 2725,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)&gt;},
 &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;)```
</code></pre>
","python, nlp, bert-language-model, nlu","<p>The input masks — allows the model to cleanly differentiate between the content and the padding. The mask has the same shape as the input ids, and contains 1 anywhere the the input ids is not padding.</p>
",2,0,589,2021-04-26 16:21:44,https://stackoverflow.com/questions/67270272/why-input-mask-is-all-the-same-number-in-bert-language-model
"What is the simplest way to continue training a pre-trained BERT model, on a specific domain?","<p>I want to use a pre-trained BERT model in order to use it on a text classification task (I'm using Huggingface library). However, the pre-trained model was trained on domains that are different than mine, and I have a large unannotated dataset that can be used for fine-tuning it. If I use only my tagged examples and fine-tune it &quot;on the go&quot; while training on the specific task (BertForSequenceClassification), the dataset is too small for adapting the language model for the specific domain. What it the best way to do so?
Thanks!</p>
","nlp, text-classification, bert-language-model, huggingface-transformers, pytorch-lightning","<p>Let's clarify a couple of points first to reduce some ambiguity.</p>
<ol>
<li>BERT uses two pretraining objectives: Masked Language Modeling (MLM) and Next Sentence Prediction.</li>
<li>You mentioned having a large unannotated dataset, which you plan on using to fine-tune your BERT model. This is not how fine-tuning works. In order to fine-tune your pretrained model, you would need an <strong>annotated</strong> dataset i.e. document &amp; class pair for sequence classification downstream task.</li>
</ol>
<p>So what can you do? First, extend your general domain tokenizer with your unannotated dataset consisting of domain-specific vocabulary. Then, using this extended tokenizer you can continue pretraining on MLM and/or NSP objectives to modify your word embeddings. Finally, fine-tune your model using an annotated dataset.</p>
",5,2,2373,2021-04-27 11:27:42,https://stackoverflow.com/questions/67282155/what-is-the-simplest-way-to-continue-training-a-pre-trained-bert-model-on-a-spe
Loss function for comparing two vectors for categorization,"<p>I am performing a NLP task where I analyze a document and classify it into one of six categories. However, I do this operation at three different time periods. So the final output is an array of three integers (sparse), where each integer is the category 0-5. So a label looks like this: <code>[1, 4, 5]</code>.</p>
<p>I am using BERT and am trying to decide what type of head I should attach to it, as well as what type of loss function I should use. Would it make sense to use BERT's output of size <code>1024</code> and run it through a <code>Dense</code> layer with 18 neurons, then reshape into something of size <code>(3,6)</code>?</p>
<p>Finally, I assume I would use Sparse Categorical Cross-Entropy as my loss function?</p>
","python, machine-learning, nlp, bert-language-model","<p>The bert final hidden state is (512,1024). You can either take the first token which is the CLS token or take the average pooling. Either way your final output is shape (1024,) now simply put 3 linear layers of shape (1024,6) as in <code>nn.Linear(1024,6)</code> and pass it into the loss function below. (you can make it more complex if you want to)</p>
<p>Simply add up the loss and call backward. Remember you can call loss.backward() on any scalar tensor.(pytorch)</p>
<pre><code>def loss(time1output,time2output,time3output,time1label,time2label,time3label):
    loss1 = nn.CrossEntropyLoss()(time1output,time1label)
    loss2 = nn.CrossEntropyLoss()(time2output,time2label)
    loss3 = nn.CrossEntropyLoss()(time3output,time3label)

    return loss1 + loss2 + loss3
</code></pre>
",1,5,1536,2021-04-30 17:00:35,https://stackoverflow.com/questions/67337774/loss-function-for-comparing-two-vectors-for-categorization
how to add tokens in vocab.txt which decoded as [UNK] bert tokenizer,"<p>i was decoding the tokenized tokens from <strong>bert tokenizer</strong> and it was giving <strong>[UNK]</strong> for € symbol. but i tried by add ##€ token in vocab.txt file. but it was not reflected in prediction result was same as previous it was giving [UNK] again. please let me know to solve this problem did i need to <strong>fine tune</strong> the model for again to reflect the changes in <strong>prediction</strong>. till now i was avoiding fine tuning again because it takes more than 10 hours.
Thanks in advance</p>
","python, nlp, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>Use the <a href=""https://huggingface.co/transformers/v4.5.1/internal/tokenization_utils.html?highlight=add_tokens#transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens"" rel=""nofollow noreferrer"">add_tokens</a> function of the tokenizer to avoid unknown tokens:</p>
<pre><code>from transformers import BertTokenizer
t = BertTokenizer.from_pretrained('bert-base-uncased')
print(t.tokenize(&quot;This is an example with an emoji 🤗.&quot;))
t.add_tokens(['🤗'])
print(t.tokenize(&quot;This is an example with an emoji 🤗.&quot;))
</code></pre>
<p>Output:</p>
<pre><code>['this', 'is', 'an', 'example', 'with', 'an', 'em', '##oj', '##i', '[UNK]', '.']
['this', 'is', 'an', 'example', 'with', 'an', 'em', '##oj', '##i', '🤗', '.']
</code></pre>
<p>Please keep in mind that you also need to resize your model to introduce this to the new token with <a href=""https://huggingface.co/transformers/v4.5.1/main_classes/model.html#transformers.PreTrainedModel.resize_token_embeddings"" rel=""nofollow noreferrer"">resize_token_embeddings</a>:</p>
<pre><code>model.resize_token_embeddings(len(t))
</code></pre>
",3,1,3278,2021-05-02 12:57:50,https://stackoverflow.com/questions/67356666/how-to-add-tokens-in-vocab-txt-which-decoded-as-unk-bert-tokenizer
BERT model bug encountered during training,"<p>So, I made a custom dataset consisting of reviews form several E-learning sites. What I am trying to do is build a model that can recognize emotions based on text and for training I am using the dataset I've made via scraping. While working on BERT, I encountered this error</p>
<p><code>normalize() argument 2 must be str, not float</code></p>
<p>here's my code:-</p>
<pre><code>import numpy as np 
import pandas as pd
import numpy as np


import tensorflow as tf
print(tf.__version__)
import ktrain
from ktrain import text

from sklearn.model_selection import train_test_split
import pickle


#class_names = [&quot;Frustration&quot;, &quot;Not satisfied&quot;, &quot;Satisfied&quot;, &quot;Happy&quot;, &quot;Excitement&quot;]


data = pd.read_csv(&quot;Final_scraped_dataset.csv&quot;)
print(data.head())

X = data['Text']
y = data['Emotions']


class_names = np.unique(data['Emotions'])
print(class_names)
        
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)


    
    
print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)
print(X_train.head(10))

encoding = {
    'Frustration': 0,
    'Not satisfied': 1,
    'Satisfied': 2,
    'Happy': 3,
    'Excitement' : 4
}

y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]



X_train = X_train.tolist()
X_test = X_test.tolist()




#print(X_train)

(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=y_train,
                                                                       x_test=X_test, y_test=y_test,
                                                                       class_names=class_names,
                                                                       preprocess_mode='bert',
                                                                       maxlen=200, 
                                                                       max_features=15000) #I've encountered the error here


'''model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)

learner = ktrain.get_learner(model, train_data=(x_train, y_train), 
                             val_data=(x_test, y_test),
                             batch_size=4)

learner.fit_onecycle(2e-5, 3)


learner.validate(val_data=(x_test, y_test))

predictor = ktrain.get_predictor(learner.model, preproc)
predictor.get_classes()

import time 

message = 'I hate you a lot'

start_time = time.time() 
prediction = predictor.predict(message)

print('predicted: {} ({:.2f})'.format(prediction, (time.time() - start_time)))

# let's save the predictor for later use
predictor.save(&quot;new_model/bert_model&quot;)


print(&quot;SAVED  _______&quot;)'''

</code></pre>
<p>here's the complete error:-</p>
<pre><code>
  File &quot;D:\Sentiment analysis\BERT_model_new_dataset.py&quot;, line 73, in &lt;module&gt;
    max_features=15000)

  File &quot;D:\Anaconda3\envs\pythy37\lib\site-packages\ktrain\text\data.py&quot;, line 373, in texts_from_array
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)

  File &quot;D:\Anaconda3\envs\pythy37\lib\site-packages\ktrain\text\preprocessor.py&quot;, line 796, in preprocess_train
    x = bert_tokenize(texts, self.tok, self.maxlen, verbose=verbose)

  File &quot;D:\Anaconda3\envs\pythy37\lib\site-packages\ktrain\text\preprocessor.py&quot;, line 166, in bert_tokenize
    ids, segments = tokenizer.encode(doc, max_len=max_length)

  File &quot;D:\Anaconda3\envs\pythy37\lib\site-packages\keras_bert\tokenizer.py&quot;, line 73, in encode
    first_tokens = self._tokenize(first)

  File &quot;D:\Anaconda3\envs\pythy37\lib\site-packages\keras_bert\tokenizer.py&quot;, line 103, in _tokenize
    text = unicodedata.normalize('NFD', text)

TypeError: normalize() argument 2 must be str, not float
</code></pre>
","python, pandas, numpy, tensorflow, bert-language-model","<p>It sounds like you may have a float value in your <code>data['Text']</code> column somehow.</p>
<p>You can try something like this to shed more light on what's happening:</p>
<pre class=""lang-py prettyprint-override""><code>for i, s in enumerate(data['Text']):
    if not isinstance(s, str):  print('Text in row %s is not a string: %s' % (i, s))
</code></pre>
",1,2,331,2021-05-02 20:49:40,https://stackoverflow.com/questions/67360987/bert-model-bug-encountered-during-training
How to build a dataset for language modeling with the datasets library as with the old TextDataset from the transformers library,"<p>I am trying to load a custom dataset that I will then use for language modeling. The dataset consists of a text file that has a whole document in each line, meaning that each line overpasses the normal 512 tokens limit of most tokenizers.</p>
<p>I would like to understand what is the process to build a text dataset that tokenizes each line, having previously split the documents in the dataset into lines of a &quot;tokenizable&quot; size, as the old <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/datasets/language_modeling.py"" rel=""nofollow noreferrer"">TextDataset</a> class would do, where you only had to do the following, and a tokenized dataset without text loss would be available to pass to a DataCollator:</p>
<pre><code>model_checkpoint = 'distilbert-base-uncased'

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

from transformers import TextDataset

dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=&quot;path/to/text_file.txt&quot;,
    block_size=512,
)
</code></pre>
<p>Instead of this way, which is to be deprecated soon, I would like to use the <a href=""https://huggingface.co/docs/datasets/"" rel=""nofollow noreferrer"">datasets</a> library. For now, what I have is the following, which, of course, throws an error because each line is longer than the maximum block size in the tokenizer:</p>
<pre><code>import datasets
dataset = datasets.load_dataset('path/to/text_file.txt')

model_checkpoint = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;])

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[&quot;text&quot;])
</code></pre>
<p>So what would be the &quot;standard&quot; way of creating a dataset in the way it was done before but with the datasets lib?</p>
<p>Thank you very much for the help :))</p>
","python, bert-language-model, huggingface-transformers","<p>I received an answer for this question on the <a href=""https://discuss.huggingface.co/t/help-understanding-how-to-build-a-dataset-for-language-as-with-the-old-textdataset/5870"" rel=""nofollow noreferrer"">HuggingFace Datasets forum</a> by @lhoestq</p>
<blockquote>
<p>Hi !</p>
<p>If you want to tokenize line by line, you can use this:</p>
<pre><code>max_seq_length = 512
num_proc = 4

def tokenize_function(examples):
    # Remove empty lines
    examples[&quot;text&quot;] = [line for line in examples[&quot;text&quot;] if len(line) &gt; 0 and not line.isspace()]
    return tokenizer(
        examples[&quot;text&quot;],
        truncation=True,
        max_length=max_seq_length,
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=num_proc,
    remove_columns=[&quot;text&quot;],
)
</code></pre>
<p>Though the TextDataset was doing a different processing by
concatenating all the texts and building blocks of size 512. If you
need this behavior, then you must apply an additional map function
after the tokenization:</p>
<pre><code># Main data processing function that will concatenate all texts from
# our dataset and generate chunks of max_seq_length.
def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop,
    # you can customize this part to your needs.
    total_length = (total_length // max_seq_length) * max_seq_length
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]
        for k, t in concatenated_examples.items()
    }
    return result

# Note that with `batched=True`, this map processes 1,000 texts together,
# so group_texts throws away a remainder for each of those groups of 1,000 texts.
# You can adjust that batch_size here but a higher value might be slower to preprocess.

tokenized_dataset = tokenized_dataset.map(
    group_texts,
    batched=True,
    num_proc=num_proc,
)
</code></pre>
<p>This code comes from the processing of the run_mlm.py example script
of transformers</p>
</blockquote>
",0,4,2830,2021-05-03 10:55:46,https://stackoverflow.com/questions/67367757/how-to-build-a-dataset-for-language-modeling-with-the-datasets-library-as-with-t
loss is NaN when using keras bert for classification,"<p>I'm using <strong>keras-bert</strong> for classification. On some datasets, it runs well and calculates the loss, while on others the loss is <code>NaN</code>.</p>
<p>The different datasets are similar in that they are augmented versions of the original one. Working with keras-bert, the original data and some augmented versions of the data run well while the other augmented versions of data don't run well.</p>
<p>When I use a regular one-layer <code>BiLSTM</code> on the augmented versions of data that don't run well with keras-bert, it works out fine which means I can rule out the possibility of the data being faulty or containing spurious values that may affect the way the loss is calculated.
The data in working with has three classes.</p>
<p>I'm using bert based uncased</p>
<pre><code>!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
</code></pre>
<p>Can anyone give me pointers as to why the loss is nan?</p>
<pre><code>inputs = model.inputs[:2]
dense = model.layers[-3].output
outputs = keras.layers.Dense(3, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(dense)
decay_steps, warmup_steps = calc_train_steps(train_y.shape[0], batch_size=BATCH_SIZE,epochs=EPOCHS,)
#(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR)
model = keras.models.Model(inputs, outputs)
model.compile(AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR), loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])
sess = tf.compat.v1.keras.backend.get_session()
uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.compat.v1.report_uninitialized_variables ())])
init_op = tf.compat.v1.variables_initializer([v for v in tf.compat.v1.global_variables() if v.name.split(':')[0] in uninitialized_variables])
sess.run(init_op)
model.fit(train_x,train_y,epochs=EPOCHS,batch_size=BATCH_SIZE)
</code></pre>
<pre><code> Train on 20342 samples
Epoch 1/10
20342/20342 [==============================] - 239s 12ms/sample - loss: nan - sparse_categorical_accuracy: 0.5572
Epoch 2/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 3/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
Epoch 4/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 5/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 6/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 7/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 8/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
Epoch 9/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
Epoch 10/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
&lt;tensorflow.python.keras.callbacks.History at 0x7f1caf9b0f90&gt;
</code></pre>
<p>Also, I'm running this on Google Colab with <code>tensorflow 2.3.0</code> and <code>keras 2.4.3</code></p>
<pre><code>UPDATE
</code></pre>
<p>I looked the data that was causing this issue again and i realised that one of the target labels were missing. I might have mistakenly edited it. Once i fixed it, the loss is NaN problem dissappeared. However, i'll be awarding the 50 points to the answer i got because it got me to think better about my code. Thanks.</p>
","tensorflow, keras, deep-learning, bert-language-model","<p>I noticed one issue in your code but I'm not sure if this the main cause; better if you can possibly provide some reproducible code.</p>
<p>In your above code snippet, you set <code>sigmoid</code> in your last layer activation with <code>unit &lt; 1</code> which indicate the problem dataset is probably <strong>multi-label</strong> and that's why the loss function should be <code>binary_crossentropy</code> but you set <code>sparse_categorical_crossentropy</code> which is typical uses <strong>multi-class</strong> problem and with <strong>integer labels</strong>.</p>
<pre><code>outputs = keras.layers.Dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),
                   name = 'real_output')(dense)

model = keras.models.Model(inputs, outputs)
model.compile(AdamWarmup(decay_steps=decay_steps, 
                            warmup_steps=warmup_steps, lr=LR),
                 loss='sparse_categorical_crossentropy',
                 metrics=['sparse_categorical_accuracy'])
  
</code></pre>
<p>So, if your problem data set is a <strong>multi-label</strong> with the last layer <code>unit = 3</code>, then the set-up should be more like</p>
<pre><code>outputs = keras.layers.Dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(AdamWarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=LR),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])
</code></pre>
<p>but if the problem set is a <strong>multi-class</strong> problem and your <strong>target labels are integer</strong> (<code>unit = 3</code>) then the set-up should more like as follows:</p>
<pre><code>outputs = keras.layers.Dense(3, activation='softmax',
                   kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(AdamWarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=LR),
                     loss='sparse_categorical_crossentropy',
                     metrics=['sparse_categorical_accuracy'])
</code></pre>
",2,1,664,2021-05-04 02:53:27,https://stackoverflow.com/questions/67378194/loss-is-nan-when-using-keras-bert-for-classification
Can&#39;t do lazy loading with allennlp,"<p>Currently I'm trying to implement lazy loading with allennlp, but can't.
My code is as the followings.</p>
<pre class=""lang-py prettyprint-override""><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    # TODO: avoid memory consumption and lazy loading
    train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>When I commented-out <code> train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code>, iterator doesn't work and training is conducted with 0 sample.</p>
<pre><code>Building the vocabulary
100it [00:00, 442.15it/s]01, 133.57it/s]
building vocab: 100it [00:01, 95.84it/s]
100it [00:00, 413.40it/s]
100it [00:00, 138.38it/s]
You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
0it [00:00, ?it/s]
0it [00:00, ?it/s]
</code></pre>
<p>I'd like to know if there is any solution for avoid this.
Thanks.</p>
<hr />
<p><strong>Supplement, added at fifth, May.</strong></p>
<p>Currently I am trying to avoid putting all of each sample data on top of memory before training the model.</p>
<p>So I have implemented the _read method as a generator. My understanding is that by calling this method and wrapping it with SimpleDataLoader, I can actually pass the data to the model.</p>
<p>In the DatasetReader, the code for the _read method looks like this. It is my understanding that this is intended to be a generator that avoids memory consumption.</p>
<pre class=""lang-py prettyprint-override""><code>    @overrides
    def _read(self, train_dev_test_flag: str) -&gt; Iterator[Instance]:
        '''
        :param train_dev_test_flag: 'train', 'dev', 'test'
        :return: list of instances
        '''
        if train_dev_test_flag == 'train':
            dataset = self._train_loader()
            random.shuffle(dataset)
        elif train_dev_test_flag == 'dev':
            dataset = self._dev_loader()
        elif train_dev_test_flag == 'test':
            dataset = self._test_loader()
        else:
            raise NotImplementedError(
                &quot;{} is not a valid flag. Choose from train, dev and test&quot;.format(train_dev_test_flag))

        if self.config.debug:
            dataset = dataset[:self.config.debug_data_num]

        for data in tqdm(enumerate(dataset)):
            data = self._one_line_parser(data=data, train_dev_test_flag=train_dev_test_flag)
            yield self.text_to_instance(data)
</code></pre>
<p>Also, <code>build_data_loaders</code> actually looks like this.</p>
<pre class=""lang-py prettyprint-override""><code>def build_data_loaders(config,
    train_data: List[Instance],
    dev_data: List[Instance],
    test_data: List[Instance]) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:

    train_loader = SimpleDataLoader(train_data, config.batch_size_for_train, shuffle=False)
    dev_loader = SimpleDataLoader(dev_data, config.batch_size_for_eval, shuffle=False)
    test_loader = SimpleDataLoader(test_data, config.batch_size_for_eval, shuffle=False)

    return train_loader, dev_loader, test_loader
</code></pre>
<p>But, by somewhat reason I don't know, this code doesn't work.</p>
<pre><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>In this code, the SimpleDataLoader will wrap the generator type as it is. I would like to do the lazy loading that allennlp did in the 0.9 version.</p>
<p>But this code iterates training over 0 instances, so currently I have added</p>
<p><code>train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code></p>
<p>before</p>
<p><code>train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)</code>.</p>
<p>And it works. But this means that I can't train or evaluate the model until I have all the instances in memory. Rather, I want each batch to be called into memory only when it is time to train.</p>
","nlp, pytorch, bert-language-model, allennlp","<p>The <code>SimpleDataLoader</code> is not capable of lazy loading. You should use the <a href=""https://docs.allennlp.org/main/api/data/data_loaders/multiprocess_data_loader/#multiprocessdataloader"" rel=""nofollow noreferrer""><code>MultiProcessDataLoader</code></a> instead. Setting <code>max_instances_in_memory</code> to a non-zero integer (usually some multiple of your batch size) will trigger lazy loading.</p>
",0,0,320,2021-05-04 04:29:28,https://stackoverflow.com/questions/67378820/cant-do-lazy-loading-with-allennlp
How to add extra dense layer on top of BertForSequenceClassification?,"<p>I want to add an extra layer (and dropout) before the classification layer (I'm using PyTorch lightning) What is the best way to do it?</p>
","text-classification, bert-language-model, pytorch-lightning","<p>The class <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer""><code>BertForSequenceClassification</code></a> (that comes from the Huggingface Transformers <a href=""https://www.pytorchlightning.ai/blog/how-to-fine-tune-bert-with-pytorch-lightning"" rel=""nofollow noreferrer"">when using PyTorch lightning</a>) implements a fixed architecture. If you want to change it (e.g., by adding layers), you need to inherit your own module.</p>
<p>This is actually quite simple. You can copy the code of <code>BertForSequenceClassification</code> and modify the code <a href=""https://github.com/huggingface/transformers/blob/v4.5.0/src/transformers/models/bert/modeling_bert.py#L1515"" rel=""nofollow noreferrer"">between getting the pooled BERT output and getting the logits</a>.</p>
<p>Note however that adding a hidden layer to a classifier does not make much difference when finetuning BERT. The capacity of the additional hidden layer is negligible compared to the entire stack of BERT layers. Even If you cannot finetune the entire model, fine-tuning just the last BERT layer is probably better than adding an extra layer to the classifier.</p>
",0,0,1988,2021-05-05 09:40:38,https://stackoverflow.com/questions/67398812/how-to-add-extra-dense-layer-on-top-of-bertforsequenceclassification
Is there any TF implementation of the Original BERT other than Google and HuggingFace?,"<p>Trying to find any <code>Tensorflow/Keras</code> implementation of the <code>original BERT model trained using MLM/NSP</code>. The official google and HuggingFace implementations are very complex and has so much of added functionalities. But I want to learn and implement <code>BERT</code> for just learning its working.</p>
<p>Any leads will be helpful?</p>
","tensorflow, keras, deep-learning, nlp, bert-language-model","<p>As mentioned in the comment, you can try the following implementation of <a href=""https://keras.io/examples/nlp/masked_language_modeling/"" rel=""nofollow noreferrer"">MLP-BERT</a> TensorFlow. It's a simplified version and easy to follow comparatively.</p>
",0,1,57,2021-05-07 05:29:29,https://stackoverflow.com/questions/67429425/is-there-any-tf-implementation-of-the-original-bert-other-than-google-and-huggin
How to preprocess a dataset for BERT model implemented in Tensorflow 2.x?,"<h2>Overview</h2>
<p>I have a dataset made for classification problem. There are two columns one is <code>sentences</code> and the other is <code>labels</code> (total: 10 labels). I'm trying to convert this dataset to implement it in a BERT model made for classification and that is implemented in Tensorflow 2.x. However, I can't preprocess correctly the dataset to make a <code>PrefetchDataset</code> used as input.</p>
<h2>What I did?</h2>
<ul>
<li>Dataframe is balanced and shuffled (every label have 18708 data)</li>
<li>Dataframe shape: (187080, 2)</li>
<li><code>from sklearn.model_selection import train_test_split</code> was used to split the dataframe</li>
<li>80% train data, 20% test data</li>
</ul>
<h3>Training data:</h3>
<p><strong>X_train</strong></p>
<pre><code>array(['i hate megavideo  stupid time limits',
       'wow this class got wild quick  functions are a butt',
       'got in trouble no cell phone or computer for a you later twitter',
       ...,
       'we lied down around am rose a few hours later party still going lt',
       'i wanna miley cyrus on brazil  i love u my diva miley rocks',
       'i know i hate it i want my dj danger bck'], dtype=object)
</code></pre>
<p><strong>y_train</strong></p>
<pre class=""lang-py prettyprint-override""><code>array(['unfriendly', 'unfriendly', 'unfriendly', ..., 'pos_hp',
       'friendly', 'friendly'], dtype=object)
</code></pre>
<p><strong>BERT preprocessing Xy_dataset</strong></p>
<pre class=""lang-py prettyprint-override""><code>AUTOTUNE = tf.data.AUTOTUNE # autotune the buffer_size: optional = 1

train_Xy_slices = tf.data.Dataset.from_tensor_slices(tensors=(X_train, y_train))
dataset_train_Xy = train_Xy_slices.batch(batch_size=32)
</code></pre>
<p><strong>output</strong></p>
<pre class=""lang-py prettyprint-override""><code>dataset_train_Xy
&lt;PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)&gt;


for i in dataset_train_Xy:
    print(i)
(
&lt;tf.Tensor: shape=(32,), dtype=string, numpy=
array([b'some of us had to work al day',
       ...
       b'feels claudia cazacus free falling feat audrey gallagher amp thomas bronzwaers look ahead are the best trance offerings this summer'], dtype=object)&gt;,
 
&lt;tf.Tensor: shape=(32,), dtype=string, numpy=
array([b'interested', b'uninterested', b'happy', b'friendly', b'neg_hp',
       ...
       b'friendly', b'insecure', b'pos_hp', b'interested', b'happy'],
      dtype=object)&gt;
)
</code></pre>
<h2>Expected output (example)</h2>
<pre class=""lang-py prettyprint-override""><code>dataset_train_Xy
&lt;PrefetchDataset shapes: ({input_word_ids: (None, 128), input_mask: (None, 128), input_type_ids: (None, 128)}, (None,)), types: ({input_word_ids: tf.int32, input_mask: tf.int32, input_type_ids: tf.int32}, tf.int64)&gt;
</code></pre>
<p><a href=""https://i.sstatic.net/iqmu6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iqmu6.png"" alt=""enter image description here"" /></a></p>
<h2>Observations/problem:</h2>
<p>I know I need to tokenize <code>X_train</code> and <code>y_train</code>, but when I tried to tokenize had an error:</p>
<pre class=""lang-py prettyprint-override""><code>AUTOTUNE = tf.data.AUTOTUNE # autotune the buffer_size: optional = 1

train_Xy_slices = tf.data.Dataset.from_tensor_slices(tensors=(X_train, y_train))
dataset_train_Xy = train_Xy_slices.batch(batch_size=batch_size) # 32

print(type(dataset_train_Xy))

# Tokenize the text to word pieces.
bert_preprocess = hub.load(tfhub_handle_preprocess)
tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')

dataset_train_Xy = dataset_train_Xy.map(lambda ex: (tokenizer(ex), ex[1])) #    print(i[1]) # correspond to labels
dataset_train_Xy = dataset_train_Xy.prefetch(buffer_size=AUTOTUNE)
</code></pre>
<h3>Traceback</h3>
<pre class=""lang-py prettyprint-override""><code>&lt;class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'&gt;
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-69-8e486f7b671b&gt; in &lt;module&gt;()
     14 tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')
     15 
---&gt; 16 dataset_train_Xy = dataset_train_Xy.map(lambda ex: (tokenizer(ex), ex[1])) #    print(i[1]) #labels
     17 dataset_train_Xy = dataset_train_Xy.prefetch(buffer_size=AUTOTUNE)

10 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):
--&gt; 670           raise e.ag_error_metadata.to_exception(e)
    671         else:
    672           raise

TypeError: in user code:


    TypeError: &lt;lambda&gt;() takes 1 positional argument but 2 were given
</code></pre>
","python, tensorflow, tokenize, bert-language-model","<p><strong>Working sample BERT model</strong></p>
<pre><code>#importing neccessary modules
import os
import tensorflow as tf
import tensorflow_hub as hub

data = {'input' :['i hate megavideo  stupid time limits',
       'wow this class got wild quick  functions are a butt',
       'got in trouble no cell phone or computer for a you later twitter',
       'we lied down around am rose a few hours later party still going lt',
       'i wanna miley cyrus on brazil  i love u my diva miley rocks',
       'i know i hate it i want my dj danger bck'],
        'label' : ['unfriendly', 'unfriendly', 'unfriendly', 'unfriendly ',
       'friendly', 'friendly']}
        
import pandas as pd
df = pd.DataFrame(data)

df['category']=df['label'].apply(lambda x: 1 if x=='friendly' else 0)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['input'],df['category'], stratify=df['category'])

bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)

def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embeding([
    &quot;we lied down around am rose&quot;, 
    &quot;i hate it i want my dj&quot;]
)

#Build model
# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)
</code></pre>
",2,1,2008,2021-05-08 20:52:53,https://stackoverflow.com/questions/67452037/how-to-preprocess-a-dataset-for-bert-model-implemented-in-tensorflow-2-x
"RuntimeError: Input, output and indices must be on the current device. (fill_mask(&quot;Random text &lt;mask&gt;.&quot;)","<p>I am getting &quot;RuntimeError: Input, output and indices must be on the current device.&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;)
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>The trainer trains your model automatically at GPU (<a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">default value no_cuda=False</a>). You can verify this by running:</p>
<pre class=""lang-py prettyprint-override""><code>model.device
</code></pre>
<p>after training. The pipeline does not this and this leads to the error you see (i.e. your model is on your GPU but your example sentence is on your CPU). You can fix that by either run the pipeline with GPU support as well:</p>
<pre class=""lang-py prettyprint-override""><code>fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer,
    device=0,
)
</code></pre>
<p>or by transferring your model to CPU before initializing the pipeline:</p>
<pre><code>model.to('cpu')
</code></pre>
",1,0,2341,2021-05-12 02:36:11,https://stackoverflow.com/questions/67496616/runtimeerror-input-output-and-indices-must-be-on-the-current-device-fill-mas
Using transformers class BertForQuestionAnswering for Extractive Question Answering,"<p>I'm using a BERT model for Extractive QA task with the <code>transformers</code> class library <code>BertForQuestionAnswering</code>.  Extractive Question Answering is the task of answering a question for a given context text and outputting the start and end indexes of where the answer matches in the context. <a href=""https://github.com/loretoparisi/hf-experiments/blob/master/src/bert/run.py"" rel=""nofollow noreferrer"">My code</a> is the following:</p>
<pre><code>model = BertForQuestionAnswering.from_pretrained('bert-base-uncased',
    cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;../../models&quot;))
question = &quot;What is the capital of Italy?&quot;
text = &quot;The capital of Italy is Rome.&quot;
inputs = tokenizer.encode_plus(question, text, return_tensors='pt')
start, end = model(**inputs)
start_max = torch.argmax(F.softmax(start, dim = -1))
end_max = torch.argmax(F.softmax(end, dim = -1)) + 1 ## add one ##because of python list indexing
answer = tokenizer.decode(inputs[&quot;input_ids&quot;][0][start_max : end_max])
print(answer)
</code></pre>
<p>I get this error</p>
<pre><code>start_max = torch.argmax(F.softmax(start, dim = -1))
  File &quot;/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 1583, in softmax
    ret = input.softmax(dim)
AttributeError: 'str' object has no attribute 'softmax'
</code></pre>
<p>I have also tried this approach, slightly different</p>
<pre><code>encoding = tokenizer.encode_plus(text=question,text_pair=text, add_special=True)
inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens
start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>but the error is likely the same:</p>
<pre><code>    start_index = torch.argmax(start_scores)
TypeError: argmax(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
<p>I assume due to the unpack of the output as</p>
<pre><code>start, end = model(**inputs)
</code></pre>
<p>If so, how to correct unpack this model's outputs?</p>
","python, bert-language-model, huggingface-transformers","<p>Due to version update, the model returns a dictionary and not a tuple of start, end.
You can add the following parameter:
<code>return_dict=False</code></p>
",1,1,1131,2021-05-12 21:38:05,https://stackoverflow.com/questions/67511285/using-transformers-class-bertforquestionanswering-for-extractive-question-answer
PipelineException: No mask_token ([MASK]) found on the input,"<p>I am getting this error &quot;PipelineException: No mask_token ([MASK]) found on the input&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer,
    device=0,
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;).     # This line is giving me the error...
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
<p>Complete error: &quot;f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,&quot;</p>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>Even if you have already found the error, a recommendation to avoid it in the future. Instead of calling</p>
<pre class=""lang-py prettyprint-override""><code>fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;)
</code></pre>
<p>you can do the following to be more flexible when you use different models:</p>
<pre class=""lang-py prettyprint-override""><code>MASK_TOKEN = tokenizer.mask_token

fill_mask(&quot;Auto Car {}.&quot;.format(MASK_TOKEN))
</code></pre>
",5,3,4073,2021-05-12 22:35:14,https://stackoverflow.com/questions/67511800/pipelineexception-no-mask-token-mask-found-on-the-input
Bert Tokenizer add_token function not working properly,"<p>tokenizer add_tokens is not adding new tokens.
Here is my code:</p>
<pre><code>  from transformers import BertTokenizer

  bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

  new_tokens = []
  text = open(&quot;parsed_data.txt&quot;, &quot;r&quot;)
  for line in text:
     for word in line.split():
        new_tokens.append(word) 

  print(len(new_tokens))      # 53966
  print(len(bert_tokenizer))  # 36369
  bert_tokenizer.add_tokens(new_tokens)
  print(len(bert_tokenizer))  # 36369
</code></pre>
","python, nlp, pytorch, bert-language-model","<p>Yes, if a token already exists, it is skipped. By the way, after changing the tokenizer you have to also update your model. See the last line below.</p>
<pre><code>bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.add_tokens(my_new_tokens)

model.resize_token_embeddings(len(bert_tokenizer))
</code></pre>
",2,0,526,2021-05-13 21:55:31,https://stackoverflow.com/questions/67526697/bert-tokenizer-add-token-function-not-working-properly
Python: BERT Tokenizer cannot be loaded,"<p>I am working on the <code>bert-base-mutilingual-uncased</code> model but when I try to set the <code>TOKENIZER</code> in the <code>config</code> it throws an <code>OSError</code>.</p>
<h3>Model Config</h3>
<pre><code>class config: 
    DEVICE = &quot;cuda:0&quot;
    MAX_LEN = 256
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 4
    EPOCHS = 1

    BERT_PATH = {&quot;bert-base-multilingual-uncased&quot;: &quot;workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased&quot;}
    MODEL_PATH = &quot;workspace/data/jigsaw-multilingual/model.bin&quot;

    TOKENIZER = transformers.BertTokenizer.from_pretrained(
            BERT_PATH[&quot;bert-base-multilingual-uncased&quot;], 
            do_lower_case=True)
</code></pre>
<h3>Error</h3>
<pre><code>    ---------------------------------------------------------------------------
    OSError                                   Traceback (most recent call last)
    &lt;ipython-input-33-83880b6b788e&gt; in &lt;module&gt;
    ----&gt; 1 class config:
          2 #     def __init__(self):
          3 
          4         DEVICE = &quot;cuda:0&quot;
          5         MAX_LEN = 256
    
    &lt;ipython-input-33-83880b6b788e&gt; in config()
         11         TOKENIZER = transformers.BertTokenizer.from_pretrained(
         12             BERT_PATH[&quot;bert-base-multilingual-uncased&quot;],
    ---&gt; 13             do_lower_case=True)
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, *inputs, **kwargs)
       1138 
       1139         &quot;&quot;&quot;
    -&gt; 1140         return cls._from_pretrained(*inputs, **kwargs)
       1141 
       1142     @classmethod
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
       1244                     &quot;, &quot;.join(s3_models),
       1245                     pretrained_model_name_or_path,
    -&gt; 1246                     list(cls.vocab_files_names.values()),
       1247                 )
       1248             )
    
    OSError: Model name 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was not  
 found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking,   
bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc,   
bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1,     
wietsedv/bert-base-dutch-cased). 

We assumed 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was a path, a model   identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such  
 vocabulary files at this path or url.
</code></pre>
<p>As I can interpret the error, it says that the <code>vocab.txt</code> file was not found at the given location but actually its present.</p>
<p>Following are the files available in the <code>bert-base-multilingual-uncased</code> folder:</p>
<ul>
<li><code>config.json</code></li>
<li><code>pytorch_model.bin</code></li>
<li><code>vocab.txt</code></li>
</ul>
<p>I am new to working with <code>bert</code>, so I am not sure if there is a different way to define the tokenizer.</p>
","python, nlp, pytorch, bert-language-model, huggingface-transformers","<p>I think this should work:</p>
<pre><code>from transformers import BertTokenizer
TOKENIZER = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)
</code></pre>
<p>It will download the tokenizer from huggingface.</p>
",2,-1,2205,2021-05-17 10:03:29,https://stackoverflow.com/questions/67567587/python-bert-tokenizer-cannot-be-loaded
Copy one layer&#39;s weights from one Huggingface BERT model to another,"<p>I have a pre-trained model which I load like so:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel
model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>
<p>I want to create a new model with the same architecture, and random initial weights, <em>except</em> for the embedding layer:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)
</code></pre>
<p>It seems I can do this to create a new model with the same architecture, but then <em>all</em> the weights are random:</p>
<pre><code>configuration   = model.config
untrained_model = BertForSequenceClassification(configuration)
</code></pre>
<p>So how do I copy over <code>model</code>'s embedding layer weights to the new <code>untrained_model</code>?</p>
","python, bert-language-model, huggingface-transformers","<p>Weights and bias are just tensor and you can simply copy them with <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_"" rel=""nofollow noreferrer"">copy_</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertForSequenceClassification, BertConfig
jetfire = BertForSequenceClassification.from_pretrained('bert-base-cased')
config = BertConfig.from_pretrained('bert-base-cased')

optimus = BertForSequenceClassification(config)

parts = ['bert.embeddings.word_embeddings.weight'
,'bert.embeddings.position_embeddings.weight'              
,'bert.embeddings.token_type_embeddings.weight'    
,'bert.embeddings.LayerNorm.weight'
,'bert.embeddings.LayerNorm.bias']

def joltElectrify (jetfire, optimus, parts):
  target = dict(optimus.named_parameters())
  source = dict(jetfire.named_parameters())

  for part in parts:
    target[part].data.copy_(source[part].data)  

joltElectrify(jetfire, optimus, parts)
</code></pre>
",4,2,4581,2021-05-25 13:41:01,https://stackoverflow.com/questions/67689219/copy-one-layers-weights-from-one-huggingface-bert-model-to-another
Are these normal speed of Bert Pretrained Model Inference in PyTorch,"<p>I am testing Bert base and Bert distilled model in Huggingface with 4 scenarios of speeds, batch_size = 1:</p>
<pre><code>1) bert-base-uncased: 154ms per request
2) bert-base-uncased with quantifization: 94ms per request
3) distilbert-base-uncased: 86ms per request
4) distilbert-base-uncased with quantifization: 69ms per request
</code></pre>
<p>I am using the IMDB text as experimental data and set the max_length=512, so it's quite long. The cpu on Ubuntu 18.04 info is below:</p>
<pre><code>cat /proc/cpuinfo  | grep 'name'| uniq
model name  : Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre>
<p>The machine has 3 GPU available for use:</p>
<pre><code>Tesla V100-SXM2
</code></pre>
<p>It seems quite slow for realtime application. Are those speeds normal for bert base model?</p>
<p>The testing code is below:</p>
<pre><code>import pandas as pd
import torch.quantization

from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel

def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    outputs = model(**inputs)
    output_tensors = outputs[0][0]
    output_numpy = output_tensors.detach().numpy()
    embedding = output_numpy.tolist()[0]

def process_text(model, tokenizer, text_lines):
    for index, line in enumerate(text_lines):
        embedding = get_embedding(model, tokenizer, line)
        if index % 100 == 0:
            print('Current index: {}'.format(index))

import time
from datetime import timedelta
if __name__ == &quot;__main__&quot;:

    df = pd.read_csv('../data/train.csv', sep='\t')
    df = df.head(1000)
    text_lines = df['review']
    text_line_count = len(text_lines)
    print('Text size: {}'.format(text_line_count))

    start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end = time.time()
    print('Total time spent with bert base: {}'.format(str(timedelta(seconds=end - start))))

    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end2 = time.time()
    print('Total time spent with bert base quantization: {}'.format(str(timedelta(seconds=end2 - end))))

    tokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end3 = time.time()
    print('Total time spent with distilbert: {}'.format(str(timedelta(seconds=end3 - end2))))

    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end4 = time.time()
    print('Total time spent with distilbert quantization: {}'.format(str(timedelta(seconds=end4 - end3))))
</code></pre>
<p>EDIT: based on suggestion I changed to the following:</p>
<pre><code>inputs = tokenizer(text_batch, padding=True, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>Where text_batch is a list of text as input.</p>
","bert-language-model, huggingface-transformers, transformer-model, huggingface-tokenizers","<p>No, you can speed it up.</p>
<p>First, why are you testing it with batch size 1?</p>
<p>Both <code>tokenizer</code> and <code>model</code> accept batched inputs. Basically, you can pass a 2D array/list that contains a single sample at each row. See the documentation for tokenizer: <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call_"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call_</a>_ The same applies for the models.</p>
<p>Also, your for loop is sequential even if you use batch size larger than 1. You can create a test data and then use <code>Trainer</code> class with <code>trainer.predict()</code></p>
<p>Also see this discussion of mine at the HF forums: <a href=""https://discuss.huggingface.co/t/urgent-trainer-predict-and-model-generate-creates-totally-different-predictions/3426"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/urgent-trainer-predict-and-model-generate-creates-totally-different-predictions/3426</a></p>
",2,2,1766,2021-05-26 06:07:27,https://stackoverflow.com/questions/67699354/are-these-normal-speed-of-bert-pretrained-model-inference-in-pytorch
XLM/BERT sequence outputs to pooled output with weighted average pooling,"<p>Let's say I have a tokenized sentence of length 10, and I pass it to a BERT model.</p>
<pre class=""lang-py prettyprint-override""><code>bert_out = bert(**bert_inp)
hidden_states = bert_out[0]
hidden_states.shape
&gt;&gt;&gt;torch.Size([1, 10, 768])
</code></pre>
<p>This returns me a tensor of shape: [<em>batch_size, seq_length, d_model</em>] where each word in sequence is encoded as a 768-dimentional vector</p>
<p>In TensorFlow BERT also returns a so called pooled output which corresponds to a vector representation of a whole sentence.<br />
I want to obtain it by taking a weighted average of sequence vectors and the way I do it is:</p>
<pre class=""lang-py prettyprint-override""><code>hidden_states.view(-1, 10).shape
&gt;&gt;&gt; torch.Size([768, 10])

pooled = nn.Linear(10, 1)(hidden_states.view(-1, 10))
pooled.shape
&gt;&gt;&gt; torch.Size([768, 1])
</code></pre>
<ul>
<li>Is it the right way to proceed, or should I just flatten the whole thing and then apply linear?</li>
<li>Any other ways to obtain a good sentence representation?</li>
</ul>
","python, nlp, pytorch, bert-language-model, attention-model","<p>There are two simple ways to get a sentence representation:</p>
<ul>
<li>Get the vector for the <code>CLS</code> token.</li>
<li>Get the <code>pooler_output</code></li>
</ul>
<p>Assuming the input is <code>[batch_size, seq_length, d_model]</code>, where <code>batch_size</code> is the number of sentences, then to get the CLS token for every sentence:</p>
<pre><code>bert_out = bert(**bert_inp)
hidden_states = bert_out['last_hidden_state']
cls_tokens = hidden_states[:, 0, :]  # 0 for the CLS token for every sentence.
</code></pre>
<p>You will have a tensor with shape (batch_size, d_model).</p>
<p>To get the <code>pooler_output</code>:</p>
<pre><code>bert_out = bert(**bert_inp)
pooler_output = bert_out['pooler_output']
</code></pre>
<p>Again you get a tensor with shape (batch_size, d_model).</p>
",1,0,664,2021-05-26 10:39:21,https://stackoverflow.com/questions/67703260/xlm-bert-sequence-outputs-to-pooled-output-with-weighted-average-pooling
Huggingface Electra - Load model trained with google implementation error: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 64: invalid start byte,"<p>I have trained an electra model from scratch using <a href=""https://github.com/google-research/electra"" rel=""nofollow noreferrer"">google implementation code</a>.</p>
<pre class=""lang-sh prettyprint-override""><code>python run_pretraining.py --data-dir gc://bucket-electra/dataset/ --model-name greek_electra --hparams hparams.json
</code></pre>
<p>with this json hyperparams:</p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;embedding_size&quot;: 768,
&quot;max_seq_length&quot;: 512,
&quot;train_batch_size&quot;: 128,
&quot;vocab_size&quot;: 100000,
&quot;model_size&quot;: &quot;base&quot;,
&quot;num_train_steps&quot;: 1500000
}
</code></pre>
<p>After having trained the model, I used the <a href=""https://github.com/huggingface/transformers/blob/d5d7d886128732091e92afff7fcb3e094c71a7ec/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">convert_electra_original_tf_checkpoint_to_pytorch.py</a> script from transformers library to convert the checkpoint.</p>
<pre class=""lang-sh prettyprint-override""><code>python convert_electra_original_tf_checkpoint_to_pytorch.py --tf_checkpoint_path output/models/transformer/greek_electra --config_file resources/hparams.json --pytorch_dump_path output/models/transformer/discriminator  --discriminator_or_generator &quot;discriminator&quot;
</code></pre>
<p>Now I am trying to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import ElectraForPreTraining

model = ElectraForPreTraining.from_pretrained('discriminator')
</code></pre>
<p>but I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 427, in get_config_dict
    config_dict = cls._dict_from_json_file(resolved_config_file)
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 510, in _dict_from_json_file
    text = reader.read()
  File &quot;/usr/lib/python3.9/codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte
</code></pre>
<p>Any ideas what's causing this &amp; how to solve it?</p>
","python, tensorflow, pytorch, bert-language-model, huggingface-transformers","<p>It seems that @npit is right. The output of the convert_electra_original_tf_checkpoint_to_pytorch.py does not contain the configuration that I gave (hparams.json), therefore I created an ElectraConfig object -- with the same parameters -- and provided it to the from_pretrained function. That solved the issue.</p>
",1,1,1271,2021-05-28 14:12:50,https://stackoverflow.com/questions/67740498/huggingface-electra-load-model-trained-with-google-implementation-error-utf
Why does Transformer&#39;s BERT (for sequence classification) output depend heavily on maximum sequence length padding?,"<p>I am using Transformer's RobBERT (the dutch version of RoBERTa) for sequence classification - trained for sentiment analysis on the Dutch Book Reviews dataset.</p>
<p>I wanted to test how well it works on a similar dataset (also on sentiment analysis), so I made annotations for a set of text fragments and checked its accuracy. When I checked what kind of sentence are misclassified, I noticed that the output for a unique sentence depends heavily on the length of padding I give when tokenizing. See code below.</p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch.nn.functional as F
import torch


model = RobertaForSequenceClassification.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, num_labels=2)
tokenizer = RobertaTokenizer.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, do_lower_case=True)

sent = 'De samenwerking gaat de laatste tijd beter'
max_seq_len = 64


test_token = tokenizer(sent,
                        max_length = max_seq_len,
                        padding = 'max_length',
                        truncation = True,
                        return_tensors = 'pt'
                        )

out = model(test_token['input_ids'],test_token['attention_mask'])

probs = F.softmax(out[0], dim=1).detach().numpy()
</code></pre>
<p>For the given sample text, which translates in English to &quot;The collaboration has been improving lately&quot;, there is a huge difference in output on classification depending on the max_seq_len. Namely, for <code>max_seq_len = 64</code> the output for <code>probs</code> is:</p>
<p>[[0.99149346 0.00850648]]</p>
<p>whilst for <code>max_seq_len = 9</code>, being the actual length including cls tokens:</p>
<p>[[0.00494814 0.9950519 ]]</p>
<p>Can anyone explain why this huge difference in classification is happening? I would think that the attention mask ensures that in the output there is no difference because of padding to the max sequence length.</p>
","sentiment-analysis, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>This is caused because your comparison isn't correct. The sentence <code>De samenwerking gaat de laatste tijd beter</code> has actually 16 tokens (+2 for the specialtokens) and not 9. You only counted the words which are not necessarily the tokens.</p>
<pre class=""lang-py prettyprint-override""><code>print(tokenizer.tokenize(sent))
print(len(tokenizer.tokenize(sent)))
</code></pre>
<p>Output:</p>
<pre><code>['De', 'Ġsam', 'en', 'wer', 'king', 'Ġga', 'at', 'Ġde', 'Ġla', 'at', 'ste', 'Ġt', 'ij', 'd', 'Ġbe', 'ter']
16
</code></pre>
<p>When you set the sequence length to 9 you are truncating the sentence to:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.decode(tokenizer(sent,
                         max_length = 9,
                         padding = 'max_length',
                         truncation = True,
                         return_tensors = 'pt', 
                         add_special_tokens=False
                         )['input_ids'][0])
</code></pre>
<p>Output:</p>
<pre><code>'De samenwerking gaat de la'
</code></pre>
<p>And as final prove, the output when you set <code>max_length</code> to 52 is also [[0.99149346 0.00850648]].</p>
",3,2,1275,2021-05-31 09:33:47,https://stackoverflow.com/questions/67771257/why-does-transformers-bert-for-sequence-classification-output-depend-heavily
How to test masked language model after training it?,"<p>I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.</p>
<p>Tutorial: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb"" rel=""noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb</a></p>
<p>I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.</p>
<p><a href=""https://i.sstatic.net/dLyex.png"" rel=""noreferrer"">Example of what I want to do with my trained model</a></p>
<p>On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:</p>
<pre><code>&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
&gt;&gt;&gt; unmasker(&quot;Hello I'm a [MASK] model.&quot;)

[{'sequence': &quot;[CLS] hello i'm a fashion model. [SEP]&quot;,
  'score': 0.1073106899857521,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': &quot;[CLS] hello i'm a role model. [SEP]&quot;,
  'score': 0.08774490654468536,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': &quot;[CLS] hello i'm a new model. [SEP]&quot;,
  'score': 0.05338378623127937,
  'token': 2047,
  'token_str': 'new'},
 {'sequence': &quot;[CLS] hello i'm a super model. [SEP]&quot;,
  'score': 0.04667217284440994,
  'token': 3565,
  'token_str': 'super'},
 {'sequence': &quot;[CLS] hello i'm a fine model. [SEP]&quot;,
  'score': 0.027095865458250046,
  'token': 2986,
  'token_str': 'fine'}
</code></pre>
<p>Any help on how to do this would be great.</p>
","python, nlp, bert-language-model, huggingface-transformers","<p>This depends a lot of your task. Your task seems to be masked language modelling, that, is to predict one or more masked words:</p>
<p><strong>today I ate ___ .</strong></p>
<p>(pizza) or (pasta) could be equally correct, so you cannot use a metric such as accuray. But (water) should be less &quot;correct&quot; than the other two.
So what you normally do is to check how &quot;surprised&quot; the language model is, on an evaluation data set. This metric is called <a href=""https://huggingface.co/transformers/perplexity.html"" rel=""noreferrer"">perplexity</a>.
Therefore, before  and after you finetune a model on you specific dataset, you would calculate the perplexity and you would expect it to be lower after finetuning. The model should be more used to your specific vocabulary etc. And that is how you <strong>test</strong> your model.</p>
<p>As you can see, they calculate the perplexity in the tutorial you mentioned:</p>
<pre><code>import math
eval_results = trainer.evaluate()
print(f&quot;Perplexity: {math.exp(eval_results['eval_loss']):.2f}&quot;) 
</code></pre>
<p>To <strong>predict</strong> samples, you need to tokenize those samples and prepare the input for the model. The Fill-mask-Pipeline can do this for you:</p>
<pre><code># if you trained your model on gpu you need to add this line:
trainer.model.to('cpu')

unmasker = pipeline('fill-mask', model=trainer.model, tokenizer=tokenizer)
unmasker(&quot;today I ate &lt;mask&gt;&quot;)
</code></pre>
<p>which results in the following output:</p>
<pre><code>[{'score': 0.23618391156196594,
  'sequence': 'today I ate it.',
  'token': 24,
  'token_str': ' it'},
 {'score': 0.03940323367714882,
  'sequence': 'today I ate breakfast.',
  'token': 7080,
  'token_str': ' breakfast'},
 {'score': 0.033759087324142456,
  'sequence': 'today I ate lunch.',
  'token': 4592,
  'token_str': ' lunch'},
 {'score': 0.025962186977267265,
  'sequence': 'today I ate pizza.',
  'token': 9366,
  'token_str': ' pizza'},
 {'score': 0.01913984678685665,
  'sequence': 'today I ate them.',
  'token': 106,
  'token_str': ' them'}]
</code></pre>
",8,6,3348,2021-06-05 15:49:48,https://stackoverflow.com/questions/67851322/how-to-test-masked-language-model-after-training-it
Huggingface SciBERT predict masked word not working,"<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to predict masked words in scientific/biomedical text.  This produces errors, and not sure how to move forward from this point.</p>
<p>Here is the code so far -</p>
<pre><code>!pip install transformers

from transformers import pipeline, AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

model = AutoModel.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>This works with BERT alone, but is not the specialized pre-trained model -</p>
<pre><code>!pip install transformers

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>The errors with SciBERT are -</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    494         kwargs[&quot;feature_extractor&quot;] = feature_extractor
    495 
--&gt; 496     return task_class(model=model, framework=framework, task=task, **kwargs)

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/fill_mask.py in __init__(self, model, tokenizer, modelcard, framework, args_parser, device, top_k, task)
     73         )
     74 
---&gt; 75         self.check_model_type(TF_MODEL_WITH_LM_HEAD_MAPPING if self.framework == &quot;tf&quot; else MODEL_FOR_MASKED_LM_MAPPING)
     76         self.top_k = top_k
     77 

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py in check_model_type(self, supported_models)
    652                 self.task,
    653                 self.model.base_model_prefix,
--&gt; 654                 f&quot;The model '{self.model.__class__.__name__}' is not supported for {self.task}. Supported models are {supported_models}&quot;,
    655             )
    656 

PipelineException: The model 'BertModel' is not supported for fill-mask. Supported models are ['BigBirdForMaskedLM', 'Wav2Vec2ForMaskedLM', 'ConvBertForMaskedLM', 'LayoutLMForMaskedLM', 'DistilBertForMaskedLM', 'AlbertForMaskedLM', 'BartForConditionalGeneration', 'MBartForConditionalGeneration', 'CamembertForMaskedLM', 'XLMRobertaForMaskedLM', 'LongformerForMaskedLM', 'RobertaForMaskedLM', 'SqueezeBertForMaskedLM', 'BertForMaskedLM', 'MegatronBertForMaskedLM', 'MobileBertForMaskedLM', 'FlaubertWithLMHeadModel', 'XLMWithLMHeadModel', 'ElectraForMaskedLM', 'ReformerForMaskedLM', 'FunnelForMaskedLM', 'MPNetForMaskedLM', 'TapasForMaskedLM', 'DebertaForMaskedLM', 'DebertaV2ForMaskedLM', 'IBertForMaskedLM']
</code></pre>
","python, bert-language-model, huggingface-transformers","<p>As the error message tells you, you need to use <a href=""https://huggingface.co/transformers/model_doc/auto.html?highlight=automodelformaskedlm#transformers.AutoModelForMaskedLM"" rel=""nofollow noreferrer"">AutoModelForMaskedLM</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>Output:</p>
<pre><code>[{'sequence': 'the patient is a 55 year old woman admitted with pneumonia',
  'score': 0.4025486707687378,
  'token': 10221,
  'token_str': 'woman'},
 {'sequence': 'the patient is a 55 year old man admitted with pneumonia',
  'score': 0.23970800638198853,
  'token': 508,
  'token_str': 'man'},
 {'sequence': 'the patient is a 55 year old female admitted with pneumonia',
  'score': 0.15444642305374146,
  'token': 3672,
  'token_str': 'female'},
 {'sequence': 'the patient is a 55 year old male admitted with pneumonia',
  'score': 0.1111455038189888,
  'token': 3398,
  'token_str': 'male'},
 {'sequence': 'the patient is a 55 year old boy admitted with pneumonia',
  'score': 0.015877680853009224,
  'token': 12481,
  'token_str': 'boy'}]
</code></pre>
",1,0,667,2021-06-07 13:42:11,https://stackoverflow.com/questions/67872803/huggingface-scibert-predict-masked-word-not-working
How to prepare text for BERT - getting error,"<p>I am trying to learn BERT for text classification. I am finding some problem in preparing data for using BERT.</p>
<p>From my Dataset, I am segregating the sentiments and reviews as:</p>
<pre><code>X = df['sentiments']
y = df['reviews'] #it contains four different class of reviews
</code></pre>
<p>Next,</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512) 
</code></pre>
<p>Here is where I get error:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-70-22714fcf7991&gt; in &lt;module&gt;()
----&gt; 1 train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)
      2 #valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)

/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2261         if not _is_valid_text_input(text):
   2262             raise ValueError(
-&gt; 2263                 ""text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) ""
   2264                 ""or `List[List[str]]` (batch of pretokenized examples).""
   2265             )

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).</code></pre>
</div>
</div>
</p>
<p>When I am trying to convert X to list and use it, I get another error:</p>
<pre><code>TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]
</code></pre>
<p>Can someone please explain where the problem is? Previously I followed a tutorial on 20 news dataset and it worked. But now when I am using it for another project, it doesn't work and I feel sad.</p>
<p>Thanks.</p>
","python-3.x, nlp, bert-language-model, transfer-learning","<p>The error is because, your <code>X = df['sentiments']</code> and <code>y = df['reviews']</code> lines, where your X and y are still dataframe columns (or dataframe series), and not list. A simplet way to change them is:</p>
<p><code>X = df['sentiments'].values</code> and <code>y = df['reviews'].values</code></p>
<p>which returns numpy array, and it works. If notit can be further converted to python list using</p>
<p><code>X = df['sentiments'].values.tolist()</code> and <code>y = df['reviews'].values.tolist()</code></p>
",7,4,7526,2021-06-09 17:27:39,https://stackoverflow.com/questions/67909030/how-to-prepare-text-for-bert-getting-error
Hugging Face: NameError: name &#39;sentences&#39; is not defined,"<p>I am following this tutorial here: <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a> - though, I am coming across an error, and I think the tutorial is missing an import, but i do not know which.</p>
<p>These are my current imports:</p>
<pre><code># Transformers installation
! pip install transformers
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/transformers.git

! pip install datasets transformers

from transformers import pipeline
</code></pre>
<p>Current code:</p>
<pre><code>from datasets import load_dataset

raw_datasets = load_dataset(&quot;imdb&quot;)
</code></pre>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<pre><code>inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)
</code></pre>
<p>The error:</p>
<pre><code>NameError                                 Traceback (most recent call last)

&lt;ipython-input-9-5a234f114e2e&gt; in &lt;module&gt;()
----&gt; 1 inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)

NameError: name 'sentences' is not defined
</code></pre>
","python, bert-language-model, huggingface-transformers, huggingface-tokenizers, huggingface-datasets","<p>The error states that you do not have a variable called <code>sentences</code> in the scope. I believe the tutorial presumes you already have a list of sentences and are tokenizing it.</p>
<p>Have a look at the <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"" rel=""nofollow noreferrer"">documentation</a> The first argument can be either a string or list of string or list of list of strings.</p>
<pre><code>__call__(text: Union[str, List[str], List[List[str]]],...)
</code></pre>
",0,-1,6950,2021-06-14 15:00:03,https://stackoverflow.com/questions/67972661/hugging-face-nameerror-name-sentences-is-not-defined
Use BERT under spaCy to get sentence embeddings,"<p>I am trying to use BERT to get sentence embeddings. Here is how I am doing it:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_trf&quot;)
nlp(&quot;The quick brown fox jumps over the lazy dog&quot;).vector 
</code></pre>
<p>This outputs an empty vector!!</p>
<pre><code>array([], dtype=float32)
</code></pre>
<p>Am I missing something?</p>
","python, nlp, spacy, bert-language-model","<p>Transformers are a bit different than the other spacy models, but you can use
<code>doc._.trf_data.tensors[1]</code>.</p>
<p>The vectors for the individual BPE (Byte Pair Encoding) token-pieces are in <code>doc._.trf_data.tensors[0]</code>. Note that I use the term <em>token-pieces</em>  rather than <em>tokens</em>, to prevent confusion between spacy tokens and the tokens that are produced by the BPE tokenizer.</p>
<p>E.g., in our case the spacy-tokens are:</p>
<pre><code>for i, spacy_tok in enumerate(doc):
  print(f&quot;spacy-token {i + 1}: {spacy_tok.text}&quot;)
</code></pre>
<pre><code>spacy-token 1: The
spacy-token 2: quick
spacy-token 3: brown
spacy-token 4: fox
spacy-token 5: jumps
spacy-token 6: over
spacy-token 7: the
spacy-token 8: lazy
spacy-token 9: dog
</code></pre>
<p>and the token-pieces are:</p>
<pre><code>for i, tok_piece in enumerate(doc._.trf_data.tokens['input_texts'][0]):
  print(f&quot;token-piece {i + 1}: {tok_piece}&quot;)
</code></pre>
<pre><code>token-piece 1: &lt;s&gt;
token-piece 2: The
token-piece 3: Ġquick
token-piece 4: Ġbrown
token-piece 5: Ġfox
token-piece 6: Ġjumps
token-piece 7: Ġover
token-piece 8: Ġthe
token-piece 9: Ġlazy
token-piece 10: Ġdog
token-piece 11: &lt;/s&gt;
</code></pre>
",7,7,2619,2021-06-14 20:42:37,https://stackoverflow.com/questions/67976977/use-bert-under-spacy-to-get-sentence-embeddings
Must the vocab size must math the vocab_size in bert_config.json exactly?,"<p>I am seeing someone other's BERT model, in which the vocab.txt's size is 22110, but the <code>vocab_size</code> parameter's value is 21128 in bert_config.json.</p>
<p>I understand that these two numbers must be exactly the same. Is that right?</p>
","bert-language-model, huggingface-transformers, transformer-model","<p>If it is really BERT that uses WordPiece tokenizer, then yes. Different lengths of vocabulary and <code>vocab_size</code> in the config would mean that there are either embeddings that can never be used or that there are vocabulary items without any embeddings.</p>
<p>In this case, you will see no error message because the model and the tokenizer are loaded separately. The embedding table of BERT has 8 embeddings that are no &quot;reachable&quot;.</p>
<p>Note, however, that the model may use some very non-standard tokenizer that saves the vocabulary in such a way, it is 8 items shorter (although it is quite unlikely).</p>
",3,3,2101,2021-06-15 04:07:50,https://stackoverflow.com/questions/67979876/must-the-vocab-size-must-math-the-vocab-size-in-bert-config-json-exactly
How do I show the other sentiment scores from text classification?,"<p>I am doing sentiment analysis, and I was wondering how to <strong>show the other sentiment scores</strong> from classifying my sentence: &quot;Tesla's stock just increased by 20%.&quot;</p>
<p>I have three sentiments: <strong>positive</strong>, <strong>negative</strong> and <strong>neutral</strong>.</p>
<p>This is my code, which contains the sentence I want to classify:</p>
<pre><code>pip install happytransformer
from happytransformer import HappyTextClassification 
happy_tc = HappyTextClassification(&quot;BERT&quot;, &quot;ProsusAI/finbert&quot;, num_labels=3)

result = happy_tc.classify_text(&quot;Tesla's stock just increased by 20%&quot;)
</code></pre>
<p>This is the result code and output:</p>
<pre><code>print(result)

TextClassificationResult(label='positive', score=0.929110586643219)
</code></pre>
<p>This is the sentiment score, which only shows the score for positive:</p>
<pre><code>print(result.label)
print(result.score)

positive
0.92
</code></pre>
<p>Now, how do I make it so that it shows the sentiment scores for negative and neutral as well as the positive?</p>
<p>Something that looks like this:</p>
<pre><code>positive
0.92

negative
0.05

neutral
0.03
</code></pre>
<p>Thanks.</p>
","python, text-classification, bert-language-model","<p>Because HappyTransformer does not support multi class probabilities I suggest to use another library. The library <code>flair</code> provides even more functionality and can give you your desired multi class probabilities, with something like this:</p>
<pre><code>from flair.models import TextClassifier
from flair.data import Sentence

tc = TextClassifier.load('en-sentiment')

sentence = Sentence('Flair is pretty neat!')

tc.predict(sentence, multi_class_prob=True)

print('Sentence above is: ', sentence.labels)

</code></pre>
<p>Just <code>pip install flair</code> for usage.</p>
<p>Note that we use a different model than BERT and only returns two labels not three.</p>
<p>Another option is to use the <code>HuggingFace</code> library. It allows for using self defined labels.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)
classifier(
    &quot;This is a course about the Transformers library&quot;,
    candidate_labels=[&quot;education&quot;, &quot;politics&quot;, &quot;business&quot;],
)
</code></pre>
<pre><code>{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
</code></pre>
<p>In your case, you switch out the labels to <code>[&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;]</code>.</p>
<p>The examples are taken from: <a href=""https://huggingface.co/course/chapter1/3?fw=pt"" rel=""nofollow noreferrer"">https://huggingface.co/course/chapter1/3?fw=pt</a></p>
",0,0,806,2021-06-15 13:59:16,https://stackoverflow.com/questions/67987738/how-do-i-show-the-other-sentiment-scores-from-text-classification
I&#39;m using bert pre-trained model for question and answering. It&#39;s returning correct result but with lot of spaces between the text,"<p><strong>I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text</strong></p>
<p>The code is below :</p>
<pre><code>def get_answer_using_bert(question, reference_text):
  
  bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  input_ids = bert_tokenizer.encode(question, reference_text)
  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)

  sep_location = input_ids.index(bert_tokenizer.sep_token_id)
  first_seg_len, second_seg_len = sep_location + 1, len(input_ids) - (sep_location + 1)
  seg_embedding = [0] * first_seg_len + [1] * second_seg_len

  model_scores = bert_model(torch.tensor([input_ids]), 
  token_type_ids=torch.tensor([seg_embedding]))
  ans_start_loc, ans_end_loc = torch.argmax(model_scores[0]), torch.argmax(model_scores[1])
  result = ' '.join(input_tokens[ans_start_loc:ans_end_loc + 1])

  result = result.replace('#', '')
  return result
</code></pre>
<p>Followed by code below :</p>
<pre><code>reference_text = 'Mukesh Dhirubhai Ambani was born on 19 April 1957 in the British Crown colony of Aden (present-day Yemen) to Dhirubhai Ambani and Kokilaben Ambani. He has a younger brother Anil Ambani and two sisters, Nina Bhadrashyam Kothari and Dipti Dattaraj Salgaonkar. Ambani lived only briefly in Yemen, because his father decided to move back to India in 1958 to start a trading business that focused on spices and textiles. The latter was originally named Vimal but later changed to Only Vimal His family lived in a modest two-bedroom apartment in Bhuleshwar, Mumbai until the 1970s. The family financial status slightly improved when they moved to India but Ambani still lived in a communal society, used public transportation, and never received an allowance. Dhirubhai later purchased a 14-floor apartment block called Sea Wind in Colaba, where, until recently, Ambani and his brother lived with their families on different floors.'
question = 'What is the name of mukesh ambani brother?'

get_answer_using_bert(question, reference_text)
</code></pre>
<p>And the output is :</p>
<pre><code>'an il am ban i'
</code></pre>
<p><strong>Can anyone help me how to fix this issue. It would be really helpful.</strong></p>
","deep-learning, nlp, bert-language-model, huggingface-transformers, nlp-question-answering","<p>You can just use the tokenizer <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.decode"" rel=""nofollow noreferrer"">decode</a> function:</p>
<pre class=""lang-py prettyprint-override""><code>bert_tokenizer.decode(input_ids[ans_start_loc:ans_end_loc +1])
</code></pre>
<p>Output:</p>
<pre><code>'anil ambani'
</code></pre>
<p>In case you do not want to use decode, you can use:</p>
<pre class=""lang-py prettyprint-override""><code>result.replace(' ##', '')
</code></pre>
",1,0,543,2021-06-15 16:57:22,https://stackoverflow.com/questions/67990545/im-using-bert-pre-trained-model-for-question-and-answering-its-returning-corr
Initialize HuggingFace Bert with random weights,"<p>How is it possible to initialize BERT with random weights? I want to compare the performance of multilingual vs monolingual vs randomly initialized BERT in a masked language modeling task. While in the former cases it is very straightforward:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM

tokenizer_multi = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model_multi = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')
model_multi.eval()

tokenizer_mono = BertTokenizer.from_pretrained('bert-base-cased')
model_mono = BertForMaskedLM.from_pretrained('bert-base-cased')
model_mono.eval()
</code></pre>
<p>I don't know how to load random weights.</p>
<p>Thanks in advance!</p>
","bert-language-model, huggingface-transformers","<p>You can initialize a random BERT model using the Hugginface capabilites (from the documentation <a href=""https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/bert#transformers.BertConfig"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/bert#transformers.BertConfig</a>)</p>
<pre><code>from transformers import BertConfig, BertModel

# Initializing a BERT bert-base-uncased style configuration
configuration = BertConfig()

# Initializing a model (with random weights) from the bert-base-uncased style configuration
model = BertModel(configuration)

# Accessing the model configuration
configuration = model.config
</code></pre>
",4,6,4642,2021-06-20 17:57:01,https://stackoverflow.com/questions/68058647/initialize-huggingface-bert-with-random-weights
HuggingFace SciBert AutoModelForMaskedLM cannot be imported,"<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to evaluate masked words in scientific/biomedical text for bias using CrowS-Pairs (<a href=""https://github.com/nyu-mll/crows-pairs/"" rel=""nofollow noreferrer"">https://github.com/nyu-mll/crows-pairs/</a>).  The CrowS-Pairs code works great with the built in models like BERT.</p>
<p>I modified the code of metric.py with the goal of allowing an option of using the SciBERT model -</p>
<pre><code>import os
import csv
import json
import math
import torch
import argparse
import difflib
import logging
import numpy as np
import pandas as pd

from transformers import BertTokenizer, BertForMaskedLM
from transformers import AlbertTokenizer, AlbertForMaskedLM
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import AutoTokenizer, AutoModelForMaskedLM
</code></pre>
<p>and get the following error</p>
<pre><code>2021-06-21 17:11:38.626413: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File &quot;metric.py&quot;, line 15, in &lt;module&gt;
    from transformers import AutoTokenizer, AutoModelForMaskedLM
ImportError: cannot import name 'AutoModelForMaskedLM' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)
</code></pre>
<p>Later in the Python file, the AutoTokenizer and AutoModelForMaskedLM are defined as</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;) 
</code></pre>
<p>Libraries</p>
<pre><code>huggingface-hub-0.0.8
sacremoses-0.0.45
tokenizers-0.10.3
transformers-4.7.0 
</code></pre>
<p>The error occurs with and without GPU support.</p>
","python, python-3.x, bert-language-model, huggingface-transformers","<p>Try this:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;, do_lower_case=True)

model = BertForMaskedLM.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
</code></pre>
",1,0,1317,2021-06-21 17:18:21,https://stackoverflow.com/questions/68072148/huggingface-scibert-automodelformaskedlm-cannot-be-imported
Processing dictionary with zip function keeping the same structure in Python,"<p>I have a dictionary where each key is mapped to a list of arrays, except the key &quot;reference&quot; which is just an array of Integers.</p>
<p>cls[&quot;input_ids&quot;], cls[&quot;attention_masks&quot;],  cls[&quot;labels&quot;], cls[&quot;reference&quot;]</p>
<p>Each row of a key is linked to the row of the other keys (this is a modified output of a Bert tokenizer)</p>
<p>I would like to filter out some rows by the reference value and keeping in output the same dictionary structure, right now the only way I manage to do this is like this:
I put some random data to give an idea</p>
<pre><code>
cls= {&quot;input_ids&quot;:[[22,22,22],[33,33,33]], &quot;attention_masks&quot;:[[22,22,22],[33,33,33]], &quot;reference&quot;:[1,0], &quot;labels&quot;:[[[22,22,22],[33,33,33]]]}
mcp = {&quot;input_ids&quot;:[], &quot;attention_masks&quot;:[], &quot;reference&quot;:[], &quot;labels&quot;:[]}

        for el in zip(cls[&quot;input_ids&quot;], cls[&quot;attention_masks&quot;], cls[&quot;reference&quot;], cls[&quot;labels&quot;]):
            if el[2] == 1:
                mcp[&quot;input_ids&quot;].append(el[0])
                mcp[&quot;attention_masks&quot;].append(el[1])
                mcp[&quot;reference&quot;].append(el[2])
                mcp[&quot;labels&quot;].append(el[3])

</code></pre>
<p>But I really don't like this code and I was wondering if there was a prettier way of doing this.</p>
","python, dictionary, bert-language-model","<p>Assuming that your values all have the same length, I'd suggest to use <a href=""https://pandas.pydata.org/"" rel=""nofollow noreferrer""><code>pandas</code></a> which is made for that kind of modification.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

cls= {&quot;input_ids&quot;:[[22,22,22],[33,33,33]], 
      &quot;attention_masks&quot;:[[22,22,22],[33,33,33]], 
      &quot;reference&quot;:[1,0], 
      &quot;labels&quot;:[[22,22,22],[33,33,33]] # I assume this is what you meant
      }

# Turn the data into a dataframe which is sth like a table
df = pd.DataFrame(cls)
</code></pre>
<p>This is what <code>df</code> looks like:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df
      input_ids attention_masks  reference        labels
0  [22, 22, 22]    [22, 22, 22]          1  [22, 22, 22]
1  [33, 33, 33]    [33, 33, 33]          0  [33, 33, 33]

</code></pre>
<p>You can access the values like <code>df['reference']</code> and filter the data, e.g., like this:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df[df['reference']==1]
      input_ids attention_masks  reference        labels
0  [22, 22, 22]    [22, 22, 22]          1  [22, 22, 22]
</code></pre>
<p>If you need it as dictionary:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df[df['reference']==1].to_dict()
{'input_ids': {0: [22, 22, 22]},
 'attention_masks': {0: [22, 22, 22]},
 'reference': {0: 1},
 'labels': {0: [22, 22, 22]}}
</code></pre>
",0,-1,48,2021-06-22 21:54:02,https://stackoverflow.com/questions/68091124/processing-dictionary-with-zip-function-keeping-the-same-structure-in-python
BERT model loss function from one hot encoded labels,"<p>For the line: loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
I have labels hot encoded such that it is a tensor of 32x17, since the batch size is 32 and there are 17 classes for the text categories. However, BERT model only takes for the label with a single dimension vector.
Hence, I get the error:</p>
<p>Expected input batch_size (32) to match target batch_size (544)</p>
<p>The 544 is the product of 32x17. However, my question is how could I use one hot encoded labels to  get the loss value in each iteration? I could use just label encoded labels, but that would not really be suitable for unordered labels.</p>
<pre><code># BERT training loop
for _ in trange(epochs, desc=&quot;Epoch&quot;):  
  
  ## TRAINING
  
  # Set our model to training mode
  model.train()  
  # Tracking variables
  tr_loss = 0
  nb_tr_examples, nb_tr_steps = 0, 0
  # Train the data for one epoch
  for step, batch in enumerate(train_dataloader):
    # Add batch to GPU
    batch = tuple(t.to(device) for t in batch)
    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels = batch
    # Clear out the gradients (by default they accumulate)
    optimizer.zero_grad()
    # Forward pass
    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
    train_loss_set.append(loss.item())    
    # Backward pass
    loss.backward()
    # Update parameters and take a step using the computed gradient
    optimizer.step()
    # Update tracking variables
    tr_loss += loss.item()
    nb_tr_examples += b_input_ids.size(0)
    nb_tr_steps += 1
  print(&quot;Train loss: {}&quot;.format(tr_loss/nb_tr_steps))
</code></pre>
","python, pytorch, loss-function, one-hot-encoding, bert-language-model","<p>As stated in the comment, Bert for sequence classification expects the target tensor as a <code>[batch]</code> sized tensors with values spanning the range <em>[0, num_labels)</em>. A one-hot encoded tensor can be converted by <code>argmax</code>ing it over the label dim, i.e. <code>labels=b_labels.argmax(dim=1)</code>.</p>
",2,0,1214,2021-06-23 17:15:44,https://stackoverflow.com/questions/68104425/bert-model-loss-function-from-one-hot-encoded-labels
KeyBERT package is not working on Google Colab,"<p>I'm using KeyBERT on Google Colab to extract keywords from the text.</p>
<pre><code>from keybert import KeyBERT

model = KeyBERT('distilbert-base-nli-mean-tokens')
text_keywords = model.extract_keywords(my_long_text)
</code></pre>
<p>But I get the following error:</p>
<p><strong>OSError:</strong> Model name 'distilbert-base-nli-mean-token' was not found in model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad). We assumed 'distilbert-base-nli-mean-token' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.</p>
<p>Any idea how to fix this?</p>
<p>Thanks</p>
<pre><code>Exception when trying to download http://sbert.net/models/distilbert-base-nli-mean-token.zip. Response 404
SentenceTransformer-Model http://sbert.net/models/distilbert-base-nli-mean-token.zip not found. Try to create it from scratch
Try to create Transformer Model distilbert-base-nli-mean-token with mean pooling
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py in __init__(self, model_name_or_path, modules, device)
     78                         zip_save_path = os.path.join(model_path_tmp, 'model.zip')
---&gt; 79                         http_get(model_url, zip_save_path)
     80                         with ZipFile(zip_save_path, 'r') as zip:

11 frames
/usr/local/lib/python3.7/dist-packages/sentence_transformers/util.py in http_get(url, path)
    241         print(&quot;Exception when trying to download {}. Response {}&quot;.format(url, req.status_code), file=sys.stderr)
--&gt; 242         req.raise_for_status()
    243         return

/usr/local/lib/python3.7/dist-packages/requests/models.py in raise_for_status(self)
    940         if http_error_msg:
--&gt; 941             raise HTTPError(http_error_msg, response=self)
    942 

HTTPError: 404 Client Error: Not Found for url: https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/distilbert-base-nli-mean-token.zip

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    133           that will be used by default in the :obj:`generate` method of the model. In order to get the tokens of the
--&gt; 134           words that should not appear in the generated text, use :obj:`tokenizer.encode(bad_word,
    135           add_prefix_space=True)`.

/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies)
    181 except importlib_metadata.PackageNotFoundError:
--&gt; 182     _timm_available = False
    183 

OSError: file distilbert-base-nli-mean-token not found

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-59-d0fa7b6b7cd1&gt; in &lt;module&gt;()
      1 doc = full_text
----&gt; 2 model = KeyBERT('distilbert-base-nli-mean-token')

/usr/local/lib/python3.7/dist-packages/keybert/model.py in __init__(self, model)
     46                       * https://www.sbert.net/docs/pretrained_models.html
     47         &quot;&quot;&quot;
---&gt; 48         self.model = select_backend(model)
     49 
     50     def extract_keywords(self,

/usr/local/lib/python3.7/dist-packages/keybert/backend/_utils.py in select_backend(embedding_model)
     40     # Create a Sentence Transformer model based on a string
     41     if isinstance(embedding_model, str):
---&gt; 42         return SentenceTransformerBackend(embedding_model)
     43 
     44     return SentenceTransformerBackend(&quot;xlm-r-bert-base-nli-stsb-mean-tokens&quot;)

/usr/local/lib/python3.7/dist-packages/keybert/backend/_sentencetransformers.py in __init__(self, embedding_model)
     33             self.embedding_model = embedding_model
     34         elif isinstance(embedding_model, str):
---&gt; 35             self.embedding_model = SentenceTransformer(embedding_model)
     36         else:
     37             raise ValueError(&quot;Please select a correct SentenceTransformers model: \n&quot;

/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py in __init__(self, model_name_or_path, modules, device)
     93                             save_model_to = model_path
     94                             model_path = None
---&gt; 95                             transformer_model = Transformer(model_name_or_path)
     96                             pooling_model = Pooling(transformer_model.get_word_embedding_dimension())
     97                             modules = [transformer_model, pooling_model]

/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/Transformer.py in __init__(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case)
     25         self.do_lower_case = do_lower_case
     26 
---&gt; 27         config = AutoConfig.from_pretrained(model_name_or_path, **model_args, cache_dir=cache_dir)
     28         self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)
     29         self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir, **tokenizer_args)

/usr/local/lib/python3.7/dist-packages/transformers/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)

/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    144           after the :obj:`decoder_start_token_id`. Useful for multilingual models like :doc:`mBART
    145           &lt;../model_doc/mbart&gt;` where the first generated token needs to be the target language token.
--&gt; 146         - **forced_eos_token_id** (:obj:`int`, `optional`) -- The id of the token to force as the last generated token
    147           when :obj:`max_length` is reached.
    148         - **remove_invalid_values** (:obj:`bool`, `optional`) -- Whether to remove possible `nan` and `inf` outputs of

OSError: Model name 'distilbert-base-nli-mean-token' was not found in model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad). We assumed 'distilbert-base-nli-mean-token' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.
</code></pre>
","google-colaboratory, bert-language-model, keyword-extraction","<p>I couldn't reproduce this issue with the code you've provided but from the provided error message I believe you're just missing an 's' in the model name so just make sure that the model name is as follows:</p>
<blockquote>
<p>distilbert-base-nli-mean-tokens</p>
</blockquote>
<p>and not</p>
<blockquote>
<p>distilbert-base-nli-mean-token</p>
</blockquote>
<p>Also refer to <a href=""https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2"" rel=""nofollow noreferrer"">this link</a> for all models available for use.</p>
",1,1,1917,2021-06-23 23:04:55,https://stackoverflow.com/questions/68107887/keybert-package-is-not-working-on-google-colab
How to concatenate new vectors into existing Bert vector?,"<p>For a sentence,I may extract a few entities and each of the entities is embedded with 256 dimension vectors. Then I compute an average for these entities to be a single vector to represent these entity representations.</p>
<p>Now, I want to concatenate the bert's 'pooled output' layer with this entity vector together as input of the next layer. This might improve the original Bert's performance. How to do this in Keras?</p>
<p>This is the fine-tune code to define a text classifier model from a tutorial:</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)
</code></pre>
<p>How to combine the pooled_output of 512 dimension with the entity vector?</p>
","tensorflow, embedding, bert-language-model, word-embedding","<p>You first need to ensure that the vector with whom you want to concatenate has the same dimension on the axis you want to concatenate (e.g. <code>(512,1)</code> <code>(512,1)</code> -&gt; <code>(1024,1)</code>).</p>
<p>Then you could use:
<code>tf.keras.layers.Concatenate([pooled_output,entity_layer],axis=axis)</code></p>
<p>on the desired axis.</p>
<p>You can also have a look here for more details: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate</a></p>
",0,0,709,2021-06-24 07:13:20,https://stackoverflow.com/questions/68111122/how-to-concatenate-new-vectors-into-existing-bert-vector
Removal of Stop Words and Stemming/Lemmatization for BERTopic,"<p>For Topic Modelling, I'm trying out the BERTopic: <a href=""https://maartengr.github.io/BERTopic/index.html"" rel=""nofollow noreferrer"">Link</a></p>
<p>I'm little confused here, I am trying out the BERTopic on my custom Dataset. <br />
Since BERT was trained in such a way that it holds the semantic meaning of the text/document,
Should I be removing the stop words and stem/lemmatize my documents before passing it onto BERTopic?
Because I'm afraid if these stopwords might land into my topics as salient terms which they are not</p>
<p>Suggestions and Advices please!</p>
","python, nlp, bert-language-model, topic-modeling","<p>No.</p>
<p>BERTopic uses transformers that are based on &quot;real and clean&quot; text, not on text without stopwords, lemmas or tokens. At the end of the calculation stop words have become noise (non-informative) and are all in topic_id = -1.</p>
<p>For the same reason you should not tokenize (done internally) or lemmatize (somewhat subjective) the text. That will mess-up your topics</p>
<p>A disadvantage of not lemmatizing is that the keywords of a topic have a lot of redundancy, like (topn=10) &quot;hotel, hotels&quot;, &quot;resort, resorts&quot; etc. It also does not handle bigrams like &quot;New York&quot; or &quot;Barack Obama&quot; elegantly</p>
<p>You can't have it all ;-)</p>
<p>Andreas</p>
<p>PS: You can ofcourse remove HTML tags; they are not in your reference corpus either</p>
",6,4,9427,2021-06-25 08:20:21,https://stackoverflow.com/questions/68127754/removal-of-stop-words-and-stemming-lemmatization-for-bertopic
Using roberta model cannot define the model .compile or summary,"<p>Using roberta model for sentiment analysis cannot define the model .compile or summary</p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import  BertConfig
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
robertamodel = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=7)
print('\nBert Model',robertamodel.summary())
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('0accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08)

robertamodel.compile(loss=loss,optimizer=optimizer,metrics=[metric])
print(robertamodel.summary())
</code></pre>
<p>i got these errors
'RobertaForSequenceClassification' object has no attribute 'summary'
'RobertaForSequenceClassification' object has no attribute 'compile'</p>
","python, keras, deep-learning, bert-language-model, roberta-language-model","<p>Roberta is based on pytorch. Check out the helper function <a href=""https://huggingface.co/transformers/model_doc/roberta.html"" rel=""nofollow noreferrer"">TFRobertaModel</a> to convert it to a tensorflow model.</p>
",1,1,978,2021-06-27 14:43:27,https://stackoverflow.com/questions/68152276/using-roberta-model-cannot-define-the-model-compile-or-summary
BERT Summarization for a column of texts,"<p>I'm trying to summarize 100 products descriptions from a dataset. To do so i simply tried to use summarizers</p>
<pre><code>!pip install summarizers -q

from summarizers import Summarizers 

import pandas as pd
</code></pre>
<p>It worked pretty well for one sentence at a time.</p>
<pre><code>textpanasonic=&quot;The NN-CS89L offers next-level cooking convenience. Its four distinct cooking methods - steaming, baking, grilling and microwaving ensure your meals are cooked or reheated to perfection. Its multi-function capabilities can be combined to save time without compromising taste, texture or nutritional value. It’s the all-in-one kitchen companion designed for people with a busy lifestyle.&quot;



summ(textpanasonic)

The NN-CS89L offers next-level cooking convenience.
</code></pre>
<p>but <strong>do you know if it is possible to create a new column with a summary for each comment ?</strong></p>
<p>ValueError: text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
<p>Thank you in advance ^^</p>
","python, nlp, bert-language-model, summarize","<p>You can simply <code>apply</code> <code>summ</code> to the column with summaries. Since <code>summ</code> can take a list as input it will also process a pandas Series. To provide an example with multiple rows:</p>
<pre><code>import pandas as pd
from summarizers import Summarizers
summ = Summarizers()

data = [&quot;The NN-CS89L offers next-level cooking convenience. Its four distinct cooking methods - steaming, baking, grilling and microwaving ensure your meals are cooked or reheated to perfection. Its multi-function capabilities can be combined to save time without compromising taste, texture or nutritional value. It’s the all-in-one kitchen companion designed for people with a busy lifestyle.&quot;, &quot;These slim and stylish bodies are packed with high performance. The attractive compact designs and energy-saving functions help Panasonic Blu-ray products consume as little power as possible. You can experience great movie quality with this ultra-fast booting DMP-BD89 Full HD Blu-ray disc player. After starting the player, the time it takes from launching the menu to playing a disc is much shorter than in conventional models. The BD89 also allows for smart home networking (DLNA) and provides access to video on demand, so that home entertainment is more intuitive, more comfortable, and lots more fun.&quot;]
df = pd.DataFrame(data, columns=['summaries'])
df['abstracts'] = df['summaries'].apply(summ)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">summaries</th>
<th style=""text-align: left;"">abstracts</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">The NN-CS89L offers next-level cooking convenience. Its four distinct cooking methods - steaming, baking, grilling and microwaving ensure your meals are cooked or reheated to perfection. Its multi-function capabilities can be combined to save time without compromising taste, texture or nutritional value. It’s the all-in-one kitchen companion designed for people with a busy lifestyle.</td>
<td style=""text-align: left;"">The NN-CS89L offers next-level cooking convenience.</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">These slim and stylish bodies are packed with high performance. The attractive compact designs and energy-saving functions help Panasonic Blu-ray products consume as little power as possible. You can experience great movie quality with this ultra-fast booting DMP-BD89 Full HD Blu-ray disc player. After starting the player, the time it takes from launching the menu to playing a disc is much shorter than in conventional models. The BD89 also allows for smart home networking (DLNA) and provides access to video on demand, so that home entertainment is more intuitive, more comfortable, and lots more fun.</td>
<td style=""text-align: left;"">Panasonic DMP-BD89 Full HD Blu-ray disc player.</td>
</tr>
</tbody>
</table>
</div>",1,0,1135,2021-07-01 15:24:33,https://stackoverflow.com/questions/68212946/bert-summarization-for-a-column-of-texts
Training SVM classifier (word embeddings vs. sentence embeddings),"<p>I want to experiment with different embeddings such Word2Vec, ELMo, and BERT but I'm a little confused about whether to use the word embeddings or sentence embeddings, and why. I'm using the embeddings as features input to SVM classifier.</p>
<p>Thank you.</p>
","svm, word2vec, bert-language-model, word-embedding, elmo","<p>Though both approaches can prove efficient for different datasets, as a rule of thumb I would advice you to use word embeddings when your input is of a few words, and sentence embeddings when your input in longer (e.g. large paragraphs).</p>
",2,4,754,2021-07-02 12:22:12,https://stackoverflow.com/questions/68225126/training-svm-classifier-word-embeddings-vs-sentence-embeddings
BERT fine tuned model for sentiment analysis highly over-fitting,"<p>I am trying to fine tune a BERT pre-trained model. I am working with the <code>yelp_polarity_reviews</code> data from <code>tensorflow_datasets</code>. I have made sure:</p>
<ol>
<li>To load the pre-trained BERT model as <code>KerasLayer</code> with
<code>tensorflow_hub</code>.</li>
<li>To use the same <code>tokenizer</code>, <code>vocab_file</code> and <code>do_lower_case</code> which
were used in training the original model.</li>
<li>Convert the dataset to <code>tf.data.Dataset</code> object and apply <code>map</code>
function with wrapping my python function in <code>tf.py_function</code>.</li>
<li>I'm also supplying the input as BERT wants i.e., <code>input_word_ids</code>,
<code>input_mask</code> and <code>input_type_ids</code> in an array.</li>
</ol>
<p>After making sure all the above is implemented correctly, while training the model overfits badly. The training accuracy goes up to ~99% while the validation accuracy barely crosses 50% mark.</p>
<p>I have tried different <code>optimizers</code>, <code>error functions</code>, <code>learning rates</code>, even tried with high as well as low <code>dropouts</code> and I've also tried with altering the size of train data but  after all this the result is no better.</p>
<p><a href=""https://colab.research.google.com/drive/1co7i68tKv16OYkkhpWWhtztV334jqFM-?authuser=2"" rel=""nofollow noreferrer"">Here</a> is the colab notebook that shows the executed code.</p>
<p>Any suggestions and help would be highly appreciated.</p>
","python, tensorflow, bert-language-model","<p>I checked your colab code and with a few trails, it appeared that there was an issue on the validation set. And it was right of course. The mistake was to load the train labels in the test data set.</p>
<pre><code>elp_test, _ = train_test_split(list(zip(yelp['test']['text'].numpy(),
                                yelp['test']['label'].numpy())), # &lt; correction
                                train_size=0.025, 
                                random_state=36)
</code></pre>
<p>Now, if you run the model, you will get</p>
<pre><code>history = model.fit(data_train, 
                    validation_data=data_valid, 
                    epochs=1,
                    batch_size=256, 
                    verbose=2)
915ms/step - loss: 0.3309 - binary_accuracy: 0.8473 - 
             val_loss: 0.1722 - val_binary_accuracy: 0.9354
</code></pre>
",1,0,1479,2021-07-04 12:12:56,https://stackoverflow.com/questions/68244607/bert-fine-tuned-model-for-sentiment-analysis-highly-over-fitting
How is get predict accuracy score in Bert Classification,"<p>I am using Bert Classifier for my Chatbot project. I perform the necessary tokenizer operations for the incoming text message. Then I insert it into the model and make a prediction. How can I get the accuracy of this estimate?</p>
<pre><code>for text in test_texts:
    encoded_dict = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
print(&quot;input_ids &quot;,input_ids)
print(&quot;attention_masks &quot;,attention_masks)

batch_size = 32

prediction_data = TensorDataset(input_ids, attention_masks)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

print(&quot;prediction_data &quot;,prediction_data)
print(&quot;prediction_sampler &quot;,prediction_sampler)
print(&quot;prediction_dataloader &quot;,prediction_dataloader)
model.eval()
predictions, true_labels = [], []

for batch in prediction_dataloader:
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask = batch
    print(&quot;b input ids&quot;,b_input_ids)
    with torch.no_grad():
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask.to(device))

    logits = outputs[0]
    logits = logits.detach().cpu().numpy()
    label_ids = b_input_mask.to('cpu').numpy()

    predictions.append(logits)
    true_labels.append(label_ids)
    print(&quot;logits &quot;,logits)

    print(&quot;label_ids &quot;,label_ids)
    print(&quot;true_labels &quot;,true_labels)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
    prediction_set.append(pred_labels_i)

prediction= [item for sublist in prediction_set for item in sublist]


print(&quot;prediction:&quot;, prediction[0])
</code></pre>
<p>I am looking for a percentage value. will respond or pass depending on the result of this percentage value.</p>
","python, machine-learning, pytorch, bert-language-model","<p>Accuracy can be directly computed using some libraries.</p>
<p>For example, you can use sklearn:</p>
<pre><code>from sklearn.metrics import accuracy_score
print(&quot;Accuracy:&quot;, accuracy_score(true_labels, predictions)) # Value between 0 and 1

print(&quot;Accuracy Percentage {} %:&quot;.format(100*accuracy_score(true_labels, predictions))) # Value between 0 and 100
``

</code></pre>
",1,1,1812,2021-07-09 06:53:06,https://stackoverflow.com/questions/68312423/how-is-get-predict-accuracy-score-in-bert-classification
Can BERT be used to train non-text sequence data for classification?,"<p>Can BERT be used for non-text sequence data? I want to try BERT for sequence classification problems. The data is not text. In other words, I want to train BERT from scratch. How do I do that?</p>
","classification, sequence, bert-language-model","<p>The Transformer architecture can be used for anything as long as it is a sequence of discrete symbols. BERT is trained using the marked language model objective, i.e., it is trained to fill in a gap in a sequence based on the rest of the sequence. If your data is of that kind, you can train a BERT-like model on it. With sequences of continuous vectors, you would need to come up with a suitable alternative to masked language modeling.</p>
<p>You can follow any of the many tutorials that you can find online, e.g., from the <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">Huggingface blog</a> or <a href=""https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6"" rel=""nofollow noreferrer"">towardsdatascience.com</a>.</p>
",1,1,1337,2021-07-13 05:45:05,https://stackoverflow.com/questions/68357054/can-bert-be-used-to-train-non-text-sequence-data-for-classification
CUDA error: CUBLAS_STATUS_INVALID_VALUE error when training BERT model using HuggingFace,"<p>I am working on sentiment analysis on steam reviews dataset using BERT model where I have 2 labels: positive and negative. I have fine-tuned the model with 2 Linear layers and the code for that is as below.</p>
<pre><code> bert = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;,
                                                 num_labels = len(label_dict),
                                                 output_attentions = False,
                                                 output_hidden_states = False)

 class bertModel(nn.Module):
   def __init__(self, bert):
     super(bertModel, self).__init__()
     self.bert = bert
     self.dropout1 = nn.Dropout(0.1)
     self.relu =  nn.ReLU()
     self.fc1 = nn.Linear(768, 512)
     self.fc2 = nn.Linear(512, 2)
     self.softmax = nn.LogSoftmax(dim = 1)

  def forward(self, **inputs):
     _, x = self.bert(**inputs)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)

  return x
</code></pre>
<p>This is my train function:</p>
<pre><code>def model_train(model, device, criterion, scheduler, optimizer, n_epochs):
  train_loss = []
  model.train()
 for epoch in range(1, epochs+1):
   total_train_loss, training_loss = 0,0 
  for idx, batch in enumerate(dataloader_train):
     model.zero_grad()
     data = tuple(b.to(device) for b in batch)
     inputs = {'input_ids':      data[0],'attention_mask': data[1],'labels':data[2]}
     outputs = model(**inputs)
     loss = criterion(outputs, labels)
     loss.backward()
     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
     #update the weights
     optimizer.step()
     scheduler.step()
     training_loss += loss.item()
     total_train_loss += training_loss
     if idx % 25 == 0:
        print('Epoch: {}, Batch: {}, Training Loss: {}'.format(epoch, idx, training_loss/10))
        training_loss = 0      
  #avg training loss
  avg_train_loss = total_train_loss/len(dataloader_train)
  #validation data loss
  avg_pred_loss = model_evaluate(dataloader_val)
  #print for every end of epoch
  print('End of Epoch {}, Avg. Training Loss: {}, Avg. validation Loss: {} \n'.format(epoch, avg_train_loss, avg_pred_loss))
</code></pre>
<p>I am running this code on Google Colab. When I run the train function, I get the following the error, I have tried with batch sizes 32, 256, 512.</p>
<pre><code>RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)`
</code></pre>
<p>Can anyone please help me on this? Thank you.</p>
<p><strong>Update on the code:</strong> I tried running the code on the CPU and the error is in the matrix shapes mismatch. The input shape, shape after the self.bert is printed in the image. Since the first linear layer (fc1) is not getting executed, the shape after that is not printed.</p>
<p><a href=""https://i.sstatic.net/vhqjC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vhqjC.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/h7sgA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/h7sgA.png"" alt=""enter image description here"" /></a></p>
","python, pytorch, sentiment-analysis, bert-language-model","<p>I suggest trying out couple of things that can possibly solve the error.</p>
<p>As shown in this <a href=""https://discuss.pytorch.org/t/runtimeerror-cuda-error-cublas-status-invalid-value-when-calling-cublassgemm-handle-opa-opb-m-n-k-alpha-a-lda-b-ldb-beta-c-ldc/124544"" rel=""nofollow noreferrer"">forum</a>, one possible solution is to lower the batch size of how you load data. Since it might be a memory error.</p>
<p>If that does not work then I suggest as shown in this github <a href=""https://github.com/pytorch/pytorch/issues/56747"" rel=""nofollow noreferrer"">issue</a> to update to a new version of Pytorch cuda that fixes a matrix multiplication bug that releases this same error that your code could be doing. Hence, as shown in this <a href=""https://discuss.pytorch.org/t/cublas-status-execution-failed-when-calling-cublassgemm-handle-opa-opb-m-n-k-alpha-a-lda-b-ldb-beta-c-ldc/116740/9"" rel=""nofollow noreferrer"">forum</a> You can update Pytorch to the nightly pip wheel, or use the CUDA10.2 or conda binaries. You can find information on such installations on the pytorch home page where it mentions how to install pytorch.</p>
<p>If none of that works, then the best thing to do is to run a smaller version of the process on CPU and recreate the error. When running it on CPU instead of CUDA, you will get a more useful traceback that can solve your error.</p>
<h1 id=""edit-based-on-comments-wplj"">EDIT (Based on Comments):</h1>
<p>You have a matrix error in your model.
The problem stems in your forward func then</p>
<p>The model BERT outputs a tensor that has torch.size (64, 2) which means if you put it in the Linear layer you have it will error since that linear layer requires input of (?, 768) b/c you initialized it as <code>nn.Linear(768, 512)</code>. In order to make the error disappear you need to either do some transformation on the tensor or initialize another linear layer as shown below:</p>
<pre><code>somewhere defined in __init__: self.fc0 = nn.Linear(2, 768)
def forward(self, **inputs):
     _, x = self.bert(**inputs)
     
    x = self.fc0(x)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)

</code></pre>
<p>Sarthak Jain</p>
",2,2,6643,2021-07-14 18:47:33,https://stackoverflow.com/questions/68383634/cuda-error-cublas-status-invalid-value-error-when-training-bert-model-using-hug
Continual pre-training vs. Fine-tuning a language model with MLM,"<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. I’ve tried the two following approaches so far:</p>
<ol>
<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>
<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we don’t need NSP for the pretraining part.)</li>
</ol>
<p>But I’m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>
<p>I've reviewed similar questions such as <a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>
","deep-learning, nlp, huggingface-transformers, bert-language-model, pre-trained-model","<p>The answer is a mere difference in the terminology used. When the model is trained on a large generic corpus, it is called 'pre-training'. When it is adapted to a particular task or dataset it is called as 'fine-tuning'.</p>
<p>Technically speaking, in either cases ('pre-training' or 'fine-tuning'), there are updates to the model weights.</p>
<p>For example, usually, you can just take the pre-trained model and then fine-tune it for a specific task (such as classification, question-answering, etc.). However, if you find that the target dataset is from a specific domain, and you have a few unlabled data that might help the model to adapt to the particular domain, then you can do a MLM or MLM+NSP 'fine-tuning' (unsupervised learning) (some researchers do call this as 'pre-training' especially when a huge corpus is used to train the model), followed by using the target corpus with target task fine-tuning.</p>
",25,14,19143,2021-07-20 20:52:44,https://stackoverflow.com/questions/68461204/continual-pre-training-vs-fine-tuning-a-language-model-with-mlm
BERT classification on imbalanced or small dataset,"<p>I have a large corpus, no labels. I trained this corpus to get my BERT tokenizer.</p>
<p>Then I want to build a <code>BertModel</code> to do a binary classification on a labeled dataset. However, this dataset is highly imbalanced, 1: 99. So my question is:</p>
<ol>
<li>Does BertModel would perform well on imbalanced dataset?</li>
<li>Does BertModel would perform well on small dataset? (as small as less than 500 data points, I bet it's not..)</li>
</ol>
","bert-language-model, imbalanced-data","<p>The objective of transferred learning using pre-trained models partially answers your questions. <code>BertModel</code> pre-trained on large corpus, which when adapted to task specific corpus, usually performs better than non pre-trained models (for example, training a simple LSTM for classification task).</p>
<p>BERT has shown that it performs well when fine-tuned on small task-specific corpus. (This answers your question 2.). However, the level of improvements also depend on the domain and task that you want to perform, and how related was the data used for pre-training is with respect to your target dataset.</p>
<p>From my experience, pre-trained BERT when fine-tuned on target task performs much better than other DNNs such as LSTM and CNNs when the datasets are highly imbalanced. However, this again depends on the task and data. 1:99 is really a huge imbalance, which might require data balancing techniques.</p>
",3,3,3755,2021-07-25 04:53:22,https://stackoverflow.com/questions/68515595/bert-classification-on-imbalanced-or-small-dataset
AttributeError: type object &#39;Language&#39; has no attribute &#39;factory&#39;,"<p>I am getting error while using &quot;spacy_sentence_bert&quot;</p>
<p><strong>Code:</strong></p>
<pre><code>import spacy_sentence_bert
# load one of the models listed at https://github.com/MartinoMensio/spacy-sentence-bert/
nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
# get two documents
doc_1 = nlp('Hi there, how are you?')
doc_2 = nlp('Hello there, how are you doing today?')
# use the similarity method that is based on the vectors, on Doc, Span or Token
print(doc_1.similarity(doc_2[0:7]))
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-42-d86d8866c1ca&gt; in &lt;module&gt;()
      1 ##### using BERT
----&gt; 2 import spacy_sentence_bert
      3 # load one of the models listed at https://github.com/MartinoMensio/spacy-sentence-bert/
      4 nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
      5 # get two documents

1 frames
/usr/local/lib/python3.7/dist-packages/spacy_sentence_bert/language.py in &lt;module&gt;()
     24 
     25 # the pipeline stage factory
---&gt; 26 @Language.factory('sentence_bert', default_config={
     27     'model_name': None,
     28     'debug': True

AttributeError: type object 'Language' has no attribute 'factory'
</code></pre>
","nlp, spacy, bert-language-model","<p>The issue seems to be with your environment.</p>
<blockquote>
<p>AttributeError: type object 'Language' has no attribute 'factory'
message</p>
</blockquote>
<p>is a spacy 2.x error.</p>
<p>Please try to run it with spacy 3.x. If you have already installed 3.x, then please verify if your virtual environment is pointing to correct python.</p>
",4,0,3377,2021-07-26 16:50:43,https://stackoverflow.com/questions/68533685/attributeerror-type-object-language-has-no-attribute-factory
How to run &#39;run_squad.py&#39; on google colab? It gives &#39;invalid syntax&#39; error,"<p>I downloaded the file first using:</p>
<pre><code>!curl -L -O https://github.com/huggingface/transformers/blob/master/examples/legacy/question-answering/run_squad.py
</code></pre>
<p>Then used following code:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train  \
    --train_file /content/train.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file /content/val.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>Also tried following:</p>
<pre><code>!python run_squad.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --do_train \
  --do_eval \
  --do_lower_case \
  --train_file /content/train.json \
  --predict_file /content/val.json \
  --per_gpu_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 584 \
  --doc_stride 128 \
  --output_dir /content/
</code></pre>
<p>The error says in both the codes:</p>
<blockquote>
<p>File &quot;run_squad.py&quot;, line 7

^ SyntaxError: invalid syntax</p>
</blockquote>
<p>What exactly is the issue? How can I run the <code>.py</code> file?</p>
","google-colaboratory, stanford-nlp, bert-language-model, huggingface-transformers, nlp-question-answering","<p>SOLVED: It was giving error because I was downloading the github link rather than the script in github. Once I copied and used 'Raw' link to download the script, the code ran.</p>
",2,0,507,2021-07-30 10:12:43,https://stackoverflow.com/questions/68589222/how-to-run-run-squad-py-on-google-colab-it-gives-invalid-syntax-error
BERT problem with context/semantic search in italian language,"<p>I am using BERT model for context search in Italian language but it does not understand the contextual meaning of the sentence and returns wrong result.</p>
<p>in below example code when I compare &quot;milk with chocolate flavour&quot; with two other type of milk and one chocolate so it returns high similarity with chocolate. it should return high similarity with other milks.</p>
<p>can anyone suggest me any improvement on the below code so that it can return semantic results?</p>
<p><strong>Code :</strong></p>
<pre><code>!python -m spacy download it_core_news_lg
!pip install sentence-transformers


import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distiluse-base-multilingual-cased') # workes with Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish

corpus = [
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr&quot;, #Alpro, Chocolate soy drink 1 ltr(soya milk)
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml(milk with chocolate flabor)
         ]
corpus_embeddings = model.encode(corpus)


queries = [
            'latte al cioccolato', #milk with chocolate flavor,
          ]
query_embeddings = model.encode(queries)


# Calculate Cosine similarity of query against each sentence i
closest_n = 10
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, &quot;cosine&quot;)[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print(&quot;\n======================\n&quot;)
    print(&quot;Query:&quot;, query)
    print(&quot;\nTop 10 most similar sentences in corpus:&quot;)

    for idx, distance in results[0:closest_n]:
        print(corpus[idx].strip(), &quot;(Score: %.4f)&quot; % (1-distance))
</code></pre>
<p><strong>Output :</strong></p>
<pre><code>======================

Query: latte al cioccolato

Top 10 most similar sentences in corpus:
Milka  cioccolato al latte 100 g (Score: 0.7714)
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.5586)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.4569)
</code></pre>
","machine-learning, nlp, bert-language-model","<p>The problem is not with your code, it is just the insufficient model performance.</p>
<p>There are a few things you can do. First, you can try Universal Sentence Encoder (USE). From my experience their embeddings are a little bit better, at least in English.</p>
<p>Second, you can try a different model, for example <code>sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1</code>. It is based on ROBERTa and might give a better performance.</p>
<p>Now you can combine together embeddings from several models (just by concatenating the representations). In some cases it helps, on expense of much heavier compute.</p>
<p>And finally you can create your own model. It is well known that single language models perform significantly better than multilingual ones. You can follow <a href=""https://www.sbert.net/examples/training/multilingual/README.html"" rel=""nofollow noreferrer"">the guide</a> and train your own Italian model.</p>
",3,1,580,2021-08-02 19:34:34,https://stackoverflow.com/questions/68627093/bert-problem-with-context-semantic-search-in-italian-language
"RuntimeError: shape &#39;[4, 512]&#39; is invalid for input of size 1024 while while evaluating test data","<p>I am trying XLnet over Jigsaw toxic dataset.</p>
<p>When I train my data with</p>
<pre><code>input_ids = d[&quot;input_ids&quot;].reshape(4,512).to(device)  # batch size x seq length
</code></pre>
<p>it trains perfectly.
But when I try to test the model with test data with reshaping the input_ids in the same way, it generates a run time error:</p>
<pre><code>shape '[4, 512]' is invalid for input of size 1024
</code></pre>
<p>This is the method I am using for training:</p>
<pre><code>def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):
    model = model.train()
    losses = []
    acc = 0
    counter = 0
  
    for d in data_loader:
        input_ids = d[&quot;input_ids&quot;].reshape(4,512).to(device)
        attention_mask = d[&quot;attention_mask&quot;].to(device)
        targets = d[&quot;targets&quot;].to(device)
        
        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)
        loss = outputs[0]
        logits = outputs[1]

        # preds = preds.cpu().detach().numpy()
        _, prediction = torch.max(outputs[1], dim=1)
        targets = targets.cpu().detach().numpy()
        prediction = prediction.cpu().detach().numpy()
        accuracy = metrics.accuracy_score(targets, prediction)

        acc += accuracy
        losses.append(loss.item())
        
        loss.backward()

        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        counter = counter + 1

    return acc / counter, np.mean(losses)
</code></pre>
<p>This is the method I am using for evaluating my test data:</p>
<pre><code>def eval_model(model, data_loader, device, n_examples):
    model = model.eval()
    losses = []
    acc = 0
    counter = 0
  
    with torch.no_grad():
        for d in data_loader:
            # print(d[&quot;input_ids&quot;])
            input_ids = d[&quot;input_ids&quot;].reshape(4,512).to(device)
            attention_mask = d[&quot;attention_mask&quot;].to(device)
            targets = d[&quot;targets&quot;].to(device)
            
            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)
            loss = outputs[0]
            logits = outputs[1]

            _, prediction = torch.max(outputs[1], dim=1)
            targets = targets.cpu().detach().numpy()
            prediction = prediction.cpu().detach().numpy()
            accuracy = metrics.accuracy_score(targets, prediction)

            acc += accuracy
            losses.append(loss.item())
            counter += 1

    return acc / counter, np.mean(losses)
</code></pre>
<p>And when I try to run the <em>eval_model</em> method with my test data, it generates a run time error.
<a href=""https://i.sstatic.net/38fHB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/38fHB.png"" alt=""enter image description here"" /></a></p>
<p>My model info:</p>
<p><a href=""https://i.sstatic.net/xkxmo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xkxmo.png"" alt=""enter image description here"" /></a></p>
<p>I am unable to understand what wrong I am doing. Can anyone please help me out with this? Thank you.</p>
","python, neural-network, nlp, bert-language-model","<p>I think the problem is that the training dataset's <code>d['input_ids']</code> was of size 4*512 = 2048 so it could be divided into 4 and 512.
But the testing dataset's <code>d['input_ids']</code> is of size 1024, which cannot be divided into 4 and 512.</p>
<p>Since you haven't given the <code>model</code> description, i can't say if you should change it to (-1, 512) or (4, -1) [using -1 in reshape tells numpy to figure that dimension out automatically.</p>
<p>e.g. reshaping an array of 2048 elements into (4, 512) can be done by <code>reshape(4,512)</code> and <code>reshape(-1, 512)</code> and <code>reshape(4, -1)</code> as well.</p>
",0,0,3336,2021-08-06 08:09:41,https://stackoverflow.com/questions/68678278/runtimeerror-shape-4-512-is-invalid-for-input-of-size-1024-while-while-eva
How can I train an XGBoost with a generator?,"<p>I'm attempting to stack a BERT tensorflow model with and XGBoost model in python.  To do this, I have trained the BERT model and and have a generator that takes the predicitons from BERT (which predicts a category) and yields a list which is the result of categorical data concatenated onto the BERT prediction. This doesn't train, however because it doesn't have a shape. The code I have is:</p>
<pre><code>...
categorical_inputs=df[cat_cols]
y=pd.get_dummies(df[target_col]).values
xgboost_labels=df[target_col].values
concatenated_text_input=df['concatenated_text']
text_model.fit(tf.constant(concatenated_text_input),tf.constant(y), epochs=8)
cat_text_generator=(list(categorical_inputs.iloc[i].values)+list(text_model.predict([concatenated_text_input.iloc[i]])[0]) for i in range(len(categorical_inputs)))


clf = xgb.XGBClassifier(max_depth=200, n_estimators=400, subsample=1, learning_rate=0.07, reg_lambda=0.1, reg_alpha=0.1,\
                       gamma=1)
clf.fit(cat_text_generator, xgboost_labels)
</code></pre>
<p>and the error I get is:</p>
<pre><code>...
-&gt; 1153         if len(X.shape) != 2:
   1154             # Simply raise an error here since there might be many
   1155             # different ways of reshaping

AttributeError: 'generator' object has no attribute 'shape'
</code></pre>
<p>Although it's possible to create a list or array to hold the data, I would prefer a solution that would work for when there's too much data to hold in memory at once. Is there a way to use generators to train an xgboost model?</p>
","python, python-3.x, tensorflow, xgboost, bert-language-model","<pre><code>def generator(X_data,y_data,batch_size):
    while True:
      for step in range(X_data.shape[0]//batch_size):
          start=step*batch_size
          end=step*(batch_size+1)
          current_x=X_data.iloc[start]
          current_y=y_data.iloc[start] 
          #Or if it's an numpy array just get the rows
          yield current_x,current_y

Generator=generator(X,y)
batch_size=32
number_of_steps=X.shape[0]//batch_size

clf = xgb.XGBClassifier(max_depth=200, n_estimators=400, subsample=1, learning_rate=0.07, reg_lambda=0.1, reg_alpha=0.1,\
                       gamma=1)
 
for step in number_of_steps:
    X_g,y_g=next(Generator)
    clf.fit(X_g, y_g)
</code></pre>
",3,1,2264,2021-08-06 15:49:14,https://stackoverflow.com/questions/68684398/how-can-i-train-an-xgboost-with-a-generator
How to increase dimension-vector size of BERT sentence-transformers embedding,"<p>I am using sentence-transformers for semantic search but sometimes it does not understand the contextual meaning and returns wrong result
eg. <a href=""https://stackoverflow.com/questions/68627093/bert-problem-with-context-semantic-search-in-italian-language"">BERT problem with context/semantic search in italian language</a></p>
<p>by default the vector side of embedding of the sentence is 78 columns, so how do I increase that dimension so that it can understand the contextual meaning in deep.</p>
<p><strong>code:</strong></p>
<pre><code># Load the BERT Model
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('bert-base-nli-mean-tokens')

# Setup a Corpus
# A corpus is a list with documents split by sentences.

sentences = ['Absence of sanity', 
             'Lack of saneness',
             'A man is eating food.',
             'A man is eating a piece of bread.',
             'The girl is carrying a baby.',
             'A man is riding a horse.',
             'A woman is playing violin.',
             'Two men pushed carts through the woods.',
             'A man is riding a white horse on an enclosed ground.',
             'A monkey is playing drums.',
             'A cheetah is running behind its prey.']

# Each sentence is encoded as a 1-D vector with 78 columns 
sentence_embeddings = model.encode(sentences) ### how to increase vector dimention 

print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))

print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])
</code></pre>
","machine-learning, nlp, artificial-intelligence, bert-language-model","<p>Unfortunately the only way to INCREASE the dimension of the embedding in a meaningful way is retraining the model. :(</p>
<p>However, maybe this is not what you need...maybe you should consider fine-tuning a model:</p>
<p>I suggest you take a look at <a href=""https://www.sbert.net"" rel=""noreferrer"">sentence-transformers</a> from UKPLabs.
They have pretrained models for sentence embedding for over 100 languages. The best part is that you can <a href=""https://www.sbert.net/docs/training/overview.html"" rel=""noreferrer"">fine tune</a> those models.</p>
<p>Good Luck!</p>
",5,4,8170,2021-08-06 18:45:55,https://stackoverflow.com/questions/68686272/how-to-increase-dimension-vector-size-of-bert-sentence-transformers-embedding
How to get probability of an answer using BERT model and is there a way to ask multiple questions for a context,"<p>I am new to AI models and currently experimenting with the QandA model. Particularly I am interested in following 2 models.
<strong>1. from transformers import BertForQuestionAnswering</strong><br />
<strong>2. from simpletransformers.question_answering import QuestionAnsweringModel</strong></p>
<p>Using option 1 <strong>BertForQuestionAnswering</strong> I am getting the desired results. However I can ask only one question at a time. Also I am not getting the probability of the answer.</p>
<p>below is the code for <strong>BertForQuestionAnswering</strong> from transformers.</p>
<pre><code>from transformers import BertTokenizer, BertForQuestionAnswering
import torch

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')


input_ids = tokenizer.encode('Sky color?', 'Today the sky is blue. and it is cold out there')
tokens = tokenizer.convert_ids_to_tokens(input_ids)
sep_index = input_ids.index(tokenizer.sep_token_id)
num_seg_a = sep_index + 1
num_seg_b = len(input_ids) - num_seg_a
segment_ids = [0]*num_seg_a + [1]*num_seg_b
assert len(segment_ids) == len(input_ids)
outputs = model(torch.tensor([input_ids]), 
                            token_type_ids=torch.tensor([segment_ids]), 
                            return_dict=True)
start_scores = outputs.start_logits
end_scores = outputs.end_logits
answer_start = torch.argmax(start_scores)
answer_end = torch.argmax(end_scores)
answer = ' '.join(tokens[answer_start:answer_end+1])
print(answer)
</code></pre>
<p>Here is the output: <strong>blue</strong></p>
<p>Where as using option 2 <strong>QuestionAnsweringModel</strong> from simpletransformers, I can put multiple questions at a time and also getting probability of the answer.</p>
<p>below is the code for <strong>QuestionAnsweringModel</strong> from simpletransformers</p>
<pre><code>from simpletransformers.question_answering import QuestionAnsweringModel
model = QuestionAnsweringModel('distilbert', 'distilbert-base-uncased-distilled-squad', use_cuda=False)

question_data = {
        'qas': [{
            'question': 'Sky color?',
            'id': 0,
        },
        {
            'question': 'weather?',
            'id': 1,
        }
        ],
        'context': 'Today the sky is blue. and it is cold out there'
    }

prediction = model.predict([question_data])
output = {'result': list(prediction)}
print(output)
</code></pre>
<p>Here is the output:</p>
<pre><code>{
   &quot;result&quot;:[
      [
         {
            &quot;id&quot;:0,
            &quot;answer&quot;:[&quot;blue&quot;, &quot;the sky is blue&quot;, &quot;blue.&quot;]
         },
         {
            &quot;id&quot;:1,
            &quot;answer&quot;:[&quot;cold&quot;, &quot;it is cold&quot;, &quot;cold out there&quot;]
         }
      ],
      [
         {
            &quot;id&quot;:0,
            &quot;probability&quot;:[0.8834650211919095,0.0653234009794176,0.031404456093241565]
         },
         {
            &quot;id&quot;:1,
            &quot;probability&quot;:[0.6851319220199236,0.18145769901523698,0.05004994980074798]
         }
      ]
   ]
}
</code></pre>
<p>As you can see, For the same context I can ask multiple questions at a time and get the probability for each answer.</p>
<p>Is there a way I can get similar output for the BERT model in option#1.
I need a way to set multiple questions to a context and also need probability for each answer in the response.</p>
<p>Any help would be greatly appreciated.</p>
","python, bert-language-model, huggingface-transformers, simpletransformers","<p>You can use the huggingface <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#transformers.QuestionAnsweringPipeline"" rel=""nofollow noreferrer"">question answering pipeline</a> to achieve that:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

model_checkpoint = 'bert-large-uncased-whole-word-masking-finetuned-squad' qa_pipeline =  pipeline('question-answering', model=model_checkpoint, tokenizer=model_checkpoint)

qa_pipeline(question=['Sky color?', 'weather?'], context='Today the sky is blue. and it is cold out there')
</code></pre>
<p>Output:</p>
<pre><code>[{'score': 0.9102755188941956, 'start': 17, 'end': 21, 'answer': 'blue'},
 {'score': 0.49744659662246704, 'start': 33, 'end': 37, 'answer': 'cold'}]
</code></pre>
",3,3,1963,2021-08-11 18:20:06,https://stackoverflow.com/questions/68747152/how-to-get-probability-of-an-answer-using-bert-model-and-is-there-a-way-to-ask-m
AttributeError: Caught AttributeError in DataLoader worker process 0. - fine tuning pre-trained transformer model,"<p>can anyone help me to resolve this error?</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-aaa58b106c77&gt; in &lt;module&gt;()
     25     output_path='fine_tuned_bert',
     26     save_best_model= True,
---&gt; 27     show_progress_bar= True
     28     )

4 frames
/usr/local/lib/python3.7/dist-packages/torch/_utils.py in reraise(self)
    423             # have message field
    424             raise self.exc_type(message=msg)
--&gt; 425         raise self.exc_type(msg)
    426 
    427 

AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py&quot;, line 518, in smart_batching_collate
    num_texts = len(batch[0].texts)
AttributeError: 'str' object has no attribute 'texts'
</code></pre>
<p><strong>Code:</strong></p>
<pre><code>import pandas as pd
# initialise data of lists.
data = {'input':[
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr&quot;, #Alpro, Chocolate soy drink 1 ltr
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml
         ]
        }
 
# Creates pandas DataFrame.
x_sample = pd.DataFrame(data)
print(x_sample['input'])

# load model
from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, evaluation
from torch.utils.data import DataLoader

embedder = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1') # or any other pretrained model
print(&quot;embedder loaded...&quot;)

# define your train dataset, the dataloader, and the train loss
train_dataset = SentencesDataset(x_sample[&quot;input&quot;].tolist(), embedder)
train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=4, num_workers=1)
train_loss = losses.CosineSimilarityLoss(embedder)

# dummy evaluator to make the api work
sentences1 = ['latte al cioccolato', 'latte al cioccolato','latte al cioccolato']
sentences2 = ['Alpro, Cioccolato bevanda a base di soia 1 ltr', 'Danone, HiPRO 25g Proteine gusto cioccolato 330 ml','Milka  cioccolato al latte 100 g']
scores = [0.99,0.95,0.4]
evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)

# tune the model
embedder.fit(train_objectives=[(train_dataloader, train_loss)], 
    epochs=5, 
    warmup_steps=500, 
    evaluator=evaluator, 
    evaluation_steps=1,
    output_path='fine_tuned_bert',
    save_best_model= True,
    show_progress_bar= True
    )
</code></pre>
","python, machine-learning, nlp, bert-language-model","<p><strong>[Updated]</strong>
I skimmed several lines of documentation <a href=""https://www.sbert.net/docs/training/overview.html?highlight=fit#sentence_transformers.SentenceTransformer.fit"" rel=""nofollow noreferrer"">here</a> about how to use the <code>fit()</code> method and I realized there is a simpler solution to do what you desired. The only changes you need to consider are to define proper <code>InputExample</code> for constructing a DataLoader &amp; create a loss!</p>
<pre><code>import pandas as pd
# initialise data of lists.
data = {'input':[
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr&quot;, #Alpro, Chocolate soy drink 1 ltr
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml
         ]
        }
 
# Creates pandas DataFrame.
x_sample = pd.DataFrame(data)
print(x_sample['input'])

# load model
from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, evaluation
from torch.utils.data import DataLoader

embedder = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1') # or any other pretrained model
print(&quot;embedder loaded...&quot;)

# define your train dataset, the dataloader, and the train loss
# train_dataset = SentencesDataset(x_sample[&quot;input&quot;].tolist(), embedder)
# train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=4, num_workers=1)
# train_loss = losses.CosineSimilarityLoss(embedder)

# dummy evaluator to make the api work
sentences1 = ['latte al cioccolato', 'latte al cioccolato','latte al cioccolato']
sentences2 = ['Alpro, Cioccolato bevanda a base di soia 1 ltr', 'Danone, HiPRO 25g Proteine gusto cioccolato 330 ml','Milka  cioccolato al latte 100 g']
scores = [0.99,0.95,0.4]
evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)

examples = []
for s1,s2,l in zip(sentences1, sentences2, scores):
  examples.append(InputExample(texts=[s1, s2], label=l))
train_dataloader = DataLoader(examples, shuffle=False, batch_size=4, num_workers=1)
train_loss = losses.CosineSimilarityLoss(embedder)
# tune the model
embedder.fit(train_objectives=[(train_dataloader, train_loss)], 
    epochs=5, 
    warmup_steps=500, 
    evaluator=evaluator, 
    evaluation_steps=1,
    output_path='fine_tuned_bert',
    save_best_model= True,
    show_progress_bar= True
    )
</code></pre>
",1,0,1772,2021-08-12 15:19:34,https://stackoverflow.com/questions/68760136/attributeerror-caught-attributeerror-in-dataloader-worker-process-0-fine-tun
How to save parameters just related to classifier layer of pretrained bert model due to the memory concerns?,"<p>I fine tuned the pretrained model <a href=""https://huggingface.co/dbmdz/bert-base-turkish-cased"" rel=""noreferrer"">here</a> by freezing all layers except the classifier layers. And I saved weight file with using pytorch as .bin format.</p>
<p>Now instead of loading the 400mb pre-trained model, is there a way to load the parameters of the just Classifier layer I retrained it? By the way, I know that I have to load the original pretrained model, I just don't want to load the entire fine tuned model. due to memory concerns.</p>
<p>I can access the last layer's parameters from state_dict as below, but how can I save them in a separate file to use them later for less memory usage?</p>
<pre><code>model = PosTaggingModel(num_pos_tag=num_pos_tag)
state_dict = torch.load(&quot;model.bin&quot;)
print(&quot;state dictionary:&quot;,state_dict)
with torch.no_grad():
    model.out_pos_tag.weight.copy_(state_dict['out_pos_tag.weight'])
    model.out_pos_tag.bias.copy_(state_dict['out_pos_tag.bias'])
</code></pre>
<p>Here is the model class:</p>
<pre><code>class PosTaggingModel(nn.Module):
    def __init__(self, num_pos_tag):
        super(PosTaggingModel, self).__init__()
        self.num_pos_tag = num_pos_tag
        self.model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-turkish-cased&quot;)
        for name, param in self.model.named_parameters():
            if 'classifier' not in name: # classifier layer
                param.requires_grad = False
        self.bert_drop = nn.Dropout(0.3)
        self.out_pos_tag = nn.Linear(768, self.num_pos_tag)
        
    def forward(self, ids, mask, token_type_ids, target_pos_tag):
        o1, _ = self.model(ids, attention_mask = mask, token_type_ids = token_type_ids)
        
        bo_pos_tag = self.bert_drop(o1)
        pos_tag = self.out_pos_tag(bo_pos_tag)

        loss = loss_fn(pos_tag, target_pos_tag, mask, self.num_pos_tag)
        return pos_tag, loss
</code></pre>
<p>I don't know if this is possible but I'm just looking for a way to save and reuse the last layer's parameters, without the need for parameters of frozen layers. I couldn't find it in the <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=save"" rel=""noreferrer"">documentation</a>.
Thanks in advance to those who will help.</p>
","python, nlp, pytorch, bert-language-model, transfer-learning","<p>You can do it like this</p>
<pre><code>import torch

# creating a dummy model
class Classifier(torch.nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    self.first = torch.nn.Linear(10, 10)
    self.second = torch.nn.Linear(10, 20)
    self.last = torch.nn.Linear(20, 1)
  
  def forward(self, x):
    pass

# Creating its object
model = Classifier()

# Extracting the layer to save
to_save = model.last

# Saving the state dict of that layer
torch.save(to_save.state_dict(), './classifier.bin')

# Recreating the object of that model
model = Classifier()

# Updating the saved layer of model
model.last.load_state_dict(torch.load('./classifier.bin'))
</code></pre>
",6,5,915,2021-08-17 08:28:46,https://stackoverflow.com/questions/68814074/how-to-save-parameters-just-related-to-classifier-layer-of-pretrained-bert-model
How to combine embeddins vectors of bert with other features?,"<p>I am working on a classification task with 3 labels (0,1,2 = neg, pos, neu). Data are sentences. So to produce vectors/embeddings of sentences, I use a Bert encoder to get embeddings for each sentence and then I used a simple knn to make predictions.</p>
<p>My data look like this : each sentence has a label and other numerical value of classification.</p>
<p>For example, my data look like this</p>
<pre><code>Sentence embeddings_BERT level sub-level label

je mange  [0.21, 0.56]    2     2.1      pos
il hait   [0.25, 0.39]   3     3.1      neg
.....
</code></pre>
<p>As you can see each sentence has other categories but the are not the final one but indices to help figure the label when a human annotated the data. I want my model to take into consideration those two values when predicting the label. I was wondering if I have to concatenate them with the embeddings generate by the bert encoding or is there another way ?</p>
","python, python-3.x, bert-language-model, word-embedding","<p>There is not one perfect way to tackle this problem, but a simple solution will be to concat the bert embeddings with hard-coded features. The BERT embeddings (sentence embeddings) will be of dimension 768 (if you have used BERT base). These embeddings can be treated as features of the sentence itself. The additional features can be concatenated to form a higher dimensional vector. If the features are categorical, it will be ideal to convert to one-hot vectors and concatenate them. For example, if you want to use <code>level</code> in your example as set of input features, it will be best to convert it into one-hot feature vector and then concatenate with BERT embeddings. However, in some cases, your hard coded features can be dominant feature to bias the classifier, and in some other cases, it can have no influence at all. It all depends on the data that you have.</p>
",4,3,3606,2021-08-17 10:45:26,https://stackoverflow.com/questions/68815926/how-to-combine-embeddins-vectors-of-bert-with-other-features
Bert model output interpretation,"<p>I searched a lot for this but havent still got a clear idea so I hope you can help me out:</p>
<p>I am trying to translate german texts to english! I udes this code:</p>
<pre><code>
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

results = model(batch)  
</code></pre>
<p>Which returned me a size error! I fixed this problem (thanks to the community: <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>) with switching the last line of code to:</p>
<pre><code>results = model(input_ids = batch,decoder_input_ids=batch)
</code></pre>
<p>Now my output looks like a really long array. What is this output precisely? Are these some sort of word embeddings? And if yes: How shall I go on with converting these embeddings to the texts in the english language? Thanks alot!</p>
","translation, bert-language-model, huggingface-transformers, word-embedding","<p>Adding to Timbus's answer,</p>
<blockquote>
<p>What is this output precisely? Are these some sort of word embeddings?</p>
</blockquote>
<p><code>results</code> is of type <code>&lt;class 'transformers.modeling_outputs.Seq2SeqLMOutput'&gt;</code> and you can do</p>
<pre><code>results.__dict__.keys()
</code></pre>
<p>to check that <code>results</code> contains the following:</p>
<pre><code>dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])
</code></pre>
<p>You can read more about this class in the <a href=""https://huggingface.co/transformers/v3.3.1/main_classes/output.html#seq2seqlmoutput"" rel=""nofollow noreferrer"">huggingface documentation</a>.</p>
<blockquote>
<p>How shall I go on with converting these embeddings to the texts in the
english language?</p>
</blockquote>
<p>To interpret the text in English, you can use <code>model.generate</code> which is easily decodable in the following way:</p>
<pre><code>predictions = model.generate(batch)
english_text = tokenizer.batch_decode(predictions)
</code></pre>
",1,1,1446,2021-08-17 13:11:18,https://stackoverflow.com/questions/68817989/bert-model-output-interpretation
Looping cosine similarity formula from one dataframe to another dataframe using pandas &amp; BERT,"<p>I am building a NLP project which compares sentence similarities between two different dataframes. Here is an example of the dataframes:</p>
<pre><code>df = pd.DataFrame({'Element Detail':['Too many competitors in market', 'Highly skilled employees']})
df1 = pd.DataFrame({'Element Details':['Our workers have a lot of talent', 
                                      'this too is a sentence',
                                      'this is very different',
                                      'another sentence is this',
                                      'not much of anything']
                    })
</code></pre>
<p>I currently have the code set up in a way that it compares the first cell in df with all the cells in df1. It then picks the highest cosine similarity score and puts that in a separate dataframe with the following code:</p>
<pre><code>import pandas as pd
import numpy as np

model_name = 'bert-base-nli-mean-tokens'
from sentence_transformers import SentenceTransformer
model = SentenceTransformer(model_name)
sentence_vecs = model.encode(df['Element Detail'])
sentence_vecs1 = model.encode(df1['Element Details'])

from sklearn.metrics.pairwise import cosine_similarity

new = cosine_similarity(
    [sentence_vecs[0]],
    sentence_vecs1[0:]
)

d = pd.DataFrame(new)
T =pd.DataFrame.transpose(d)
df_new = T.insert(0, 'New_ID', range(1, 1 + len(T)))
Tnew = (T.add_prefix('X'))
Final = (Tnew[Tnew.X0 == Tnew.X0.max()])
</code></pre>
<p>The end product is this dataframe:</p>
<pre><code>    XNew_ID     X0  
0   1           0.615005 
</code></pre>
<p>How can I write a piece of code so it will loop through the rest of the elements in df and write the to the 'Final' dataframe in the same manner?</p>
","python, pandas, nlp, bert-language-model, cosine-similarity","<p>Cosign similarity can perform well on two lists, so you can pass the whole embeddings list as arguments and extract maximum similarities afterward.</p>
<pre><code>import pandas as pd
import numpy as np

model_name = 'bert-base-nli-mean-tokens'
from sentence_transformers import SentenceTransformer
model = SentenceTransformer(model_name)
sentence_vecs = model.encode(df1['Element Detail'])
sentence_vecs1 = model.encode(df2['Element Details'])

from sklearn.metrics.pairwise import cosine_similarity

new = cosine_similarity(
    sentence_vecs,
    sentence_vecs1
)
max_similarities = np.amax(new, axis=1)
d = pd.DataFrame(new)
T =pd.DataFrame.transpose(d)
df_new = T.insert(0, 'New_ID', range(1, 1 + len(T)))
Tnew = (T.add_prefix('X'))
Final = (Tnew[Tnew.X0 == Tnew.X0.max()])
Final
</code></pre>
<p>output:</p>
<pre><code>    XNew_ID     X0          X1
0   1           0.615005    0.868932
</code></pre>
",1,1,667,2021-08-17 17:26:50,https://stackoverflow.com/questions/68821642/looping-cosine-similarity-formula-from-one-dataframe-to-another-dataframe-using
Transfer Learning using HuggingFace and Tensorflow with AutoModel does not work,"<p>I try to do transfer learning using a <code>HuggingFace</code> pretrained BERT model. I want to use tensorflow API with it. I do not understand why the last line produces an error</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

model_name = &quot;distilbert-base-uncased&quot;
text = &quot;this is a test&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)    
text_tensor = tokenizer.encode(text, return_tensors=&quot;tf&quot;)

model = AutoModel.from_pretrained(model_name).to(&quot;cuda&quot;)
output = model(text_tensor) # ERROR!!, but why?
</code></pre>
","tensorflow2.0, bert-language-model, huggingface-transformers, transfer-learning","<p>You are mixing Tensorflow and Pytorch.</p>
<p>Use <code>TFAutoModel</code> instead of default (Pytorch) <code>AutoModel</code></p>
<pre><code>from transformers import AutoTokenizer, TFAutoModel

model_name = &quot;distilbert-base-uncased&quot;
text = &quot;this is a test&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)    
text_tensor = tokenizer.encode(text, return_tensors=&quot;tf&quot;)

model = TFAutoModel.from_pretrained(model_name).to(&quot;cuda&quot;)
output = model(text_tensor)
</code></pre>
",2,0,1163,2021-08-18 08:51:46,https://stackoverflow.com/questions/68829277/transfer-learning-using-huggingface-and-tensorflow-with-automodel-does-not-work
Type errors with BERT example,"<p>I'm new to BERT QA model &amp; was trying to follow the example found in <a href=""https://medium.com/saarthi-ai/build-a-smart-question-answering-system-with-fine-tuned-bert-b586e4cfa5f5"" rel=""nofollow noreferrer"">this article</a>. The problem is when I run the code attached to the example it produces a Type error as follows <code>TypeError: argmax(): argument 'input' (position 1) must be Tensor, not str</code>.</p>
<p>Here is the code that I've tried running :</p>
<pre><code>import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#Model
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''SAMPLE QUESTION&quot;'''

paragraph = '''SAMPLE PARAGRAPH'''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True)

inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)

end_index = torch.argmax(end_scores)

answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>The issue appears at line 13 of this code where I'm trying to get the maximum element in <code>start_scores</code> saying that this is not a tensor. When I tried printing this variable it showed &quot;start_logits&quot; as a string. Does anyone know a solution to this issue?</p>
","python, bert-language-model","<p>So after referring to the <a href=""https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering"" rel=""nofollow noreferrer"">BERT Documentation</a> we identified that the model output object contains multiple properties not only start &amp; end scores. Thus, we applied the following changes to the code.</p>
<pre class=""lang-py prettyprint-override""><code>
outputs = model(input_ids=torch.tensor([inputs]),token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(outputs.start_logits)

end_index = torch.argmax(outputs.end_logits)

answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>Always refer to the documentation first :&quot;D</p>
",0,0,426,2021-08-21 05:28:56,https://stackoverflow.com/questions/68870383/type-errors-with-bert-example
Bert with Padding and Masked Token Predicton,"<p>I am Playing around with Bert Pretrained Models (bert-large-uncased-whole-word-masking)
I used Huggingface to try it I first Used this Piece of Code</p>
<pre><code>m = TFBertLMHeadModel.from_pretrained(&quot;bert-large-cased-whole-word-masking&quot;)
logits = m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]).logits
</code></pre>
<p>I then used Argmax to get max probabilities after applying softmax,
Things works fine Until now.</p>
<p>When I used padding with max_length = 100 The model started making false prediction and not working well and all predicted tokens were the same i.e 119-Token ID</p>
<p>Code I used for Argmax</p>
<pre><code>tf.argmax(tf.keras.activations.softmax(m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;,max_length=,padding=&quot;max_length&quot;)[&quot;input_ids&quot;]).logits)[0],axis=-1)
</code></pre>
<p>Output Before using padding</p>
<pre><code>&lt;tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 9800, 19082,  1362,   146,  1176,  1122,   119])&gt;
</code></pre>
<p>Output After using padding with max_length of 100</p>
<pre><code>&lt;tf.Tensor: shape=(100,), dtype=int64, numpy=
array([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119])&gt;
</code></pre>
<p>I wonder if this problem prevail even training a new model as It is mandatory to set Input shape for training new model I Padded and tokenized the data but, now I want to know if this problem continues with it too.</p>
","tensorflow, keras, bert-language-model, huggingface-transformers, language-model","<p>As already mentioned in the comments, you forgot to pass the attention_mask to BERT and it, therefore, treated the added padding tokens like ordinary tokens.</p>
<p>You also asked in the comments how you can rid of the padding token prediction. There are several ways to do it depending on your actual task. One of them is removing them with <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer"">boolean_mask</a> and the attention_mask as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from transformers import TFBertLMHeadModel, BertTokenizerFast

ckpt = &quot;bert-large-cased-whole-word-masking&quot;

t = BertTokenizerFast.from_pretrained(ckpt)
m = TFBertLMHeadModel.from_pretrained(ckpt)

e = t(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;)
e_padded = t(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;, padding=&quot;max_length&quot;, max_length = 100)

def prediction(encoding):
  logits = m(**encoding).logits
  token_mapping = tf.argmax(tf.keras.activations.softmax(logits),axis=-1)
  return tf.boolean_mask(token_mapping, encoding[&quot;attention_mask&quot;])

token_predictions = prediction(e) 
token_predictions_padded = prediction(e_padded) 

print(token_predictions)
print(token_predictions_padded)
</code></pre>
<p>Output:</p>
<pre><code>tf.Tensor([ 9800 19082  1362   146  1176  1122   119], shape=(7,), dtype=int64)
tf.Tensor([ 9800 19082  1362   146  1176  1122   119], shape=(7,), dtype=int64)
</code></pre>
",1,2,1744,2021-08-24 12:23:04,https://stackoverflow.com/questions/68907519/bert-with-padding-and-masked-token-predicton
String comparison with BERT seems to ignore &quot;not&quot; in sentence,"<p>I implemented a string comparison method using SentenceTransformers and BERT like following</p>
<pre><code>from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')

sentences = [
    &quot;I'm a good person&quot;,
    &quot;I'm not a good person&quot;
]

sentence_embeddings = model.encode(sentences)

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)
</code></pre>
<p>Notice how my sentence examples are very similar but with the opposite meaning. The problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when I expected it to return something closer to zero, as they have the opposite meanings.</p>
<p>How can I adapt my code to return a more accurate result?</p>
","nlp, bert-language-model, transformer-model, sentence-similarity, sentence-transformers","<p><em>TL;DR: NLI is all you need</em></p>
<p>First, <strong>the cosine similarity is reasonably high</strong>, because the sentences are similar in the following sense:</p>
<ul>
<li>They are about the same topic (evaluation of a person)</li>
<li>They are about the same subject (&quot;I&quot;) and the same property (&quot;being a good person&quot;)</li>
<li>They have similar syntactic structure</li>
<li>They have almost the same vocabulary</li>
</ul>
<p>So, from the formal point of view, they should be considered similar. Moreover, from the practical point of view, they should often be considered similar. For example, if you google &quot;GMO are causing cancer&quot;, you might find that the text with label &quot;GMO are <em>not</em> causing cancer&quot; is relevant.</p>
<p>Second, if you want to measure logical connection between sentences, <strong>cosine similarity of embeddings is just not expressive enough</strong>. This is because embeddings contain lots of semantic stylistic, lexical and syntactic information, but they are fixed-size (768-dimensional, in your case), so they cannot contain complete information about the meaning of both sentences. So you need another model with the following properties:</p>
<ol>
<li>It encodes both texts simultaneously, so it compares the texts themselves, not just their fixed-size embeddings</li>
<li>It is explicitly trained to evaluate logical connection between sentences</li>
</ol>
<p>The task of assesing logical connection between texts is called <em>natural language inference</em> (NLI), and its most common formulation is <em>recognizing textual entailment</em> (RTE): it is the problem of predicting whether the first sentence entails the second one.</p>
<p>There are lots of models trained for this task in the Huggingface repo, with <a href=""https://huggingface.co/roberta-large-mnli"" rel=""noreferrer"">roberta-large-mnli</a> being a good one. You can use it to evaluate equivalence of two texts. If each text entails another, they are equivalent, so you can estimate the degree of equivalence as the product of the entailment scores in both directions.</p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&quot;roberta-large-mnli&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;roberta-large-mnli&quot;)

def test_entailment(text1, text2):
    batch = tokenizer(text1, text2, return_tensors='pt').to(model.device)
    with torch.no_grad():
        proba = torch.softmax(model(**batch).logits, -1)
    return proba.cpu().numpy()[0, model.config.label2id['ENTAILMENT']]

def test_equivalence(text1, text2):
    return test_entailment(text1, text2) * test_entailment(text2, text1)

print(test_equivalence(&quot;I'm a good person&quot;, &quot;I'm not a good person&quot;))  # 2.0751484e-07
print(test_equivalence(&quot;I'm a good person&quot;, &quot;You are a good person&quot;))  # 0.49342492
print(test_equivalence(&quot;I'm a good person&quot;, &quot;I'm not a bad person&quot;))   # 0.94236994
</code></pre>
",8,4,1407,2021-09-07 16:18:05,https://stackoverflow.com/questions/69091576/string-comparison-with-bert-seems-to-ignore-not-in-sentence
Load a model as DPRQuestionEncoder in HuggingFace,"<p>I would like to load the BERT's weights (or whatever transformer) into a <a href=""https://huggingface.co/transformers/model_doc/dpr.html#transformers.DPRQuestionEncoder"" rel=""nofollow noreferrer"">DPRQuestionEncoder</a> architecture, such that I can use the HuggingFace <em>save_pretrained</em> method and plug the saved model into the <a href=""https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag"" rel=""nofollow noreferrer"">RAG architecture to do end-to-end fine-tuning</a>.</p>
<pre><code>from transformers import DPRQuestionEncoder
model = DPRQuestionEncoder.from_pretrained('bert-base-uncased')
</code></pre>
<p>But I got the following error</p>
<pre><code>You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.

NotImplementedErrorTraceback (most recent call last)
&lt;ipython-input-27-1f1b990b906b&gt; in &lt;module&gt;
----&gt; 1 model = DPRQuestionEncoder.from_pretrained(model_name)
      2 # https://github.com/huggingface/transformers/blob/41cd52a768a222a13da0c6aaae877a92fc6c783c/src/transformers/models/dpr/modeling_dpr.py#L520

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1211                     )
   1212 
-&gt; 1213             model, missing_keys, unexpected_keys, error_msgs = cls._load_state_dict_into_model(
   1214                 model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init
   1215             )

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init)
   1286             )
   1287             for module in unintialized_modules:
-&gt; 1288                 model._init_weights(module)
   1289 
   1290         # copy state_dict so _load_from_state_dict can modify it

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _init_weights(self, module)
    515         Initialize the weights. This method should be overridden by derived class.
    516         &quot;&quot;&quot;
--&gt; 517         raise NotImplementedError(f&quot;Make sure `_init_weigths` is implemented for {self.__class__}&quot;)
    518 
    519     def tie_weights(self):

NotImplementedError: Make sure `_init_weigths` is implemented for &lt;class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'&gt;
</code></pre>
<p>I am using the last version of Transformers.</p>
","python, nlp, huggingface-transformers, bert-language-model, transformer-model","<p>As already mentioned in the comments, <code>DPRQuestionEncoder</code> does currently not provide any functionality to load other models. I still recommend creating your own class that inherits from <code>DPRQuestionEncoder</code> that loads your custom model and adjusts its method.</p>
<p>But you asked in the comments if there is another way, and yes there is in case the parameters of your model and the model that your <code>DPRQuestionEncoder</code> object is holding are <strong>completely the same</strong>. Please have a look at the commented example below:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertModel

# here I am just loading a bert model that represents your model
ordinary_bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
ordinary_bert.save_pretrained(&quot;this_is_a_bert&quot;)

import torch
from transformers import DPRQuestionEncoder

# now we load the state dict (i.e. weights and bias) of your model
ordinary_bert_state_dict = torch.load('this_is_a_bert/pytorch_model.bin')

# here we create a DPRQuestionEncoder object 
# the facebook/dpr-question_encoder-single-nq-base has the same parameters as bert-base-uncased
# You can compare the respective configs or model.parameters to be sure 
model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')

# we need to create the same keys (i.e. layer names) as your target model facebook/dpr-question_encoder-single-nq-base
ordinary_bert_state_dict = {f&quot;question_encoder.bert_model.{k}&quot;:v for k,v in ordinary_bert_state_dict.items()}

# now we can load the bert-base-uncased weights into your DPRQuestionEncoder object 
model.load_state_dict(ordinary_bert_state_dict)
</code></pre>
<p>It works from a technical perspective but I can not tell you how it will perform for your task.</p>
",2,1,2489,2021-09-13 08:26:35,https://stackoverflow.com/questions/69159507/load-a-model-as-dprquestionencoder-in-huggingface
Should feature embeddings be taken before or after dropout layer in neural network?,"<p>I am training a binary text classification model using BERT as follows:</p>
<pre><code>def create_model():
   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
   preprocessed_text = bert_preprocess(text_input)
   outputs = bert_encoder(preprocessed_text)

   # Neural network layers
   l1 = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(outputs['pooled_output'])
   l2 = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l1)

   # Use inputs and outputs to construct a final model
   model = tf.keras.Model(inputs=[text_input], outputs=[l2])
   return model
</code></pre>
<p>This code is borrowed from the example on tfhub: <a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"" rel=""nofollow noreferrer"">https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4</a>.</p>
<p>I want to extract feature embeddings from the penultimate layer and use them for comparison, clustering, visualization, etc between examples. Should this be done before dropout (l1 in the model above) or after dropout (l2 in the model above)?</p>
<p>I am trying to figure out whether this choice makes a significant difference, or is it fine either way? For example, if I extract feature embeddings after dropout and compute feature similarities between two examples, this might be affected by which nodes are randomly set to 0 (but perhaps this is okay).</p>
","tensorflow, neural-network, embedding, bert-language-model, dropout","<p>In order to answer your question let's recall how a Dropout layer works:</p>
<p>The Dropout layer is usually used as a means to mitigate overfitting. Suppose two layers, A and B, are connected through a Dropout layer. Then during the training phase, neurons in layer A are being randomly dropped. That prevents layer B from becoming too dependent upon specific neurons in layer A, as these neurons are not always available. Therefore, layer B has to take into consideration the overall signal coming from layer A, and (hopefully) cannot cling to some noise which is specific to the training set.</p>
<p>An important point to note is that the Dropout mechanism is activated only during the training phase. While predicting, Dropout does nothing.</p>
<p>If I understand you correctly, you want to know whether to take the features before or after the Dropout (note that in your network <code>l1</code> denotes the features after Dropout has been applied). If so, I would take the features before Dropout, because technically it does not really matter (Dropout is inactive during prediction) and it is more reasonable to do so (Dropout is not meaningful without a following layer).</p>
",1,-1,1064,2021-09-13 22:03:57,https://stackoverflow.com/questions/69169595/should-feature-embeddings-be-taken-before-or-after-dropout-layer-in-neural-netwo
How to add new special token to the tokenizer?,"<p>I want to build a multi-class classification model for which I have conversational data as input for the BERT model (using bert-base-uncased).</p>
<blockquote>
<p>QUERY: I want to ask a question.<br />
ANSWER: Sure, ask away.<br />
QUERY: How is the weather today?<br />
ANSWER: It is nice and sunny.<br />
QUERY: Okay, nice to know.<br />
ANSWER: Would you like to know anything else?</p>
</blockquote>
<p>Apart from this I have two more inputs.</p>
<p>I was wondering if I should put special token in the conversation to make it more meaning to the BERT model, like:</p>
<blockquote>
<p>[CLS]QUERY: I want to ask a question.  [EOT]<br />
ANSWER: Sure, ask away.  [EOT]<br />
QUERY: How is the weather today?  [EOT]<br />
ANSWER: It is nice and sunny.  [EOT]<br />
QUERY: Okay, nice to know.  [EOT]<br />
ANSWER: Would you like to know anything else?  [SEP]</p>
</blockquote>
<p>But I am not able to add a new [EOT] special token.<br />
Or should I use [SEP] token for this?</p>
<p>EDIT: steps to reproduce</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)

print(tokenizer.all_special_tokens) # --&gt; ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(tokenizer.all_special_ids)    # --&gt; [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[EOT]'])
model.resize_token_embeddings(len(tokenizer))  # --&gt; Embedding(30523, 768)

tokenizer.convert_tokens_to_ids('[EOT]')  # --&gt; 30522

text_to_encode = '''QUERY: I want to ask a question. [EOT]
ANSWER: Sure, ask away. [EOT]
QUERY: How is the weather today? [EOT]
ANSWER: It is nice and sunny. [EOT]
QUERY: Okay, nice to know. [EOT]
ANSWER: Would you like to know anything else?'''

enc = tokenizer.encode_plus(
  text_to_encode,
  max_length=128,
  add_special_tokens=True,
  return_token_type_ids=False,
  return_attention_mask=False,
)['input_ids']

print(tokenizer.convert_ids_to_tokens(enc))
</code></pre>
<p>Result:</p>
<blockquote>
<p>['[CLS]', 'query', ':', 'i', 'want', 'to', 'ask', 'a', 'question',
'.', '[', 'e', '##ot', ']', 'answer', ':', 'sure', ',', 'ask', 'away',
'.', '[', 'e', '##ot', ']', 'query', ':', 'how', 'is', 'the',
'weather', 'today', '?', '[', 'e', '##ot', ']', 'answer', ':', 'it',
'is', 'nice', 'and', 'sunny', '.', '[', 'e', '##ot', ']', 'query',
':', 'okay', ',', 'nice', 'to', 'know', '.', '[', 'e', '##ot', ']',
'answer', ':', 'would', 'you', 'like', 'to', 'know', 'anything',
'else', '?', '[SEP]']</p>
</blockquote>
","bert-language-model, huggingface-tokenizers, sentencepiece","<p>As the intention of the <code>[SEP]</code> token was to act as a separator between two sentence, it fits your objective of using <code>[SEP]</code> token to separate sequences of QUERY and ANSWER.</p>
<p>You also try to add different tokens to mark the beginning and end of QUERY or ANSWER as <code>&lt;BOQ&gt;</code> and <code>&lt;EOQ&gt;</code> to mark the beginning and end of QUERY. Likewise, <code>&lt;BOA&gt;</code> and <code>&lt;EOA&gt;</code> to mark the beginning and end of ANSWER.</p>
<p>Sometimes, using the existing token works much better than adding new tokens to the vocabulary, as it requires huge number of training iterations as well as the data to learn the new token embedding.</p>
<p>However, if you want to add a new token if your application demands so, then it can be added as follows:</p>
<pre><code>num_added_toks = tokenizer.add_tokens(['[EOT]'], special_tokens=True) ##This line is updated
model.resize_token_embeddings(len(tokenizer))

###The tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(&lt;output_dir&gt;)
</code></pre>
",17,12,30617,2021-09-15 10:24:19,https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer
Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace&#39;s transformers,"<p>I'm trying to build the model illustrated in this picture:
<a href=""https://i.sstatic.net/4eiAK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/4eiAK.png"" alt=""enter image description here"" /></a></p>
<p>I obtained a pre-trained BERT and respective tokenizer from HuggingFace's <code>transformers</code> in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TFBertModel
model_name = &quot;dbmdz/bert-base-italian-xxl-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
bert = TFBertModel.from_pretrained(model_name)
</code></pre>
<p>The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.</p>
<p>I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT.</p>
<p>I can do it outside of the model-building context:</p>
<pre><code>my_phrase = &quot;Ciao, come va?&quot;
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=True, return_tensors='tf', max_length=110, padding='max_length', truncation=True) 
attention_mask = bert_input &gt; 0
outputs = bert(bert_input, attention_mask)['pooler_output']
</code></pre>
<p>but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ):</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(net)
  X = tf.keras.layers.Concatenate(axis=-1)([X, input_layer])
  X = tf.keras.layers.MaxPooling1D(20)(X)
  X = tf.keras.layers.SpatialDropout1D(0.4)(X)
  X = tf.keras.layers.Flatten()(X)
  X = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(X)
  X = tf.keras.layers.Dropout(0.25)(X)
  X = tf.keras.layers.Dense(2, activation='softmax')(X)

  model = tf.keras.Model(inputs=text_input, outputs = X) 
  
  return model
</code></pre>
<p>And when I call the function for creating this model I get this error:</p>
<blockquote>
<p>text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
</blockquote>
<p>One thing I thought was that maybe I had to use the <code>tokenizer.batch_encode_plus</code> function which works with lists of strings:</p>
<pre class=""lang-py prettyprint-override""><code>class BertPreprocessingLayer(tf.keras.layers.Layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=True)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = BertPreprocessingLayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above
</code></pre>
<p>but I get this error:</p>
<blockquote>
<p>batch_text_or_text_pairs has to be a list (got &lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;)</p>
</blockquote>
<p>and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way.</p>
<p>I've also looked up on the huggingface's <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">documentation</a> but there is only a single usage example, with a single phrase, and what they do is analogous at my &quot;out of model-building context&quot; example.</p>
<p>EDIT:</p>
<p>I also tried with <code>Lambda</code>s in this way:</p>
<pre><code>tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.Lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)
</code></pre>
<p>but I get the following error:</p>
<blockquote>
<p>'Tensor' object has no attribute 'numpy'</p>
</blockquote>
<p>EDIT 2:</p>
<p>I also tried the approach suggested by @mdaoust of wrapping everything in a <code>tf.py_function</code> and got this error.</p>
<pre class=""lang-py prettyprint-override""><code>def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], Tout=[tf.int32, tf.int32, tf.int32])
</code></pre>
<blockquote>
<p>eager_py_func() missing 1 required positional argument: 'Tout'</p>
</blockquote>
<p>Then I defined Tout as the type of the value returned by the tokenizer:</p>
<p><code>transformers.tokenization_utils_base.BatchEncoding</code></p>
<p>and got the following error:</p>
<blockquote>
<p>Expected DataType for argument 'Tout' not &lt;class
'transformers.tokenization_utils_base.BatchEncoding'&gt;</p>
</blockquote>
<p>Finally I unpacked the value in the BatchEncoding in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask
</code></pre>
<p>And get an error in the line below:</p>
<pre class=""lang-py prettyprint-override""><code>...
outputs = bert(encoder_inputs)
</code></pre>
<blockquote>
<p>ValueError: Cannot take the length of shape with unknown rank.</p>
</blockquote>
","tensorflow, keras, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>For now I solved by taking the tokenization step out of the model:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in sentences:
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=True)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        
        
    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')
</code></pre>
<p>The model takes two inputs which are the first two values returned by the tokenize funciton.</p>
<pre class=""lang-py prettyprint-override""><code>def build_classifier_model():
   input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')
   input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') 

   embedding_layer = bert(input_ids_in, attention_mask=input_masks_in)[0]
...
   model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

   for layer in model.layers[:3]:
     layer.trainable = False
   return model
</code></pre>
<p>I'd still like to know if someone has a solution which integrates the tokenization step inside the model-building context so that an user of the model can simply feed phrases to it to get a prediction or to train the model.</p>
",3,8,6872,2021-09-15 15:28:40,https://stackoverflow.com/questions/69195950/problem-with-inputs-when-building-a-model-with-tfbertmodel-and-autotokenizer-fro
Using Hugging-face transformer with arguments in pipeline,"<p>I am working on using a transformer. Pipeline to get BERT embeddings to my input. using this without a pipeline i am able to get constant outputs but not with pipeline since I was not able to pass arguments to it.</p>
<p>How can I pass transformer-related arguments for my Pipeline?</p>
<pre class=""lang-py prettyprint-override""><code># These are BERT and tokenizer definitions
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

inputs = ['hello world']

# Normally I would do something like this to initialize the tokenizer and get the result with constant output
tokens = tokenizer(inputs,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model(**tokens)[0].detach().numpy().shape


# using the pipeline 
pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# or other option
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

nlp=pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# to call the pipeline
nlp(&quot;hello world&quot;)
</code></pre>
<p>I have tried several ways like the options listed above but was not able to get results with constant output size. one can achieve constant output size by setting the tokenizer arguments but have no idea how to give arguments for the pipeline.</p>
<p>any idea?</p>
","pytorch, huggingface-transformers, bert-language-model, transformer-model, huggingface-tokenizers","<p>The max_length tokenization parameter is not supported <a href=""https://github.com/huggingface/transformers/blob/a5fc34437dc2cfbd9e91f732820304c017b5525d/src/transformers/pipelines/base.py#L742"" rel=""nofollow noreferrer"">per default</a> (i.e. no padding to max_length is applied), but you can create your own class and overwrite this behavior:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
from transformers import FeatureExtractionPipeline
from transformers.tokenization_utils import TruncationStrategy

tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

inputs = ['hello world']

class MyFeatureExtractionPipeline(FeatureExtractionPipeline):
      def _parse_and_tokenize(
        self, inputs, max_length, padding=True, add_special_tokens=True, truncation=TruncationStrategy.DO_NOT_TRUNCATE, **kwargs
    ):
        &quot;&quot;&quot;
        Parse arguments and tokenize
        &quot;&quot;&quot;
        # Parse arguments
        if getattr(self.tokenizer, &quot;pad_token&quot;, None) is None:
            padding = False
        inputs = self.tokenizer(
            inputs,
            add_special_tokens=add_special_tokens,
            return_tensors=self.framework,
            padding=padding,
            truncation=truncation,
            max_length=max_length
        )
        return inputs

mynlp = MyFeatureExtractionPipeline(model=model, tokenizer=tokenizer)
o = mynlp(&quot;hello world&quot;, max_length = 500, padding='max_length', truncation=True)
</code></pre>
<p>Let us compare the size of the output:</p>
<pre class=""lang-py prettyprint-override""><code>print(len(o))
print(len(o[0]))
print(len(o[0][0]))
</code></pre>
<p>Output:</p>
<pre><code>1
500
768
</code></pre>
<p><strong>Please note</strong>: that this will only work with transformers 4.10.X and previous versions. The team is currently refactoring the pipeline classes and future releases will require different adjustments (i.e. that will not work as soon as the refactored pipelines are released).</p>
",2,4,2546,2021-09-15 16:47:04,https://stackoverflow.com/questions/69196995/using-hugging-face-transformer-with-arguments-in-pipeline
OOM error when training the BERT Keras model,"<p>I am working on training the fine-tuned BERT model using Keras. However when I start the training on GPU, I am facing the OOM error. The below is the code of my model.</p>
<pre><code>  max_len = 256
  input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
  input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
  segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;segment_ids&quot;)
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) 
  x = Dense(128, activation = &quot;relu&quot;)(sequence_output)
  x = Dropout(0.1)(x)
  out = Dense(1, activation = &quot;sigmoid&quot;)(x)

  model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)
  model.compile(Adam(lr = 2e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])
  model.summary()

  train_history = model.fit(train_input, train_labels, 
                      validation_split = 0.2,
                      epochs = 3,
                      batch_size = 16)
</code></pre>
<p>The error I get is</p>
<pre><code>   ---------------------------------------------------------------------------
   ResourceExhaustedError                    Traceback (most recent call last)
   &lt;timed exec&gt; in &lt;module&gt;

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py  in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split,     validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch,  validation_steps, validation_batch_size, validation_freq, max_queue_size, workers,  use_multiprocessing)
    1098                 _r=1):
    1099               callbacks.on_train_batch_begin(step)
 -&gt; 1100               tmp_logs = self.train_function(iterator)
    1101               if data_handler.should_sync:
    1102                 context.async_wait()

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in  __call__(self, *args, **kwds)
   826     tracing_count = self.experimental_get_tracing_count()
   827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
   829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;
   830       new_tracing_count = self.experimental_get_tracing_count()

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
   886         # Lifting succeeded, so variables are initialized and we can run the
   887         # stateless function.
--&gt; 888         return self._stateless_fn(*args, **kwds)
   889     else:
   890       _, _, _, filtered_flat_args = \

  /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
  2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
  2942     return graph_function._call_flat(
-&gt; 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  #  pylint: disable=protected-access
  2944 
  2945   @property

  /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
  1917       # No tape is watching; skip to running the function.
  1918       return self._build_call_outputs(self._inference_function.call(
-&gt; 1919           ctx, args, cancellation_manager=cancellation_manager))
  1920     forward_backward = self._select_forward_and_backward_functions(
  1921         args,

 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
  558               inputs=args,
  559               attrs=attrs,
--&gt; 560               ctx=ctx)
  561         else:
  562           outputs = execute.execute_with_cancellation(

 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
 58     ctx.ensure_initialized()
 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
 ---&gt; 60                                         inputs, attrs, num_outputs)
 61   except core._NotOkStatusException as e:
 62     if name is not None:

 ResourceExhaustedError:  OOM when allocating tensor with shape[16,160,1024] and type  float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
 [[{{node  model_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/encoder/layer_23/output_layer_norm/moments/SquaredDifference}}]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add  report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__inference_train_function_105694]

 Function call stack:
 train_function
</code></pre>
<p>Also, when I run the code with only the last dense layer, then I don't seem to have the OOM error. It happens only when I add the Dense(units = 128) and Dropout(0.1) layers.</p>
<p>Can anyone please help me on this?
Thank you in advance.</p>
","python, keras, nlp, sentiment-analysis, bert-language-model","<p>A quick google search led me to <a href=""https://datascience.stackexchange.com/a/76871"">this discussion</a> where he states the splitting of the data with <code>validation_split</code> parameter can lead to this OOM Error and the resolution was to split the data before calling <code>model.fit()</code> by using <code>sklearn.preprocessing.train_test_split()</code> or any other way you prefer.</p>
",1,0,242,2021-09-15 21:25:43,https://stackoverflow.com/questions/69199961/oom-error-when-training-the-bert-keras-model
TypeError in torch.argmax() when want to find the tokens with the highest `start` score,"<p>I want to run this code for question answering using hugging face transformers.</p>
<pre><code>import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#Model
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''Why was the student group called &quot;the Methodists?&quot;'''

paragraph = ''' The movement which would become The United Methodist Church began in the mid-18th century within the Church of England.
            A small group of students, including John Wesley, Charles Wesley and George Whitefield, met on the Oxford University campus.
            They focused on Bible study, methodical study of scripture and living a holy life.
            Other students mocked them, saying they were the &quot;Holy Club&quot; and &quot;the Methodists&quot;, being methodical and exceptionally detailed in their Bible study, opinions and disciplined lifestyle.
            Eventually, the so-called Methodists started individual societies or classes for members of the Church of England who wanted to live a more religious life. '''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)

inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)
</code></pre>
<p>but I get this error at the last line:</p>
<pre><code>Exception has occurred: TypeError
argmax(): argument 'input' (position 1) must be Tensor, not str
  File &quot;D:\bert\QuestionAnswering.py&quot;, line 33, in &lt;module&gt;
    start_index = torch.argmax(start_scores)
</code></pre>
<p>I don't know what's wrong. can anyone help me?</p>
","python, pytorch, torch, huggingface-transformers, bert-language-model","<p><code>BertForQuestionAnswering</code> returns a <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_outputs.html#QuestionAnsweringModelOutput"" rel=""nofollow noreferrer""><code>QuestionAnsweringModelOutput</code></a> object.</p>
<p>Since you set the output of <code>BertForQuestionAnswering</code> to <code>start_scores, end_scores</code>, the return <code>QuestionAnsweringModelOutput</code> object is forced convert to a tuple of strings <code>('start_logits', 'end_logits')</code> causing the type mismatch error.</p>
<p>The following should work:</p>
<pre><code>outputs = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(outputs.start_logits)
</code></pre>
",2,2,894,2021-09-19 02:52:22,https://stackoverflow.com/questions/69239925/typeerror-in-torch-argmax-when-want-to-find-the-tokens-with-the-highest-start
BERT text clasisification using pytorch,"<p>I am trying to build a BERT model for text classification with the help of this code<a href=""https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"" rel=""nofollow noreferrer""> [https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b]</a>. My dataset contains two columns(label, text).
The labels can have three values of (0,1,2). The code works without any error but all values of confusion matrix are 0. Is there something wrong with my code?</p>
<pre><code>import matplotlib.pyplot as plt
import pandas as pd
import torch
from torchtext.data import Field, TabularDataset, BucketIterator, Iterator
import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import seaborn as sns

torch.manual_seed(42)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

MAX_SEQ_LEN = 128
PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)


label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True, fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_t&gt;
fields = [('label', label_field), ('text', text_field)]
CLASSIFICATION_REPORT = &quot;classification_report.jsonl&quot;


train, valid, test = TabularDataset.splits(path='', train='train.csv', validation='validate.csv', test='test.csv', format='CSV', fields=fields, skip_header=True)

train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=True, sort=True, sort_within_batch=True)
valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=True, sort=True, sort_within_batch=True)
test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)

class BERT(nn.Module):
        def __init__(self):
                super(BERT, self).__init__()
                options_name = &quot;bert-base-uncased&quot;
                self.encoder = BertForSequenceClassification.from_pretrained(options_name, num_labels = 3)

        def forward(self, text, label):
                loss, text_fea = self.encoder(text, labels=label)[:2]
                return loss, text_fea

def train(model, optimizer, criterion = nn.BCELoss(), train_loader = train_iter, valid_loader = valid_iter, num_epochs = 5, eval_every = len(train_iter) // 2, file_pat&gt;        running_loss = 0.0
        valid_running_loss = 0.0
        global_step = 0
        train_loss_list = []
        valid_loss_list = []
        global_steps_list = []

        model.train()

        for epoch in range(num_epochs):
                for (label, text), _ in train_loader:
                        label = label.type(torch.LongTensor)
                        label = label.to(device)
                        text = text.type(torch.LongTensor)
                        text = text.to(device)
                        output = model(text, label)
                        loss, _ = output
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        global_step += 1
                        if global_step % eval_every == 0:
                                model.eval()
                                with torch.no_grad():
                                        for (label, text), _ in valid_loader:
                                                label = label.type(torch.LongTensor)
                                                label = label.to(device)
                                                text = text.type(torch.LongTensor)
                                                text = text.to(device)
                                                output = model(text, label)
                                                loss, _ = output
                                                valid_running_loss += loss.item()

                                average_train_loss = running_loss / eval_every
                                average_valid_loss = valid_running_loss / len(valid_loader)
                                train_loss_list.append(average_train_loss)
                                valid_loss_list.append(average_valid_loss)
                                global_steps_list.append(global_step)


                                # resetting running values
                                running_loss = 0.0
                                valid_running_loss = 0.0
                                model.train()

                                # print progress
                                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'.format(epoch+1, num_epochs, global_step, num_epochs*len(tra&gt;
                                if best_valid_loss &gt; average_valid_loss:
                                        best_valid_loss = average_valid_loss
        print('Finished Training!')

model = BERT().to(device)
optimizer = optim.Adam(model.parameters(), lr=2e-5)

train(model=model, optimizer=optimizer)


def evaluate(model, test_loader):
        y_pred = []
        y_true = []
        model.eval()
        with torch.no_grad():
                for (label, text), _ in test_loader:
                        label = label.type(torch.LongTensor)
                        label = label.to(device)
                        text = text.type(torch.LongTensor)
                        text = text.to(device)
                        output = model(text, label)

                        _, output = output
                        y_pred.extend(torch.argmax(output, 2).tolist())
                        y_true.extend(label.tolist())
        print('Classification Report:')
        print(classification_report(y_true, y_pred, labels=[0,1,2], digits=4))
best_model = BERT().to(device)
evaluate(best_model, test_iter)
</code></pre>
","python, nlp, pytorch, classification, bert-language-model","<p>you are using criterion = nn.BCELoss(), binary cross entropy for a multi class classification problem, &quot;the labels can have three values of (0,1,2)&quot;. use suitable loss function for multiclass classification.</p>
",0,0,828,2021-09-20 05:57:24,https://stackoverflow.com/questions/69249665/bert-text-clasisification-using-pytorch
transformers and BERT downloading to your local machine,"<p>I am trying to replicates the code from <a href=""https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX"" rel=""nofollow noreferrer"">this page</a>.</p>
<p>At my workplace we have access to transformers and pytorch library but cannot connect to internet from our python environment. Could anyone help with how we could get the script working after manually downloading files to my machine?</p>
<p>my specific questions are -</p>
<ol>
<li><p>should I go to the location <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">bert-base-uncased at main</a> and download all the files? Do I have put them in a folder with a specific name?</p>
</li>
<li></li>
</ol>
<p>How should I change the below code</p>
<pre><code># Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# Tokenize our sentence with the BERT tokenizer.
tokenized_text = tokenizer.tokenize(marked_text)
</code></pre>
<ol start=""3"">
<li></li>
</ol>
<p>How should I change the below code</p>
<pre><code># Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased',

                                  output_hidden_states = True, # Whether the model returns all hidden-states.

                              )
</code></pre>
<p>Please let me know if anyone has done this…thanks</p>
<p>###update1</p>
<p>I went to the <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">link</a> and manually downloaded all files to a folder and specified path of that folder in my code. Tokenizer works but this line <code>model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, # Whether the model returns all hidden-states. )</code> fails. Any idea what should i do? I noticed that 4 big files when downloaded have very strange name...should I rename them to same names as shown on the above page? Do I need to download any other files?</p>
<p>the error message is <code>OSErrr: unable to load weights from pytorch checkpoint file for bert-base-uncased2/ at bert-base-uncased/pytorch_model.bin If you tried to load a pytroch model from a TF 2 checkpoint, please set from_tf=True</code></p>
","python, torch, bert-language-model, transformer-model, doc2vec","<p>clone the model repo for downloading all the files</p>
<pre><code>git lfs install
git clone https://huggingface.co/bert-base-uncased

# if you want to clone without large files – just their pointers
# prepend your git clone with the following env var:
GIT_LFS_SKIP_SMUDGE=1
</code></pre>
<p>git usage:</p>
<ol>
<li><p>download git from here <a href=""https://git-scm.com/downloads"" rel=""nofollow noreferrer"">https://git-scm.com/downloads</a></p>
</li>
<li><p>paste these to your cli(terminal):<br>
a. git lfs install<br>
b. git clone <a href=""https://huggingface.co/bert-base-uncased"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased</a></p>
</li>
<li><p>wait for download, it will take time. if you want monitor your web performance</p>
</li>
<li><p>find the current directory simply pasting cd to your cli and get the file path(e.g &quot;C:/Users/........./bert-base-uncased&quot; )</p>
</li>
<li><p>use it as:</p>
<pre><code> from transformers import BertModel, BertTokenizer
 model = BertModel.from_pretrained(&quot;C:/Users/........./bert-base-uncased&quot;)
 tokenizer = BertTokenizer.from_pretrained(&quot;C:/Users/........./bert-base-uncased&quot;)
</code></pre>
</li>
</ol>
<p><strong>Manual download, without git:</strong></p>
<ol>
<li><p>Download all the files from here <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/tree/main</a></p>
</li>
<li><p>Put them in a folder named &quot;yourfoldername&quot;</p>
</li>
<li><p>use it as:</p>
<pre><code> model = BertModel.from_pretrained(&quot;C:/Users/........./yourfoldername&quot;)
 tokenizer = BertTokenizer.from_pretrained(&quot;C:/Users/........./yourfoldername&quot;)
</code></pre>
</li>
</ol>
<p><strong>For only model(manual download, without git):</strong></p>
<ol>
<li><p>just click the download button here and download only pytorch pretrained model. its about 420mb
<a href=""https://huggingface.co/bert-base-uncased/blob/main/pytorch_model.bin"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/blob/main/pytorch_model.bin</a></p>
</li>
<li><p>download config.json file from here <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/tree/main</a></p>
</li>
<li><p>put both of them in a folder named &quot;yourfilename&quot;</p>
</li>
<li><p>use it as:</p>
<pre><code> model = BertModel.from_pretrained(&quot;C:/Users/........./yourfilename&quot;)
</code></pre>
</li>
</ol>
",4,1,19910,2021-09-22 15:07:44,https://stackoverflow.com/questions/69286889/transformers-and-bert-downloading-to-your-local-machine
"Loading tf.keras model, ValueError: The two structures don&#39;t have the same nested structure","<p>I created a <code>tf.keras model</code> that has <strong>BERT</strong> and I want to train and save it for further use.
Loading this model is a big issue cause I keep getting error: <code>ValueError: The two structures don't have the same nested structure.</code></p>
<p>I simplified the model a lot, to see where is the problem exactly. The code is pretty simple:</p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;bert-base-german-cased&quot;)

model_name = &quot;Model&quot;
txt12_input_ids = tf.keras.layers.Input(shape=(max_length,),  name='txt12_input_ids', dtype='int32')
txt12_mask      = tf.keras.layers.Input(shape=(max_length,),  name='txt12_mask', dtype='int32')
txt12_outputs = bert(txt12_input_ids, txt12_mask).pooler_output

model_K = tf.keras.Model(inputs=(txt12_input_ids,  txt12_mask), outputs=txt12_outputs, name=model_name)
model_K.compile(optimizer=Adam(1e-5), loss=&quot;binary_crossentropy&quot;, metrics=&quot;accuracy&quot;)


model_K.save(dir_path+'Prob')
model_2 = tf.keras.models.load_model(dir_path+'Prob')
</code></pre>
<p><em>Some notes before you start replying:</em></p>
<ol>
<li><p>I did specified <code>dtype</code>.</p>
</li>
<li><p>No, I don't want to save just weights.</p>
</li>
<li><p>I tried to use <code>tf.keras.models.save_model(model_K, dir_path+'Prob')</code> instead and it gives the same error.</p>
</li>
</ol>
<p>And the last thing, I work with <code>tf version: 2.6.0</code>. Does anyone knows how to solve it?</p>
<p>Full error message:</p>
<pre><code>ValueError: The two structures don't have the same nested structure.

First structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, None, False), {})

Second structure: type=tuple str=((TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 120), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, None, False), {})

More specifically: Substructure &quot;type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}&quot; is a sequence, while substructure &quot;type=TensorSpec str=TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids')&quot; is not
Entire first structure:
(({'input_ids': .}, ., ., ., ., ., ., ., ., .), {})
Entire second structure:
((., ., ., ., ., ., ., ., ., .), {})
</code></pre>
","python, tensorflow, keras, huggingface-transformers, bert-language-model","<p>Check out the <a href=""https://github.com/huggingface/transformers/issues/3627"" rel=""nofollow noreferrer"">issue here</a> on GitHub. It should help you to solve the problem with the shape.</p>
",3,4,3882,2021-09-28 14:57:06,https://stackoverflow.com/questions/69364068/loading-tf-keras-model-valueerror-the-two-structures-dont-have-the-same-neste
How to use SciBERT in the best manner?,"<p>I'm trying to use BERT models to do text classification. As the text is about scientific texts, I intend to use the <strong>SicBERT</strong> pre-trained model: <a href=""https://github.com/allenai/scibert"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert</a></p>
<p>I have faced several limitations which I want to know if there is any solutions for them:</p>
<ol>
<li><p>When I want to do tokenization and batching, it only allows me to use <code>max_length</code> of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</li>
<li><p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</li>
<li><p>I know this is a general question, but any suggestion that I can improve my fine tuning (from data to hyper parameter, etc)? Currently, I'm getting ~75% accuracy. Thanks</p>
</li>
</ol>
<p><strong>Codes:</strong></p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

encoded_data_train = tokenizer.batch_encode_plus(
    df_train.text.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    padding=True,
    max_length=256
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df_train.label.values)

dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased',
                                                      num_labels=len(labels),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

epochs = 1

optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)

scheduler = get_linear_schedule_with_warmup(optimizer,
num_training_steps=len(dataloader_train)*epochs)
</code></pre>
","nlp, pytorch, text-classification, huggingface-transformers, bert-language-model","<blockquote>
<p>When I want to do tokenization and batching, it only allows me to use max_length of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</blockquote>
<p>Yes, you are not using the complete text. And this is one of the limitations of BERT and T5 models, which limit to using 512 and 1024 tokens resp. to the best of my knowledge.</p>
<p>I can suggest you to use <code>Longformer</code> or <code>Bigbird</code> or <code>Reformer</code> models, which can handle sequence lengths up to <code>16k</code>, <code>4096</code>, <code>64k</code> tokens respectively. These are really good for processing longer texts like scientific documents.</p>
<blockquote>
<p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</blockquote>
<p><code>SciBERT</code> is actually a pre-trained BERT model.
See this <a href=""https://github.com/huggingface/transformers/issues/2902"" rel=""nofollow noreferrer"">issue</a> for more details where they mention the feasibility of converting BERT to ROBERTa:</p>
<p><code>Since you're working with a BERT model that was pre-trained, you unfortunately won't be able to change the tokenizer now from a WordPiece (BERT) to a Byte-level BPE (RoBERTa).</code></p>
<blockquote>
<p>I know this is a general question, but any suggestion that I can
improve my fine tuning (from data to hyper parameter, etc)? Currently,
I'm getting ~79% accuracy.</p>
</blockquote>
<p>I would first try to tune the most important hyperparameter <code>learning_rate</code>. I would then explore different values for hyperparameters of <code>AdamW</code> optimizer and <code>num_warmup_steps</code> hyperparamter of the scheduler.</p>
",0,0,2485,2021-10-01 13:44:52,https://stackoverflow.com/questions/69406937/how-to-use-scibert-in-the-best-manner
How does BERT word embedding preprocess work,"<p>I'm trying to figure out what <code>BERT</code> preprocess does. I mean, how it is done. But I can't find a good explanation. I would appreciate, if somebody know, a link to a better and deeply explained solution.
If someone, by the other hand, wants to solve it here, I would be also extremly thankful!</p>
<p>My question is, how does <code>BERT</code> mathematically convert a string input into a vector of numbers with fixed size? Which are the logical steps that follows?</p>
","nlp, huggingface-transformers, bert-language-model, transformer-model","<p>BERT provides its own tokenizer. Because BERT is a pretrained model that expects input data in a specific format, following are required:</p>
<ul>
<li>A special token, <code>[SEP]</code>, to mark the end of a sentence, or the
separation between two sentences</li>
<li>A special token, <code>[CLS]</code>, at the
beginning of our text. This token is used for classification tasks,
but BERT expects it no matter what your application is.</li>
<li>Tokens that conform with the fixed vocabulary used in BERT</li>
<li>The Token IDs for the tokens, from BERT’s tokenizer</li>
<li>Mask IDs to indicate which elements in the sequence are tokens and which are padding elements</li>
<li>Segment IDs used to distinguish different sentences</li>
<li>Positional Embeddings used to show token position within the sequence</li>
</ul>
<p>.</p>
<pre><code>from transformers import BertTokenizer

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# An example sentence 
text = &quot;Sentence to embed&quot;

# Add the special tokens.
marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot;

# Split the sentence into tokens.
tokenized_text = tokenizer.tokenize(marked_text)

# Map the token strings to their vocabulary indices.
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) 
</code></pre>
<p>Have a look at this excellent <a href=""https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"" rel=""nofollow noreferrer"">tutorial</a> for more details.</p>
",3,1,1970,2021-10-03 20:54:45,https://stackoverflow.com/questions/69428811/how-does-bert-word-embedding-preprocess-work
BERT Heads Count,"<p>From the literature I read,</p>
<p>Bert Base has 12 encoder layers and 12 attention heads.  Bert Large has 24 encoder layers and 16 attention heads.</p>
<p>Why is Bert large having 16 attentions heads ?</p>
","bert-language-model, transformer-model","<p>The number of attention heads is irrespective of the number of (encoder) layers.
However, there is an inherent tie between the hidden size of each model (768 for <code>bert-base</code>, and 1024 for <code>bert-large</code>), which is explained in <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">the original Transformers paper</a>.
Essentially, the choice made by the authors is that the self-attention block size (<code>d_k</code>) equals the hidden dimension (<code>d_hidden</code>), divided by the number of heads (<code>h</code>), or formally</p>
<pre><code>d_k = d_hidden / h
</code></pre>
<p>Since the standard choice seems to be <code>d_k = 64</code>, we can infer the final size from our parameters:</p>
<pre><code>h = d_hidden / d_k = 1024 / 64 = 16
</code></pre>
<p>which is exactly the value you are looking at in <code>bert-large</code>.</p>
",6,3,1897,2021-10-04 13:29:17,https://stackoverflow.com/questions/69436845/bert-heads-count
How to find the similarity of sentences in 2 columns of a dataframe using spacy,"<p>I pulled this code from <a href=""https://spacy.io/universe/project/spacy-sentence-bert"" rel=""nofollow noreferrer"">https://spacy.io/universe/project/spacy-sentence-bert</a></p>
<pre><code>import spacy_sentence_bert
# load one of the models listed at https://github.com/MartinoMensio/spacy-sentence-bert/
nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
# get two documents
doc_1 = nlp('Hi there, how are you?')
doc_2 = nlp('Hello there, how are you doing today?')
# use the similarity method that is based on the vectors, on Doc, Span or Token
print(doc_1.similarity(doc_2[0:7]))
</code></pre>
<p>I have a dataframe with 2 columns containing sentences like below. I'm trying to find the similarity between the sentences in each row. I've tried a few different methods but not having much luck so figured I would ask here. Thank you all.</p>
<p><strong>Current df</strong></p>
<pre><code>Sentence1 | Sentence2

Another-Sentence1 | Another-Sentence2

Yet-Another-Sentence1 | Yet-Another-Sentence2
</code></pre>
<p><strong>Goal output:</strong></p>
<pre><code>Sentence1 | Sentence2 | Similarity-Score-Sentence1-Sentence2

Another-Sentence1 | Another-Sentence2 | Similarity-Score-Another-Sentence1-Another-Sentence2

Yet-Another-Sentence1 | Yet-Another-Sentence2 | Similarity-Score-Yet-Another-Sentence1-Yet-Another-Sentence2
</code></pre>
","python, pandas, spacy, similarity, bert-language-model","<p>I assume that your first row consists of headers, the data will start from the next row after header, and also assume that you are using panda to convert csv to dataframe, the below code works in my environment.</p>
<pre><code>import spacy_sentence_bert
import pandas as pd
nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
df = pd.read_csv('testing.csv')
similarityValue = []

for i in range(df.count()[0]):
    sentence_1 = nlp(df.iloc[i][0])
    sentence_2 = nlp(df.iloc[i][1])
    similarityValue.append(sentence_1.similarity(sentence_2))
    print(sentence_1, '|', sentence_2, '|', sentence_1.similarity(sentence_2))

df['Similarity'] = similarityValue
print(df)
</code></pre>
<p>Input CSV:</p>
<p><a href=""https://i.sstatic.net/fg2Zy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fg2Zy.png"" alt=""enter image description here"" /></a></p>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/xAlAc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xAlAc.png"" alt=""enter image description here"" /></a></p>
",2,3,2330,2021-10-05 00:56:39,https://stackoverflow.com/questions/69443805/how-to-find-the-similarity-of-sentences-in-2-columns-of-a-dataframe-using-spacy
ImportError: cannot import name &#39;SAVE_STATE_WARNING&#39; from &#39;torch.optim.lr_scheduler&#39;,"<p>I am attempting to issue this statement in a jupyter Notebook.</p>
<pre><code>from transformers import BertForQuestionAnswering
</code></pre>
<p>I get the error:</p>
<blockquote>
<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (C:\Users\sbing.conda\envs\Tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)</p>
</blockquote>
<p>Here is the complete stack:</p>
<blockquote>
<p>ImportError                               Traceback (most recent call last)
 in 
----&gt; 1 from transformers import BertForQuestionAnswering</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers_<em>init</em>_.py in 
624
625     # Trainer
--&gt; 626     from .trainer import Trainer
627     from .trainer_pt_utils import torch_distributed_zero_first
628 else:</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers\trainer.py in 
67     TrainerState,
68 )
---&gt; 69 from .trainer_pt_utils import (
70     DistributedTensorGatherer,
71     SequentialDistributedSampler,</p>
<p>~.conda\envs\Tensorflow\lib\site-packages\transformers\trainer_pt_utils.py in 
38     SAVE_STATE_WARNING = &quot;&quot;
39 else:
---&gt; 40     from torch.optim.lr_scheduler import SAVE_STATE_WARNING
41
42 logger = logging.get_logger(<strong>name</strong>)</p>
<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (C:\Users\sbing.conda\envs\Tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)</p>
</blockquote>
","import, huggingface-transformers, bert-language-model, nlp-question-answering","<p>You need to update the transformer package to the latest version. You can achieve it by running this code:</p>
<pre><code>!pip install transformers==4.11.3.
</code></pre>
<p>For me, there is no error after updating. Refer to these links <a href=""https://github.com/huggingface/transformers/pull/8979"" rel=""nofollow noreferrer"">official resource</a> and <a href=""https://github.com/huggingface/transformers/issues/9060"" rel=""nofollow noreferrer"">this</a></p>
",2,0,3616,2021-10-06 21:52:19,https://stackoverflow.com/questions/69473082/importerror-cannot-import-name-save-state-warning-from-torch-optim-lr-schedu
TF BERT input packer on more than two inputs,"<p>Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to &quot;pack&quot; inputs. E.g. in <a href=""https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb"" rel=""nofollow noreferrer"">this example</a>,</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))</code></p>
<p>The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like:</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20))</code></p>
<p>However, so doing causes the error at the bottom[1] of this post.</p>
<p>I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab?</p>
<p>1.</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * [tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_2:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_1:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_3:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_5:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_4:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_6:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_8:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_7:0&quot;, shape=(2,), dtype=int64))]
    * Tensor(&quot;seq_length:0&quot;, shape=(), dtype=int32)
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 3:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64), RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 4:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}```
</code></pre>
","tensorflow, bert-language-model","<p>BERT model expects specific input shape.</p>
<p><strong>Working sample code:</strong></p>
<pre><code>bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;,trainable=True)

def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embeding([
    &quot;How to find which version of TensorFlow is&quot;, 
    &quot;TensorFlow not found using pip&quot;]
)
</code></pre>
",1,3,642,2021-10-09 18:11:22,https://stackoverflow.com/questions/69509388/tf-bert-input-packer-on-more-than-two-inputs
BERT get sentence embedding,"<p>I am replicating code from <a href=""https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX"" rel=""noreferrer"">this page</a>. I have downloaded the BERT model to my local system and getting sentence embedding.</p>
<p>I have around 500,000 sentences for which I need sentence embedding and it is taking a lot of time.</p>
<ol>
<li>Is there a way to expedite the process?</li>
<li>Would sending batches of sentences rather than one sentence at a time help?</li>
</ol>
<p>.</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

corpa=[&quot;i am a boy&quot;,&quot;i live in a city&quot;]



storage=[]#list to store all embeddings

for text in corpa:
    # Add the special tokens.
    marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot;

    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

    segments_ids = [1] * len(tokenized_text)

    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)

    storage.append((text,sentence_embedding))
</code></pre>
<p>######update 1</p>
<p>I modified my code based upon the answer provided. It is not doing full batch processing</p>
<pre><code>#!pip install transformers
import torch
import transformers
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

# Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation.
model.eval()

batch_sentences = [&quot;Hello I'm a single sentence&quot;,
                    &quot;And another sentence&quot;,
                    &quot;And the very very last one&quot;]
encoded_inputs = tokenizer(batch_sentences)


storage=[]#list to store all embeddings
for i,text in enumerate(encoded_inputs['input_ids']):
    
    tokens_tensor = torch.tensor([encoded_inputs['input_ids'][i]])
    segments_tensors = torch.tensor([encoded_inputs['attention_mask'][i]])
    print (tokens_tensor)
    print (segments_tensors)

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # Calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)
    print (sentence_embedding[:10])
    storage.append((text,sentence_embedding))
</code></pre>
<p>I could update first 2 lines from the for loop to below. But they work only if all sentences have same length after tokenization</p>
<pre><code>tokens_tensor = torch.tensor([encoded_inputs['input_ids']])
segments_tensors = torch.tensor([encoded_inputs['attention_mask']])
</code></pre>
<p>moreover in that case <code>outputs = model(tokens_tensor, segments_tensors) </code> fails.</p>
<p>How could I fully perform batch processing in such case?</p>
","python, nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>About your original question: there is not much you can do. BERT is pretty computationally demanding algorithm. Your best shot is to use <code>BertTokenizerFast</code> instead of the regular <code>BertTokenizer</code>. The &quot;fast&quot; version is much more efficient and you will see the difference for large amounts of text.</p>
<p>Saying that, I have to warn you that averaging BERT word embeddings  does not create good embeddings for the sentence. See <a href=""https://medium.com/dair-ai/tl-dr-sentencebert-8dec326daf4e"" rel=""nofollow noreferrer"">this</a> post. From your questions I assume you want to do some kind of <a href=""https://www.sbert.net/examples/applications/semantic-search/README.html"" rel=""nofollow noreferrer"">semantic similarity</a> search. Try using one of those <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">open-sourced models</a>.</p>
",2,6,12612,2021-10-10 17:32:31,https://stackoverflow.com/questions/69517460/bert-get-sentence-embedding
Input/output format for Fine Tuning Huggingface RobertaForQuestionAnswering,"<p>I'm trying to fine-tune &quot;RobertaForQuestionAnswering&quot; on my custom dataset and I'm confused about the input params it takes. Here's the sample code.</p>
<pre><code>&gt;&gt;&gt; from transformers import RobertaTokenizer, RobertaForQuestionAnswering
&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
&gt;&gt;&gt; model = RobertaForQuestionAnswering.from_pretrained('roberta-base')

&gt;&gt;&gt; question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;
&gt;&gt;&gt; inputs = tokenizer(question, text, return_tensors='pt')
&gt;&gt;&gt; start_positions = torch.tensor([1])
&gt;&gt;&gt; end_positions = torch.tensor([3])

&gt;&gt;&gt; outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
&gt;&gt;&gt; loss = outputs.loss
&gt;&gt;&gt; start_scores = outputs.start_logits
&gt;&gt;&gt; end_scores = outputs.end_logits
</code></pre>
<p>I'm not able to understand variables <strong>start_positions</strong> &amp; <strong>end_positions</strong> which are being given in the model as input and variables <strong>start_scores</strong> &amp; <strong>end_scores</strong> that are being generated.</p>
","nlp, huggingface-transformers, bert-language-model, nlp-question-answering, roberta-language-model","<p>A Question Answering ot is basically a DL model that creates an answer by <em>extracting part of the context</em> (in you case what is called <code>text</code>). This means that the goal of the QAbot is to identify the <strong>start</strong> and the <strong>end</strong> of the answer.</p>
<hr />
<p>Basic functioning of a QAbot:</p>
<p>First of all, every word of the question and context is tokenized. This means it is (possibily divided into characters/subwords and then) convertend into a number. It really depends on the type of tokenizer (which means it depends on the model you are using, since you will be using the same tokenizer - it's what the third line of your code is doing). I suggest <a href=""https://huggingface.co/course/chapter2/4?fw=pt"" rel=""nofollow noreferrer"">this very useful guide</a>.</p>
<p>Then, the tokenized <code>question + text</code> are passed into the model which performs its internal operations. Remember when I told at the beginning that the model will identify the <code>start</code> and the <code>end</code> of the answer? Well, it does so by calculating for every token of the <code>question + text</code> the probability that that particular token is the start of the answer. This probabilities are the softmaxed version of the <code>start_logits</code>. After that, the same operations are performed for the end token.</p>
<p>So, this is what <code>start_scores</code> and <code>end_scores</code> are: the pre-softmax scores that every token is start and end of the answer, respectively.</p>
<hr />
<p>So, what are <code>start_position</code> and <code>stop_position</code>?</p>
<p>As stated <a href=""https://huggingface.co/transformers/model_doc/roberta.html#robertaforquestionanswering"" rel=""nofollow noreferrer"">here</a>, they are:</p>
<blockquote>
<p><code>start_positions</code> (<code>torch</code>.<code>LongTensor</code> of shape (<code>batch_size</code>,), optional) –
Labels for position (index) of the start of the labelled span for
computing the token classification loss. Positions are clamped to the
length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.</p>
<p><code>end_positions</code> (<code>torch</code>.<code>LongTensor</code> of shape (<code>batch_size</code>,), optional) –
Labels for position (index) of the end of the labelled span for
computing the token classification loss. Positions are clamped to the
length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.</p>
</blockquote>
<hr />
<p>Moreover, the model you are using (<code>roberta-base</code>, see <a href=""https://huggingface.co/roberta-base"" rel=""nofollow noreferrer"">the model on the HuggingFace repository</a> and the <a href=""https://arxiv.org/abs/1907.11692"" rel=""nofollow noreferrer"">RoBERTa official paper</a>) has <strong>NOT</strong> been fine-tuned for QuestionAnswering. It is &quot;just&quot; a model trained by using MaskedLanguageModeling, which means that the model has a general understanding of the english language, but it is not suitable for question asnwering. You can use it of course, but it would probably give non optimal results.</p>
<p>I suggest you use the same model, inthe version specifically fine-tuned on QuestionAnswering: <code>roberta-base-squad2</code>, see <a href=""https://huggingface.co/deepset/roberta-base-squad2?context=Jim%20Henson%20was%20a%20nice%20puppet&amp;question=Who%20was%20Jim%20Henson%3F"" rel=""nofollow noreferrer"">it on HuggingFace</a>.</p>
<p>In practical terms, you have to replace the lines where you load the model and the tokenizer with:</p>
<pre><code>tokenizer = RobertaTokenizer.from_pretrained('roberta-base-squad2')
model = RobertaForQuestionAnswering.from_pretrained('roberta-base-squad2')
</code></pre>
<p>This will give much more accurate results.</p>
<p>Bonus read: <a href=""https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"" rel=""nofollow noreferrer"">what fine-tuning is and how it works</a></p>
",1,2,930,2021-10-12 17:18:29,https://stackoverflow.com/questions/69544570/input-output-format-for-fine-tuning-huggingface-robertaforquestionanswering
Using Sentence-Bert with other features in scikit-learn,"<p>I have a dataset, one feature is text and 4 more features. Sentence-Bert vectorizer transforms text data into tensors. I can use these sparse matrices directly with a machine learning classifier. Can I replace the text column with tensors? And, how can I train the model. The code below is how I transform the text into vectors.</p>
<pre><code>model = SentenceTransformer('sentence-transformers/LaBSE')
sentence_embeddings = model.encode(X_train['tweet'], convert_to_tensor=True, show_progress_bar=True)
sentence_embeddings1 = model.encode(X_test['tweet'], convert_to_tensor=True, show_progress_bar=True)
</code></pre>
","python, machine-learning, nlp, embedding, bert-language-model","<p>Let's assume this is your data</p>
<pre><code>X_train = pd.DataFrame({
    'tweet':['foo', 'foo', 'bar'],
    'feature1':[1, 1, 0],
    'feature2':[1, 0, 1],
})
y_train = [1, 1, 0]
</code></pre>
<p>and you are willing to use it with <code>sklearn</code> API (cross-validation, pipeline, grid-search, and so on). There is a utility named <code>ColumnTransformer</code> which can map pandas data frames to the desired data using user-defined arbitrary functions! what you have to do is define a function and create an official <code>sklearn.transformer</code> from it.</p>
<pre><code>model = SentenceTransformer('mrm8488/bert-tiny-finetuned-squadv2') # model named is changed for time and computation gians :)
embedder = FunctionTransformer(lambda item:model.encode(item, convert_to_tensor=True, show_progress_bar=False).detach().cpu().numpy())
</code></pre>
<p>After that, you would be able to use the transformer like any other transformer and map your text column into semantic space, like:</p>
<pre><code>preprocessor = ColumnTransformer(
    transformers=[('embedder', embedder, 'tweet')],
    remainder='passthrough'
    )
X_train = preprocessor.fit_transform(X_train) # X_train.shape =&gt; (len(df), your_transformer_model_hidden_dim + your_features_count)
</code></pre>
<p><code>X_train</code> would be the data you wanted. It's proper to use with <code>sklearn</code> ecosystem.</p>
<pre><code>gnb = GaussianNB()
gnb.fit(X_train, y_train) 
</code></pre>
<p>output:
<code>GaussianNB(priors=None, var_smoothing=1e-09)</code></p>
<p><strong>caveat</strong>: Numerical features and the tweets embeddings should belong to the same <strong>SCALE</strong> otherwise some would dominate others and degrade the performance</p>
",4,3,2058,2021-10-15 13:03:14,https://stackoverflow.com/questions/69585176/using-sentence-bert-with-other-features-in-scikit-learn
"Is it necessary to re-train BERT models, specifically RoBERTa model?","<p>I am looking for a sentiment analysis code with atleast 80%+ accuracy. I tried Vader and it I found it easy and usable, however it was giving accuracy of 64% only.</p>
<p>Now, I was looking at some BERT models and I noticed it needs to be re-trained? Is that correct? Isn't it pre-trained? is re-training necessary?</p>
","python, tensorflow, sentiment-analysis, bert-language-model, roberta-language-model","<p>You can use pre-trained models from  <code>HuggingFace</code>. There are plenty to choose from. Search for <code>emotion</code> or <code>sentiment</code> <a href=""https://huggingface.co/models?language=en&amp;pipeline_tag=text-classification&amp;sort=likes&amp;search=Emotion"" rel=""nofollow noreferrer"">models</a></p>
<p>Here is an example of a model with 26 emotions. The current implementation works but is very slow for large datasets.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;arpanghoshal/EmoRoBERTa&quot;)
model = TFRobertaForSequenceClassification.from_pretrained(&quot;arpanghoshal/EmoRoBERTa&quot;)


emotion = pipeline('sentiment-analysis', 
                    model='arpanghoshal/EmoRoBERTa')

# example data
DATA_URI = &quot;https://github.com/AFAgarap/ecommerce-reviews-analysis/raw/master/Womens%20Clothing%20E-Commerce%20Reviews.csv&quot;
dataf = pd.read_csv(DATA_URI, usecols=[&quot;Review Text&quot;,])

# This is super slow, I will find a better optimization ASAP


dataf = (dataf
         .head(50) # comment this out for the whole dataset
         .assign(Emotion = lambda d: (d[&quot;Review Text&quot;]
                                       .fillna(&quot;&quot;)
                                       .map(lambda x: emotion(x)[0].get(&quot;label&quot;, None))
                                  ),
             
            )
)

</code></pre>
<p>We could also refactor it a bit</p>
<pre class=""lang-py prettyprint-override""><code>...
# a bit faster than the previous but still slow

def emotion_func(text:str) -&gt; str:
    if not text:
        return None
    return emotion(text)[0].get(&quot;label&quot;, None)
    



dataf = (dataf
         .head(50) # comment this out for the whole dataset
         .assign(Emotion = lambda d: (d[&quot;Review Text&quot;]
                                        .map(emotion_func)
                                     ),

            )
)

</code></pre>
<h2>Results:</h2>
<pre class=""lang-sh prettyprint-override""><code>    Review Text Emotion
0   Absolutely wonderful - silky and sexy and comf...   admiration
1   Love this dress! it's sooo pretty. i happene... love
2   I had such high hopes for this dress and reall...   fear
3   I love, love, love this jumpsuit. it's fun, fl...   love
...
6   I aded this in my basket at hte last mintue to...   admiration
7   I ordered this in carbon for store pick up, an...   neutral
8   I love this dress. i usually get an xs but it ...   love
9   I'm 5&quot;5' and 125 lbs. i ordered the s petite t...   love
...
16  Material and color is nice. the leg opening i...    neutral
17  Took a chance on this blouse and so glad i did...   admiration
...
26  I have been waiting for this sweater coat to s...   excitement
27  The colors weren't what i expected either. the...   disapproval
...
31  I never would have given these pants a second ...   love
32  These pants are even better in person. the onl...   disapproval
33  I ordered this 3 months ago, and it finally ca...   disappointment
34  This is such a neat dress. the color is great ...   admiration
35  Wouldn't have given them a second look but tri...   love
36  This is a comfortable skirt that can span seas...   approval
...
40  Pretty and unique. great with jeans or i have ...   admiration
41  This is a beautiful top. it's unique and not s...   admiration
42  This poncho is so cute i love the plaid check ...   love
43  First, this is thermal ,so naturally i didn't ...   love

</code></pre>
",1,2,1519,2021-10-21 04:44:03,https://stackoverflow.com/questions/69655995/is-it-necessary-to-re-train-bert-models-specifically-roberta-model
"Inputting some data for BERT model, using tf.data.Dataset.from_tensor_slices","<p>Here's my model:</p>
<pre><code>def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='features')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(3, activation=&quot;softmax&quot;, name='classifier')(net)
    return tf.keras.Model(text_input, net)
</code></pre>
<p>In the preprocessing layer, I'm using a BERT preprocessor from TF-Hub.</p>
<p>I've already divided the data into <code>corpus_train, corpus_test, labels_train, labels_test</code>.
The corpuses are panda dataframes with the texts that will be used as features, and the labels are NumPy arrays.</p>
<pre><code>corpus=df_speech_EN_merged[&quot;contents&quot;]
corpus.shape
(1768,)

labels=np.asarray(df_speech_EN_merged[&quot;Classes&quot;].astype(&quot;int&quot;))
labels.shape
(1768,)
</code></pre>
<p>To create the train and test data set, I've used the following:</p>
<pre><code>train_dataset = (
    tf.data.Dataset.from_tensor_slices(
        {
            &quot;features&quot;:tf.cast(corpus_train.values, tf.string),
            &quot;labels&quot;:tf.cast(labels_train, tf.int32) #labels is already an array, no need for .values
        }
    )
test_dataset = tf.data.Dataset.from_tensor_slices(
    {&quot;features&quot;:tf.cast(corpus_test.values, tf.string),
     &quot;labels&quot;:tf.cast(labels_test, tf.int32)
    } #labels is already an array, no need for .values
    )
)
</code></pre>
<p>After building and compiling the model without any error message, when I fit the model with:</p>
<pre><code>classifier_model.fit(x=train_dataset,
                               validation_data=test_dataset,
                               epochs=2)
</code></pre>
<p>I get the following error:</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (3 total):
        * Tensor(&quot;inputs:0&quot;, shape=(), dtype=string)
        * False
        * None
      Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')
    * False
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')
    * True
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')
    * False
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')
    * True
    * None
  Keyword arguments: {}
</code></pre>
<p>I think this error occurs because I'm either building train_dataset/test_dataset wrong or because the text_input layer is expecting the wrong type of data. Any help would be appreciated.</p>
","tensorflow, nlp, bert-language-model","<p>When using <code>tf.data.Dataset.from_tensor_slices</code>, try providing a batch_size, since the <code>Bert</code> preprocessing layer expects a very specific shape. Here is a simplified, working example based on the Bert models used in this <a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert"" rel=""nofollow noreferrer"">tutorial</a> and your specific details:</p>
<pre><code>def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='features')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(3, activation=&quot;softmax&quot;, name='classifier')(net)
    return tf.keras.Model(text_input, net)

sentences = tf.constant([
&quot;Improve the physical fitness of your goldfish by getting him a bicycle&quot;,
&quot;You are unsure whether or not to trust him but very thankful that you wore a turtle neck&quot;,
&quot;Not all people who wander are lost&quot;, 
&quot;There is a reason that roses have thorns&quot;,
&quot;Charles ate the french fries knowing they would be his last meal&quot;,
&quot;He hated that he loved what she hated about hate&quot;,
])

labels = tf.random.uniform((6, ), minval=0, maxval=2, dtype=tf.dtypes.int32)

classifier_model = build_classifier_model()
classifier_model.compile(optimizer=tf.keras.optimizers.Adam(),
                         loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                         metrics=tf.keras.metrics.SparseCategoricalAccuracy())
BATCH_SIZE = 1
train_dataset = tf.data.Dataset.from_tensor_slices(
        (sentences, labels)).shuffle(
        sentences.shape[0]).batch(
        BATCH_SIZE)
    
classifier_model.fit(x=train_dataset, epochs=2)
</code></pre>
<pre><code>Epoch 1/2
6/6 [==============================] - 7s 446ms/step - loss: 2.4348 - sparse_categorical_accuracy: 0.5000
Epoch 2/2
6/6 [==============================] - 3s 447ms/step - loss: 1.3977 - sparse_categorical_accuracy: 0.5000
</code></pre>
",1,1,646,2021-10-21 10:24:04,https://stackoverflow.com/questions/69660201/inputting-some-data-for-bert-model-using-tf-data-dataset-from-tensor-slices
Which choice does BertForMultipleChoice presume correct the most?,"<p>The following code is taken from <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMultipleChoice
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMultipleChoice.from_pretrained('bert-base-uncased')

prompt = &quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;
choice0 = &quot;It is eaten with a fork and a knife.&quot;
choice1 = &quot;It is eaten while held in the hand.&quot;
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True)
outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits
</code></pre>
<p>I guess <code>logits</code> in final line contains how well the model thought each choice likely to be correct, but don't know whether one with max or min value is presumed to be correct.</p>
","python, bert-language-model","<p><a href=""https://stackoverflow.com/a/43577384"">Seeing the definition of the word &quot;logit&quot;</a>, I guess item with the highest one is what the model guesses is likely to be correct the most.</p>
",0,0,193,2021-10-24 02:59:53,https://stackoverflow.com/questions/69693550/which-choice-does-bertformultiplechoice-presume-correct-the-most
Could not find function &#39;spacy-transformers.TransformerModel.v3&#39; in function registry &#39;architectures&#39;,"<p>I was trying to create a custom NER model. I used spacy library to create the model. And this line of code is to create the config file from the <code>base.config</code> file.
<strong>My code is :</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>!python -m spacy init fill-config /content/drive/MyDrive/NER_RE_New/NER/base_config.cfg /content/drive/MyDrive/NER_RE_New/NER/config.cfg</code></pre>
</div>
</div>
</p>
<p><strong>Error</strong> :</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>catalogue.RegistryError: [E893] Could not find function 'spacy-transformers.TransformerModel.v3' in function registry 'architectures'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.</code></pre>
</div>
</div>
</p>
<p><strong>Available names:</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>spacy-legacy.CharacterEmbed.v1, 
spacy-legacy.HashEmbedCNN.v1, 
spacy-legacy.MaxoutWindowEncoder.v1, 
spacy-legacy.MishWindowEncoder.v1, 
spacy-legacy.MultiHashEmbed.v1, 
spacy-legacy.TextCatBOW.v1, 
spacy-legacy.TextCatCNN.v1, 
spacy-legacy.TextCatEnsemble.v1, 
spacy-legacy.Tok2Vec.v1, 
spacy-legacy.TransitionBasedParser.v1, 
spacy-transformers.Tok2VecTransformer.v1,
spacy-transformers.TransformerListener.v1, 
spacy-transformers.TransformerModel.v1, 
spacy.CharacterEmbed.v1, 
spacy.EntityLinker.v1, 
spacy.HashEmbedCNN.v1, 
spacy.MaxoutWindowEncoder.v2, 
spacy.MishWindowEncoder.v2, 
spacy.MultiHashEmbed.v1, 
spacy.PretrainCharacters.v1, 
spacy.PretrainVectors.v1, 
spacy.Tagger.v1, 
spacy.TextCatBOW.v1, 
spacy.TextCatCNN.v1, 
spacy.TextCatEnsemble.v2, 
spacy.TextCatLowData.v1, 
spacy.Tok2Vec.v2, 
spacy.Tok2VecListener.v1, 
spacy.TorchBiLSTMEncoder.v1, 
spacy.TransitionBasedParser.v1, 
spacy.TransitionBasedParser.v2</code></pre>
</div>
</div>
</p>
","named-entity-recognition, bert-language-model, spacy-3, spacy-transformers","<p>This happened since spacy had a new update 3.1 recently. And the base_config file have the architecture mentioned as &quot;spacy-transformers.TransformerModel.v3&quot;. Change it into &quot;spacy-transformers.TransformerModel.v1&quot;</p>
<pre><code>[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v1&quot;
name = &quot;roberta-base&quot;
tokenizer_config = {&quot;use_fast&quot;: true}
</code></pre>
",2,2,5514,2021-10-24 06:10:49,https://stackoverflow.com/questions/69694277/could-not-find-function-spacy-transformers-transformermodel-v3-in-function-reg
How to get the corresponding character or string that has been labelled as &#39;UNK&#39; token in BERT?,"<p>After tokenization of a string it returns the token list consisting of separate words and special tokens. For instance, how to decode which word/character has been termed as <strong>'UNK'</strong> token if there is any?</p>
","python, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>The fast tokenizers return a <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#batchencoding"" rel=""nofollow noreferrer"">Batchencoding</a> object that has a built-in <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.word_ids"" rel=""nofollow noreferrer"">word_ids</a> and <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.token_to_chars"" rel=""nofollow noreferrer"">token_to_chars</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizerFast

t = BertTokenizerFast.from_pretrained('bert-base-uncased')

tokens = t('word embeddings are vectors 😀')
print(tokens['input_ids'])
print(t.decode(tokens['input_ids']))
print(tokens.word_ids())
print(tokens.token_to_chars(8))
</code></pre>
<p>Output:</p>
<pre><code>[101, 2773, 7861, 8270, 4667, 2015, 2024, 19019, 100, 102]
[CLS] word embeddings are vectors [UNK] [SEP]
[None, 0, 1, 1, 1, 1, 2, 3, 4, None]
CharSpan(start=28, end=29)

</code></pre>
",1,0,903,2021-10-27 05:27:25,https://stackoverflow.com/questions/69733197/how-to-get-the-corresponding-character-or-string-that-has-been-labelled-as-unk
How to use existing huggingface-transformers model into spacy?,"<p>I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.</p>
<p>My first naive attempt was to load it via <code>spacy.load('bert-base-uncased')</code>, it didn't work because spacy demands a certain structure, which is understandable.</p>
<p>Now I'm trying to figure out how to use the <code>spacy-transformers</code> library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.</p>
<p>I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.</p>
<p>Can I get some help from you?</p>
<p>Thanks.</p>
","spacy, huggingface-transformers, bert-language-model, spacy-transformers","<p>What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is <a href=""https://spacy.io/usage/embeddings-transformers#transformers-training"" rel=""noreferrer"">covered in the docs</a>, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.</p>
<p>To pull from the docs, this is how you specify a custom model in a config:</p>
<pre><code>[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v3&quot;
# XXX You can change the model name here
name = &quot;bert-base-cased&quot;
tokenizer_config = {&quot;use_fast&quot;: true}
</code></pre>
<p>Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features.  If your HuggingFace model has an NER head or something <strong>it will not work.</strong> So if you use a custom model, you'll need to train other components, like NER, on top of it.</p>
<p>Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.</p>
<blockquote>
<p>all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?</p>
</blockquote>
<p>Did you see the <a href=""https://spacy.io/usage/training#quickstart"" rel=""noreferrer"">quickstart</a>? The pretrained models are created using configs similar to what you get from that.</p>
",9,11,3855,2021-10-27 12:44:40,https://stackoverflow.com/questions/69738938/how-to-use-existing-huggingface-transformers-model-into-spacy
"Error in &#39;from torchtext.data import Field, TabularDataset, BucketIterator, Iterator&#39;","<p>I am trying to implement this article <a href=""https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"" rel=""nofollow noreferrer"">https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b</a>, but I have the following problem.</p>
<pre><code># Preliminaries
from torchtext.data import Field, TabularDataset, BucketIterator, Iterator
</code></pre>
<p>Error</p>
<pre><code>ImportError: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.7/dist-packages/torchtext/data/__init__.py)


OSError: /usr/local/lib/python3.7/dist-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtESt10shared_ptrIS0_EPSo
</code></pre>
","python, pytorch, bert-language-model","<p>Try</p>
<pre><code>from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator
</code></pre>
<p>Field is a legacy functionality of Torchtext since the 0.9 release. That article you linked was from before that release. If you've got the newest torchtext, but are trying to use legacy functionality, you need to use torchtext.legacy.*</p>
<p>Check out <a href=""https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb"" rel=""nofollow noreferrer"">this tutorial</a> for more info.</p>
<p>Also <a href=""https://github.com/pytorch/text/releases"" rel=""nofollow noreferrer"">check this for full torchtext release notes</a>.</p>
",3,0,7084,2021-10-29 08:17:08,https://stackoverflow.com/questions/69765669/error-in-from-torchtext-data-import-field-tabulardataset-bucketiterator-iter
The inputs into BERT are token IDs. How do I get the corresponding the input token VECTORs into BERT?,"<p>I am new and learning about transformers.</p>
<p>In a lot of BERT tutorials, I see the input is just the token id of the words. But surely we need to convert this token ID to a vector representation (it can be one hot encoding, or any initial vector representation for each token ID) so that it can be used by the model.</p>
<p>My question is: Where cam I find this initial vector representation for each token?</p>
","nlp, huggingface-transformers, bert-language-model, word-embedding","<p>In BERT, the input is a <code>string</code> itself. THen, BERT manages to convert it into a token and then, create its vector. Let's see an example:</p>
<pre class=""lang-py prettyprint-override""><code>prep_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
enc_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4' 
bert_preprocess = hub.KerasLayer(prep_url)
bert_encoder = hub.KerasLayer(enc_url)

text = ['Hello I&quot;m new to stack overflow']

# First, you need to preprocess the data

preprocessed_text = bert_preprocess(text)
# this will give you a dict with a few keys such us input_word_ids, that is, the tokenizer

encoded = bert_encoder(preprocessed_text)
# and this will give you the (1, 768) vector with the context value of the previous text. the output is encoded['pooled_output']

# you can play with both dicts, printing its keys()
</code></pre>
<p>I recommend you to go to both links above and do a little of research. To recap, BERT uses string as inputs and then tokenize it (with its own tokenzer!). If you want to tokenize with the same values, you need the same vocab file, but for a fresh start like you are doing this should be enough.</p>
",0,0,2718,2021-11-01 21:19:12,https://stackoverflow.com/questions/69802895/the-inputs-into-bert-are-token-ids-how-do-i-get-the-corresponding-the-input-tok
BERT outputs explained,"<p>The keys of the BERT encoder's output are <code>default</code>, <code>encoder_outputs</code>, <code>pooled_output</code> and <code>sequence_output</code></p>
<p>As far as I can know, <code>encoder_outputs</code> are the output of each encoder, <code>pooled_output</code> is the output of the global context and <code>sequence_output</code> is the output context of each token (correct me if I'm wrong please). But what is <code>default</code>? Can you give me a more detailed explanation of each one?</p>
<p><a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"" rel=""nofollow noreferrer"">This is the link to the encoder</a></p>
","python, tensorflow, deep-learning, nlp, bert-language-model","<p>The Tensorflow <a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert"" rel=""nofollow noreferrer"">docs</a> provide a very good explanation to the outputs you are asking about:</p>
<blockquote>
<p>The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:</p>
<p>pooled_output represents each input sequence as a whole. The shape is
[batch_size, H]. You can think of this as an embedding for the entire
movie review.</p>
<p>sequence_output represents each input token in the context. The shape
is [batch_size, seq_length, H]. You can think of this as a contextual
embedding for every token in the movie review.</p>
<p>encoder_outputs are the
intermediate activations of the L Transformer blocks.
outputs[&quot;encoder_outputs&quot;][i] is a Tensor of shape [batch_size,
seq_length, 1024] with the outputs of the i-th Transformer block, for
0 &lt;= i &lt; L. The last value of the list is equal to sequence_output</p>
</blockquote>
<p><a href=""https://www.kaggle.com/questions-and-answers/86510"" rel=""nofollow noreferrer"">Here</a> is another interesting discussion on the difference between the <code>pooled_output</code> and <code>sequence_output</code>, if you are interested.</p>
<p>The <code>default</code> output is equal to the <code>pooled_output</code>, which you can confirm here:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_hub as hub

tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'

def build_classifier_model(name):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='features')    
    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = bert_preprocess_model(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder) 
    outputs = encoder(encoder_inputs)
    net = outputs[name]
    return tf.keras.Model(text_input, net)

sentence = tf.constant([
&quot;Improve the physical fitness of your goldfish by getting him a bicycle&quot;
])

classifier_model = build_classifier_model(name='default')
default_output = classifier_model(sentence)

classifier_model = build_classifier_model(name='pooled_output')
pooled_output = classifier_model(sentence)

print(default_output == pooled_output)
</code></pre>
",4,4,7729,2021-11-04 08:41:17,https://stackoverflow.com/questions/69836422/bert-outputs-explained
BERT: Unable to reproduce sentence-to-embedding operation,"<p>I am trying to convert sentence to embedding, with the following code.</p>
<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = &quot;[CLS] This is a sentence. [SEP]&quot;
tokens = tokenizer.tokenize(text)
input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))])
encoded_layers, pooled_output = model(input_ids, output_all_encoded_layers=False)
</code></pre>
<p>The code worked. However, each time I run this code, it gives a different result. <code>encoded_layers</code> and <code>pooled_output</code> changes every time, for the same input.</p>
<p>Thank you for your help!</p>
","nlp, bert-language-model","<p>Maybe &quot;dropout&quot; works while inferencing. You can try model.eval()</p>
<p>In addition, &quot;transformers&quot; is Long-Time-Support. Stop using pytorch_pretrained_bert</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizerFast, BertModel

bert_path = &quot;/Users/Caleb/Desktop/codes/ptms/bert-base&quot;
tokenizer = BertTokenizerFast.from_pretrained(bert_path)
model = BertModel.from_pretrained(bert_path)

max_length = 32
test_str = &quot;This is a sentence.&quot;
tokenized = tokenizer(test_str, max_length=max_length, padding=&quot;max_length&quot;)
input_ids = tokenized['input_ids']
input_ids = torch.unsqueeze(torch.LongTensor(input_ids), 0)
attention_mask = tokenized['attention_mask']
attention_mask = torch.unsqueeze(torch.IntTensor(attention_mask), 0)
res = model(input_ids, attention_mask=attention_mask)
print(res.last_hidden_state)
</code></pre>
",1,0,116,2021-11-17 13:42:55,https://stackoverflow.com/questions/70005473/bert-unable-to-reproduce-sentence-to-embedding-operation
output from bert gives str not tensor in forward function,"<pre><code>class BertModel(nn.Module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert = AutoModel.from_pretrained(pre_trained)
        self.dropout = nn.Dropout(0.1)
        self.relu =  nn.ReLU()
        self.fc1 = nn.Linear(768,512)
        self.fc2 = nn.Linear(512,6)
      
    
        
    def forward(self,inputs, mask, labels):
        
        pooled, cls_hs = self.bert(input_ids=inputs,attention_mask=mask)
        print(pooled)
        print(cls_hs)  
        print(inputs) 
        print(mask)   
        x = self.fc1(cls_hs)
        print(1) 
        x = self.relu(x)
        print(2) 
        x = self.dropout(x)
        print(3) 
      # output layer
        x = self.fc2(x)
        print(4)
      # apply softmax activation
        x = self.softmax(x)
        print(5)
</code></pre>
<blockquote>
<p>last_hidden_state
pooler_output</p>
</blockquote>
<blockquote>
<p>tensor([[  101,  2342,  2393,  ...,     0,     0,     0],
[  101, 14477,  4779,  ...,  4839,  6513,   102],
[  101, 14777,  2111,  ..., 13677,  3613,   102],
...,
[  101,  2113, 14047,  ...,     0,     0,     0],
[  101,  5683,  3008,  ...,     0,     0,     0],
[  101, 19046,  2075,  ...,  2050,  3308,   102]])
tensor([[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 1, 1, 1],
[1, 1, 1,  ..., 1, 1, 1],
...,
[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 1, 1, 1]])</p>
</blockquote>
<p>in linear(input, weight, bias)
if has_torch_function_variadic(input, weight, bias):
return handle_torch_function(linear, (input, weight, bias), input, weight,
bias=bias)
return torch._C._nn.linear(input, weight, bias)</p>
<p>TypeError: linear(): argument 'input' (position 1) must be Tensor, not str</p>
<blockquote>
<p>pooled, cls_hs printed as string last_hidden_state, pooler_output tensor
with out any tensor</p>
</blockquote>
","python, function, model, bert-language-model","<p>Try to replace the line where you downloaded the pretrained bert model with:</p>
<pre><code>self.model = AutoModel.from_pretrained(pre_trained, return_dict=False)
</code></pre>
",1,0,212,2021-11-17 15:23:51,https://stackoverflow.com/questions/70007129/output-from-bert-gives-str-not-tensor-in-forward-function
How to get cosine similarity of word embedding from BERT model,"<p>I was interesting in how to get the similarity of word embedding in different sentences from BERT model (actually, that means words have different meanings in different scenarios).</p>
<p>For example:</p>
<pre><code>sent1 = 'I like living in New York.'
sent2 = 'New York is a prosperous city.'
</code></pre>
<p>I want to get the cos(New York, New York)'s value from sent1 and sent2, even if the phrase 'New York' is same, but it appears in different sentence. I got some intuition from <a href=""https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2"" rel=""noreferrer"">https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2</a></p>
<p>But I still do not know which layer's embedding I need to extract and how to caculate the cos similarity for my above example.</p>
<p>Thanks in advance for any suggestions!</p>
","python, bert-language-model, word-embedding, transformer-model","<p>Okay let's do this.</p>
<p>First you need to understand that BERT has 13 layers. The first layer is basically just the embedding layer that BERT gets passed during the initial training. You can use it but probably don't want to since that's essentially a static embedding and you're after a dynamic embedding. For simplicity I'm going to only use the last hidden layer of BERT.</p>
<p>Here you're using two words: &quot;New&quot; and &quot;York&quot;. You could treat this as one during preprocessing and combine it into &quot;New-York&quot; or something if you really wanted. In this case I'm going to treat it as two separate words and average the embedding that BERT produces.</p>
<p>This can be described in a few steps:</p>
<ol>
<li>Tokenize the inputs</li>
<li>Determine where the tokenizer has word_ids for New and York (suuuuper important)</li>
<li>Pass through BERT</li>
<li>Average</li>
<li>Cosine similarity</li>
</ol>
<p>First, what you need to import: <code>from transformers import AutoTokenizer, AutoModel</code></p>
<p>Now we can create our tokenizer and our model:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
model = model = AutoModel.from_pretrained('bert-base-cased', output_hidden_states=True).eval()
</code></pre>
<p>Make sure to use the model in evaluation mode unless you're trying to fine tune!</p>
<p>Next we need to tokenize (step 1):</p>
<pre><code>tok1 = tokenizer(sent1, return_tensors='pt')
tok2 = tokenizer(sent2, return_tensors='pt')
</code></pre>
<p>Step 2. Need to determine where the index of the words match</p>
<pre><code># This is where the &quot;New&quot; and &quot;York&quot; can be found in sent1
sent1_idxs = [4, 5]
sent2_idxs = [0, 1]

tok1_ids = [np.where(np.array(tok1.word_ids()) == idx) for idx in sent1_idxs]
tok2_ids = [np.where(np.array(tok2.word_ids()) == idx) for idx in sent2_idxs]
</code></pre>
<p>The above code checks where the word_ids() produced by the tokenizer overlap the word indices from the original sentence. This is necessary because the tokenizer splits rare words. So if you have something like &quot;aardvark&quot;, when you tokenize it and look at it you actually get this:</p>
<pre><code>In [90]: tokenizer.convert_ids_to_tokens( tokenizer('aardvark').input_ids)
Out[90]: ['[CLS]', 'a', '##ard', '##var', '##k', '[SEP]']

In [91]: tokenizer('aardvark').word_ids()
Out[91]: [None, 0, 0, 0, 0, None]
</code></pre>
<p>Step 3. Pass through BERT</p>
<p>Now we grab the embeddings that BERT produces across the token ids that we've produced:</p>
<pre><code>with torch.no_grad():
    out1 = model(**tok1)
    out2 = model(**tok2)

# Only grab the last hidden state
states1 = out1.hidden_states[-1].squeeze()
states2 = out2.hidden_states[-1].squeeze()

# Select the tokens that we're after corresponding to &quot;New&quot; and &quot;York&quot;
embs1 = states1[[tup[0][0] for tup in tok1_ids]]
embs2 = states2[[tup[0][0] for tup in tok2_ids]]
</code></pre>
<p>Now you will have two embeddings. Each is shape <code>(2, 768)</code>. The first size is because you have two words we're looking at: &quot;New&quot; and &quot;York. The second size is the embedding size of BERT.</p>
<p>Step 4. Average</p>
<p>Okay, so this isn't necessarily what you want to do but it's going to depend on how you treat these embeddings. What we have is two <code>(2, 768)</code> shaped embeddings. You can either compare New to New and York to York or you can combine New York into an average. I'll just do that but you can easily do the other one if it works better for your task.</p>
<pre><code>avg1 = embs1.mean(axis=0)
avg2 = embs2.mean(axis=0)
</code></pre>
<p>Step 5. Cosine sim</p>
<p>Cosine similarity is pretty easy using <code>torch</code>:</p>
<pre><code>torch.cosine_similarity(avg1.reshape(1,-1), avg2.reshape(1,-1))

# tensor([0.6440])

</code></pre>
<p>This is good! They point in the same direction. They're not exactly 1 but that can be improved in several ways.</p>
<ol>
<li>You can fine tune on a training set</li>
<li>You can experiment with averaging different layers rather than just the last hidden layer like I did</li>
<li>You can try to be creative in combining New and York. I took the average but maybe there's a better way for your exact needs.</li>
</ol>
",23,6,15576,2021-11-21 19:39:24,https://stackoverflow.com/questions/70057975/how-to-get-cosine-similarity-of-word-embedding-from-bert-model
How padding in huggingface tokenizer works?,"<p>I tried following tokenization example:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)
sent = &quot;I hate this. Not that.&quot;,        
_tokenized = tokenizer(sent, padding=True, max_length=20, truncation=True)
print(_tknzr.decode(_tokenized['input_ids'][0]))
print(len(_tokenized['input_ids'][0]))
</code></pre>
<p>The output was:</p>
<pre><code>[CLS] i hate this. not that. [SEP]
9
</code></pre>
<p>Notice the parameter to <code>tokenizer</code>: <code>max_length=20</code>. How can I make Bert tokenizer to append 11 <code>[PAD]</code> tokens to this sentence to make it total <code>20</code>?</p>
","nlp, huggingface-transformers, bert-language-model, transformer-model, huggingface-tokenizers","<p>One should set <code>padding=&quot;max_length&quot;</code>:</p>
<pre><code>_tokenized = tokenizer(sent, padding=&quot;max_length&quot;, max_length=20, truncation=True)
</code></pre>
",9,7,17817,2021-11-22 14:43:54,https://stackoverflow.com/questions/70067608/how-padding-in-huggingface-tokenizer-works
How to combine 2 different shaped pytorch tensors in training?,"<p>At the moment my model gives 3 output tensors. I want two of them to be more cooperative.
I want to use the combination of self.dropout1(hs) and self.dropout2(cls_hs) to pass through the self.entity_out Linear Layer. The issue is mentioned 2 tensors are in different shapes.</p>
<p>Current Code</p>
<pre><code>class NLUModel(nn.Module):
def __init__(self, num_entity, num_intent, num_scenarios):
    super(NLUModel, self).__init__()
    self.num_entity = num_entity
    self.num_intent = num_intent
    self.num_scenario = num_scenarios

    self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL)

    self.dropout1 = nn.Dropout(0.3)
    self.dropout2 = nn.Dropout(0.3)
    self.dropout3 = nn.Dropout(0.3)

    self.entity_out = nn.Linear(768, self.num_entity)
    self.intent_out = nn.Linear(768, self.num_intent)
    self.scenario_out = nn.Linear(768, self.num_scenario)

def forward(self, ids, mask, token_type_ids):
    out = self.bert(input_ids=ids, attention_mask=mask,
                    token_type_ids=token_type_ids)

    hs, cls_hs = out['last_hidden_state'], out['pooler_output']

    entity_hs = self.dropout1(hs)
    intent_hs = self.dropout2(cls_hs)
    scenario_hs = self.dropout3(cls_hs)

    entity_hs = self.entity_out(entity_hs)
    intent_hs = self.intent_out(intent_hs)
    scenario_hs = self.scenario_out(scenario_hs)

    return entity_hs, intent_hs, scenario_hs
</code></pre>
<p>Required</p>
<pre><code>def forward(self, ids, mask, token_type_ids):
    out = self.bert(input_ids=ids, attention_mask=mask,
                    token_type_ids=token_type_ids)

    hs, cls_hs = out['last_hidden_state'], out['pooler_output']

    entity_hs = self.dropout1(hs)
    intent_hs = self.dropout2(cls_hs)
    scenario_hs = self.dropout3(cls_hs)

    entity_hs = self.entity_out(concat(entity_hs, intent_hs)) # Concatination
    intent_hs = self.intent_out(intent_hs)
    scenario_hs = self.scenario_out(scenario_hs)

    return entity_hs, intent_hs, scenario_hs
</code></pre>
<p>Let's say I was successful in concatenating... will the backward propagation work?</p>
","python, nlp, pytorch, bert-language-model, nlu","<p>Shape of entity_hs (last_hidden_state) is [batch_size, sequence_length, hidden_size], and shape of intent_hs (pooler_output) is just [batch_size, hidden_size] and putting them together may not make sense. It depends on what you want to do.</p>
<p>If, for some reason, you want to get output [batch_size, sequence_length, channels], you could tile the intent_hs tensor:</p>
<pre><code>intent_hs = torch.tile(intent_hs[:, None, :], (1, sequence_lenght, 1))
... = torch.cat([entity_hs, intent_hs], dim=2) 
</code></pre>
<p>If you want to get [batch_size, channels], you can reduce the entity_hs tensor for example by averaging:</p>
<pre><code>entity_hs = torch.mean(entity_hs, dim=1) 
... = torch.cat([entity_hs, intent_hs], dim=1) 
</code></pre>
<p>Yes, the the backward pass will propagate gradients through the concatenation (and the rest).</p>
",0,0,415,2021-11-24 20:52:47,https://stackoverflow.com/questions/70102790/how-to-combine-2-different-shaped-pytorch-tensors-in-training
Integrated Gradients Error - Attempt to convert a value (None) with an unsupported type (&lt;class &#39;NoneType&#39;&gt;) to a Tensor,"<p>I am new to transformers, but I managed to create a Bert classifier using tensorflow and now I want to implement Integreted Gradients to explain the model, ,but I get this error:</p>
<blockquote>
<p>Attempt to convert a value (None) with an unsupported type (&lt;class 'NoneType'&gt;) to a Tensor</p>
</blockquote>
<p>If you think that you can help, please find my code here: <a href=""https://colab.research.google.com/drive/18imP8yxpuenwWFGzdYQ1sWejRJd1vbOy?authuser=1#scrollTo=Le2Sl2QP_KSO"" rel=""nofollow noreferrer"">link</a> because it is a lot of code to paste in here.</p>
","nlp, gradient, bert-language-model","<p>In your notebook, the error is triggered by</p>
<pre><code>explanation = ig.explain(a,
                         baselines=None,
                         target=predictions)
</code></pre>
<p>Therefore, it seems that <code>ig</code> requires non-empty baselines.</p>
",0,-1,119,2021-11-26 12:42:17,https://stackoverflow.com/questions/70124620/integrated-gradients-error-attempt-to-convert-a-value-none-with-an-unsupport
Convert column of lists to integer,"<p>Trying to convert after encoding to integers but they are objects so i first turn them into strings</p>
<p>train_df[&quot;labels&quot;] = train_df[&quot;labels&quot;].astype(str).astype(int)</p>
<p>I am getting this error</p>
<p>invalid literal for int() with base 10: '[0, 1, 0, 0]</p>
<p>An example of a row from the dataset is</p>
<pre><code>text                        labels
[word1,word2,word3,word4]    [1,0,1,0]
</code></pre>
","python, pandas, dataframe, bert-language-model","<p>It's because after <code>train_df[&quot;labels&quot;].astype(str)</code>, this Series became a Series of lists, so you can't convert a list into <code>type int</code>.</p>
<p>If each element in <code>train_df[&quot;labels&quot;]</code> is of type <code>list</code>, you can do:</p>
<pre><code>train_df[&quot;labels&quot;].apply(lambda x: [int(el) for el in x])
</code></pre>
<p>If it's of type <code>str</code>, you can do:</p>
<pre><code>train_df[&quot;labels&quot;].apply(lambda x: [int(el) for el in x.strip(&quot;[]&quot;).split(&quot;,&quot;)])
</code></pre>
<p>You presumably you want to train some model but you can't use pd.Series of lists to do it. You'll need to convert this into a DataFrame. I can't say how to do that without looking at more than 1 line of data.</p>
",1,0,5293,2021-11-30 03:56:41,https://stackoverflow.com/questions/70164103/convert-column-of-lists-to-integer
BERT Domain Adaptation,"<p>I am using <code>transformers.BertForMaskedLM</code> to further pre-train the BERT model on my custom dataset. I first serialize all the text to a <code>.txt</code> file by separating the words by a whitespace. Then, I am using <code>transformers.TextDataset</code> to load the serialized data with a BERT tokenizer given as <code>tokenizer</code> argument. Then, I am using <code>BertForMaskedLM.from_pretrained()</code> to load the pre-trained model (which is what <code>transformers</code> library presents). Then, I am using <code>transformers.Trainer</code> to further pre-train the model on my custom dataset, i.e., domain adaptation, for 3 epochs. I save the model with <code>trainer.save_model()</code>. Then, I want to load the further pre-trained model to get the embeddings of the words in my custom dataset. To load the model, I am using <code>AutoModel.from_pretrained()</code> but this pops up a warning.</p>
<pre><code>Some weights of the model checkpoint at {path to my further pre-trained model} were not used when initializing BertModel
</code></pre>
<p>So, I know why this pops up. Because I further pre-trained using <code>transformers.BertForMaskedLM</code> but when I load with <code>transformers.AutoModel</code>, it loads it as <code>transformers.BertModel</code>. What I do not understand is if this is a problem or not. I just want to get the embeddings, e.g., embedding vector with a size of 768.</p>
","python, nlp, pytorch, huggingface-transformers, bert-language-model","<p>You saved a <code>BERT</code> model with LM head attached. Now you are going to load the serialized file into a standalone <code>BERT</code> structure without any extra element and the warning is issued. This is pretty normal and there is no Fatal error to do so! You can check the list of unloaded params like below:</p>
<pre><code>from transformers import BertTokenizer, BertModel
from transformers import BertTokenizer, BertLMHeadModel, BertConfig
import torch

lmbert = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)
lmbert.save_pretrained('you_desired_path/BertLMHeadModel')
lmbert_params = []
for name, param in lmbert.named_parameters():
    lmbert_params.append(name)
bert = BertModel.from_pretrained('you_desired_path/BertLMHeadModel')
bert_params = []
for name, param in bert.named_parameters():
    bert_params.append(name)
params_ralated_to_lm_head = [param_name for param_name in lmbert_params if param_name.replace('bert.', '') not in bert_params]
params_ralated_to_lm_head
</code></pre>
<p>output:</p>
<pre><code>['cls.predictions.bias',
 'cls.predictions.transform.dense.weight',
 'cls.predictions.transform.dense.bias',
 'cls.predictions.transform.LayerNorm.weight',
 'cls.predictions.transform.LayerNorm.bias']
</code></pre>
",1,1,650,2021-12-02 15:11:15,https://stackoverflow.com/questions/70201921/bert-domain-adaptation
AttributeError: module &#39;keras.engine&#39; has no attribute &#39;InputSpec&#39;,"<p>I am trying to load the bert-language-model:</p>
<pre><code>import numpy as np
from tensorflow import keras
from keras_bert import load_trained_model_from_checkpoint
import tokenization

folder = &quot;multi_cased_L-12_H-768_A-12&quot;

config_path = folder+'/bert_config.json'
checkpoint_path = folder+'/bert_model.ckpt'
vocab_path = folder+'/vocab.txt'

tokenizer =  tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)
model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)
model.summary()
</code></pre>
<p>and I got the error:</p>
<pre><code>----&gt; 4 model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)
</code></pre>
<p>in <em>layer_normalization.py</em> from Anaconda packages:</p>
<pre><code>---&gt; 70         self.input_spec = keras.engine.InputSpec(shape=input_shape)

AttributeError: module 'keras.engine' has no attribute 'InputSpec'

</code></pre>
<p>Installed:</p>
<pre><code>Tensorflow version Version: 2.7.0
Keras  Version Version: 2.7.0
</code></pre>
<p>Please help me to sort out</p>
","python, tensorflow, keras, bert-language-model","<p>Tensorflow 2.7 has API <code>tensorflow.keras.layers.InputSpec</code></p>
<p>Replace</p>
<pre><code>keras.engine.InputSpec(shape=input_shape)
</code></pre>
<p>with</p>
<pre><code>tf.keras.layers.InputSpec(shape=input_shape)
</code></pre>
",1,0,974,2021-12-06 14:51:58,https://stackoverflow.com/questions/70247408/attributeerror-module-keras-engine-has-no-attribute-inputspec
BertModel and BertForMaskedLM weights count,"<p>I want understand BertForMaskedLM model, in huggingface github code, BertForMaskedLM is bert model with additional 2 linear layers with shape (input 768, output 768) and (input 768, output 30522). Count of all weights will be weights of BertModel + 768 * 768 + 768 * 30522, but when I check the numbers don't match.</p>
<pre><code>from transformers import BertModel, BertForMaskedLM
import torch

bertmodel = BertModel.from_pretrained('bert-base-uncased')
bertForMaskedLM = BertForMaskedLM.from_pretrained('bert-base-uncased')

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

count_parameters(bertmodel)
#output 109482240
count_parameters(bertForMaskedLM)
#output 109514298
</code></pre>
<p>109482240 + 768 * 768 + 768 * 30522 != 109514298</p>
<p>what am I doing wrong?</p>
","machine-learning, deep-learning, nlp, pytorch, bert-language-model","<p>Using <code>numel()</code> along with <code>model.parameters()</code> is not a reliable method for counting the total number of parameters and may fail for recursive configuration of layers. This is exactly what is happening in your case. Instead, try following:</p>
<pre><code>from torchinfo import summary

print(summary(bertmodel))
</code></pre>
<p>Output:
<a href=""https://i.sstatic.net/UrQX3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UrQX3.png"" alt=""enter image description here"" /></a></p>
<br>
<pre><code>print(summary(bertForMaskedLM))
</code></pre>
<p>Output:
<a href=""https://i.sstatic.net/2z5cT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2z5cT.png"" alt=""enter image description here"" /></a></p>
<p>From the above outputs we can see that total number of trainable params for the two models are:<br>
bertmodel: 109,482,240<br>
bertForMaskedLM: 132,955,194<br></p>
<p>In order to understand the difference, lets have a look at the last module of both the models (rest of the base model is exactly the same):</p>
<h3>bertmodel:<br></h3>
<pre><code>(pooler): BertPooler(
(dense): Linear(in_features=768, out_features=768, bias=True)
(activation): Tanh())
</code></pre>
<h4>bertForMaskedLM: <br></h4>
<pre><code>(cls): BertOnlyMLMHead((predictions): BertLMPredictionHead(
(transform): BertPredictionHeadTransform(
  (dense): Linear(in_features=768, out_features=768, bias=True)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
)
(decoder): Linear(in_features=768, out_features=30522, bias=True)))
</code></pre>
<p>Only additions are the <em>LayerNorm</em> layer (2 * 768 params for layer gammas and betas) and the <em>decoder</em> layer (769 * 30522, using the y=A*X + B, where A is of size (nxm) and B of (nx1) with a total params of nx(m+1).<br></p>
<p>Params for bertForMaskedLM = 109482240 + 2 * 768 + 769 * 30522 = 132955194</p>
",4,1,1174,2021-12-08 14:02:34,https://stackoverflow.com/questions/70276298/bertmodel-and-bertformaskedlm-weights-count
How to Run Pytorch Bert with AMD,"<p>github code: <a href=""https://github.com/bellowman/Deep-Learning-Practice/blob/main/BioBert%20for%20Multi%20Label%20AMD.ipynb"" rel=""nofollow noreferrer"">https://github.com/bellowman/Deep-Learning-Practice/blob/main/BioBert%20for%20Multi%20Label%20AMD.ipynb</a></p>
<p>Hello everyone,</p>
<p>I am a beginner with pytorch, tensorflow, and BERT. I have a machine at home with an AMD Ryzen 7 1800x and a Radeon RX 6600 video card.</p>
<p>I am trying to run a bioBERT model at home. I have trouble leveraging my model to use my AMD card. I posted my github notebook. I have troubles in cell 3 and 9.</p>
<ol>
<li>First Question: In cell 3,I am trying to convert the bioBERT weight to PyTorch with transformmer-cli. I get the warning of &quot;Could not load dynamic library 'cudart64_110.dll'&quot;. Does this affect performance later?</li>
<li>Second Question: In cell 9, My model load is really slow because it is using just the CPU. How can I get the model to run on my AMD GPU</li>
</ol>
","python, deep-learning, pytorch, bert-language-model, amd-gpu","<p>Thank you to <a href=""https://stackoverflow.com/users/2599709/chrispresso"">chrispresso</a></p>
<p>AMD ROCm seems to be the way to go, but it requires one to run under linux</p>
",1,1,1039,2021-12-08 18:11:44,https://stackoverflow.com/questions/70279801/how-to-run-pytorch-bert-with-amd
separately save the model weight in pytorch,"<p>I am using PyTorch to train a deep learning model. I wonder if it is possible for me to separately save the model weight. For example:</p>
<pre><code>class my_model(nn.Module):
def __init__(self):
    super(my_model, self).__init__()
    self.bert = transformers.AutoModel.from_pretrained(BERT_PATH)
    self.out = nn.Linear(768,1)
    
def forward(self, ids, mask, token_type):
    x = self.bert(ids, mask, token_type)[1]
    x = self.out(x)
    
    return x
</code></pre>
<p>I have the BERT model as the base model and an additional linear layer on the top. After I train this model, can I save the weight for the BERT model and this linear layer separately?</p>
","python, machine-learning, deep-learning, pytorch, bert-language-model","<p>Alternatively to the previous answer, You can create two separated class of nn.module. One for the BERT model and another one for the linear layer:</p>
<pre><code>class bert_model(nn.Module):
  def __init__(self):
  super(bert_model, self).__init__()
  self.bert = transformers.AutoModel.from_pretrained(BERT_PATH)

  def forward(self, ids, mask, token_type):
    x = self.bert(ids, mask, token_type)[1]

    return x

class linear_layer(nn.Module):
  def __init__(self):
  super(linear_layer, self).__init__()
  self.out = nn.Linear(768,1)

  def forward(self, x):
    x = self.out(x)

    return x
</code></pre>
<p>Then you can save the two part of the model separately with:</p>
<pre><code>bert_model = bert_model()
linear_layer = linear_layer()
#train
torch.save(bert_model.state_dict(), PATH)
torch.save(linear_layer.state_dict(), PATH)
</code></pre>
",0,0,577,2021-12-13 10:40:21,https://stackoverflow.com/questions/70333321/separately-save-the-model-weight-in-pytorch
"Machine Learning, Transformer, Multi-class classification, number of classes is inconsistent in test data and training data","<p>For example, suppose I'm building a transformer model (from huggingface) and there are 20 classes in the training data however there are only 5 classes in the testing data. To configure the transformer model from huggingface, for example, BertConfig, we need to provide a parameter: num_labels. Should I set num_labels to 20 or 5 because there are only 5 classes in the testing data?</p>
","machine-learning, huggingface-transformers, bert-language-model","<p>20.because your trained network has 20 classes.</p>
",0,-1,138,2021-12-15 22:34:12,https://stackoverflow.com/questions/70371140/machine-learning-transformer-multi-class-classification-number-of-classes-is
How to calculate perplexity of a sentence using huggingface masked language models?,"<p>I have several masked language models (mainly Bert, Roberta, Albert, Electra). I also have a dataset of sentences. How can I get the perplexity of each sentence?</p>
<p>From the huggingface documentation <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""noreferrer"">here</a> they mentioned that perplexity &quot;is not well defined for masked language models like BERT&quot;, though I still see people somehow calculate it.</p>
<p>For example in this <a href=""https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence"">SO</a> question they calculated it using the function</p>
<pre><code>def score(model, tokenizer, sentence,  mask_token_id=103):
  tensor_input = tokenizer.encode(sentence, return_tensors='pt')
  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
  masked_input = repeat_input.masked_fill(mask == 1, 103)
  labels = repeat_input.masked_fill( masked_input != 103, -100)
  loss,_ = model(masked_input, masked_lm_labels=labels)
  result = np.exp(loss.item())
  return result

score(model, tokenizer, '我爱你') # returns 45.63794545581973
</code></pre>
<p>However, when I try to use the code I get <code>TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'</code>.</p>
<p>I tried it with a couple of my models:</p>
<pre><code>from transformers import pipeline, BertForMaskedLM, BertForMaskedLM, AutoTokenizer, RobertaForMaskedLM, AlbertForMaskedLM, ElectraForMaskedLM
import torch

1)
tokenizer = AutoTokenizer.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
2)
tokenizer = AutoTokenizer.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
model = ElectraForMaskedLM.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
</code></pre>
<p><a href=""https://stackoverflow.com/questions/61470768/how-does-masked-lm-labels-argument-work-in-bertformaskedlm"">This</a> SO question also used the <code>masked_lm_labels</code> as an input and it seemed to work somehow.</p>
","nlp, pytorch, huggingface-transformers, bert-language-model, transformer-model","<p>There is a paper <a href=""https://arxiv.org/abs/1910.14659"" rel=""noreferrer"">Masked Language Model Scoring</a> that explores pseudo-perplexity from masked language models and shows that pseudo-perplexity, while not being theoretically well justified, still performs well for comparing &quot;naturalness&quot; of texts.</p>
<p>As for the code, your snippet is perfectly correct but for one detail: in recent implementations of Huggingface BERT, <code>masked_lm_labels</code> are renamed to simply <code>labels</code>, to make interfaces of various models more compatible. I have also replaced the hard-coded <code>103</code> with the generic <code>tokenizer.mask_token_id</code>. So the snippet below should work:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch
import numpy as np

model_name = 'cointegrated/rubert-tiny'
model = AutoModelForMaskedLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def score(model, tokenizer, sentence):
    tensor_input = tokenizer.encode(sentence, return_tensors='pt')
    repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id)
    labels = repeat_input.masked_fill( masked_input != tokenizer.mask_token_id, -100)
    with torch.inference_mode():
        loss = model(masked_input, labels=labels).loss
    return np.exp(loss.item())

print(score(sentence='London is the capital of Great Britain.', model=model, tokenizer=tokenizer)) 
# 4.541251105675365
print(score(sentence='London is the capital of South America.', model=model, tokenizer=tokenizer)) 
# 6.162017238332462
</code></pre>
<p>You can try this code in Google Colab by running <a href=""https://gist.github.com/avidale/f574c014cd686709636b89208f2259ce"" rel=""noreferrer"">this gist</a>.</p>
",22,8,11061,2021-12-23 15:50:06,https://stackoverflow.com/questions/70464428/how-to-calculate-perplexity-of-a-sentence-using-huggingface-masked-language-mode
InternalError when using TPU for training Keras model,"<p>I am attempting to fine-tune a BERT model on Google Colab from the Tensorflow Hub using this <a href=""https://www.tensorflow.org/text/tutorials/bert_glue"" rel=""nofollow noreferrer"">link</a>.</p>
<p>However, I run into the following error:</p>
<pre><code>InternalError: RET_CHECK failure (third_party/tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:2047) arg_shape.handle_type != DT_INVALID  input edge: [id=2693 model_preprocessing_67660:0 -&gt; cluster_train_function:628]
</code></pre>
<p>When I run my <code>model.fit(...)</code> function.</p>
<p>This error only occurs when I try to use TPU (runs fine on CPU, but has a very long training time).</p>
<p>Here is my code for setting up the TPU and model:</p>
<p><strong>TPU Setup:</strong></p>
<pre><code>import os
os.environ[&quot;TFHUB_MODEL_LOAD_FORMAT&quot;]=&quot;UNCOMPRESSED&quot;

cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
strategy = tf.distribute.TPUStrategy(cluster_resolver)
</code></pre>
<p><strong>Model Setup:</strong></p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/sst2/2', trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)
</code></pre>
<p><strong>Model Training</strong></p>
<pre><code>with strategy.scope():

  bert_model = build_classifier_model()
  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
  metrics = tf.metrics.BinaryAccuracy()
  epochs = 1
  steps_per_epoch = 1280000
  num_train_steps = steps_per_epoch * epochs
  num_warmup_steps = int(0.1*num_train_steps)

  init_lr = 3e-5
  optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')
  bert_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)
  print(f'Training model')
  history = bert_model.fit(x=X_train, y=y_train,
                               validation_data=(X_val, y_val),
                               epochs=epochs)
</code></pre>
<p>Note that <code>X_train</code> is a numpy array of type <code>str</code> with shape <code>(1280000,)</code> and <code>y_train</code> is a numpy array of shape <code>(1280000, 1)</code></p>
","python, tensorflow, machine-learning, bert-language-model, tpu","<p>As I don't exactly know what changes you have made in the code... I don't have idea about your dataset. But I can see that you are trying to train the whole datset with one epoch and passing the steps per epoch directly. I would recommend to write it like this</p>
<blockquote>
<p>set some batch_size 2^n power (for example 16 or 32 or etc) if you don't want to batch the dataset just set batch_size to 1</p>
</blockquote>
<pre><code>batch_size = 16
steps_per_epoch = training_data_size // batch_size
</code></pre>
<p>The problem with the code is most probably the training dataset size. I think that you're making a mistake by passing the value of the training dataset manually.</p>
<p>If you're loading the dataset from tfds use (as shown in the link):</p>
<pre><code>train_dataset, train_data_size = load_dataset_from_tfds(
  in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)
</code></pre>
<p>If you're using a custom dataset take the size of the cleaned dataset in a variable and then use that variable for using the size of the training data. Try to avoid manually putting values in the code as far as possible.</p>
",3,3,702,2021-12-25 10:11:00,https://stackoverflow.com/questions/70479279/internalerror-when-using-tpu-for-training-keras-model
SimpleTransformers model not training on whole data (As shown in brackets under epochs bar)?,"<p>My data has 1751 sentences however when training a number appears under the epochs bars. Sometimes it is 1751 which makes sense it's the number of sentences I have, but most of the times it's 50% the number of data (sentences I have as shown in the figure below).</p>
<p>I tried to look in the documentation to understand if the number should be the same as my training set size but I couldn't find an answer.
<a href=""https://i.sstatic.net/HIfMe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HIfMe.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/1Obqi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1Obqi.png"" alt=""enter image description here"" /></a>
<img src=""https://user-images.githubusercontent.com/47028011/147410064-4ae15d7b-856d-4859-8b83-03fd781f5b90.png"" alt=""image"" /></p>
<p>I am using Kaggle with GPU backend. Does this means that the model is indeed not training on all data?</p>
","python, bert-language-model, simpletransformers","<p>In short: no, it is training on all data.</p>
<p>First let's look at some of the parameters:</p>
<p><code>num_of_train_epochs: 4</code>: your setting, meaning the whole dataset would be trained 4 times. Which is why you have 4 bars in the output.</p>
<p><code>train_batch_size: 8</code>: this is the <a href=""https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model"" rel=""nofollow noreferrer"">default setting</a>, meaning that for each update on the weights, you use 8 records in your training data (out of a total of 1751)</p>
<p>So this means, you have a total of 1751/8 = 218.875  batches per epoch, which is the 219/219 you see in the output.</p>
<p>The 876 that you sees in the bottom simply means it went through a total of 219(batch per epoch) * 4(number of epoch) = 876 number of batches/updates.</p>
<p>One way to prove this is to change <code>num_of_train_epochs</code> to 1. And you should see 219 instead of 876.</p>
<p>Definition of <a href=""https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/#:%7E:text=batches%20and%20epochs.-,What%20Is%20the%20Difference%20Between%20Batch%20and%20Epoch%3F,passes%20through%20the%20training%20dataset."" rel=""nofollow noreferrer"">batch and epoch</a>:</p>
<blockquote>
<p>The batch size is a number of samples processed before the model is updated.</p>
</blockquote>
<blockquote>
<p>The number of epochs is the number of complete passes through the training dataset.</p>
</blockquote>
",3,1,390,2021-12-26 13:53:49,https://stackoverflow.com/questions/70486977/simpletransformers-model-not-training-on-whole-data-as-shown-in-brackets-under
Can we calculate feature importance in Huggingface Bert?,"<blockquote>
<p>We can fit a LinearRegression model on the regression dataset and retrieve the coeff_ property that contains the coefficients found for each input variable. These coefficients can provide the basis for a crude feature importance score. This assumes that the input variables have the same scale or have been scaled prior to fitting a model.</p>
</blockquote>
<p>What about Bert? Can we get coef_ variable from the model and use it to calculate feature importance like LinearRegression model in text classification task?</p>
","nlp, huggingface-transformers, bert-language-model","<p>Captum is a prominent tool (from  pytorch/Facebook) for interpreting transformers and you can get a score for the attention the model pays to specific tokens at specific layers. See a tutorial here: <a href=""https://captum.ai/tutorials/Bert_SQUAD_Interpret"" rel=""nofollow noreferrer"">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a> or here <a href=""https://github.com/pytorch/captum"" rel=""nofollow noreferrer"">https://github.com/pytorch/captum</a></p>
",2,3,2639,2021-12-27 13:29:35,https://stackoverflow.com/questions/70496137/can-we-calculate-feature-importance-in-huggingface-bert
InvalidConfigException: Can&#39;t load class for name &#39;HFTransformersNLP&#39;. in rasa,"<h1>how to implement BERT in rasa with huggingface transformers and what are needed for running the Bert model in rasa ?</h1>
<pre><code>recipe: default.v1
*# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/*
language: en
pipeline:
*# how to implement this BERT in rasa* 
  - name: HFTransformersNLP
    model_weights: &quot;bert-base-uncased&quot;
    model_name: &quot;bert&quot;
  - name: LanguageModelTokenizer
  - name: LanguageModelFeaturizer
  - name: DIETClassifier
    epochs: 200
</code></pre>
","nlp, huggingface-transformers, bert-language-model, rasa","<p>This error could be due to the Rasa version you're using (output of <code>rasa --version</code>). In the current versions (<code>&gt;2.1</code>), <code>HFTransformersNLP</code> and <code>LanguageModelTokenizer</code> are deprecated. Using a BERT model can be achieved with any tokenizer and</p>
<pre><code>pipeline:
  - name: LanguageModelFeaturizer
    model_name: &quot;bert&quot;
    model_weights: &quot;rasa/LaBSE&quot;
</code></pre>
<p>See the <a href=""https://rasa.com/docs/rasa/2.x/components#languagemodelfeaturizer"" rel=""noreferrer"">documentation</a> for further details.</p>
",5,1,2091,2022-01-04 12:18:22,https://stackoverflow.com/questions/70578679/invalidconfigexception-cant-load-class-for-name-hftransformersnlp-in-rasa
Does torch.nn.MultiheadAttention contain normalisation layer and feed forward layer?,"<p>Tried to find the source code of multihead attention but could not find any implementation details. I wonder if this module only contains the attention part rather than the whole transformer block (i.e. It does not contain the normalisation layer, residual connection and an additional feedforward neural network)?</p>
","python, pytorch, bert-language-model, transformer-model, attention-model","<p>According to the <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"" rel=""nofollow noreferrer"">source code</a>, the answer is no. <code>MultiheadAttention</code> unsurprisingly implements only the attention function.</p>
",0,0,753,2022-01-06 11:26:08,https://stackoverflow.com/questions/70606412/does-torch-nn-multiheadattention-contain-normalisation-layer-and-feed-forward-la
Use Quantization on HuggingFace Transformers models,"<p>I'm learning <strong>Quantization</strong>, and am experimenting with <strong>Section 1</strong> of this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>I want to use this code on my own models.</p>
<p>Hypothetically, I only need to assign to <code>model</code> variable in <strong>Section 1.2</strong></p>
<hr />
<pre class=""lang-py prettyprint-override""><code># load model
model = BertForSequenceClassification.from_pretrained(configs.output_dir)
model.to(configs.device)
</code></pre>
<p>My models are from a different library: <code>from transformers import pipeline</code>. So <code>.to()</code> throws an <code>AttributeError</code>.</p>
<p>My Model:</p>
<pre><code>pip install transformers
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
model = unmasker(&quot;Hello I'm a [MASK] model.&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<hr />
<p><strong>How might I run the linked Quantization code on my example model?</strong></p>
<p>Please let me know if there's anything else I should clarify in this post.</p>
","python, deep-learning, huggingface-transformers, bert-language-model, quantization","<p>The <code>pipeline</code> approach won't work for Quantisation as we need the models to be returned. You can however, use <code>pipeline</code> for testing the original <a href=""https://huggingface.co/models?pipeline_tag=fill-mask&amp;language=en&amp;library=pytorch&amp;dataset=dataset:bookcorpus&amp;sort=downloads"" rel=""nofollow noreferrer"">models</a> for timing etc.</p>
<hr />
<p><strong>Quantisation Code:</strong></p>
<p><code>token_logits</code> contains the tensors of the quantised model.</p>
<p>You could place a <code>for-loop</code> around this code, and replace <code>model_name</code> with <code>string</code> from a <code>list</code>.</p>
<pre class=""lang-py prettyprint-override""><code>model_name = bert-base-uncased
tokenizer = AutoTokenizer.from_pretrained(model_name )
model = AutoModelForMaskedLM.from_pretrained(model_name)
    
sequence = &quot;Distilled models are smaller than the models they mimic. Using them instead of the large &quot; \
f&quot;versions would help {tokenizer.mask_token} our carbon footprint.&quot;

inputs = tokenizer(sequence, return_tensors=&quot;pt&quot;)
mask_token_index = torch.where(inputs[&quot;input_ids&quot;] == tokenizer.mask_token_id)[1]
    
token_logits = model(**inputs).logits

# &lt;- can stop here
</code></pre>
<p><a href=""https://huggingface.co/models?sort=downloads"" rel=""nofollow noreferrer"">Source</a></p>
",1,1,2506,2022-01-06 15:36:37,https://stackoverflow.com/questions/70609579/use-quantization-on-huggingface-transformers-models
"ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape ()","<p>I got word-embedding using BERT and need to feed it as an embedding layer in the Keras model, and the error I got is</p>
<pre><code>ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape ()
</code></pre>
<p>the model is</p>
<pre><code>embedding = Embedding(30522, 768, mask_zero=True)(sentence)
model.layers[1].set_weights([embedding_matrix])
</code></pre>
","tensorflow, keras, deep-learning, huggingface-transformers, bert-language-model","<p>You are passing to <code>set_weights</code> a list of list:</p>
<pre><code>embedding_matrix = [np.random.uniform(0,1, (30522, 768))]

sentence = Input((20,))
embedding = Embedding(30522, 768, mask_zero=True)(sentence)
model = Model(sentence, embedding)

model.layers[1].set_weights([embedding_matrix])
</code></pre>
<p>while you should simply pass a list of arrays:</p>
<pre><code>embedding_matrix = np.random.uniform(0,1, (30522, 768))

sentence = Input((20,))
embedding = Embedding(30522, 768, mask_zero=True)(sentence)
model = Model(sentence, embedding)

model.layers[1].set_weights([embedding_matrix])
</code></pre>
",2,0,670,2022-01-11 08:34:07,https://stackoverflow.com/questions/70663782/valueerror-layer-weight-shape-30522-768-not-compatible-with-provided-weight
IndexError: Target is out of bounds,"<p>I am currently trying to replicate the article</p>
<p><a href=""https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f</a></p>
<p>to get an introduction to PyTorch and BERT.</p>
<p>I used some own sample corpus and corresponding tragets as practise, but the code throws the following:</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-4-8577755f37de&gt; in &lt;module&gt;()
    201 LR = 1e-6
    202 
--&gt; 203 trainer(model, df_train, df_val, LR, EPOCHS)

3 frames
&lt;ipython-input-4-8577755f37de&gt; in trainer(model, train_data, val_data, learning_rate, epochs)
    162                 output = model(input_id, mask)
    163 
--&gt; 164                 batch_loss = criterion(output, torch.max(train_label,1)[1])
    165                 total_loss_train += batch_loss.item()
    166 

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1102             return forward_call(*input, **kwargs)
   1103         # Do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1150         return F.cross_entropy(input, target, weight=self.weight,
   1151                                ignore_index=self.ignore_index, reduction=self.reduction,
-&gt; 1152                                label_smoothing=self.label_smoothing)
   1153 
   1154 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not None or reduce is not None:
   2845         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2846     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

IndexError: Target 32 is out of bounds.
</code></pre>
<p>The code is mostly identical to the one in the article, except of course the more extensive lable-dict.</p>
<p>Orginial:</p>
<pre><code>labels = {'business':0,
          'entertainment':1,
          'sport':2,
          'tech':3,
          'politics':4
          }
</code></pre>
<p>Mine:</p>
<pre><code>labels = 
{'Macroeconomics': 0,
 'Microeconomics': 1,
 'Labor Economics': 2,
 'Subnational Fiscal Issues': 3,
 'Econometrics': 4,
 'International Economics': 5,
 'Financial Economics': 6,
 'Health, Education, and Welfare': 7,
 'Public Economics': 8,
 'Development and Growth': 9,
 'Industrial Organization': 10,
 'Other': 11,
 'Environmental and Resource Economics': 12,
 'History': 13,
 'Regional and Urban Economics': 14,
 'Development Economics': 15,
 'Corporate Finance': 16,
 'Children': 17,
 'Labor Studies': 18,
 'Economic Fluctuations and Growth': 19,
 'Economics of Aging': 20,
 'Economics of Education': 21,
 'International Trade and Investment': 22,
 'Asset Pricing': 23,
 'Health Economics': 24,
 'Law and Economics': 25,
 'International Finance and Macroeconomics': 26,
 'Monetary Economics': 27,
 'Technical Working Papers': 28,
 'Political Economy': 29,
 'Development of the American Economy': 30,
 'Health Care': 31,
 'Productivity, Innovation, and Entrepreneurship': 32}
</code></pre>
<p>Code:</p>
<pre><code>class Dataset(torch.utils.data.Dataset):

    def __init__(self, df):

        self.labels = torch.LongTensor([labels[label] for label in df[&quot;category&quot;]])
        self.texts = [tokenizer(text, 
                               padding='max_length', max_length = 512, truncation=True,
                                return_tensors=&quot;pt&quot;) for text in df['text']]

    def classes(self):
        return self.labels

    def __len__(self):
        return len(self.labels)

    def get_batch_labels(self, idx):
        # Fetch a batch of labels
        return np.array(self.labels[idx])

    def get_batch_texts(self, idx):
        # Fetch a batch of inputs
        return self.texts[idx]

    def __getitem__(self, idx):
        batch_texts = self.get_batch_texts(idx)
        batch_y = np.array(range(0,len(labels)))

        return batch_texts, batch_y
    
#Splitting the sample into trainingset, validationset and testset (80,10,10)
np.random.seed(112)
df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), 
                                     [int(.8*len(df)), int(.9*len(df))])

print(len(df_train),len(df_val), len(df_test))


from torch import nn

class BertClassifier(nn.Module):

    def __init__(self, dropout=0.5):

        super(BertClassifier, self).__init__()

        self.bert = BertModel.from_pretrained('bert-base-cased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 5)
        self.relu = nn.ReLU()

    def forward(self, input_id, mask):

        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        final_layer = self.relu(linear_output)

        return final_layer
    
from torch.optim import Adam
from tqdm import tqdm

def trainer(model, train_data, val_data, learning_rate, epochs):

    train, val = Dataset(train_data), Dataset(val_data)
    
    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)
    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)

    use_cuda = torch.cuda.is_available()
    device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)

    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr= learning_rate)

    if use_cuda:

            model = model.cuda()
            criterion = criterion.cuda()

    for epoch_num in range(epochs):

            total_acc_train = 0
            total_loss_train = 0

            for train_input, train_label in tqdm(train_dataloader):

                train_label = train_label.to(device)
                mask = train_input['attention_mask'].to(device)
                input_id = train_input['input_ids'].squeeze(1).to(device)

                output = model(input_id, mask)
                
                batch_loss = criterion(output, torch.max(train_label,1)[1])
                total_loss_train += batch_loss.item()
                
                acc = (output.argmax(dim=1) == train_label).sum().item()
                total_acc_train += acc

                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
            
            total_acc_val = 0
            total_loss_val = 0

            with torch.no_grad():

                for val_input, val_label in val_dataloader:

                    val_label = val_label.to(device)
                    mask = val_input['attention_mask'].to(device)
                    input_id = val_input['input_ids'].squeeze(1).to(device)

                    output = model(input_id, mask)

                    batch_loss = criterion(output, val_label)
                    total_loss_val += batch_loss.item()
                    
                    acc = (output.argmax(dim=1) == val_label).sum().item()
                    total_acc_val += acc
            
            print(
                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \
                | Train Accuracy: {total_acc_train / len(train_data): .3f} \
                | Val Loss: {total_loss_val / len(val_data): .3f} \
                | Val Accuracy: {total_acc_val / len(val_data): .3f}')
                  
EPOCHS = 5
model = BertClassifier()
LR = 1e-6
              
trainer(model, df_train, df_val, LR, EPOCHS)
</code></pre>
","python, pytorch, text-classification, bert-language-model","<p>You're creating a list of length 33 in your <code>__getitem__</code> call which is one more than the length of the labels list, hence the out of bounds error. In fact, you create <em>the same</em> list each time this method is called. You're supposed to fetch the associated <code>y</code> with the <code>X</code> found at <code>idx</code>.</p>
<p>If you replace <code>batch_y = np.array(range(...))</code> with <code>batch_y = np.array(self.labels[idx])</code>, you'll fix your error. Indeed, this is already implemented in your <code>get_batch_labels</code> method.</p>
",1,0,17052,2022-01-12 10:53:59,https://stackoverflow.com/questions/70680290/indexerror-target-is-out-of-bounds
extract and concanate the last 4 hidden states from bert model for each input,"<p>I want to extract and concanate 4 last hidden states from bert for each input sentance and save them
I use this code but i got last hidden state only</p>
<pre><code>class MixModel(nn.Module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert =  AutoModel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= False,  output_hidden_states=True)        
        print(cls_hs)        
                   
        encoded_layers = cls_hs[0]
        print(len(encoded_layers))

        print(encoded_layers.size())
        #output is [1,64,768]
       
        return encoded_layers
</code></pre>
<p>batch size is 1
padding size is 64</p>
<p>how to extract the last four?</p>
","python, deep-learning, nlp, feature-extraction, bert-language-model","<p>grab the last 4 hidden states, now a tuple of 4 tensors of shape (batch_size, seq_len, hidden_size)</p>
<pre><code>encoded_layers = cls_hs['hidden_states'][-4:]
</code></pre>
<p>and concatenate them (here over the last dimension) to a single tensor of shape (batch_size, seq_len, 4 * hidden_size)</p>
<pre><code>concatenated = torch.cat(encoded_layers, -1)
</code></pre>
",1,0,1812,2022-01-12 13:37:55,https://stackoverflow.com/questions/70682546/extract-and-concanate-the-last-4-hidden-states-from-bert-model-for-each-input
"ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name","<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>Albert</strong> and <strong>Distilbert</strong> models</p>
<p>Kernel: <code>conda_pytorch_p36</code>. I did Restart &amp; Run All, and refreshed file view in working directory.</p>
<p>Error occurs in <strong>Section 1.2</strong>, only for these 2 new models.</p>
<p>For filenames etc., I've created a variable used everywhere:</p>
<pre class=""lang-py prettyprint-override""><code>MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'
</code></pre>
<p>I replaced imports with:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)
</code></pre>
<p>As suggested in <a href=""https://huggingface.co/docs/transformers/model_doc/auto"" rel=""nofollow noreferrer"">Transformers Documentation - Auto Classes</a>.</p>
<blockquote>
<p>Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.</p>
</blockquote>
<hr />
<p><strong>Section 1.2:</strong></p>
<pre class=""lang-py prettyprint-override""><code># load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), &quot;temp.p&quot;)
    print('Size (MB):', os.path.getsize(&quot;temp.p&quot;)/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)
</code></pre>
<p>Traceback:</p>
<pre><code>ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
","python, tensorflow, huggingface-transformers, bert-language-model, onnx","<h3>Explanation:</h3>
<p>When instantiating <code>AutoModel</code>, you must specify a <code>model_type</code> parameter in <code>./MRPC/config.json</code> file (downloaded during Notebook runtime).</p>
<p>List of <code>model_types</code> can be found <a href=""https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoConfig"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<h3>Solution:</h3>
<p>Code that appends <code>model_type</code> to <code>config.json</code>, in the same format:</p>
<pre class=""lang-py prettyprint-override""><code>import json

json_filename = './MRPC/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))
</code></pre>
<p><code>config.json</code>:</p>
<pre><code>{
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;finetuning_task&quot;: &quot;mrpc&quot;,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;num_labels&quot;: 2,
  &quot;output_attentions&quot;: false,
  &quot;output_hidden_states&quot;: false,
  &quot;pruned_heads&quot;: {},
  &quot;torchscript&quot;: false,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 30522,
  &quot;model_type&quot;: &quot;albert&quot;
}
</code></pre>
<hr />
",1,3,8085,2022-01-13 13:32:15,https://stackoverflow.com/questions/70697470/valueerror-unrecognized-model-in-mrpc-should-have-a-model-type-key-in-its
focal loss for imbalanced data using pytorch,"<p>I want to use focal loss with multiclass imbalanced data using pytorch . I searched got and try to use this code but I got  error</p>
<pre><code>
class_weights=tf.constant([0.21, 0.45, 0.4, 0.46, 0.48, 0.49])

loss_fn=nn.CrossEntropyLoss(weight=class_weights,reduction='mean')
</code></pre>
<p>and use this in train function</p>
<pre><code>

    preds = model(sent_id, mask, labels)
   
     # compu25te the validation loss between actual and predicted values
    alpha=0.25
    gamma=2
    ce_loss = loss_fn(preds, labels)
    pt = torch.exp(-ce_loss)
    focal_loss = (alpha * (1-pt)**gamma * ce_loss).mean()
</code></pre>
<p>the error is</p>
<pre><code>TypeError: cannot assign 'tensorflow.python.framework.ops.EagerTensor' object to buffer 'weight' (torch Tensor or None required)
</code></pre>
<p>in this line</p>
<pre><code>loss_fn=nn.CrossEntropyLoss(weight=class_weights,reduction='mean')
</code></pre>
","python, pytorch, loss-function, bert-language-model, cross-entropy","<p>You're mixing tensorflow and pytorch objects.</p>
<p>Try:</p>
<pre><code>class_weights=torch.tensor([0.21, ...], requires_grad=False)
</code></pre>
",1,0,1947,2022-01-13 14:38:11,https://stackoverflow.com/questions/70698416/focal-loss-for-imbalanced-data-using-pytorch
Building the output layer of NLP model (is the &quot;embedding&quot; layer),"<p>I was looking through some notebooks in Kaggle just to get a deeper understanding of how NLP works. I came across a notebook for the natural language inference task of predicting the relationship between a given premise and hypothesis. It uses the pretrained BERT model for this task</p>
<p>I had a question about the <code>build_model()</code> function:</p>
<pre><code>max_len = 50

def build_model():
   bert_encoder = TFBertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
   input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
   input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
   input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
   
   embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0] # confused about this line
   output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
   
   model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
   model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
   
   return model 
</code></pre>
<p>I am confused about this line: <code>embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]</code></p>
<p>What does this &quot;embedding&quot; represent and why is there a [0] infront of the function call? Why is the bert_encoder used to instantiate this &quot;embedding&quot;?</p>
<p>Thanks in advance!</p>
","tensorflow, nlp, bert-language-model, kaggle","<p><strong>logits</strong></p>
<p><a href=""https://i.sstatic.net/w6ATA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w6ATA.png"" alt=""scrnsht1"" /></a></p>
<p><a href=""https://i.sstatic.net/jTkvm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jTkvm.png"" alt=""scrnsht2"" /></a></p>
<p>You have to put <code>[0]</code> in order to have <code>torch.Tensor</code> for computation.
You can also try <code>output.logits</code> instead of <code>output[0]</code></p>
<p>ps. I used <code>AutoModelForMaskedLM</code>, not <code>TFBertModel</code>. It might be little different, but just try to print out your <code>embedding</code> first = ]</p>
",0,0,115,2022-01-13 16:19:53,https://stackoverflow.com/questions/70699795/building-the-output-layer-of-nlp-model-is-the-embedding-layer
Optimize Albert HuggingFace model,"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model</p>
<p>Kernel: <code>conda_pytorch_p36</code>.</p>
<p><strong>Section 2.1</strong> exports the finalised model. It too uses a BERT specific function. However, I cannot find an equivalent for Albert.</p>
<p>I've successfully implemented alternatives for Albert up until this section.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code># optimize transformer-based models with onnxruntime-tools
from onnxruntime_tools import optimizer
from onnxruntime_tools.transformers.onnx_model_bert import BertOptimizationOptions

# disable embedding layer norm optimization for better model size reduction
opt_options = BertOptimizationOptions('bert')
opt_options.enable_embed_layer_norm = False
...
</code></pre>
<p><strong>Do functions for Optimizing and Quantizing an Albert model exist?</strong></p>
<p>Update: You can run Quantization in the notebook, without running Optimization. You just need to remove '.opt.' from code, that is an indicative of optimised filenames.</p>
","python, huggingface-transformers, bert-language-model, onnx, huggingface-tokenizers","<p>Optimise any PyTorch model, using <strong>torch_optimizer</strong>.</p>
<p>Installation:</p>
<pre class=""lang-sh prettyprint-override""><code>pip install torch_optimizer
</code></pre>
<p>Implementation:</p>
<pre class=""lang-py prettyprint-override""><code>import torch_optimizer as optim

# model = ...
optimizer = optim.DiffGrad(model.parameters(), lr=0.001)
optimizer.step()
</code></pre>
<p><a href=""https://github.com/jettify/pytorch-optimizer#simple-example"" rel=""nofollow noreferrer"">Source</a></p>
<pre class=""lang-py prettyprint-override""><code>torch.save(model.state_dict(), PATH)
</code></pre>
<p><a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended"" rel=""nofollow noreferrer"">Source</a></p>
",0,1,293,2022-01-17 11:25:01,https://stackoverflow.com/questions/70740565/optimize-albert-huggingface-model
"TypeError: nll_loss_nd(): argument &#39;input&#39; (position 1) must be Tensor, not tuple","<p>So I'm trying to train my BigBird model (BigBirdForSequenceClassification) and I got to the moment of the training, which ends with below error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\######&quot;, line 189, in &lt;module&gt;
    train_loss, _ = train()  
  File &quot;C:\Users\######&quot;, line 152, in train
    loss = cross_entropy(preds, labels)
  File &quot;C:\Users\#####\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\######\venv\lib\site-packages\torch\nn\modules\loss.py&quot;, line 211, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File &quot;C:\Users\######\venv\lib\site-packages\torch\nn\functional.py&quot;, line 2532, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
TypeError: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not tuple
</code></pre>
<p>From what I understand, the problem happens because the train() function returns the tuple. Now - my question is how I should approach such issue? How do I change the output of train() function to return tensor instead of tuple?
I have seen similar issues posted here but none of the solutions seems to be helpful in my case, not even</p>
<pre><code>model = BigBirdForSequenceClassification(config).from_pretrained(checkpoint, return_dict=False)
</code></pre>
<p>(When I don't add return_dict=False I got similiar error message but it says &quot;<code>TypeError: nll_loss_nd(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput</code>&quot;
Please see my train code below:</p>
<pre><code>def train():
    model.train()
    total_loss = 0
    total_preds = []
    
    for step, batch in enumerate(train_dataloader):
        
        if step % 10 == 0 and not step == 0:
            print('Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
            
        batch = [r.to(device) for r in batch]
        sent_id, mask, labels = batch

        preds = model(sent_id, mask)

        loss = cross_entropy(preds, labels)
        total_loss = total_loss + loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()
        preds = preds.detach().cpu().numpy()
        total_preds.append(preds)

    avg_loss = total_loss / len(train_dataloader)
    total_preds = np.concatenate(total_preds, axis=0)
    return avg_loss, total_preds
</code></pre>
<p>and then:</p>
<pre><code>for epoch in range(epochs):
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    train_loss, _ = train()  
    train_losses.append(train_loss)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
</code></pre>
<p>I will really appreciate any help on this case and thank you in advance!</p>
","python, pytorch, huggingface-transformers, bert-language-model","<p>Ok, so it seems like I should have used BigBirdModel instead of BigBirdForSequenceClassification - issue solved</p>
",0,0,1737,2022-01-17 19:46:37,https://stackoverflow.com/questions/70746737/typeerror-nll-loss-nd-argument-input-position-1-must-be-tensor-not-tupl
ValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?,"<p>Goal: Amend this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">Notebook</a> to work with <strong>albert-base-v2</strong> model</p>
<p>Kernel: <code>conda_pytorch_p36</code>.</p>
<p><strong>Section 1.2</strong> instantiates a model from files in <code>./MRPC/</code> dir.</p>
<p>However, I <em>think</em> it is for a <strong>BERT</strong> model, not <strong>Albert</strong>. So, I downloaded an Albert <code>config.json</code> file from <a href=""https://huggingface.co/albert-base-v2/tree/main"" rel=""nofollow noreferrer"">here</a>. It is this chnage that causes the error.</p>
<p><strong>What else do I need to do in order to instantiate an Albert model?</strong></p>
<hr />
<p><code>./MRPC/</code> dir:</p>
<pre><code>!curl https://download.pytorch.org/tutorial/MRPC.zip --output MPRC.zip
!unzip -n MPRC.zip
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from os import listdir
from os.path import isfile, join
​
mypath = './MRPC/'
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]
onlyfiles
---

['tokenizer_config.json',
 'special_tokens_map.json',
 'pytorch_model.bin',
 'config.json',
 'training_args.bin',
 'added_tokens.json',
 'vocab.txt']
</code></pre>
<p>Configs:</p>
<pre class=""lang-py prettyprint-override""><code># The output directory for the fine-tuned model, $OUT_DIR.
configs.output_dir = &quot;./MRPC/&quot;

# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.
configs.data_dir = &quot;./glue_data/MRPC&quot;

# The model name or path for the pre-trained model.
configs.model_name_or_path = &quot;albert-base-v2&quot;
# The maximum length of an input sequence
configs.max_seq_length = 128

# Prepare GLUE task.
configs.task_name = &quot;MRPC&quot;.lower()
configs.processor = processors[configs.task_name]()
configs.output_mode = output_modes[configs.task_name]
configs.label_list = configs.processor.get_labels()
configs.model_type = &quot;albert&quot;.lower()
configs.do_lower_case = True

# Set the device, batch size, topology, and caching flags.
configs.device = &quot;cpu&quot;
configs.eval_batch_size = 1
configs.n_gpu = 0
configs.local_rank = -1
configs.overwrite_cache = False
</code></pre>
<p>Model:</p>
<pre class=""lang-py prettyprint-override""><code>model = AlbertForSequenceClassification.from_pretrained(configs.output_dir)  # !
model.to(configs.device)
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-0936fd8cbb17&gt; in &lt;module&gt;
      1 # load model
----&gt; 2 model = AlbertForSequenceClassification.from_pretrained(configs.output_dir)
      3 model.to(configs.device)
      4 
      5 # quantize model

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1460                     pretrained_model_name_or_path,
   1461                     ignore_mismatched_sizes=ignore_mismatched_sizes,
-&gt; 1462                     _fast_init=_fast_init,
   1463                 )
   1464 

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)
   1601             if any(key in expected_keys_not_prefixed for key in loaded_keys):
   1602                 raise ValueError(
-&gt; 1603                     &quot;The state dictionary of the model you are training to load is corrupted. Are you sure it was &quot;
   1604                     &quot;properly saved?&quot;
   1605                 )

ValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?
</code></pre>
","python, huggingface-transformers, bert-language-model, onnx, huggingface-tokenizers","<p>Exactly what I was looking for, <a href=""https://huggingface.co/textattack/albert-base-v2-MRPC"" rel=""nofollow noreferrer"">textattack/albert-base-v2-MRPC</a></p>
<p>How to use from the 🤗/transformers library</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&quot;textattack/albert-base-v2-MRPC&quot;)

model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/albert-base-v2-MRPC&quot;)
</code></pre>
<p>Or just clone the model repo</p>
<pre class=""lang-sh prettyprint-override""><code>git lfs install
git clone https://huggingface.co/textattack/albert-base-v2-MRPC
# if you want to clone without large files – just their pointers
# prepend your git clone with the following env var:
GIT_LFS_SKIP_SMUDGE=1
</code></pre>
",1,0,3769,2022-01-18 10:33:29,https://stackoverflow.com/questions/70754085/valueerror-the-state-dictionary-of-the-model-you-are-trying-to-load-is-corrupte
Span-Aste with allennlp - testing against new unseen and unlabeled data,"<p>I am trying to use this <a href=""https://colab.research.google.com/drive/1F9zW_nVkwfwIVXTOA_juFDrlPz5TLjpK?usp=sharing"" rel=""nofollow noreferrer"">colab</a> of this <a href=""https://github.com/xuuuluuu/Span-ASTE"" rel=""nofollow noreferrer"">github</a> page to extract the triplet [term, opinion, value] from a sentence from my custom dataset.</p>
<p>Here is an overview of the system architecture:
<a href=""https://i.sstatic.net/bReOT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bReOT.png"" alt=""Span-Aste system"" /></a></p>
<p>While I can use the sample offered in the colab and also train the model with my data, I don't know I should re-use this against an unlabeled sample.</p>
<p>If I try to run the colab as-is changing only the test and dev data with unlabeled data, I encounter this error:</p>
<pre><code>    DEVICE=0 {   &quot;names&quot;: &quot;sample&quot;,   &quot;seeds&quot;: [
        0   ],   &quot;sep&quot;: &quot;,&quot;,   &quot;name_out&quot;: &quot;results&quot;,   &quot;kwargs&quot;: {
        &quot;trainer__cuda_device&quot;: 0,
        &quot;trainer__num_epochs&quot;: 10,
        &quot;trainer__checkpointer__num_serialized_models_to_keep&quot;: 1,
        &quot;model__span_extractor_type&quot;: &quot;endpoint&quot;,
        &quot;model__modules__relation__use_single_pool&quot;: false,
        &quot;model__relation_head_type&quot;: &quot;proper&quot;,
        &quot;model__use_span_width_embeds&quot;: true,
        &quot;model__modules__relation__use_distance_embeds&quot;: true,
        &quot;model__modules__relation__use_pair_feature_multiply&quot;: false,
        &quot;model__modules__relation__use_pair_feature_maxpool&quot;: false,
        &quot;model__modules__relation__use_pair_feature_cls&quot;: false,
        &quot;model__modules__relation__use_span_pair_aux_task&quot;: false,
        &quot;model__modules__relation__use_span_loss_for_pruners&quot;: false,
        &quot;model__loss_weights__ner&quot;: 1.0,
        &quot;model__modules__relation__spans_per_word&quot;: 0.5,
        &quot;model__modules__relation__neg_class_weight&quot;: -1   },   &quot;root&quot;: &quot;aste/data/triplet_data&quot; } {   &quot;root&quot;: &quot;/content/Span-ASTE/aste/data/triplet_data/sample&quot;,   &quot;train_kwargs&quot;: {
        &quot;seed&quot;: 0,
        &quot;trainer__cuda_device&quot;: 0,
        &quot;trainer__num_epochs&quot;: 10,
        &quot;trainer__checkpointer__num_serialized_models_to_keep&quot;: 1,
        &quot;model__span_extractor_type&quot;: &quot;endpoint&quot;,
        &quot;model__modules__relation__use_single_pool&quot;: false,
        &quot;model__relation_head_type&quot;: &quot;proper&quot;,
        &quot;model__use_span_width_embeds&quot;: true,
        &quot;model__modules__relation__use_distance_embeds&quot;: true,
        &quot;model__modules__relation__use_pair_feature_multiply&quot;: false,
        &quot;model__modules__relation__use_pair_feature_maxpool&quot;: false,
        &quot;model__modules__relation__use_pair_feature_cls&quot;: false,
        &quot;model__modules__relation__use_span_pair_aux_task&quot;: false,
        &quot;model__modules__relation__use_span_loss_for_pruners&quot;: false,
        &quot;model__loss_weights__ner&quot;: 1.0,
        &quot;model__modules__relation__spans_per_word&quot;: 0.5,
        &quot;model__modules__relation__neg_class_weight&quot;: -1   },   &quot;path_config&quot;: &quot;/content/Span-ASTE/training_config/aste.jsonnet&quot;,   &quot;repo_span_model&quot;: &quot;/content/Span-ASTE&quot;,   &quot;output_dir&quot;: &quot;model_outputs/aste_sample_c7b00b66bf7ec669d23b80879fda043d&quot;,   &quot;model_path&quot;: &quot;models/aste_sample_c7b00b66bf7ec669d23b80879fda043d/model.tar.gz&quot;,   &quot;data_name&quot;: &quot;sample&quot;,   &quot;task_name&quot;: &quot;aste&quot; }
    # of original triplets:  11
    # of triplets for current setup:  11
    # of original triplets:  7
    # of triplets for current setup:  7 Traceback (most recent call last):   File &quot;/usr/lib/python3.7/pdb.py&quot;, line 1699, in main
        pdb._runscript(mainpyfile)   
File &quot;/usr/lib/python3.7/pdb.py&quot;, line 1568, in _runscript
        self.run(statement)   
File &quot;/usr/lib/python3.7/bdb.py&quot;, line 578, in run
        exec(cmd, globals, locals)   File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 1, in &lt;module&gt;
        import json   
File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 138, in Fire
        component_trace = _Fire(component, args, parsed_flag_args, context, name)   File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 468, in
    _Fire
        target=component.__name__)   
File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 672, in
    _CallAndUpdateTrace
        component = fn(*varargs, **kwargs)   File &quot;/content/Span-ASTE/aste/main.py&quot;, line 278, in main
        scores = main_single(p, overwrite=True, seed=seeds[i], **kwargs)   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 254, in main_single
        trainer.train(overwrite=overwrite)   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 185, in train
        self.setup_data()   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 177, in setup_data
        data.load()   
File &quot;aste/data_utils.py&quot;, line 214, in load
        opinion_offset=self.opinion_offset,   
File &quot;aste/evaluation.py&quot;, line 165, in read_inst
        o_output = line[2].split()  # opinion IndexError: list index out of range Uncaught exception. Entering post mortem debugging Running 'cont' or 'step' will restart the program
    &gt; /content/Span-ASTE/aste/evaluation.py(165)read_inst()
    -&gt; o_output = line[2].split()  # opinion (Pdb)
</code></pre>
<p>From my understanding, it seems that it is searching for the labels to start the evaluation. The problem is that I don't have those labels - although I have provided training set with similar data and labels associated.</p>
<p>I am new in deep learning and also allennlp so I am probably missing knowledge. I have tried to solve this for the past 2 weeks but I am still stuck, so here I am.</p>
","deep-learning, sentiment-analysis, bert-language-model, reproducible-research, allennlp","<p>KeyPi, this is a supervised learning model, it needs labelled data for your text corpus in the form sentence(ex: I charge it at night and skip taking the cord with me because of the good battery life .)  followed by '#### #### ####' as a separator and list of labels(include aspect/target word index in first list and the openion token index in the sentence followed by 'POS' for Positive and 'NEG' for negitive.) [([16, 17], [15], 'POS')]
16 and 17- battery life and in index 15, we have openion word &quot;good&quot;.
I am not sure if you have figures this out already and find some way to label the corpus.</p>
",0,0,94,2022-01-19 19:02:59,https://stackoverflow.com/questions/70776362/span-aste-with-allennlp-testing-against-new-unseen-and-unlabeled-data
NER Classification Deberta Tokenizer error : You need to instantiate DebertaTokenizerFast,"<p>I'm trying to perform a NER Classification task using Deberta, but I'm stacked with a Tokenizer error. This is my code (my input sentence must be splitted word by word by &quot;,:):</p>
<pre><code>from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;])
</code></pre>
<p>I have this results:</p>
<pre><code>{'input_ids': [[1, 31414, 2], [1, 6, 2], [1, 9226, 2], [1, 354, 2], [1, 1264, 2], [1, 19530, 4086, 2], [1, 44154, 2], [1, 12473, 2], [1, 30938, 2], [1, 4, 2]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]} 
</code></pre>
<p>Then I proceed but I have this error:</p>
<pre><code>tokenized_input = tokenizer(example[&quot;tokens&quot;])
tokens = tokenizer.convert_ids_to_tokens(tokenized_input[&quot;input_ids&quot;])
print(tokens)
</code></pre>
<pre><code>TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'
</code></pre>
<p>And I think the reasons is that I need to have the results of the token in the following format(that is not possible because I have the sentence splitted by &quot;,&quot;:</p>
<pre><code>tokenizer(&quot;Hello, this is one sentence!&quot;)

{'input_ids': [1, 31414, 6, 42, 16, 65, 3645, 328, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>So I tried in this both way but I'm stack and don't know how to do. There are very few documentation online about Deberta.</p>
<pre><code>tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;], is_split_into_words=True)

AssertionError: You need to instantiate DebertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs.
</code></pre>
<pre><code>tokenizer([&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;], is_split_into_words=True,add_prefix_space=True)
</code></pre>
<p>And the error is still the same.
Thank you so much !</p>
","python, tokenize, bert-language-model, named-entity-recognition, roberta","<p>Lets try this:</p>
<pre><code>input_ids = [1, 31414, 6, 42, 16, 65, 3645, 328, 2]
input_ids  = ','.join(map(str, input_ids ))


input_ids = [&quot;Hello&quot;, &quot;,&quot;, &quot;this&quot;, &quot;is&quot;, &quot;one&quot;, &quot;sentence&quot;, &quot;split&quot;, &quot;into&quot;, &quot;words&quot;, &quot;.&quot;]
input_ids  = ','.join(map(str, input_ids ))
input_ids
</code></pre>
",1,1,633,2022-01-21 09:42:10,https://stackoverflow.com/questions/70799226/ner-classification-deberta-tokenizer-error-you-need-to-instantiate-debertatoke
Is there a faster way to convert sentences to TFHUB embeddings?,"<p>So I am involved in a project that involves feeding a combination of text embeddings and image vectors into a DNN to arrive at the result. Now for the word embedding part, I am using TFHUB's Electra while for the image part I am using a NASNet Mobile network.</p>
<p>However, the issue I am facing is that while running the word embedding part, using the code shown below, the code just keeps running nonstop. It has been over 2 hours now and my training dataset has just 14900 rows of tweets.</p>
<p>Note - The input to the function is just a list of 14900 tweets.</p>
<pre><code>tfhub_handle_encoder=&quot;https://tfhub.dev/google/electra_small/2&quot; 
tfhub_handle_preprocess=&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3

# Load Models 
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) 
bert_model = hub.KerasLayer(tfhub_handle_encoder)

def get_text_embedding(text):

  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='Preprocessor')   
  encoder_inputs = preprocessing_layer(text)   encoder = 
  hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='Embeddings')   outputs = 
  encoder(encoder_inputs)   text_repr = outputs['pooled_output']   text_repr = 
  tf.keras.layers.Dense(128, activation='relu')(text_repr)

  return text_repr

text_repr = get_text_embedding(train_text)
</code></pre>
<p>Is there a faster way to get text representation using these models?</p>
<p>Thanks for the help!</p>
","deep-learning, nlp, bert-language-model, word-embedding, tensorflow-hub","<p>The operation performed in the code is quadratic in its nature. While I managed to execute your snippet with 10000 samples within a few minutes, a 14900 long input ran out of memory on 32GB RAM runtime. Is it possible that your runtime is experiencing swapping?</p>
<p>It is not clear what is the snippet trying to achieve. Do you intend to train model? In such case you can define the text_input as an Input layer and use fit to train. Here is an example: <a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert#define_your_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/classify_text_with_bert#define_your_model</a></p>
",0,0,104,2022-01-23 07:30:21,https://stackoverflow.com/questions/70820006/is-there-a-faster-way-to-convert-sentences-to-tfhub-embeddings
Can I fine-tune BERT using only masked language model and next sentence prediction?,"<p>So if I understand correctly there are mainly two ways to adapt BERT to a specific task: fine-tuning (all weights are changed, even pretrained ones) and feature-based (pretrained weights are frozen). However, I am confused.</p>
<ol>
<li>When to use which one? If you have unlabeled data (unsupervised learning), should you then use fine-tuning?</li>
<li>If I want to fine-tuned BERT, isn't the only option to do that using masked language model and next sentence prediction? And also: is it necessary to put another layer of neural network on top?</li>
</ol>
<p>Thank you.</p>
","nlp, bert-language-model","<p>Your first approach should be to try the pre-trained weights. Generally it works well. However if you are working on a different domain (e.g.: Medicine), then you'll need to fine-tune on data from new domain. Again you might be able to find pre-trained models on the domains (e.g.: <a href=""https://arxiv.org/abs/1901.08746"" rel=""nofollow noreferrer"">BioBERT</a>).</p>
<p>For adding layer, there are slightly different approaches depending on your task. E.g.: For question-answering, have a look at <a href=""https://arxiv.org/abs/1911.04118"" rel=""nofollow noreferrer"">TANDA</a> paper (Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection). It is a very nice easily readable paper which explains the transfer and adaptation strategy. Again, hugging-face has modified and pre-trained models for most of the standard tasks.</p>
",0,1,1228,2022-02-01 11:35:17,https://stackoverflow.com/questions/70939904/can-i-fine-tune-bert-using-only-masked-language-model-and-next-sentence-predicti
Custom query with Deepstackai haystack,"<p>I am exploring <a href=""https://haystack.deepset.ai/overview/get-started"" rel=""nofollow noreferrer"">deepset haystack</a> and found it very interesting for multiple use cases like a chatbot, search engine, document search, etc</p>
<p>But have not found any reference where I can create multiple indexes for different documents and search based on indexes. I thought of using meta tags for conditional search(on a particular area) by tagging the documents first and then using the <code>params</code> parameter of query API but the same doesn't seem to work and throws an error(I used its vanilla docker-compose based setup)</p>
<p><a href=""https://i.sstatic.net/PefMN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PefMN.png"" alt=""enter image description here"" /></a></p>
","elasticsearch, bert-language-model, nlp-question-answering, haystack","<p>You can use multiple indices in the same document store if you want to support multiple use cases, indeed. The <code>write_documents</code> method of the document store has a parameter <code>index</code> so that you can store documents for your different use cases in different indices. In the same way, you can pass an <code>index</code> parameter to the <code>query</code> method.</p>
<p>As you expected, there is an alternative solution that uses the <code>meta</code> field of documents. However, the format needs to be slightly different. Your query needs to have the following format:</p>
<pre><code>{&quot;query&quot;: &quot;What's the capital town?&quot;, &quot;params&quot;: {&quot;filters&quot;: {&quot;name&quot;: &quot;75_Algeria75.txt&quot;}}}
</code></pre>
<p>and your documents need to have the following format:</p>
<pre><code>{'text': 'Algeria is...', 'meta':{'name': &quot;75_Algeria75.txt&quot;}}
</code></pre>
",2,2,705,2022-02-03 04:37:05,https://stackoverflow.com/questions/70965996/custom-query-with-deepstackai-haystack
"RuntimeError: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1","<p>When running this code:</p>
<pre class=""lang-py prettyprint-override""><code>embedding_matrix = torch.stack(embeddings)
</code></pre>
<p>I got this error:</p>
<blockquote>
<p>RuntimeError: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1</p>
</blockquote>
<p>I'm trying to get embedding using BERT via:</p>
<pre class=""lang-py prettyprint-override""><code>split_sent = sent.split()
tokens_embedding = []
j = 0
for full_token in split_sent:
    curr_token = ''
    x = 0
    for i,_ in enumerate(tokenized_sent[1:]): 
        token = tokenized_sent[i+j]
        piece_embedding = bert_embedding[i+j]
        if token == full_token and curr_token == '' :
            tokens_embedding.append(piece_embedding)
            j += 1
            break                                     
sent_embedding = torch.stack(tokens_embedding)
embeddings.append(sent_embedding)
embedding_matrix = torch.stack(embeddings)
</code></pre>
<p>Does anyone know how I can fix this?</p>
","python, pytorch, runtime-error, tensor, bert-language-model","<p>As per <a href=""https://pytorch.org/docs/1.9.1/generated/torch.stack.html"" rel=""noreferrer"">PyTorch Docs</a> about <code>torch.stack()</code> function, it needs the input tensors in the same shape to stack. I don't know how will you be using the <code>embedding_matrix</code> but either you can add padding to your tensors (which will be a list of zeros at the end till a certain user-defined length and is recommended if you will train with this stacked tensor, refer <a href=""https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/"" rel=""noreferrer"">this tutorial</a>) to make them equidimensional or you can simply use something like <code>torch.cat(data,dim=0)</code>.</p>
",6,6,40437,2022-02-06 20:23:36,https://stackoverflow.com/questions/71011333/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-7-768-at-en
"When doing pre-training of a transformer model, how can I add words to the vocabulary?","<p>Given a DistilBERT trained language model for a given language, taken from the Huggingface hub, I want to pre-train the model on a specific domain, and I want to add new words that are:</p>
<ul>
<li>definitely non existing in the original training set</li>
<li>and impossible to handle via word piece toeknization - basically you can think of these words as &quot;codes&quot; that are a normalized form of a named entity</li>
</ul>
<p>Consider that:</p>
<ul>
<li>I would like to avoid to learn a <em>new</em> tokenizer: I am fine to add the new words, and then let the model learn their embeddings via pre-training</li>
<li>the number of the &quot;words&quot; is way larger that the &quot;unused&quot; tokens in the &quot;stock&quot; vocabulary</li>
</ul>
<p>The only advice that I have found is the one reported <a href=""https://github.com/huggingface/transformers/issues/237"" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>Append it to the end of the vocab, and write a script which generates a new checkpoint that is identical to the pre-trained checkpoint, but but with a bigger vocab where the new embeddings are randomly initialized (for initialized we used tf.truncated_normal_initializer(stddev=0.02)). This will likely require mucking around with some tf.concat() and tf.assign() calls.</p>
</blockquote>
<p>Do you think this is the only way of achieve my goal?</p>
<p>If yes, I do not have any idea of how to write this &quot;script&quot;: does someone has some hints at how to proceeed (sample code, documentation etc)?</p>
","pytorch, huggingface-transformers, bert-language-model","<p>As per my comment, I'm assuming that you go with a pre-trained checkpoint, if only to &quot;avoid [learning] a new tokenizer.&quot;
Also, the solution works with PyTorch, which might be more suitable for such changes. I haven't checked Tensorflow (which is mentioned in one of your quotes), so no guarantees that this works across platforms.<br />
To solve your problem, let us divide this into two sub-problems:</p>
<ul>
<li>Adding the new tokens to the tokenizer, and</li>
<li>Re-sizing the token embedding matrix of the model accordingly.</li>
</ul>
<p>The first can actually be achieved quite simply by using <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L905"" rel=""nofollow noreferrer""><code>.add_tokens()</code></a>. I'm referencing the slow tokenizer's implementation of it (because it's in Python), but from what I can see, this also exists for the faster Rust-based tokenizers.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
# Will return an integer corresponding to the number of added tokens
# The input could also be a list of strings instead of a single string
num_new_tokens = tokenizer.add_tokens(&quot;dennlinger&quot;)  
</code></pre>
<p>You can quickly verify that this worked by looking at the encoded input ids:</p>
<pre class=""lang-py prettyprint-override""><code>print(tokenizer(&quot;This is dennlinger.&quot;))
# 'input_ids': [101, 2023, 2003, 30522, 1012, 102]
</code></pre>
<p>The index <code>30522</code> now corresponds to the new token with my username, so we can check the first part. However, if we look at the function docstring of <code>.add_tokens()</code>, it also says:</p>
<blockquote>
<p>Note, hen adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer.<br />
In order to do that, please use the <code>PreTrainedModel.resize_token_embeddings</code> method.</p>
</blockquote>
<p>Looking at <a href=""https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings"" rel=""nofollow noreferrer"">this particular function</a>, the description is a bit confusing, but we can get a correctly resized matrix (with randomly initialized weights for new tokens), by simply passing the previous model size, plus the number of new tokens:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel

model = AutoModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
model.resize_token_embeddings(model.config.vocab_size + num_new_tokens)

# Test that everything worked correctly
model(**tokenizer(&quot;This is dennlinger&quot;, return_tensors=&quot;pt&quot;))
</code></pre>
<p>EDIT: Notably, <code>.resize_token_embeddings()</code> also takes care of any associated weights; this means, if you are pre-training, it will also adjust the size of the language modeling head (which should have the same number of tokens), or fix tied weights that would be affected by an increased number of tokens.</p>
",2,1,2920,2022-02-10 14:57:38,https://stackoverflow.com/questions/71067376/when-doing-pre-training-of-a-transformer-model-how-can-i-add-words-to-the-vocab
HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128),"<p>I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:</p>
<pre><code>from datasets import load_dataset
from transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('Tokenizing data. This may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = AutoModel.from_pretrained(model_name)

training_args = TrainingArguments(checkpoints_dir)

print('Training the model...')
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()
</code></pre>
<p>I get the following error:</p>
<pre><code>  File &quot;train_lm_hf.py&quot;, line 44, in &lt;module&gt;
    trainer.train()
...
  File &quot;/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py&quot;, line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 165 at dim 1 (got 128)
</code></pre>
<p>What am I doing wrong?</p>
","python, deep-learning, pytorch, huggingface-transformers, bert-language-model","<p>I fixed this solution by changing the tokenize function to:</p>
<pre><code>def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)
</code></pre>
<p>(note the <code>padding</code> argument). Also, I used a data collator like so:</p>
<pre><code>data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)
trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
)
</code></pre>
",19,13,26472,2022-02-17 23:45:30,https://stackoverflow.com/questions/71166789/huggingface-valueerror-expected-sequence-of-length-165-at-dim-1-got-128
How to change AllenNLP BERT based Semantic Role Labeling to RoBERTa in AllenNLP,"<p>Currently i'm able to train a <a href=""https://demo.allennlp.org/semantic-role-labeling"" rel=""nofollow noreferrer"">Semantic Role Labeling</a> model using the config file below. This config file is based on the <a href=""https://raw.githubusercontent.com/allenai/allennlp-models/main/training_config/structured_prediction/bert_base_srl.jsonnet"" rel=""nofollow noreferrer"">one provided by AllenNLP</a> and works for the default <code>bert-base-uncased</code> model and also <code>GroNLP/bert-base-dutch-cased</code>.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;srl_custom&quot;,
    &quot;bert_model_name&quot;: &quot;GroNLP/bert-base-dutch-cased&quot;
  },
  &quot;data_loader&quot;: {
    &quot;batch_sampler&quot;: {
      &quot;type&quot;: &quot;bucket&quot;,
      &quot;batch_size&quot;: 32
    }
  },
  &quot;train_data_path&quot;: &quot;./data/SRL/SONAR_1_SRL/MANUAL500/&quot;,
  &quot;validation_data_path&quot;: &quot;./data/SRL/SONAR_1_SRL/MANUAL500/&quot;,
  &quot;model&quot;: {
    &quot;type&quot;: &quot;srl_bert&quot;,
    &quot;embedding_dropout&quot;: 0.1,
    &quot;bert_model&quot;: &quot;GroNLP/bert-base-dutch-cased&quot;
  },
  &quot;trainer&quot;: {
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;huggingface_adamw&quot;,
      &quot;lr&quot;: 5e-5,
      &quot;correct_bias&quot;: false,
      &quot;weight_decay&quot;: 0.01,
      &quot;parameter_groups&quot;: [
        [
          [
            &quot;bias&quot;,
            &quot;LayerNorm.bias&quot;,
            &quot;LayerNorm.weight&quot;,
            &quot;layer_norm.weight&quot;
          ],
          {
            &quot;weight_decay&quot;: 0.0
          }
        ]
      ]
    },
    &quot;learning_rate_scheduler&quot;: {
      &quot;type&quot;: &quot;slanted_triangular&quot;
    },
    &quot;checkpointer&quot;: {
      &quot;keep_most_recent_by_count&quot;: 2
    },
    &quot;grad_norm&quot;: 1.0,
    &quot;num_epochs&quot;: 3,
    &quot;validation_metric&quot;: &quot;+f1-measure-overall&quot;
  }
}
</code></pre>
<p>Swapping the values of <code>bert_model_name</code> and <code>bert_model</code> parameters from <code>GroNLP/bert-base-dutch-cased</code> to <code>roberta-base</code> won't work out of the box since the SRL datareader <a href=""https://github.com/allenai/allennlp-models/blob/1e89d5e51cb45f3e77a48d4983bf980088334fac/allennlp_models/structured_prediction/dataset_readers/srl.py#L146-L148"" rel=""nofollow noreferrer"">only supports the BertTokenizer</a> and not the RobertaTokenizer. So I changed the config file to the following:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;srl_custom&quot;,
    &quot;token_indexers&quot;: {
      &quot;tokens&quot;: {
        &quot;type&quot;: &quot;pretrained_transformer&quot;,
        &quot;model_name&quot;: &quot;roberta-base&quot;
      }
    }
  },
  &quot;data_loader&quot;: {
    &quot;batch_sampler&quot;: {
      &quot;type&quot;: &quot;bucket&quot;,
      &quot;batch_size&quot;: 32
    }
  },
  &quot;train_data_path&quot;: &quot;./data/SRL/SONAR_1_SRL/MANUAL500/&quot;,
  &quot;validation_data_path&quot;: &quot;./data/SRL/SONAR_1_SRL/MANUAL500/&quot;,
  &quot;model&quot;: {
    &quot;type&quot;: &quot;srl_bert&quot;,
    &quot;embedding_dropout&quot;: 0.1,
    &quot;bert_model&quot;: &quot;roberta-base&quot;
  },
  &quot;trainer&quot;: {
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;huggingface_adamw&quot;,
      &quot;lr&quot;: 5e-5,
      &quot;correct_bias&quot;: false,
      &quot;weight_decay&quot;: 0.01,
      &quot;parameter_groups&quot;: [
        [
          [
            &quot;bias&quot;,
            &quot;LayerNorm.bias&quot;,
            &quot;LayerNorm.weight&quot;,
            &quot;layer_norm.weight&quot;
          ],
          {
            &quot;weight_decay&quot;: 0.0
          }
        ]
      ]
    },
    &quot;learning_rate_scheduler&quot;: {
      &quot;type&quot;: &quot;slanted_triangular&quot;
    },
    &quot;checkpointer&quot;: {
      &quot;keep_most_recent_by_count&quot;: 2
    },
    &quot;grad_norm&quot;: 1.0,
    &quot;num_epochs&quot;: 15,
    &quot;validation_metric&quot;: &quot;+f1-measure-overall&quot;
  }
}
</code></pre>
<p>However, this is still not working. I'm receiving the following error:</p>
<pre class=""lang-sh prettyprint-override""><code>2022-02-22 16:19:34,122 - INFO - allennlp.training.gradient_descent_trainer - Training
  0%|          | 0/1546 [00:00&lt;?, ?it/s]2022-02-22 16:19:34,142 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-02-22 16:19:34,142 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
  0%|          | 0/1546 [00:00&lt;?, ?it/s]
2022-02-22 16:19:34,526 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File &quot;C:\Program Files\Python39\lib\runpy.py&quot;, line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;C:\Program Files\Python39\lib\runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\Scripts\allennlp.exe\__main__.py&quot;, line 7, in &lt;module&gt;
    sys.exit(run())
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\__main__.py&quot;, line 39, in run
    main(prog=&quot;allennlp&quot;)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\__init__.py&quot;, line 119, in main
    args.func(args)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\train.py&quot;, line 111, in train_model_from_args
    train_model_from_file(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\train.py&quot;, line 177, in train_model_from_file
    return train_model(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\train.py&quot;, line 258, in train_model
    model = _train_worker(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\train.py&quot;, line 508, in _train_worker
    metrics = train_loop.run()
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\commands\train.py&quot;, line 581, in run
    return self.trainer.train()
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\training\gradient_descent_trainer.py&quot;, line 771, in train
    metrics, epoch = self._try_train()
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\training\gradient_descent_trainer.py&quot;, line 793, in _try_train
    train_metrics = self._train_epoch(epoch)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\training\gradient_descent_trainer.py&quot;, line 510, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=True)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp\training\gradient_descent_trainer.py&quot;, line 403, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp_models\structured_prediction\models\srl_bert.py&quot;, line 141, in forward
    bert_embeddings, _ = self.bert_model(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\transformers\models\bert\modeling_bert.py&quot;, line 989, in forward
    embedding_output = self.embeddings(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\transformers\models\bert\modeling_bert.py&quot;, line 215, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\sparse.py&quot;, line 156, in forward
    return F.embedding(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\torch\nn\functional.py&quot;, line 1916, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>I don't fully understand whats going wrong and couldn't find any documentation on how to change the config file to load in a 'custom' BERT/RoBERTa model (one thats not mentioned <a href=""https://github.com/huggingface/transformers/blob/f9582c205afaa4bb117bb67a4bf5184b053417b3/src/transformers/models/bert/tokenization_bert.py#L31-L52"" rel=""nofollow noreferrer"">here</a>). I'm running the default <code>allennlp train config.jsonnet</code> command to start training. <code>allennlp train config.jsonnet --dry-run</code> produces no errors however.</p>
<p>Thanks in advance!
Thijs</p>
<p><strong>EDIT:</strong>
I've now swapped out and inherited the &quot;srl_bert&quot; for a custom &quot;srl_roberta&quot; class to make use of the <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py"" rel=""nofollow noreferrer"">RobertaModel</a>. This however still produces the same error.</p>
<p><strong>EDIT2:</strong> I'm now using the AutoTokenizer as suggested by Dirk Groeneveld. It looks like changing the SrlReader class to support RoBERTa based models involves way more changes like swapping BERTs wordpiece tokenizer to RoBERTa's BPE tokenizer. Is there an easy way to adapt the SrlReader class or is it better to write a new RobertaSrlReader from scratch?</p>
<p>I've inherited the SrlReader class and changed <a href=""https://github.com/allenai/allennlp-models/blob/1e89d5e51cb45f3e77a48d4983bf980088334fac/allennlp_models/structured_prediction/dataset_readers/srl.py#L147"" rel=""nofollow noreferrer"">this line</a> to the following:</p>
<pre class=""lang-py prettyprint-override""><code>self.bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
</code></pre>
<p>It produces the following error since RoBERTa tokenization differs from BERT:</p>
<pre class=""lang-sh prettyprint-override""><code>  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py&quot;, line 255, in text_to_instance
    wordpieces, offsets, start_offsets = self._wordpiece_tokenize_input(
  File &quot;C:\Users\denbe\AppData\Roaming\Python\Python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py&quot;, line 196, in _wordpiece_tokenize_input
    word_pieces = self.bert_tokenizer.wordpiece_tokenizer.tokenize(token)
AttributeError: 'RobertaTokenizerFast' object has no attribute 'wordpiece_tokenizer'
</code></pre>
","bert-language-model, allennlp, roberta-language-model, srl","<p>The easiest way to resolve this is to patch <code>SrlReader</code> so that it uses <code>PretrainedTransformerTokenizer</code> (from AllenNLP) or <code>AutoTokenizer</code> (from Huggingface) instead of <code>BertTokenizer</code>. <code>SrlReader</code> is an old class, and was written against an old version of the Huggingface tokenizer API, so it's not so easy to upgrade.</p>
<p>If you want to submit a pull request in the AllenNLP project, I'd be happy to help you get it merged into AllenNLP!</p>
",1,2,445,2022-02-22 15:24:27,https://stackoverflow.com/questions/71223907/how-to-change-allennlp-bert-based-semantic-role-labeling-to-roberta-in-allennlp
Print Bert model summary using Pytorch,"<p>Hi I would like to print the model summary of my BERT model for text classification. I am using command print(summary(model, inputsize=(channels, height, width)). I would like to know what would be the dimensions of input_size in case of text classification?
I have use print(model) as well but the output is confusing and I want to see the output in the layered form.
Below is my model summary.</p>
<pre><code>BertClassifier(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (linear1): Linear(in_features=768, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=141, bias=True)
  (relu): ReLU()
)
</code></pre>
","input, model, pytorch, bert-language-model, summary","<p>I used torch-summary module-</p>
<pre><code>pip install torch-summary

summary(model,input_size=(768,),depth=1,batch_dim=1, dtypes=[‘torch.IntTensor’]) 
</code></pre>
",3,1,3370,2022-02-24 07:51:18,https://stackoverflow.com/questions/71248696/print-bert-model-summary-using-pytorch
output from bert into cnn model,"<p>i am trying to concatenate bert model with Cnn 1d using pytorch . I used this code but I do not understand what is meaning of in_channels and out_channels in function conv1d
if input shape into cnn model is torch(256,64,768)</p>
<pre><code>class MixModel(nn.Module):
    def __init__(self,pre_trained='distilbert-base-uncased'):
        super().__init__()        
        self.bert =  AutoModel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        self.conv = nn.Conv1d(in_channels=1, out_channels=256, kernel_size=5, padding='valid', stride=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size= 256- 5 + 1)
        self.dropout = nn.Dropout(0.3)
        self.clf = nn.Linear(self.hidden_size*2,6)
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= False) 
        x=cls_hs
       # x = torch.cat(cls_hs[0]) # x= [416, 64, 768]
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.clf(x)
        
        
      
        return x
</code></pre>
<p>Edit
I use recommended answer and change the parameters but i got error</p>
<pre><code>class MixModel(nn.Module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert =  AutoModel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        self.conv = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=5, padding='valid', stride=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size= 64- 5 + 1)
        print(11)
        self.dropout = nn.Dropout(0.3)
        print(12)
        self.clf = nn.Linear(self.hidden_size*2,6)
        print(13)
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= False) 
        x=cls_hs[0]
        print(cls_hs[0]) 
        print(len(cls_hs[0]))
        print(cls_hs[0].size())
        #x = torch.cat(cls_hs,0) # x= [416, 64, 768]
        x = x.permute(0, 2, 1)
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.clf(x)
return x
</code></pre>
<p>the error is
5 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
1846     if has_torch_function_variadic(input, weight, bias):
1847         return handle_torch_function(linear, (input, weight, bias), input, weight, bias=bias)
-&gt; 1848     return torch._C._nn.linear(input, weight, bias)
1849
1850</p>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1536x6)</p>
<pre><code></code></pre>
","python, pytorch, conv-neural-network, bert-language-model","<p>The dimension of the output prediction of BERT (and many other transformer-based models) is of shape <code>batch</code>x<code>seq-len</code>x<code>feature-dim</code>: That is, your input is a batch of 256 sequences of length (probably with padding) of 64 tokens, each token is represented by a feature vector of dimension 768.</p>
<p>In order to apply 1-d convolution along the sequence-len dimension, you will need first to <a href=""https://pytorch.org/docs/stable/generated/torch.permute.html"" rel=""nofollow noreferrer""><code>permute</code></a> <code>x</code> to be of shape <code>batch</code>x<code>dim</code>x<code>len</code>:</p>
<pre class=""lang-py prettyprint-override""><code>x = x.permute(0, 2, 1)
</code></pre>
<p>Now you can apply <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"" rel=""nofollow noreferrer""><code>nn.Conv1d</code></a>, where the <code>in_channels</code> is the dimension of <code>x</code> <code>= 768</code>. the <code>out_channels</code> is up to you - what is going to be the hidden dimension of your model.</p>
",1,0,1092,2022-02-28 21:32:19,https://stackoverflow.com/questions/71301279/output-from-bert-into-cnn-model
"BERT Classifier ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2]))","<p>I'm training a Classifier Model but it's a few days that I cannot overcame a problem!
I have the ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2])) error but actually it seems correct to me ! Indeed I used unsqueeze(1) to put them of the same size. WHat else I can try? Thank you!</p>
<pre><code>class SequenceClassifier(nn.Module):

  def __init__(self, n_classes):
    super(SequenceClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    ) 
    output = self.drop(pooled_output)
    return self.out(output)

model = SequenceClassifier(len(class_names))
model = model.to(device)

EPOCHS = 10

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)
weights=[0.5,1]
pos_weight=torch.FloatTensor(weights).to(device)
loss_fn=nn.BCEWithLogitsLoss(pos_weight=pos_weight)

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d[&quot;input_ids&quot;].to(device)
    attention_mask = d[&quot;attention_mask&quot;].to(device)
    targets = d[&quot;targets&quot;].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    
    targets = targets.unsqueeze(1)
    loss = loss_fn(outputs, targets)
    

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)
  

%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(df_train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn, 
    device, 
    len(df_val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc &gt; best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc
</code></pre>
<pre><code>ValueError: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 2]))
</code></pre>
<p><strong>EDIT</strong>
I have a binary classifier problem, indeed i have 2 classes encoded 0 (&quot;bad&quot;) and 1 (&quot;good&quot;).</p>
","python, machine-learning, pytorch, huggingface-transformers, bert-language-model","<p>In case anyone stumbles on this like I did, I'll write out an answer since there aren't a lot of google hits for this target size/input size error and the previous answer has some factual inaccuracies.</p>
<p>Unlike the previous answer would suggest, the real problem isn't with the loss function but with the output of the model.<code>nn.BCEWithLogitsLoss</code> is completely fine for multi-label and multi-class applications. Chiara updated her post saying that in fact she has a binary classification problem, but even that should not be a problem for this loss function. So why the error?</p>
<p>The original code has:</p>
<pre><code>outputs = model(
  input_ids=input_ids,
  attention_mask=attention_mask
)
_, preds = torch.max(outputs, dim=1)
</code></pre>
<p>This means &quot;Run the model, then create <code>preds</code> with the row indeces of the highest output of the model&quot;. Obviously, there is only a &quot;index of highest&quot; if there are multiple  predicted values. Multiple output values usually means multiple input classes, so I can see why Shai though this was multi-class. But why would we get multiple outputs from a binary classifier?</p>
<p>As it turns out, BERT (or Huggingface anyway) for binary problems expects that <code>n_classes</code> is set to 2 -- setting classes to 1 puts the model in regression mode. This means that under the hood, binary problems are treated like a two-class problem, outputting predictions with the size [2, <em>batch size</em>] -- one column predicting the chance of it being a 1 and one for the chance of it being 0. The loss fucntion throws an error because it is supplied with only one row of one-hot encoded labels: <code>targets = d[&quot;targets&quot;].to(device)</code> so the  labels have dimensions [<em>batch size</em>] or after the unsqueeze, [1, <em>batch size</em>]. Either way, the dimensions don't match up.</p>
<p>Some loss functions can deal with this fine, but others require the exact same dimensions. To make things more frustrating, for version 1.10, <code>nn.BCEWithLogitsLoss</code> requires matching dimensions but later versions do not.</p>
<p>One solution may therefore be to update your pytorch (version 1.11 would work for example).</p>
<p>For me, this was not an option, so I ended up going with a different loss function. <code>nn.CrossEntropyLoss</code>, as suggested by Shai, indeed does the trick as it accepts any input with the same length. In other words, they had a working solution for the wrong reasons.</p>
",3,0,4444,2022-03-02 07:02:24,https://stackoverflow.com/questions/71318599/bert-classifier-valueerror-target-size-torch-size4-1-must-be-the-same-as
How to fix random seed for BERTopic?,"<p>I'd like to fix the random seed from <a href=""https://github.com/MaartenGr/BERTopic"" rel=""noreferrer"">BERTopic</a> library to get reproducible results. Looking at the code of BERTopic I see it uses numpy. Will using <code>np.random.seed(123)</code> be enough? or do I also need to other libraries as random or pytorch as in this <a href=""https://stackoverflow.com/questions/59486629/bert-binary-textclassification-get-different-results-every-run"">question</a>.</p>
","python, bert-language-model","<p>You can fix the <code>random_state</code> variable using UMAP, but you have to also send the other default parameters to the UMAP constructor or the model will break.</p>
<p>What this looks like in practice is:</p>
<pre><code>umap = UMAP(n_neighbors=15,
            n_components=5,
            min_dist=0.0,
            metric='cosine',
            low_memory=False,
            random_state=1337) 
model = BERTopic(language=&quot;multilingual&quot;, umap_model=umap)
topics, probs = model.fit_transform(content)
</code></pre>
<p>By default, <code>umap_model</code> is set to <code>None</code> in the <code>BERTopic</code> constructor. Internally if that is not provided, it sets one up with default params <a href=""https://github.com/MaartenGr/BERTopic/blob/362ccc6b9acbf9e15a24edcbcfdeb81a2dd007ff/bertopic/_bertopic.py#L229C5-L229C5"" rel=""noreferrer"">here in the code</a>.</p>
<p>Note that <code>low_memory</code> is a param in both constructors, and if the <code>BERTopic</code> constructor isn't called with that in it, it internally sets it to <code>False</code>.</p>
",6,8,5254,2022-03-02 09:19:47,https://stackoverflow.com/questions/71320201/how-to-fix-random-seed-for-bertopic
"why do pooler use tanh as a activation func in bert, rather than gelu?","<p>class BERTPooler(nn.Module):
def <strong>init</strong>(self, config):
super(BERTPooler, self).<strong>init</strong>()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
self.activation = nn.Tanh()</p>
<pre><code>def forward(self, hidden_states):
    # We &quot;pool&quot; the model by simply taking the hidden state corresponding
    # to the first token.
    first_token_tensor = hidden_states[:, 0]
    pooled_output = self.dense(first_token_tensor)
    pooled_output = self.activation(pooled_output)
    return pooled_output
</code></pre>
","nlp, bert-language-model","<p>The author of the <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">original BERT paper</a> answered it (kind of) in a <a href=""https://github.com/google-research/bert/issues/43#issuecomment-435980269"" rel=""nofollow noreferrer"">comment on GitHub</a>.</p>
<blockquote>
<p>The tanh() thing was done early to try to make it more interpretable but it probably doesn't matter either way.</p>
</blockquote>
<p>I agree it doesn't fully answer &quot;whether&quot; <code>tanh</code> is preferable, but from the looks of it, it'll probably work with any activation.</p>
",0,0,820,2022-03-03 02:22:36,https://stackoverflow.com/questions/71331411/why-do-pooler-use-tanh-as-a-activation-func-in-bert-rather-than-gelu
How to freeze some layers of BERT in fine tuning in tf2.keras,"<p>I am trying to fine-tune 'bert-based-uncased' on a dataset for a text classification task. Here is the way I am downloading the model:</p>
<pre><code>import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

model = TFAutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=num_labels)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>As bert-base has 12 layers, I wanted to just fine-tune the last 2 layers to prevent overfitting. <code> model.layers[i].trainable = False</code> will not help. Because <code>model.layers[0]</code> gives the whole bert base model and if I set the <code>trainable</code> parameter to <code>False</code>, then all layers of bert will be frozen. Here is the architecture of <code>model</code>:</p>
<pre><code>Model: &quot;tf_bert_for_sequence_classification&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bert (TFBertMainLayer)      multiple                  109482240 
                                                                 
 dropout_37 (Dropout)        multiple                  0         
                                                                 
 classifier (Dense)          multiple                  9997      
                                                                 
=================================================================
Total params: 109,492,237
Trainable params: 109,492,237
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>Also, I wanted to use <code>model.layers[0].weights[j]._trainable = False</code>; but <code>weights</code> list has 199 elements in shape of <code>TensorShape([30522, 768])</code>. So I could not figure out that which weights are related to the last 2 layers.
Can any-one help me to fix this?</p>
","python-3.x, keras, tensorflow2.0, huggingface-transformers, bert-language-model","<p>I found the answer and I share it here. Hope it can help others.
By the help of <a href=""https://raphaelb.org/posts/freezing-bert/"" rel=""nofollow noreferrer"">this article</a>, which is about fine tuning bert using pytorch, the equivalent in tensorflow2.keras is as below:</p>
<pre><code>model.bert.encoder.layer[i].trainable = False
</code></pre>
<p>where i is the index of the proper layer.</p>
",2,3,3242,2022-03-03 11:04:40,https://stackoverflow.com/questions/71336067/how-to-freeze-some-layers-of-bert-in-fine-tuning-in-tf2-keras
How to get number of tokens in the sentence in keras,"<p>I have a sentence and a pre-trained tokenizer. I want to calculate the number of tokens in the sentence, <strong>without special tokens</strong>. I use the <a href=""https://huggingface.co/bert-base-cased"" rel=""nofollow noreferrer"">code</a> from HuggingFace.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertModel.from_pretrained(&quot;bert-base-cased&quot;)
text = &quot;I want to know the number of tokens in this sentence!!!&quot;
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
</code></pre>
<p>How can I do it?</p>
","python, nlp, token, huggingface-transformers, bert-language-model","<p>You can either use <code>encode</code> method with setting <code>add_special_tokens</code> to <code>False</code> or basically use <code>tokenize</code> method.</p>
<pre><code>encoded_input = tokenizer(text, return_tensors='tf', add_special_tokens=False)
encoded_input.input_ids.shape[1]
</code></pre>
<p>and</p>
<pre><code>tokenized_input = tokenizer.tokenize(text)
len(tokenized_input)
</code></pre>
",3,1,5049,2022-03-05 05:09:14,https://stackoverflow.com/questions/71359679/how-to-get-number-of-tokens-in-the-sentence-in-keras
how to save and load custom siamese bert model,"<p>I am following this tutorial on how to train a siamese bert network:</p>
<p><a href=""https://keras.io/examples/nlp/semantic_similarity_with_bert/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/semantic_similarity_with_bert/</a></p>
<p>all good, but I am not sure what is the best way to save the model after train it and save it.
any suggestion?</p>
<p>I was trying with</p>
<p><code>model.save('models/bert_siamese_v1')</code></p>
<p>which creates a folder with save_model.bp keras_metadata.bp and two subfolders (variables and assets)</p>
<p>then I try to load it with:</p>
<pre><code>model.load_weights('models/bert_siamese_v1/')
</code></pre>
<p>and it gives me this error:</p>
<pre><code>2022-03-08 14:11:52.567762: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open models/bert_siamese_v1/: Failed precondition: models/bert_siamese_v1; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?
</code></pre>
<p>what is the best way to proceed?</p>
","python, tensorflow, keras, deep-learning, bert-language-model","<p>Try using <code>tf.saved_model.save</code> to save your model:</p>
<pre><code>tf.saved_model.save(model, 'models/bert_siamese_v1')
model = tf.saved_model.load('models/bert_siamese_v1')
</code></pre>
<p>The warning you get during saving can <a href=""https://stackoverflow.com/questions/65697623/tensorflow-warning-found-untraced-functions-such-as-lstm-cell-6-layer-call-and"">apparently</a> be ignored. After loading your model, you can use it for inference <code>f(test_data)</code>:</p>
<pre><code>f = model.signatures[&quot;serving_default&quot;]
x1 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
x2 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
x3 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
print(f)
print(f(attention_masks = x1, input_ids = x2, token_type_ids = x3))
</code></pre>
<pre><code>ConcreteFunction signature_wrapper(*, token_type_ids, attention_masks, input_ids)
  Args:
    attention_masks: int32 Tensor, shape=(None, 128)
    input_ids: int32 Tensor, shape=(None, 128)
    token_type_ids: int32 Tensor, shape=(None, 128)
  Returns:
    {'dense': &lt;1&gt;}
      &lt;1&gt;: float32 Tensor, shape=(None, 3)
{'dense': &lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.40711606, 0.13456087, 0.45832306]], dtype=float32)&gt;}
</code></pre>
",1,3,1904,2022-03-08 14:20:18,https://stackoverflow.com/questions/71396540/how-to-save-and-load-custom-siamese-bert-model
"Hugginface Dataloader BERT ValueError: too many values to unpack (expected 2), AX hyperparameters tuning with Pytorch","<p>I'm stacked with this error from one week now, I tried everything so the fact is that I'm not understanding deeply what is happening (I'm new at pytorch implementation). Anyway I'm trying to implement a Bert Classifier to discriminate between 2 sequences classes, with AX hyperparameters tuning.
This is all my code implemented anticipated by a sample of my datasets ( I have 3 csv, train-test-val). Thank you very much !</p>
<pre><code>                                           0        1
M A T T D R P T P D G T D A I D L T T R V R R...    1
M K K L F Q T E P L L E L F N C N E L R I I G...    0
M L V A A A V C P H P P L L I P E L A A G A A...    1
M I V A W G N S G S G L L I L I L S L A V S A...    0
M V E E G R R L A A L H P N I V V K L P T T E...    1
M G S K V S K N A L V F N V L Q A L R E G L T...    1
M P S K E T S P A E R M A R D E Y Y M R L A M...    1
M V K E Y A L E W I D G Y R E R L V K V S D A...    1
M G T A A S Q D R A A M A E A A Q R V G D S F...    0
</code></pre>
<pre><code>
df_train=pd.read_csv('CLASSIFIER_train',sep=',',header=None)
df_train
class SequenceDataset(Dataset):

  def __init__(self, sequences, targets, tokenizer, max_len):
    self.sequences = sequences
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.sequences)
  
  def __getitem__(self, item):
    sequences = str(self.sequences[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      sequences,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )


    return {
      'sequences_text': sequences,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = SequenceDataset(
    sequences=df[0].to_numpy(),
    targets=df[1].to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=2,
    shuffle=True
  )

BATCH_SIZE = 16

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

def net_train(net, train_data_loader, parameters, dtype, device):
  net.to(dtype=dtype, device=device)

  # Define loss and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(net.parameters(), # or any optimizer you prefer 
                        lr=parameters.get(&quot;lr&quot;, 0.001), # 0.001 is used if no lr is specified
                        momentum=parameters.get(&quot;momentum&quot;, 0.9)
  )

  scheduler = optim.lr_scheduler.StepLR(
      optimizer,
      step_size=int(parameters.get(&quot;step_size&quot;, 30)),
      gamma=parameters.get(&quot;gamma&quot;, 1.0),  # default is no learning rate decay
  )

  num_epochs = parameters.get(&quot;num_epochs&quot;, 3) # Play around with epoch number
  # Train Network
  for _ in range(num_epochs):
      for inputs, labels in train_data_loader:
          # move data to proper dtype and device
          inputs = inputs.to(dtype=dtype, device=device)
          labels = labels.to(device=device)

          # zero the parameter gradients
          optimizer.zero_grad()

          # forward + backward + optimize
          outputs = net(inputs)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
          scheduler.step()
  return net
  
def init_net(parameterization):

    model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) 

    # The depth of unfreezing is also a hyperparameter
    for param in model.parameters():
        param.requires_grad = False # Freeze feature extractor
        
    Hs = 512 # Hidden layer size; you can optimize this as well
                                  
    model.fc = nn.Sequential(nn.Linear(2048, Hs), # attach trainable classifier
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(Hs, 10),
                                 nn.LogSoftmax(dim=1))
    return model # return untrained model

def train_evaluate(parameterization):

    # constructing a new training data loader allows us to tune the batch size


    train_data_loader=create_data_loader(df_train, tokenizer, MAX_LEN, batch_size=parameterization.get(&quot;batchsize&quot;, 32))
    
    
    # Get neural net
    untrained_net = init_net(parameterization) 
    
    # train
    trained_net = net_train(net=untrained_net, train_data_loader=train_data_loader, 
                            parameters=parameterization, dtype=dtype, device=device)
    
    # return the accuracy of the model as it was trained in this run
    return evaluate(
        net=trained_net,
        data_loader=test_data_loader,
        dtype=dtype,
        device=device,
    )

classes=('0','1')

dtype = torch.float
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

best_parameters, values, experiment, model = optimize(
    parameters=[
        {&quot;name&quot;: &quot;lr&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [1e-6, 0.4], &quot;log_scale&quot;: True},
        {&quot;name&quot;: &quot;batchsize&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [16, 128]},
        {&quot;name&quot;: &quot;momentum&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0]},
        #{&quot;name&quot;: &quot;max_epoch&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [1, 30]},
        #{&quot;name&quot;: &quot;stepsize&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [20, 40]},        
    ],
  
    evaluation_function=train_evaluate,
    objective_name='accuracy',
)

print(best_parameters)
means, covariances = values
print(means)
print(covariances)
</code></pre>
<pre><code>  File &quot;&lt;ipython-input-71-e52ebc0d7b5b&gt;&quot;, line 14, in train_evaluate
    parameters=parameterization, dtype=dtype, device=device)
  File &quot;&lt;ipython-input-61-66c57e7138fa&gt;&quot;, line 20, in net_train
    for inputs, labels in train_data_loader:
ValueError: too many values to unpack (expected 2)
</code></pre>
","pytorch, huggingface-transformers, bert-language-model, hyperparameters, dataloader","<p>your dataloader returns a dictionary therefore the way you loop and access it is wrong should be done as such:</p>
<pre class=""lang-py prettyprint-override""><code># Train Network
  for _ in range(num_epochs):
      # Your dataloader returns a dictionary
      # so access it as such
      for batch in train_data_loader:
          # move data to proper dtype and device
          labels = batch['targets'].to(device=device)
          atten_mask = batch['attention_mask'].to(device=device)
          input_ids = batch['input_ids'].to(device=device)

          # zero the parameter gradients
          optimizer.zero_grad()

          # forward + backward + optimize
          outputs = net(input_ids, attention_mask=atten_mask)
</code></pre>
",2,0,1483,2022-03-08 21:09:01,https://stackoverflow.com/questions/71401458/hugginface-dataloader-bert-valueerror-too-many-values-to-unpack-expected-2-a
How to fed last 4 concatenated hidden layers of BERT to FC layers,"<p>I am trying to do classification task and I got last 4 layers from BERT and concatenate them.</p>
<pre><code>out = model(...)
out=torch.cat([out['hidden_states'][-i] for i in range(1,5)],dim=-1)
</code></pre>
<p>Now the shape is <code>(12,200,768*4)</code> which is <code>batch,max_length,concatenation layer</code> but for fully connected layer we need to have two dimension. So one way is to average like <code>torch.mean((12,200,768*4),dim=1)</code> and get the output as <code>(12,768*4)</code>.
But i am confused what is the original BERT approach</p>
","python, pytorch, huggingface-transformers, bert-language-model","<p>There is no &quot;original&quot; BERT approach for classification with concatenated hidden layers. You have several options to proceed and I will just describe a comment on your approach and suggest an alternative in the following.</p>
<p>Preliminary:</p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn
from transformers import BertTokenizerFast, BertModel

t = BertTokenizerFast.from_pretrained(&quot;bert-base-cased&quot;)
m = BertModel.from_pretrained(&quot;bert-base-cased&quot;)
fc = nn.Linear(768, 5)

s = [&quot;This is a random sentence&quot;, &quot;This is another random sentence with more words&quot;]

i = t(s, padding=True,return_tensors=&quot;pt&quot;)

with torch.no_grad():
  o = m(**i, output_hidden_states=True)

print(i)
</code></pre>
<p>At first, you should look at your input:</p>
<pre class=""lang-py prettyprint-override""><code>#print(I)
{'input_ids': 
tensor([[ 101, 1188, 1110,  170, 7091, 5650,  102,    0,    0,    0],
        [ 101, 1188, 1110, 1330, 7091, 5650, 1114, 1167, 1734,  102]]), 
'token_type_ids': 
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': 
tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>What you should notice here, is that the shorter sentence gets padded. That is relevant because simply pooling the mean with <code>torch.mean</code>, will result in different sentence embeddings for the same sentence depending on the number of padding tokens. Of course, the model will learn to handle that to some extent after sufficient training, but you should, however, use a more <a href=""https://github.com/UKPLab/sentence-transformers/blob/b90970c190658f7588c86e8a3fb9cc400716ecd4/sentence_transformers/models/Pooling.py#L84"" rel=""nofollow noreferrer"">sophisticated mean function</a> that removes the padding tokens right away :</p>
<pre class=""lang-py prettyprint-override""><code>def mean_pooling(model_output, attention_mask):
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()
    return torch.sum(model_output * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


o_mean = [mean_pooling(o.hidden_states[-x],i.attention_mask) for x in range(1,5)]
#we want a tensor and not a list
o_mean = torch.stack(o_mean, dim=1)
#we want only one tensor per sequence
o_mean = torch.mean(o_mean,dim=1)

print(o_mean.shape)
with torch.no_grad():
  print(fc(o_mean))
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>torch.Size([2, 768])
tensor([[ 0.0677, -0.0261, -0.3602,  0.4221,  0.2251],
        [-0.0328, -0.0161, -0.5209,  0.5825,  0.2405]])
</code></pre>
<p>These operations are pretty expensive and people often use an approach called cls pooling as a cheaper alternative with comparable performance:</p>
<pre class=""lang-py prettyprint-override""><code>#We only use the cls token (i.e. first token of the sequence)
#id 101
o_cls = [o.hidden_states[-x][:, 0] for x in range(1,5)]
#we want a tensor and not a list
o_cls = torch.stack(o_cls, dim=1)
#we want only one tensor per sequence
o_cls = torch.mean(o_cls,dim=1)
print(o_cls.shape)
with torch.no_grad():
  print(fc(o_cls))
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>torch.Size([2, 768])
tensor([[-0.3731,  0.0473, -0.4472,  0.3804,  0.4057],
        [-0.3468,  0.0685, -0.5885,  0.4994,  0.4182]])
</code></pre>
",0,1,1452,2022-03-11 07:08:30,https://stackoverflow.com/questions/71434804/how-to-fed-last-4-concatenated-hidden-layers-of-bert-to-fc-layers
Training CamelBERT model for token classification,"<p>I am trying to use a huggingface model (<a href=""https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca"" rel=""nofollow noreferrer"">CamelBERT</a>) for token classification using <a href=""https://camel.abudhabi.nyu.edu/anercorp/"" rel=""nofollow noreferrer"">ANERCorp</a> Dataset. I fed the training set from <a href=""https://camel.abudhabi.nyu.edu/anercorp/"" rel=""nofollow noreferrer"">ANERCorp</a> to train the model, but I am getting the following error.</p>
<p>Error:</p>
<pre><code>Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/16/2022 07:31:01 - INFO - utils -   Creating features from dataset file at /content/drive/MyDrive/ANERcorp-CamelLabSplits
03/16/2022 07:31:01 - INFO - utils -   Writing example 0 of 3973
Traceback (most recent call last):
  File &quot;/content/CAMeLBERT/token-classification/run_token_classification.py&quot;, line 381, in &lt;module&gt;
    main()
  File &quot;/content/CAMeLBERT/token-classification/run_token_classification.py&quot;, line 226, in main
    if training_args.do_train
  File &quot;/content/CAMeLBERT/token-classification/utils.py&quot;, line 132, in __init__
    pad_token_label_id=self.pad_token_label_id,
  File &quot;/content/CAMeLBERT/token-classification/utils.py&quot;, line 210, in convert_examples_to_features
    label_ids.extend([label_map[label]] +
KeyError: 'B-LOC'
</code></pre>
<p>Please note: I am using Google Colab to train the model.
Code:</p>
<pre><code>DATA_DIR=&quot;/content/drive/MyDrive/ANERcorp-CamelLabSplits&quot;
MAX_LENGTH=512
BERT_MODEL=&quot;CAMeL-Lab/bert-base-arabic-camelbert-ca&quot;
OUTPUT_DIR=&quot;/content/Output&quot;
BATCH_SIZE=32
NUM_EPOCHS=3
SAVE_STEPS=750
SEED=12345

!python /content/CAMeLBERT/token-classification/run_token_classification.py \
--data_dir $DATA_DIR \
--task_type ner \
--labels $DATA_DIR/train.txt \
--model_name_or_path $BERT_MODEL \
--output_dir $OUTPUT_DIR \
--max_seq_length $MAX_LENGTH \
--num_train_epochs $NUM_EPOCHS \
--per_device_train_batch_size $BATCH_SIZE \
--save_steps $SAVE_STEPS \
--seed $SEED \
--overwrite_output_dir \
--overwrite_cache \
--do_train \
--do_predict
</code></pre>
","deep-learning, nlp, bert-language-model, named-entity-recognition","<p>The script you are using loads the labels from <code>$DATA_DIR/train.txt</code>.</p>
<p>See <a href=""https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L105"" rel=""nofollow noreferrer"">https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L105</a> for what the model expects.</p>
<p>It then tries to load the label list as first file file from the corpus (even before loading the training data), see <a href=""https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L183"" rel=""nofollow noreferrer"">https://github.com/CAMeL-Lab/CAMeLBERT/blob/master/token-classification/run_token_classification.py#L183</a> and put it into label_map.</p>
<p>But that fails for some reason. My assumption would be that it doensnt find anything and label_map is an empty dict, so the first attempt to get the labels from it fails with KeyError. Probably either your input data is not there or not in the path as expected (check if you have the right files and the right value for <code>$DATA_DIR</code>). From my experience relative paths in Google Drive can be tricky. Try something simple to see if it works, like <code>os.listdir($DATA_DIR)</code> to see if that is actually the directly you expect it to be.</p>
<p>If that is not the problem then probably something about the labels is actually wrong. Does ANERCorp use this exact way of writing labels (<code>B-LOC</code> etc.)? If it is different (e.g. <code>B-Location</code> or something) it would fail too.</p>
",0,0,276,2022-03-16 08:37:55,https://stackoverflow.com/questions/71493915/training-camelbert-model-for-token-classification
stacking LSTM layer on top of BERT encoder in Keras,"<p>I have been trying to stack a single LSTM layer on top of Bert embeddings, but whilst my model starts to train it fails on the last batch and throws the following error message:</p>
<pre><code>    Node: 'model/tf.reshape/Reshape'
Input to reshape is a tensor with 59136 values, but the requested shape has 98304
         [[{{node model/tf.reshape/Reshape}}]] [Op:__inference_train_function_70500]
</code></pre>
<p>This is how I build the model and I honestly cannot figure out what is going wrong here:</p>
<pre><code>batch_size = 128


bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True)

text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)  #shape=(None, 768)
bert_output = outputs['pooled_output']

l = tf.reshape(bert_output, [batch_size, 1, 768])

l = tf.keras.layers.LSTM(32, activation='relu')(l)


l = tf.keras.layers.Dropout(0.1, name='dropout')(l)
l = tf.keras.layers.Dense(8, activation='softmax', name=&quot;output&quot;)(l)
model = tf.keras.Model(inputs=[text_input], outputs = [l])

print(model.summary())
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=1, batch_size = batch_size)
</code></pre>
<p>this is the full output:</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 text (InputLayer)              [(None,)]            0           []                               
                                                                                                  
 keras_layer (KerasLayer)       {'input_mask': (Non  0           ['text[0][0]']                   
                                e, 128),                                                          
                                 'input_word_ids':                                                
                                (None, 128),                                                      
                                 'input_type_ids':                                                
                                (None, 128)}                                                      
                                                                                                  
 keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[0][0]',            
                                 (None, 128, 768),                'keras_layer[0][1]',            
                                 'default': (None,                'keras_layer[0][2]']            
                                768),                                                             
                                 'encoder_outputs':                                               
                                 [(None, 128, 768),                                               
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768)],                                               
                                 'pooled_output': (                                               
                                None, 768)}                                                       
                                                                                                  
 tf.reshape (TFOpLambda)        (128, 1, 768)        0           ['keras_layer_1[0][13]']         
                                                                                                  
 lstm (LSTM)                    (128, 32)            102528      ['tf.reshape[0][0]']             
                                                                                                  
 dropout (Dropout)              (128, 32)            0           ['lstm[0][0]']                   
                                                                                                  
 output (Dense)                 (128, 8)             264         ['dropout[0][0]']                
                                                                                                  
==================================================================================================
Total params: 109,585,033
Trainable params: 102,792
Non-trainable params: 109,482,241
__________________________________________________________________________________________________
None
    WARNING:tensorflow:AutoGraph could not transform &lt;function Model.make_train_function.&lt;locals&gt;.train_function at 0x7fc4ff809440&gt; and will run it as-is.
    Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
    Cause: 'arguments' object has no attribute 'posonlyargs'
    To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
    WARNING:tensorflow:AutoGraph could not transform &lt;function Model.make_train_function.&lt;locals&gt;.train_function at 0x7fc4ff809440&gt; and will run it as-is.
    Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
    Cause: 'arguments' object has no attribute 'posonlyargs'
    To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
    18/19 [===========================&gt;..] - ETA: 25s - loss: 1.5747 - accuracy: 0.5456Traceback (most recent call last):
      File &quot;bert-test-lstm.py&quot;, line 62, in &lt;module&gt;
        model.fit(x_train, y_train, epochs=1, batch_size = batch_size)
      File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
      File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow/python/eager/execute.py&quot;, line 55, in quick_execute
        inputs, attrs, num_outputs)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
Detected at node 'model/tf.reshape/Reshape' defined at (most recent call last):
    File &quot;bert-test-lstm.py&quot;, line 62, in &lt;module&gt;
      model.fit(x_train, y_train, epochs=1, batch_size = batch_size)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/training.py&quot;, line 1384, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/training.py&quot;, line 1021, in train_function
      return step_function(self, iterator)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/training.py&quot;, line 1010, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/training.py&quot;, line 1000, in run_step
      outputs = model.train_step(data)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/training.py&quot;, line 859, in train_step
      y_pred = self(x, training=True)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/functional.py&quot;, line 452, in call
      inputs, training=training, mask=mask)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/functional.py&quot;, line 589, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/layers/core/tf_op_layer.py&quot;, line 226, in _call_wrapper
      return self._call_wrapper(*args, **kwargs)
    File &quot;/Users/user/opt/anaconda3/envs/project/lib/python3.7/site-packages/keras/layers/core/tf_op_layer.py&quot;, line 261, in _call_wrapper
      result = self.function(*args, **kwargs)
Node: 'model/tf.reshape/Reshape'
Input to reshape is a tensor with 59136 values, but the requested shape has 98304
</code></pre>
<p>The code runs perfectly fine if I just drop the LSTM and reshape layers - any help is appreciated.</p>
","python, keras, tensorflow2.0, tf.keras, bert-language-model","<p>You should use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape"" rel=""nofollow noreferrer"">tf.keras.layers.Reshape</a> in order to reshape <code>bert_output</code> into a 3D tensor and automatically taking into account the batch dimension.</p>
<p>Simply changing:</p>
<pre><code>l = tf.reshape(bert_output, [batch_size, 1, 768])
</code></pre>
<p>into:</p>
<pre><code>l = tf.keras.layers.Reshape((1,768))(bert_output)
</code></pre>
<p>should work.</p>
",3,2,1059,2022-03-23 11:34:08,https://stackoverflow.com/questions/71586498/stacking-lstm-layer-on-top-of-bert-encoder-in-keras
BERT Vocabulary : Why every word has &#39;▁&#39; before?,"<p>my question is related to camemBERT model (french version of BERT) and its Tokenizer :</p>
<p>Why every word of the vocabulary has a &quot;▁&quot; character before ?
For example, it's not &quot;sirop&quot; but &quot;▁sirop&quot; (sirop =&gt; syrup).</p>
<pre><code>from transformers import CamembertTokenizer
tokenizer = Camembert.Tokenizer.from_pretrained(&quot;camembert-base&quot;)
voc = tokenizer.get_vocab() #Vocabulary of the model

print(&quot;sirop&quot; in voc) # Will display False
print(&quot;▁sirop&quot; in voc) # Will display True
</code></pre>
<p>Thank you for answering :)</p>
","nlp, bert-language-model, french","<p>If I understand it correctly the CamembertTokenizer uses this special character from SentencePiece, see the <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/camembert/tokenization_camembert.py#L42"" rel=""nofollow noreferrer"">source code</a>.</p>
<p>SentencePiece on the other hand uses Subword Tokens (splitting of words into smaller tokens), but to internally always keep track of what is a &quot;real&quot; split between words (where there was a whitespace) and what is a Subword splitting, they use this character before the start of each &quot;real&quot; token, follow-up subword tokens (but not punctuation) don't have this token, see the <a href=""https://github.com/google/sentencepiece#whitespace-is-treated-as-a-basic-symbol"" rel=""nofollow noreferrer"">Explaination in the Github Repository</a>. Basically the whitespace is always part of the tokenization, but to avoid problems it is internally escaped as &quot;▁&quot;.</p>
<p>They use this example: <code>&quot;Hello World.&quot;</code> becomes <code>[Hello] [▁Wor] [ld] [.]</code>, which can then be used by the model and later transformed back into the original string (<code>detokenized = ''.join(pieces).replace('▁', ' ')</code>) --&gt; <code>&quot;Hello World.&quot;</code> without ambiguity and without having to save the original string seperately.</p>
",1,0,650,2022-03-25 13:00:46,https://stackoverflow.com/questions/71617394/bert-vocabulary-why-every-word-has-before
Is it possible to freeze some params in a single layer of TFBertModel,"<p>I am trying to utilize the pretrained Bert model of tensorflow which has approx 110 million params and it is near impossible to train these params using my gpu. And freezing the entire layer makes all these params untrainable.</p>
<p>Is it possible to make the layer partially trainable? Like have a couple million params trainable and the rest untrainable?</p>
<pre><code>input_ids_layer = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')

input_attention_layer = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')

model = TFAutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

for layer in model.layers:

    for i in range(len(layer.weights)):
//assuming there are 199 weights
        if i&gt;150:
            layer.weights[i]._trainable = True
        else:
            layer.weights[i]._trainable = False
</code></pre>
","machine-learning, nlp, bert-language-model, transfer-learning","<p>I don't know about training some weights inside a layers, but I still suggest you to do the &quot;standard way&quot;: freezing the layers is what is usually done in these cases to avoid retraining everything. However, <strong>you must not freeze all the layers</strong>, since it would be useless. What you want to do is to freeze <strong>everything except the last few layers</strong>, and then train the neural network.</p>
<p>This works since the first layers usually learn very <em>abstract</em> features, and therefore are <em>transferrable</em> across many problems. On the other hand, the last layers usually learn the features that really solves the task at hand, based on the current dataset.</p>
<p>Therefore, if you want to re-train a pretrained model in another dataset, you just need to retrain the last few layers. You can also edit the last layers of the neural network by adding some Dense layers and changing the output of the last layer, which is useful if for example the number of classes to predict is different w.r.t the original dataset. There are a lot of short and easy tutorials that you can follow online to do that.</p>
<p>To summarize:</p>
<ol>
<li>Freeze all the layers <em>expect the last one</em></li>
<li>(optional) Create new layers and link them with the output of the second-last layer</li>
<li>Train the network</li>
</ol>
",0,0,735,2022-03-25 23:36:35,https://stackoverflow.com/questions/71624150/is-it-possible-to-freeze-some-params-in-a-single-layer-of-tfbertmodel
what is so special about special tokens?,"<p>what exactly is the difference between &quot;token&quot; and a &quot;special token&quot;?</p>
<p>I understand the following:</p>
<ul>
<li>what is a typical token</li>
<li>what is a typical special token: MASK, UNK, SEP, etc</li>
<li>when do you add a token (when you want to expand your vocab)</li>
</ul>
<p>What I don't understand is, under what kind of capacity will you want to create a new special token, any examples what we need it for and when we want to create a special token other than those default special tokens? If an example uses a special token, why can't a normal token achieve the same objective?</p>
<pre><code>tokenizer.add_tokens(['[EOT]'], special_tokens=True)
</code></pre>
<p>And I also dont quite understand the following description in the source documentation.
what difference does it do to our model if we set add_special_tokens to False?</p>
<pre><code>add_special_tokens (bool, optional, defaults to True) — Whether or not to encode the sequences with the special tokens relative to their model.
</code></pre>
","nlp, tokenize, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>Special tokens are called special because they are not derived from your input. They are added for a certain purpose and are independent of the specific input.</p>
<blockquote>
<p>What I don't understand is, under what kind of capacity will you want
to create a new special token, any examples what we need it for and
when we want to create a special token other than those default
special tokens?</p>
</blockquote>
<p>Just an example, in extractive conversational question-answering it is not unusual to add the question and answer of the previous dialog-turn to your input to provide some context for your model. Those previous dialog turns are separated with special tokens from the current question. Sometimes people use the separator token of the model or introduce new special tokens. The following is an example with a new special token <code>[Q]</code></p>
<pre class=""lang-py prettyprint-override""><code>#first dialog turn - no conversation history
[CLS] current question [SEP] text [EOS]
#second dialog turn - with previous question to have some context
[CLS] previous question [Q] current question [SEP] text [EOS]
</code></pre>
<blockquote>
<p>And I also dont quite understand the following description in the
source documentation. what difference does it do to our model if we
set add_special_tokens to False?</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer
t = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)

t(&quot;this is an example&quot;)
#{'input_ids': [0, 9226, 16, 41, 1246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}

t(&quot;this is an example&quot;, add_special_tokens=False)
#{'input_ids': [9226, 16, 41, 1246], 'attention_mask': [1, 1, 1, 1]}
</code></pre>
<p>As you can see here, the input misses two tokens (the special tokens). Those special tokens have a meaning for your model since it was trained with it. The last_hidden_state will be different due to the lack of those two tokens and will therefore lead to a different result for your downstream task.</p>
<p>Some tasks, like sequence classification, often use the [CLS] token to make their predictions. When you remove them, a model that was pre-trained with a [CLS] token will struggle.</p>
",12,13,8514,2022-03-30 14:58:06,https://stackoverflow.com/questions/71679626/what-is-so-special-about-special-tokens
How to deploy a Question Answering BERT Model as a chat bot on MS Teams,"<p>I have a Text2SQL model (EditSQL: <a href=""https://github.com/ryanzhumich/editsql"" rel=""nofollow noreferrer"">https://github.com/ryanzhumich/editsql</a>) which I have configured to take a sentence as input and return a SQL query as output.</p>
<p>Now, I want to deploy this program as a chat bot application in Microsoft Teams.</p>
<p>I understand there's Microsoft bot framework that enables publishing a bot and the 3 options are described here.
<a href=""https://learn.microsoft.com/en-us/learn/modules/choose-bot-building-tool/"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/learn/modules/choose-bot-building-tool/</a></p>
<p>However, I am not finding any of them suitable for my use case since I need to deploy a Question-Answering Bot where the Questions from users need to be sent to an external server like AWS and the response from AWS (could be an excel file) needs to be sent back to the user. Multiple questions can be the part of a conversation, so the chat client should be able to mark start and end of a conversation.</p>
<p>My problem:</p>
<ul>
<li>What are the basic steps of exposing a ml model via a server so that it can be queried in production.</li>
<li>What are the tools that will allow me to make a client on Teams and a server for this model on AWS?</li>
</ul>
<p>Please let me know if I should add more information on this.</p>
<p>Thanks</p>
","microsoft-teams, chatbot, bert-language-model","<p>As you've seen, there are a bunch of tools/approaches to creating bots in the Microsoft world, for Teams or otherwise. Underneath, these all use the Bot Framework, but you can develop directly (i.e. write code), or use a higher-level tool like Bot Framework Composer - the choice is yours depending on your own internal skills. If you want to work with code directly, here are a bunch of bot samples, in multiple languages: <a href=""https://github.com/microsoft/BotBuilder-Samples/tree/main/samples"" rel=""nofollow noreferrer"">https://github.com/microsoft/BotBuilder-Samples/tree/main/samples</a> . For isntance, here is an example of integrating the Microsoft QnAMaker service into your bot: <a href=""https://github.com/microsoft/BotBuilder-Samples/tree/main/samples/python/49.qnamaker-all-features"" rel=""nofollow noreferrer"">https://github.com/microsoft/BotBuilder-Samples/tree/main/samples/python/49.qnamaker-all-features</a></p>
<p>Basically, if you go the development approach, your bot is just a web service. Once it receives the message, it can call out to any other service behind the scenes. That means it can receive a message, call out to an AWS service, receive the response, and send a reply to the user.</p>
<p>For multiple questions as part of a 'set' of chats, Bot Framework provides an idea called &quot;dialogs&quot; that should work for you.</p>
",1,1,548,2022-03-30 15:45:29,https://stackoverflow.com/questions/71680285/how-to-deploy-a-question-answering-bert-model-as-a-chat-bot-on-ms-teams
Bert prediction shape not equal to num_samples,"<p>I have a text classification that I am trying to do using BERT. Below is the code I am using. The model training code(below) works fine but I am facing issue with the prediction part</p>
<pre><code>from transformers import TFBertForSequenceClassification
import tensorflow as tf

# recommended learning rate for Adam 5e-5, 3e-5, 2e-5
learning_rate = 5e-5
nlabels = 26

# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 1


# model initialization
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=nlabels,
                                                      output_attentions=False,
                                                      output_hidden_states=False)

# optimizer Adam
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)

# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

bert_history = model.fit(ds_tr_encoded, epochs=number_of_epochs)
</code></pre>
<p>I am getting the output using the following</p>
<pre><code>preds = model.predict(ds_te_encoded)
pred_labels_idx = np.argmax(preds['logits'], axis=1)
</code></pre>
<p>The issue I am facing is that the shape of <code>pred_labels_idx</code> is not the same as <code>ds_te_encoded</code></p>
<pre><code>len(pred_labels_idx) #426820
tf.data.experimental.cardinality(ds_te_encoded) #&lt;tf.Tensor: shape=(), dtype=int64, numpy=21341&gt;
</code></pre>
<p>Not sure why this is happening.</p>
","python, tensorflow, tensorflow-datasets, bert-language-model, transformer-model","<p>Since <code>ds_te_encoded</code> is of type <code>tf.data.Dataset</code> and you call <code>cardinality(...)</code>, the cardinality in your case is simply the rounded number of <strong>batches</strong> and <em>not</em> the number of samples. So I am assuming you are using a batch size of 20, because <code>426820/20 = 21341</code>. That is probably what is causing the confusion.</p>
",1,1,208,2022-04-01 10:06:33,https://stackoverflow.com/questions/71705218/bert-prediction-shape-not-equal-to-num-samples
Creating word embedings from bert and feeding them to random forest for classification,"<p>I have used bert base pretrained model with 512 dimensions to generate contextual features. Feeding those vectors to random forest classifier is providing 83 percent accuracy but in various researches i have seen that bert minimal gives 90 percent.
I have some other features too like word2vec, lexicon, TFIDF and punctuation features.
Even when i merged all the features i got 83 percent accuracy. The research paper which i am using as base paper mentioned an accuracy score of 92 percent but they have used an ensemble based approach in which they classified through bert and trained random forest on weights.
But i was willing to do some innovation thus didn't followed that approach.
My dataset is biased to positive reviews so according to me the accuracy is less as model is also biased for positive labels but still I am looking for an expert advise</p>
<p>Code implementation of bert</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Bert_Features.ipynb</code></p>
<p>Random forest on all features independently</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/RandomForestClassifier.ipynb</code></p>
<p>Random forest on all features jointly</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Merging_Feature.ipynb</code></p>
","machine-learning, nlp, data-science, classification, bert-language-model","<p>Regarding the &quot;no improvements despite adding more features&quot; - some researchers believe that the BERT word embeddings already contain all the available information presented in text, so then it doesn't matter how fancy a classification head you add to it, doesn't matter if it is a linear model that uses the embeddings, or a complicated ML algorithm with a number of other features, they will not provide significant improvements in many tasks. They argue, that since BERT is a context-aware, bidirectional language model - that is trained extensively on MLM and NSP tasks, it already grasps most of the things that additional features for punctuation, word2vec and tfidf could convey. The lexicon could probably help a little in the sentiment task, if it is relevant, but the one or two extra variables, that you likely use to represent it, probably get drowned in all the other features.</p>
<p>Other than that, the accuracy of BERT-based models depends on the dataset used, sometimes the data is simply too diverse to obtain a perfect score, e.g. if there are some instances of observations that are very similar, but with different class labels etc. You can see in the BERT papers, that the accuracy widely depends on the task, e.g. in some tasks it is indeed 90+%, but for some tasks, e.g. Masked Language Modeling, where the model needs to choose a particular word from a vocab of over 30K words, the accuracy of 20% could be impressive in some cases. So in order to obtain a reliable comparison with bert papers, you'd need to pick a dataset that they've used and then compare.</p>
<p>Regarding the dataset balance, for deep learning models in general, the rule of thumb is that the training set should be more or less balanced w.r.t. the fraction of data covered by each class label. So if you have 2 labels, should be ~50-50, if 5 labels, then each should be at around 20% of training dataset, etc.
That is because most NN's work in batches, where they update the model weights based on the feedback from each batch. So if you have too many values of one class, the batch updates will be dominated by that one class, effectively worsening the quality of your training.</p>
<p>So, if you want to improve the accuracy of your model, balancing the dataset could be an easy fix. And if you have e.g. 5 ordered classes with differing sizes, you may consider merging some of them (e.g. reviews from 1-2 as bad, 3 as neutral, 4-5 as good) and then rebalancing, if still necessary.</p>
<p>(Unless it's a situation where e.g. 1 class has 80% of data, and 4 classes share the remaining 20%. In such a case you should probably consider some more advanced options, such as partitioning the algo to two parts, one predicting whether or not an instance is in class 1 (so a binary classifier), the other to distinguish between the 4 underrepresented classes. )</p>
",1,0,3689,2022-04-01 16:34:09,https://stackoverflow.com/questions/71710186/creating-word-embedings-from-bert-and-feeding-them-to-random-forest-for-classifi
RuntimeError: Boolean value of Tensor with more than one value is ambiguous in python,"<p>I'm facing the following error, and I don't know why:
This code is on GitHub, I ran it correctly on Collab, but it gives me the following error here:</p>
<pre><code>device=&quot;cpu&quot;
lr=3e-5#1e-3
num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)

model=Bert_Classification_Model().to(device)
optimizer=AdamW(model.parameters(), lr=lr)
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                        num_warmup_steps = 0,
                                        num_training_steps = num_training_steps)
val_losses=[]
batches_losses=[]
val_acc=[]
for epoch in range(EPOCH):
    t0 = time.time()    
    print(f&quot;\n=============== EPOCH {epoch+1} / {EPOCH} ===============\n&quot;)
    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)
    epoch_loss=np.mean(batches_losses_tmp)
    print(f&quot;\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\n&quot;)
    t1=time.time()
    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)
    print(f&quot;==&gt; evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\n&quot;)
    tmp_evaluate=evaluate(target.reshape(-1), output)
    print(f&quot;=====&gt;\t{tmp_evaluate}&quot;)
    val_acc.append(tmp_evaluate['accuracy'])
    val_losses.append(val_losses_tmp)
    batches_losses.append(batches_losses_tmp)
    print(&quot;\t§§ model has been saved §§&quot;)
    torch.save(model, f&quot;model1/model_epoch{epoch+1}.pt&quot;)    




Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).


=============== EPOCH 1 / 3 ===============

---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-33-aa98faac385e&gt; in &lt;module&gt;()
     14     t0 = time.time()
     15     print(f&quot;\n=============== EPOCH {epoch+1} / {EPOCH} ===============\n&quot;)
---&gt; 16     batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)
     17     epoch_loss=np.mean(batches_losses_tmp)
     18     print(f&quot;\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\n&quot;)

6 frames

/content/RoBERT_Recurrence_over_BERT/Custom_Dataset_Class.py in long_terms_tokenizer(self, data_tokenize, targets)
    158         targets_list.append(targets)
    159 
--&gt; 160         if remain and self.approach != 'head':
    161             remain = torch.tensor(remain, dtype=torch.long)
    162             idxs = range(len(remain)+self.chunk_len)

RuntimeError: Boolean value of Tensor with more than one value is ambiguous
</code></pre>
<p>This is the file link:
<a href=""https://github.com/helmy-elrais/RoBERT_Recurrence_over_BERT/blob/master/train.ipynb"" rel=""nofollow noreferrer"">https://github.com/helmy-elrais/RoBERT_Recurrence_over_BERT/blob/master/train.ipynb</a></p>
","python, pytorch, bert-language-model","<p>Your tensor <code>remain</code> (in your <code>Dataset</code> class) is a boolean tensor and not a boolean variable. Therefore, the condition <code>if remain</code> is not well-defined.</p>
",1,0,3694,2022-04-02 18:14:03,https://stackoverflow.com/questions/71720038/runtimeerror-boolean-value-of-tensor-with-more-than-one-value-is-ambiguous-in-p
How do I load a fine-tuned AllenNLP BERT-SRL model using BertPreTrainedModel.from_pretrained()?,"<p>I have fine-tuned a BERT model for semantic role labeling, using AllenNLP. This produces a model directory (serialization directory, if I recall?) that contains the following:</p>
<pre><code>best.th
config.json
meta.json
metrics_epoch_0.json
metrics_epoch_10.json
metrics_epoch_11.json
metrics_epoch_12.json
metrics_epoch_13.json
metrics_epoch_14.json
metrics_epoch_1.json
metrics_epoch_2.json
metrics_epoch_3.json
metrics_epoch_4.json
metrics_epoch_5.json
metrics_epoch_6.json
metrics_epoch_7.json
metrics_epoch_8.json
metrics_epoch_9.json
metrics.json
model_state_e14_b0.th
model_state_e15_b0.th
model.tar.gz
out.log
training_state_e14_b0.th
training_state_e15_b0.th
vocabulary
</code></pre>
<p>Where <code>vocabulary</code> is a folder with <code>labels.txt</code> and <code>non_padded_namespaces.txt</code>.</p>
<p>I'd now like to use this fine-tuned model BERT model as the initialization when learning a related task, event extraction, using this library: <a href=""https://github.com/wilsonlau-uw/BERT-EE"" rel=""nofollow noreferrer"">https://github.com/wilsonlau-uw/BERT-EE</a> (ie I want to exploit some transfer learning). The <code>config.ini</code> file has a line for <code>fine_tuned_path</code>, where I can specify an already-fine-tuned model that I want to use here. I provided the path to the AllenNLP serialization directory, and I got the following error:</p>
<pre><code>2022-04-05 13:07:28,112 -  INFO - setting seed 23
2022-04-05 13:07:28,113 -  INFO - loading fine tuned model in /data/projects/SRL/ser_pure_clinical_bert-large_thyme_and_ontonotes/
Traceback (most recent call last):
  File &quot;main.py&quot;, line 65, in &lt;module&gt;
    model = BERT_EE()
  File &quot;/data/projects/SRL/BERT-EE/model.py&quot;, line 88, in __init__
    self.__build(self.use_fine_tuned)
  File &quot;/data/projects/SRL/BERT-EE/model.py&quot;, line 118, in __build
    self.__get_pretrained(self.fine_tuned_path)
  File &quot;/data/projects/SRL/BERT-EE/model.py&quot;, line 110, in __get_pretrained
    self.__model = BERT_EE_model.from_pretrained(path)
  File &quot;/home/richier/anaconda3/envs/allennlp/lib/python3.7/site-packages/transformers/modeling_utils.py&quot;, line 1109, in from_pretrained
    f&quot;Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in &quot;
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory /data/projects/SRL/ser_pure_clinical_bert-large_thyme_and_ontonotes/ or `from_tf` and `from_flax` set to False.
</code></pre>
<p>Of course, the serialization directory doesn't have any of those files, hence the error. I tried unzipping <code>model.tar.gz</code> but it only has:</p>
<pre><code>config.json
weights.th
vocabulary/
vocabulary/.lock
vocabulary/labels.txt
vocabulary/non_padded_namespaces.txt
meta.json
</code></pre>
<p>Digging into the codebase of the GitHub repo I linked above, I can see that <code>BERT_EE_model</code> inherits from <code>BertPreTrainedModel</code> from the transformers library, so the trick would seem to be getting the AllenNLP model into a format that <code>BertPreTrainedModel.from_pretrained()</code> can load...?</p>
<p>Any help would be greatly appreciated!</p>
","python, pytorch, huggingface-transformers, bert-language-model, allennlp","<p>I believe I have figured this out. Basically, I had to re-load my model archive, access the underlying model and tokenizer, and then save those:</p>
<pre><code>from allennlp.models.archival import load_archive
from allennlp_models.structured_prediction import SemanticRoleLabeler, srl, srl_bert

archive = load_archive('ser_pure_clinical_bert-large_thyme_and_ontonotes/model.tar.gz')

bert_model = archive.model.bert_model #type is transformers.models.bert.modeling_bert.BertModel
bert_model.save_pretrained('ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained/')

bert_tokenizer = archive.dataset_reader.bert_tokenizer
bert_tokenizer.save_pretrained('ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained/')
</code></pre>
<p>(This last part is probably less interesting to most folks, but also, in the config.ini I mentioned, the directory 'ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained' needed to be passed to the line <code>pretrained_model_name_or_path</code> not to <code>fine_tuned_path</code>.)</p>
",0,0,663,2022-04-05 17:21:21,https://stackoverflow.com/questions/71755917/how-do-i-load-a-fine-tuned-allennlp-bert-srl-model-using-bertpretrainedmodel-fro
open_model_zoo demos stuck at reading model,"<p>I execute some open_model_zoo demos, it works successfully when I choose the CPU device.
But when I change the device to MYRIAD or GPU, it will stuck and do nothing.
(I've used hello_query_device.py to checked, my PC can detected the neural compute stick 2 )</p>
<p>Version : openvino_2022.1.0.643 at Windows10</p>
<p><a href=""https://i.sstatic.net/6BXvM.png"" rel=""nofollow noreferrer"">picture of error message</a></p>
","python, bert-language-model, openvino","<p>The bert-large-uncased-whole-word-masking-squad-0001 model is supported on CPU and GPU, while wav2vec2-base model is supported on CPU only.</p>
<p>Refer to <a href=""https://docs.openvino.ai/2022.1/omz_models_intel_device_support.html#"" rel=""nofollow noreferrer"">Intel’s Pre-Trained Models Device Support</a> and P<a href=""https://docs.openvino.ai/2022.1/omz_models_public_device_support.html"" rel=""nofollow noreferrer"">ublic Pre-Trained Models Device Support</a> for Open Model Zoo models' compatibility with CPU, GPU and MYRIAD devices.</p>
",2,-1,153,2022-04-14 01:25:36,https://stackoverflow.com/questions/71865451/open-model-zoo-demos-stuck-at-reading-model
"BertTokenizer error ValueError: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers","<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split

# read text data
df = pd.read_csv('E:/bert4keras-master/resume_data/111.txt', header=None,encoding='utf-8', sep='\t',names=['label', 'sentence'])

print(df)


# split text data
train, valid_test = train_test_split(df, test_size=0.3, shuffle=True, random_state=123, stratify=df['label'])
print(valid_test.head)
valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['label'])
train.reset_index(drop=True, inplace=True)
valid.reset_index(drop=True, inplace=True)
test.reset_index(drop=True, inplace=True)


class CreateDataset(Dataset):
  def __init__(self, X, y, tokenizer, max_len):
    self.X = X
    self.y = y
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):  # len(Dataset)
    return len(self.y)

  def __getitem__(self, index):  # Dataset[index]
    text = self.X[index]
    inputs = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=self.max_len,
      pad_to_max_length=True
    )
    ids = inputs['input_ids']
    mask = inputs['attention_mask']

    return {
      'ids': torch.LongTensor(ids),
      'mask': torch.LongTensor(mask),
      'labels': torch.Tensor(self.y[index])
    }




    # label one-hot
y_train = pd.get_dummies(train, columns=['label'])[['label_Exp','label_PI','label_Sum','label_Edu', 'label_QC', 'label_Skill', 'label_Obj']].values
y_valid = pd.get_dummies(valid, columns=['label'])[['label_Exp','label_PI','label_Sum','label_Edu', 'label_QC', 'label_Skill', 'label_Obj']].values
y_test = pd.get_dummies(test, columns=['label'])[['label_Exp','label_PI','label_Sum','label_Edu', 'label_QC', 'label_Skill', 'label_Obj']].values

# make dataset 


max_len = 256

tokenizer = BertTokenizer.from_pretrained('E:/bert4keras-master/pytorch_bert_large/')
dataset_train = CreateDataset(train['sentence'], y_train, tokenizer, max_len)
dataset_valid = CreateDataset(valid['sentence'], y_valid, tokenizer, max_len)
dataset_test = CreateDataset(test['sentence'], y_test, tokenizer, max_len)

      # dataloader
      dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)
      dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)

</code></pre>
<p>and I get this error when I try to train model <code>log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)</code></p>
<p>ERROR</p>
<pre><code>&gt;&gt;&gt; log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;&lt;stdin&gt;&quot;, line 29, in train_model
  File &quot;&lt;stdin&gt;&quot;, line 8, in calculate_loss_and_accuracy
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 521, in __next__
    data = self._next_data()
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;&lt;stdin&gt;&quot;, line 11, in __getitem__
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\transformers\tokenization_utils_base.py&quot;, line 2556, in encode_plus
    return self._encode_plus(
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\transformers\tokenization_utils.py&quot;, line 647, in _encode_plus
    first_ids = get_input_ids(text)
  File &quot;E:\anaconda3\envs\py38pytorch\lib\site-packages\transformers\tokenization_utils.py&quot;, line 634, in get_input_ids
    raise ValueError(
ValueError: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
</code></pre>
<p>any suggestions please Thank you very much</p>
","python, bert-language-model, huggingface-tokenizers","<p>yoy should debug in
inputs = self.tokenizer.encode_plus(
text,
add_special_tokens=True,
max_length=self.max_len,
pad_to_max_length=True
)
'text' should string or list string...</p>
",-1,0,875,2022-04-17 14:38:06,https://stackoverflow.com/questions/71902793/berttokenizer-error-valueerror-input-nan-is-not-valid-should-be-a-string-a-li
How to get the sentence embeddings with DeBERTa.deberta.pooling?,"<p>How to get the sentence embeddings with DeBERTa.deberta.pooling?</p>
<p>Hi everyone, I applied a DeBERTa model to analyze sentences, this is what my code looks like:</p>
<pre><code>from transformers import DebertaTokenizer, DebertaModel
import torch
# downloading the models
tokenizer = DebertaTokenizer.from_pretrained(&quot;microsoft/deberta-base&quot;)
model = DebertaModel.from_pretrained(&quot;microsoft/deberta-base&quot;)
# tokenizing the input text and converting it into pytorch tensors
inputs = tokenizer([&quot;The cat cought the mouse&quot;, &quot;This is the second sentence&quot;], return_tensors=&quot;pt&quot;, padding=True)
# pass through the model 
outputs = model(**inputs)
</code></pre>
<p>I realize that one option to get the sentence embeddings is to look at the CLS hidden state using</p>
<pre><code>outputs.last_hidden_state[:,0,:]
</code></pre>
<p>However, I would prefer to get the pooled output. As I learned, <code>pooled_output</code> is not supported, but there seems to be an implementation in DeBERTa named DeBERTa.deberta.pooling (see <a href=""https://deberta.readthedocs.io/en/latest/_modules/DeBERTa/deberta/pooling.html"" rel=""nofollow noreferrer"">https://deberta.readthedocs.io/en/latest/_modules/DeBERTa/deberta/pooling.html</a>). Does anyone know how to use it?</p>
","python, nlp, spyder, bert-language-model","<p>First, you need to import pooler for Deberta, and then it is better to create a separate class to make it more convenient to work.</p>
<pre><code>from transformers.models.deberta.modeling_deberta import ContextPooler
from transformers.models.deberta.modeling_deberta import StableDropout
from transformers import DebertaTokenizer, DebertaModel
import torch

tokenizer = DebertaTokenizer.from_pretrained(&quot;microsoft/deberta-base&quot;)
model = DebertaModel.from_pretrained(&quot;microsoft/deberta-base&quot;)

class CustomModel(nn.Module): # deberta
    def __init__(self, backbone):
        super(CustomModel, self).__init__()
        self.model = backbone
        self.config = self.model.config
        self.pooler = ContextPooler(self.config) 
        # output_dim = self.pooler.output_dim
        # self.classifier = nn.Linear(output_dim, num_classes)
        # drop_out = getattr(self.config, &quot;cls_dropout&quot;, None)
        # drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out
        # self.dropout = StableDropout(drop_out)
   def forward('.....'):
        encoder_layer = outputs[0]
        pooled_output = self.pooler(encoder_layer)#, flat_attention_mask)
        # x = '......'
        return x # pooled_output
model = CustomModel(model)
</code></pre>
<p>The details depend on your task. A more precise implementation can be found in the model repository. I hope it helps.
<a href=""https://github.com/huggingface/transformers/blob/v4.32.0/src/transformers/models/deberta/modeling_deberta.py#L66"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.32.0/src/transformers/models/deberta/modeling_deberta.py#L66</a></p>
",2,2,1392,2022-04-18 09:25:30,https://stackoverflow.com/questions/71909945/how-to-get-the-sentence-embeddings-with-deberta-deberta-pooling
"Pre-trained BERT not the right shape for LSTM layer: Value Error, total size of new array must be unchanged","<p>I am attempting to use a pre-trained BERT model on a Siamese neural network. However, I am having issues passing the BERT model to the shared LSTM layer.  I encounter the below error:</p>
<pre><code>ValueError: Exception encountered when calling layer &quot;reshape_4&quot; (type Reshape).

total size of new array must be unchanged, input_shape = [768], output_shape = [64, 768, 1]

Call arguments received:
  • inputs=tf.Tensor(shape=(None, 768), dtype=float32)

</code></pre>
<p>I read in several other posts that the dimensions I feed into the LSTM should be <code>[batch_size, 768, 1]</code>. However, when I attempt to reshape, I run into the error. How can I resolve this error?</p>
<pre><code>input_1 = Input(shape=(), dtype=tf.string, name='text')
preprocessed_text_1 = bert_preprocess(input_1)
outputs_1 = bert_encoder(preprocessed_text_1)
e1 = tf.keras.layers.Reshape((64, 768, 1))(outputs_1['pooled_output'])

input_2 = Input(shape=(), dtype=tf.string, name='text')
preprocessed_text_2 = bert_preprocess(input_2)
outputs_2 = bert_encoder(preprocessed_text_2)
e2 = Reshape((64, 768, 1))(outputs_2['pooled_output'])

lstm_layer = Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2)) # Won't work on GPU

x1 = lstm_layer(e1)
x2 = lstm_layer(e2)

mhd = lambda x: exponent_neg_cosine_distance(x[0], x[1]) 
merged = Lambda(function=mhd, output_shape=lambda x: x[0], name='cosine_distance')([x1, x2])
preds = Dense(1, activation='sigmoid')(merged)
model = Model(inputs=[input_1, input_2], outputs=preds)
</code></pre>
","python, tensorflow, keras, bert-language-model","<p>You have to remove the batch size (=64) from the Reshape layers.</p>
",0,0,237,2022-04-19 22:00:13,https://stackoverflow.com/questions/71931827/pre-trained-bert-not-the-right-shape-for-lstm-layer-value-error-total-size-of
Scikit Learn fit(): Setting an array element with a sequence fit,"<p>I am trying to call scikit learn fit functions on dataframes where the elements of each column are numpy arrays. However, I get the error &quot;setting an array element with a sequence,&quot; presumably because I am trying to call fit on a dataframe of arrays rather than scalar values. How do I work around this? I'd really appreciate some help.</p>
<p>Here is my code. You can find the data I'm using here: <a href=""https://competitions.codalab.org/competitions/21163"" rel=""nofollow noreferrer"">https://competitions.codalab.org/competitions/21163</a></p>
<pre><code>training_data = pd.read_csv('/train.tsv', sep='\t')
testing_data = pd.read_csv('/dev.tsv', sep='\t')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True,max_length=1024)
model = BertModel.from_pretrained('bert-base-uncased')
model = model.to(device)
</code></pre>
<pre><code># These are used to map the data to their appropriate column on each pass
pomt_train_x = pd.DataFrame(columns=[&quot;claim&quot;, &quot;reason&quot;, &quot;category&quot;, &quot;speaker&quot;, &quot;checker&quot;, &quot;tags&quot;, &quot;claim entities&quot;, &quot;article title&quot;])
feature_dict = {1: &quot;claim&quot;, 4: &quot;reason&quot;, 5: &quot;category&quot;, 6: &quot;speaker&quot;, 7: &quot;checker&quot;, 8: &quot;tags&quot;, 9: &quot;claim entities&quot;, 10: &quot;article title&quot;}

# Sort the data appropriately.
for i, data in enumerate(training_data[training_data.columns].to_numpy()):
    if 'pomt' in data[0]:
        appended_data = {}
        for j, sentence in enumerate(data):
            if j in feature_dict:
                inputs = tokenizer(str(sentence), return_tensors=&quot;pt&quot;, max_length=512, pad_to_max_length=True).to(device)
                outputs = model(**inputs)
                appended_data[feature_dict[j]] = outputs.last_hidden_state[:,0][0].cpu().detach().numpy()
        pomt_train_x = pomt_train_x.append(appended_data, ignore_index=True)
        print(f&quot;{i + 1} out of {training_data.index.stop} from training&quot;)

count = 0
# append testing data to training data
for i, data in enumerate(testing_data[testing_data.columns].to_numpy()):
    if 'pomt' in data[0]:
        appended_data = {}
        for j, sentence in enumerate(data):
            if j in feature_dict:
                inputs = tokenizer(str(sentence), return_tensors=&quot;pt&quot;, max_length=512, pad_to_max_length=True).to(device)
                outputs = model(**inputs)
                appended_data[feature_dict[j]] = outputs.last_hidden_state[:,0][0].cpu().detach().numpy()
        pomt_train_x = pomt_train_x.append(appended_data, ignore_index=True)
        print(f&quot;{i + 1} out of {testing_data.index.stop} from testing&quot;)
        count += 1
</code></pre>
<pre><code># Map the possible labels to an emotion
positive_set = set(['half-true', 'correct attribution!', 'correct', 'determination: barely true', 'factscan score: true',
                'correct attribution', 'mostly true', 'mostly-correct', 'truth!', 'partially true', 'half true',
                'mostly truth!', 'determination: true', 'true messages', 'authorship confirmed!', 'verdict: true',
                'mostly_true', 'determination: mostly true', 'confirmed authorship!', 'conclusion: accurate', 'accurate',
                'true', 'partly true', 'fact', 'full flop', 'in-the-green', 'verified'])
negative_set = set({'fake news', 'verdict: false', '3 pinnochios', 'fiction!', 'bogus warning', 'we rate this claim false',
                'determination: false', 'disputed!', 'false', 'fiction', 'a lot of baloney', '2 pinnochios', 'some baloney',
                'mostly_false', 'cherry picks', 'miscaptioned', 'misleading!', 'misleading recommendations', 'mostly fiction!',
                'mostly false', 'a little baloney', 'fiction! &amp; satire!', 'conclusion: false', 'rating: false',
                'determination: misleading', 'promise broken', '4 pinnochios', 'misleading', 'promise kept',
                'misattributed', 'fake', 'previously truth! now resolved!','incorrect attribution!', 'incorrect',
                'spins the facts', 'determination: a stretch', 'factscan score: misleading', 'pants on fire!',
                'factscan score: false', 'exaggerates', 'outdated', 'facebook scams', 'unsupported', 'opinion!',
                'verdict: unsubstantiated', 'scam', 'virus!', 'no flip', 'scam!', 'unverified', 'distorts the facts', 'outdated!'
                'understated', 'no evidence', 'unproven!', 'inaccurate attribution!', 'statirical reports', 'unproven', 'exaggerated', 
                'determination: huckster propaganda', 'grass roots movement!', 'commentary!', 'in-the-red', 'unsubstantiated messages',})
neutral_set = set({'truth! &amp; fiction!', 'conclusion: unclear', '1', 'unobservable', 'needs context', 'truth! &amp; disputed!', 'half flip',
               '0',  'in-between', '4', 'None', '2', 'none',  'investigation pending!','not the whole story', '10','in the works',
               'truth! &amp; misleading!', '3',  'mixture', 'not yet rated', 'legend', 'stalled', 'truth! &amp; unproven!', 'truth! &amp; outdated!',
               'compromise'})
</code></pre>
<pre><code># Read in the labels for the appropriate data
pomt_train_y = pd.DataFrame(columns=[&quot;label&quot;])

sign_to_append = 0

for i, data in enumerate(training_data[training_data.columns].to_numpy()):
    if 'pomt' in data[0]:
        if data[2] in positive_set:
            sign_to_append = 1
        elif data[2] in negative_set:
            sign_to_append = -1
        else:
            sign_to_append = 0
        pomt_train_y = pomt_train_y.append({'label':sign_to_append}, ignore_index=True)
        print(f&quot;{i + 1} out of {training_data.index.stop} from training&quot;)

# append testing data to training data
for i, data in enumerate(testing_data[testing_data.columns].to_numpy()):
    if 'pomt' in data[0]:
        if data[2] in positive_set:
            sign_to_append = 1
        elif data[2] in negative_set:
            sign_to_append = -1
        else:
            sign_to_append = 0
        pomt_train_y = pomt_train_y.append({'label':sign_to_append}, ignore_index=True)
        print(f&quot;{i + 1} out of {testing_data.index.stop} from testing&quot;)
</code></pre>
<pre><code>pomt_X_train, pomt_X_test, pomt_Y_train, pomt_Y_test = train_test_split(pomt_train_x, pomt_train_y, test_size= (count / pomt_train_x.shape[0]), stratify=pomt_train_y)
pomt_Y_train = pomt_Y_train.astype(&quot;int&quot;)
pomt_Y_test = pomt_Y_test.astype(&quot;int&quot;)
</code></pre>
<pre><code># One Vs. One Multiclass Classification
clf = OneVsOneClassifier(SVC(C = 1, verbose=True))
</code></pre>
<pre><code># Fit to Training Data
clf.fit(pomt_X_train, pomt_Y_train)
</code></pre>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
TypeError: only size-1 arrays can be converted to Python scalars

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-22-3314e23093e3&gt; in &lt;module&gt;()
      1 # Fit to Training Data
----&gt; 2 clf.fit(pomt_X_train.squeeze(), pomt_Y_train)
      3 
      4 # Training data accuracy
      5 X_train_prediction = clf.predict(pomt_X_train)

4 frames
/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py in __array__(self, dtype)
   1991 
   1992     def __array__(self, dtype: NpDtype | None = None) -&gt; np.ndarray:
-&gt; 1993         return np.asarray(self._values, dtype=dtype)
   1994 
   1995     def __array_wrap__(

ValueError: setting an array element with a sequence.
</code></pre>
","python, pandas, numpy, tensorflow, bert-language-model","<p>I figured out what to do on my own end. I basically just created a column in the dataframe to reflect each element of the list, not each list itself. It's a bit unintuitive but it works.</p>
",0,1,423,2022-05-01 17:58:34,https://stackoverflow.com/questions/72079172/scikit-learn-fit-setting-an-array-element-with-a-sequence-fit
Pytorch model object has no attribute &#39;predict&#39; BERT,"<p>I had train a BertClassifier model using pytorch. After creating my best.pt I would like to make in production my model and using it to predict and classifier starting from a sample, so I resume them from the checkpoint. Otherwise after put it in evaluation and freeze model, I use .predict to make in work on my sample but I'm encountering this Attribute Error. I had also inizialize it before calling the checkpoint. When I am wrong? Thank you for your help!</p>
<pre><code>def save_ckp(state, is_best, checkpoint_path, best_model_path):
    &quot;&quot;&quot;
    function created to save checkpoint, the latest one and the best one. 
    This creates flexibility: either you are interested in the state of the latest checkpoint or the best checkpoint.
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    &quot;&quot;&quot;
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    &quot;&quot;&quot;
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into       
    optimizer: optimizer we defined in previous training
    &quot;&quot;&quot;
    # load check point
    checkpoint = torch.load(checkpoint_fpath)
    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])
    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])
    # initialize valid_loss_min from checkpoint to valid_loss_min
    valid_loss_min = checkpoint['valid_loss_min']
    # return model, optimizer, epoch value, min validation loss 
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

#Create the BertClassfier class
class BertClassifier(nn.Module):
    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
    def __init__(self, freeze_bert=True):
        &quot;&quot;&quot;
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        &quot;&quot;&quot;
        super(BertClassifier, self).__init__()
        
        .......
        
    def forward(self, input_ids, attention_mask):
        ''' Feed input to BERT and the classifier to compute logits.
         @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                       max_length)
         @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                       information with shape (batch_size, max_length)
         @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                       num_labels) '''
         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

def initialize_model(epochs):
    &quot;&quot;&quot; Initialize the Bert Classifier, the optimizer and the learning rate scheduler.&quot;&quot;&quot;
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier = bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=lr,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler
    

def train(model, train_dataloader, val_dataloader, valid_loss_min_input, checkpoint_path, best_model_path, start_epochs, epochs, evaluation=True):

    &quot;&quot;&quot;Train the BertClassifier model.&quot;&quot;&quot;
    # Start training loop
    logging.info(&quot;--Start training...\n&quot;)

    # Initialize tracker for minimum validation loss
    valid_loss_min = valid_loss_min_input 


    for epoch_i in range(start_epochs, epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        logging.info((f&quot;{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}&quot;))

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            # Zero out any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass. This will return logits.
            logits = model(b_input_ids, b_attn_mask)

            # Compute loss and accumulate the loss values
            loss = loss_fn(logits, b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent &quot;exploding gradients&quot;
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                logging.info(f&quot;{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}&quot;)

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        logging.info(&quot;-&quot;*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            logging.info(f&quot;{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^10.6f} | {time_elapsed:^9.2f}&quot;)

            logging.info(&quot;-&quot;*70)
        logging.info(&quot;\n&quot;)


         # create checkpoint variable and add important data
        checkpoint = {
            'epoch': epoch_i + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        
        # save checkpoint
        save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
        ## TODO: save the model if validation loss has decreased
        if val_loss &lt;= valid_loss_min:
            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
            # save checkpoint as best model
            save_ckp(checkpoint, True, checkpoint_path, best_model_path)
            valid_loss_min = val_loss
    
    logging.info(&quot;-----------------Training complete--------------------------&quot;)

def evaluate(model, val_dataloader):
    &quot;&quot;&quot;After the completion of each training epoch, measure the model's performance on our validation set.&quot;&quot;&quot;
    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
train(model = bert_classifier ......)



bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
model, optimizer, start_epoch, valid_loss_min = load_ckp(r&quot;./best_model/best_model.pt&quot;, bert_classifier, optimizer)

model.eval()
model.freeze()

sample = {
  &quot;seq&quot;: &quot;ABCDE&quot;,}

predictions = model.predict(sample)
</code></pre>
<pre><code>AttributeError: 'BertClassifier' object has no attribute 'predict'
</code></pre>
","python, pytorch, huggingface-transformers, bert-language-model, sentence-transformers","<p>Generally, people wrote the prediction function for you.
If not, you need to handle the low level stuff.
After this line, you loaded the trained parameters.</p>
<pre class=""lang-py prettyprint-override""><code>model, optimizer, start_epoch, valid_loss_min = load_ckp(r&quot;./best_model/best_model.pt&quot;, bert_classifier, optimizer)
</code></pre>
<p>After that, you need to do the <code>model.forward(intput_seq,this_attention_mask_maybe_null)</code>.
You can see the forward method here is the: <code>def forward(self, input_ids, attention_mask)</code> in the model.</p>
",1,0,4739,2022-05-06 20:51:14,https://stackoverflow.com/questions/72147225/pytorch-model-object-has-no-attribute-predict-bert
How to apply max_length to truncate the token sequence from the left in a HuggingFace tokenizer?,"<p>In the HuggingFace tokenizer, applying the <code>max_length</code> argument specifies the length of the tokenized text. I believe it truncates the sequence to <code>max_length-2</code> (if <code>truncation=True</code>) by cutting the excess tokens from the <strong>right</strong>. For the purposes of utterance classification, I need to cut the excess tokens from the <strong>left</strong>, i.e. the start of the sequence in order to preserve the last tokens. How can I do that?</p>
<pre><code>from transformers import AutoTokenizer

train_texts = ['text 1', ...]
tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
encodings = tokenizer(train_texts, max_length=128, truncation=True)
</code></pre>
","python, pytorch, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>Tokenizers have a <code>truncation_side</code> parameter that should set exactly this.
See the <a href=""https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.truncation_side"" rel=""noreferrer"">docs</a>.</p>
",6,5,10537,2022-05-11 13:52:23,https://stackoverflow.com/questions/72202295/how-to-apply-max-length-to-truncate-the-token-sequence-from-the-left-in-a-huggin
"`logits` and `labels` must have the same shape, received ((None, 512, 768) vs (None, 1)) when using transformers","<p>I get the next error when im trying to fine tuning a bert model to predict sentiment analysis.</p>
<p>Im using as input:
X-A list of strings that contains tweets
y-a numeric list (0 - negative, 1 - positive)</p>
<p>I am trying to fine tuning a bert model to predict sentiment analysis but i always get the same error in logits and labels when im trying to fit the model. I load a pretrained model and then build the dataset but when i am trying to fit it, it is impossible.</p>
<p>The text used as input is a list of strings made of tweets and the labels used as input are a list of categories (negative and positive) but transformed to 0 and 1.</p>
<pre><code>from sklearn.preprocessing import MultiLabelBinarizer

#LOAD MODEL

hugging_face_model = 'distilbert-base-uncased-finetuned-sst-2-english'
batches = 32
epochs = 1 

tokenizer = BertTokenizer.from_pretrained(hugging_face_model)
model = TFBertModel.from_pretrained(hugging_face_model, num_labels=2)

#PREPARE THE DATASET

#create a list of strings (tweets)


lst = list(X_train_lower['lower_text'].values) 
encoded_input  = tokenizer(lst, truncation=True, padding=True, return_tensors='tf')

y_train['sentimentNumber'] = y_train['sentiment'].replace({'negative': 0, 'positive': 1})
label_list = list(y_train['sentimentNumber'].values) 

#CREATE DATASET

train_dataset = tf.data.Dataset.from_tensor_slices((dict(encoded_input), label_list))

#COMPILE AND FIT THE MODEL

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=BinaryCrossentropy(from_logits=True),metrics=[&quot;accuracy&quot;])
model.fit(train_dataset.shuffle(len(df)).batch(batches),epochs=epochs,batch_size=batches) ```




ValueError                                Traceback (most recent call last)
&lt;ipython-input-158-e5b63f982311&gt; in &lt;module&gt;()
----&gt; 1 model.fit(train_dataset.shuffle(len(df)).batch(batches),epochs=epochs,batch_size=batches)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1021, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1000, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1000, in train_step
        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py&quot;, line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 1932, in binary_crossentropy
        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/backend.py&quot;, line 5247, in binary_crossentropy
        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)

    ValueError: `logits` and `labels` must have the same shape, received ((None, 512, 768) vs (None, 1)).
</code></pre>
","tensorflow, machine-learning, keras, sentiment-analysis, bert-language-model","<p>As described in this <a href=""https://www.kaggle.com/code/dhruv1234/huggingface-tfbertmodel/notebook"" rel=""nofollow noreferrer"">kaggle notebook</a>, you must  build a custom Keras Model around the pre-trained BERT model to perform classification,</p>
<blockquote>
<p>The bare Bert Model transformer outputing raw hidden-states without
any specific head on top</p>
</blockquote>
<p>Here is a copy of a piece of code:</p>
<pre><code>def create_model(bert_model):
  input_ids = tf.keras.Input(shape=(60,),dtype='int32')
  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')
  
  output = bert_model([input_ids,attention_masks])
  output = output[1]
  output = tf.keras.layers.Dense(32,activation='relu')(output)
  output = tf.keras.layers.Dropout(0.2)(output)

  output = tf.keras.layers.Dense(1,activation='sigmoid')(output)
  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
  model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])
  return model
</code></pre>
<p>Note: you might have to adapt this code and in particular modify the Input shape (60 to <strong>512</strong> seemingly from the error message, your tokenizer maximum length)</p>
<p>Load BERT model and build the classifier :</p>
<pre><code>from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained(hugging_face_model)
model = create_model(bert_model)
model.summary()
</code></pre>
<p>Summary:</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_1[0][0]',                
                                thPoolingAndCrossAt               'input_2[0][0]']                
                                tentions(last_hidde                                               
                                n_state=(None, 60,                                                
                                768),                                                             
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dense (Dense)                  (None, 32)           24608       ['tf_bert_model_1[0][1]']        
                                                                                                  
 dropout_74 (Dropout)           (None, 32)           0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 1)            33          ['dropout_74[0][0]']             
                                                                                                  
==================================================================================================
Total params: 109,506,881
Trainable params: 109,506,881
Non-trainable params: 0
</code></pre>
",0,0,598,2022-05-18 12:02:15,https://stackoverflow.com/questions/72288839/logits-and-labels-must-have-the-same-shape-received-none-512-768-vs-n
Combining BERT and other types of embeddings,"<p>The flair model can give a representation of any word (it can handle the OOV problem), while the BERT model splits the unknown word into several sub-words.</p>
<p>For example, the word &quot;hjik&quot; will have one vector represented in flair, while in BERT it will be divided into several words (because it's OOV) and therefore we will have several vectors for each sub word. So from flair we'll have one vector while from BERT we might have two or more vectors.</p>
<p>The question here is how did the <a href=""https://github.com/flairNLP"" rel=""nofollow noreferrer"">flairNLP</a> library handle this issue?</p>
<p>NOTE:If you have no idea, can you at least suggest me a proper way to handle it?</p>
","python, torch, bert-language-model, embedding, flair","<p>The TransformerWordEmbeddings class has default handling for words split into multiple subwords which you control with the subtoken_pooling parameter (your choices are &quot;first&quot;, &quot;last&quot;, &quot;first_last&quot; and &quot;mean&quot;), see the info here: <a href=""https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md#pooling-operation"" rel=""nofollow noreferrer"">https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md#pooling-operation</a></p>
",0,0,672,2022-05-19 16:56:55,https://stackoverflow.com/questions/72308412/combining-bert-and-other-types-of-embeddings
How to calculate per document probabilities under respective topics with BERTopics?,"<p>I am trying to use <code>BERTopic</code> to analyze the topic distribution of documents, after <code>BERTopic</code> is performed, I would like to calculate the probabilities under respective topics per document, how should I did it?</p>
<pre class=""lang-py prettyprint-override""><code># define model
model = BERTopic(verbose=True,
                 vectorizer_model=vectorizer_model,
                 embedding_model='paraphrase-MiniLM-L3-v2',
                 min_topic_size= 50,
                 nr_topics=10)

#  train model
headline_topics, _ = model.fit_transform(df1.review_processed3)

# examine one of the topic
a_topic = freq.iloc[0][&quot;Topic&quot;] # Select the 1st topic
model.get_topic(a_topic) # Show the words and their c-TF-IDF scores
</code></pre>
<p>Below is the words and their c-TF-IDF scores for one of the Topics
<a href=""https://i.sstatic.net/XOwsD.png"" rel=""noreferrer"">image 1</a></p>
<p>How should I change the result into Topic Distribution as below in order to calculate the topic distribution score and also identify the main topic?
<a href=""https://i.sstatic.net/4MX0j.png"" rel=""noreferrer"">image 2</a></p>
","python, nlp, bert-language-model, topic-modeling","<p>First, to compute probabilities, you have to add to your model definition <code>calculate_probabilities=True</code> (this could slow down the extraction of topics if you have many documents, &gt; 100000).</p>
<pre><code># define model
model = BERTopic(verbose=True,
                 vectorizer_model=vectorizer_model,
                 embedding_model='paraphrase-MiniLM-L3-v2',
                 min_topic_size= 50,
                 nr_topics=10,
                 calculate_probabilities=True)
</code></pre>
<p>Then, calling <code>fit_transform</code>, you should save the probabilities:</p>
<pre><code>headline_topics, probs = model.fit_transform(df1.review_processed3)
</code></pre>
<p>Now, you can create a pandas dataframe which shows probabilities under respective topics per document.</p>
<pre><code>import pandas as pd
probs_df=pd.DataFrame(probs)
probs_df['main percentage'] = pd.DataFrame({'max': probs_df.max(axis=1)})
</code></pre>
",7,5,2819,2022-05-22 15:11:47,https://stackoverflow.com/questions/72338808/how-to-calculate-per-document-probabilities-under-respective-topics-with-bertopi
how can we get the attention scores of multimodal models via hugging face library?,"<p>I was wondering if we could get the attention scores of any multimodal model using the api provided  by the hugging face library, as it's relatively easier to get such scores of normal language bert model, but what about lxmert? If anyone can answer this, it would help the understanding of such models.</p>
","image-processing, huggingface-transformers, bert-language-model, transformer-model, attention-model","<p>I am not sure if this is true for all of the models, but most including LXMERT of them support the parameter <a href=""https://huggingface.co/docs/transformers/model_doc/lxmert#transformers.LxmertModel.forward.output_attentions"" rel=""nofollow noreferrer"">output_attentions</a>.</p>
<p>Example with CLIP:</p>
<pre class=""lang-py prettyprint-override""><code>from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(
    text=[&quot;a photo of a cat&quot;, &quot;a photo of a dog&quot;], images=image, return_tensors=&quot;pt&quot;, padding=True
)

outputs = model(**inputs, output_attentions=True)
print(outputs.keys())
print(outputs.text_model_output.keys())
print(outputs.vision_model_output.keys())
</code></pre>
<p>Output:</p>
<pre><code>odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])
odict_keys(['last_hidden_state', 'pooler_output', 'attentions'])
odict_keys(['last_hidden_state', 'pooler_output', 'attentions'])
</code></pre>
",2,3,986,2022-05-28 09:51:10,https://stackoverflow.com/questions/72414634/how-can-we-get-the-attention-scores-of-multimodal-models-via-hugging-face-librar
Getting random output every time on running Next Sentence Prediction code using BERT,"<p>Based on the code provided below, I am trying to run NSP (Next Sentence Prediction) on a custom dataset. The loss after training the model is different every time and the model give different accuracies every time. What am I missing or doing wrong?</p>
<pre><code>pip install transformers[torch]
from transformers import BertTokenizer, BertForNextSentencePrediction 
import torch  
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') 
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
with open('clean.txt', 'r') as fp:
    text = fp.read().split('\n')
bag = [item for sentence in text for item in sentence.split('.') if item != '']
bag_size = len(bag)
import random
 
sentence_a = []
sentence_b = []
label = []
 
for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences &gt; 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() &gt;= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)
 
inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
inputs['labels'] = torch.LongTensor([label]).T
class MeditationsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
 
dataset = MeditationsDataset(inputs)
loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
from transformers import AdamW
 
# activate training mode
model.train()
# initialize optimizer
optim = AdamW(model.parameters(), lr=5e-6)
 
from tqdm import tqdm  # for our progress bar
 
epochs = 2
 
for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())
</code></pre>
<p>In the code below I am testing the model on unseen data:</p>
<pre><code>from torch.nn import functional as f
from torch.nn.functional import softmax
prompt = &quot;sentence 1 text&quot;
prompt2 = &quot;sentence 2 text&quot;
output = tokenizer.encode_plus(prompt,prompt2, return_tensors=&quot;pt&quot;)
result = model(**output)[0]
prob = softmax(result, dim=1)
print(prob)
</code></pre>
<p>So, the value of prob and loss is different every single time for the same unseen data which to the best of my knowledge should be same.</p>
","nlp, pytorch, huggingface-transformers, bert-language-model, attention-model","<p>You need to put the model in evaluation mode. if you use i.e. dropout layers while testing the model you should turn it off.
You can do this with</p>
<pre><code>model.eval()
</code></pre>
<p>If you don't use this, you will get a different output and loss value because the dropout in your model will close different neurons each time.</p>
",1,0,386,2022-06-03 12:28:05,https://stackoverflow.com/questions/72489570/getting-random-output-every-time-on-running-next-sentence-prediction-code-using
Save a Bert model with custom forward function and heads on Hugginface,"<p>I have created my own BertClassifier model, starting from a pretrained and then added my own classification heads composed by different layers. After the fine-tuning, I want to save the model using model.save_pretrained() but when I print it upload it from pretrained i don't see my classifier head.
The code is the following. How can I save the all structure on my model and make it full accessible with <code> AutoModel.from_preatrained('folder_path')</code> ?
. Thanks!</p>
<pre><code>class BertClassifier(PreTrainedModel):
    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
    config_class = AutoConfig
    def __init__(self,config, freeze_bert=True): #tuning only the head
        &quot;&quot;&quot;
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        &quot;&quot;&quot;
        #super(BertClassifier, self).__init__()
        super().__init__(config)

        # Instantiate BERT model
        # Specify hidden size of BERT, hidden size of our classifier, and number of labels
        self.D_in = 1024 #hidden size of Bert
        self.H = 512
        self.D_out = 2
 
        # Instantiate the classifier head with some one-layer feed-forward classifier
        self.classifier = nn.Sequential(
            nn.Linear(self.D_in, 512),
            nn.Tanh(),
            nn.Linear(512, self.D_out),
            nn.Tanh()
        )
 


    def forward(self, input_ids, attention_mask):


         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

</code></pre>
<pre><code>configuration=AutoConfig.from_pretrained('Rostlab/prot_bert_bfd')
model = BertClassifier(config=configuration,freeze_bert=False)
</code></pre>
<p>Saving the model after fine-tuning</p>
<pre><code>model.save_pretrained('path')
</code></pre>
<p>Loading the fine-tuned model</p>
<pre><code>model = AutoModel.from_pretrained('path') 
</code></pre>
<p>Printing the model after loading shows I have as the last layer the following and missing my 2 linear layer:</p>
<pre><code> (output): BertOutput(
          (dense): Linear(in_features=4096, out_features=1024, bias=True)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (adapters): ModuleDict()
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (activation): Tanh()
  )
  (prefix_tuning): PrefixTuningPool(
    (prefix_tunings): ModuleDict()
  )
)
</code></pre>
","python, nlp, pytorch, huggingface-transformers, bert-language-model","<p>Maybe something is wrong with the <code>config_class</code> attribute inside your <code>BertClassifier</code> class. According to the documentation you need to create an additional config class which inherits form <code>PretrainedConfig</code> and initialises the <code>model_type</code> attribute with the name of your custom model.</p>
<p>The <code>BertClassifier's config_class</code> has to be consistent with your custom config class type.
Afterwards you can register your config and model with the following calls:</p>

<pre><code>AutoConfig.register('CustomModelName', CustomModelConfigClass)
AutoModel.register(CustomModelConfigClass, CustomModelClass)
</code></pre>
<p>And load your finetuned model with <code>AutoModel.from_pretrained('YourCustomModelName')</code></p>
<p>An incomplete example based on your code could look like this:</p>

<pre><code>class BertClassifierConfig(PretrainedConfig):
    model_type=&quot;BertClassifier&quot;


class BertClassifier(PreTrainedModel):
    config_class = BertClassifierConfig
    # ...


configuration = BertClassifierConfig()
bert_classifier = BertClassifier(configuration)

# do your finetuning and save your custom model
bert_classifier.save_pretrained(&quot;CustomModels/BertClassifier&quot;)

# register your config and your model
AutoConfig.register(&quot;BertClassifier&quot;, BertClassifierConfig)
AutoModel.register(BertClassifierConfig, BertClassifier)

# load your model with AutoModel
bert_classifier_model = AutoModel.from_pretrained(&quot;CustomModels/BertClassifier&quot;)
</code></pre>
<p>Printing the model output should be similiar to this:</p>
<pre><code>    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): Tanh()
    (2): Linear(in_features=512, out_features=2, bias=True)
    (3): Tanh()
    (4): Linear(in_features=2, out_features=512, bias=True)
    (5): Tanh()
  )
</code></pre>
<p>Hope this helps.</p>
<p><a href=""https://huggingface.co/docs/transformers/custom_models#registering-a-model-with-custom-code-to-the-auto-classes"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/custom_models#registering-a-model-with-custom-code-to-the-auto-classes</a></p>
",3,5,3710,2022-06-04 21:38:22,https://stackoverflow.com/questions/72503309/save-a-bert-model-with-custom-forward-function-and-heads-on-hugginface
AttributeError: &#39;MaskedLMOutput&#39; object has no attribute &#39;view&#39;,"<p>Sorry to bother,  I met this error when I evaluate some models,  and I didn't find a good method to fix it.<br />
What does 'MaskedLMOutput'means?Could somebody tell me How to fix this please? Thank you.</p>
<p>(AttributeError: 'MaskedLMOutput' object has no attribute 'view')</p>
<pre><code>from transformers import BertForMaskedLM

class BertPunc(nn.Module):  
    
    def __init__(self, segment_size, output_size, dropout):
        super(BertPunc, self).__init__()
        self.bert = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')
        self.bert_vocab_size = 32000
        self.bn = nn.BatchNorm1d(segment_size*self.bert_vocab_size)
        self.fc = nn.Linear(segment_size*self.bert_vocab_size, output_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input):
        
        x = self.bert(input)             
        x = x.view(x.shape[0], -1)                   # wrong thing here
        x = self.fc(self.dropout(self.bn(x)))
        return x
</code></pre>
<p>I ran this in jupyter notebook:</p>
<pre><code>    def predictions(data_loader):
        y_pred = []
        y_true = []
        for inputs, labels in tqdm(data_loader, total=len(data_loader)):
            with torch.no_grad():
                inputs, labels = inputs.cuda(), labels.cuda()
                output = bert_punc(inputs)
                y_pred += list(output.argmax(dim=1).cpu().data.numpy().flatten())
                y_true += list(labels.cpu().data.numpy().flatten())
        return y_pred, y_true

def evaluation(y_pred, y_test):
    precision, recall, f1, _ = metrics.precision_recall_fscore_support(
        y_test, y_pred, average=None, labels=[1, 2, 3])
    overall = metrics.precision_recall_fscore_support(
        y_test, y_pred, average='macro', labels=[1, 2, 3])
    result = pd.DataFrame(
        np.array([precision, recall, f1]), 
        columns=list(punctuation_enc.keys())[1:], 
        index=['Precision', 'Recall', 'F1']
    )
    result['OVERALL'] = overall[:3]
    return result


y_pred_test, y_true_test = predictions(data_loader_test)
eval_test = evaluation(y_pred_test, y_true_test)
eval_test
</code></pre>
<p>wrong here:</p>
<pre><code>   ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    Input In [12], in &lt;cell line: 1&gt;()
    ----&gt; 1 y_pred_test, y_true_test = predictions(data_loader_test)
          2 eval_test = evaluation(y_pred_test, y_true_test)
          3 eval_test
    
    Input In [10], in predictions(data_loader)
          5 with torch.no_grad():
          6     inputs, labels = inputs.cuda(), labels.cuda()
    ----&gt; 7     output = bert_punc(inputs)
          8     y_pred += list(output.argmax(dim=1).cpu().data.numpy().flatten())
          9     y_true += list(labels.cpu().data.numpy().flatten())
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File ~\anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py:166, in DataParallel.forward(self, *inputs, **kwargs)
        163     kwargs = ({},)
        165 if len(self.device_ids) == 1:
    --&gt; 166     return self.module(*inputs[0], **kwargs[0])
        167 replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        168 outputs = self.parallel_apply(replicas, inputs, kwargs)
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File F:\BertPunc-master\model.py:19, in BertPunc.forward(self, input)
         16 def forward(self, input):
         18     x = self.bert(input)
    ---&gt; 19     x = x.view(x.shape[0], -1)
         20     x = self.fc(self.dropout(self.bn(x)))
         21     return x
    
    AttributeError: 'MaskedLMOutput' object has no attribute 'view'
</code></pre>
","python, nlp, model, pytorch, bert-language-model","<p>You can refer to the <a href=""https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"" rel=""nofollow noreferrer"">documentation</a> of <code>MaskedLMOutput</code>. Basically, it is an object holding the <code>loss</code>, <code>logits</code>, <code>hidden_states</code> and <code>attentions</code>. It is not a tensor so you are getting this error. I think you are interested in <code>logits</code>, i.e., score for each token before applying softmax. Then, in the <code>forward</code> function, you can just access to the <code>logits</code> tensor like this</p>
<pre class=""lang-py prettyprint-override""><code>x = self.bert(input).logits
</code></pre>
",1,0,1493,2022-06-13 10:06:20,https://stackoverflow.com/questions/72601057/attributeerror-maskedlmoutput-object-has-no-attribute-view
Translation between different tokenizers,"<p>Sorry if this question is too basic to be asked here. I tried but I couldn't find solutions.</p>
<p>I'm now working on an NLP project that requires using two different models (BART for summarization and BERT for QA). I want to concatenate the two models so that I can train them at the same time instead of separately. However, the two models use different tokenizers. Is there a way to create a layer mapping the output of the first layer to the second one? Would creating a dictionary by hand (mapping every single available value of the tokenizer) work?  Thanks a lot!</p>
","neural-network, nlp, tokenize, bert-language-model","<p>It's not clear whether you are asking &quot;How to translate one tokenizers output into another ones output&quot;,&quot;How to use same tokenizer on both models&quot; or &quot;How to link one model after other for training&quot;. So I will answer covering both cases.</p>
<p>Before giving a direct answer to your question, lets have a look into your ML model.</p>
<p>I think this is the model you want to build (correct me in comment section if I am wrong)</p>
<pre><code>                                                         |
                                                         |
                                                         Q
                                                         |
         ,-------------------,                    ,-------------,
--text--&gt;|  BART (summarize) |--summarized text--&gt;|  BERT (QA)  |---&gt; A
         `-------------------`                    `-------------`
</code></pre>
<p>OK, this would be little easy if you have basic idea on Tokenizers and machine learning pipe lines.</p>
<p>I assume you are using huggingface library (even if not there won't be a significant difference) . When we are training an NLP model, we have to tokenize them first.</p>
<p>What do the tokenizers do?</p>
<p>Actually, a tokenizer also has been gone through a training process. It learns how ti break sentences and words into chunks. After that it automatically builds a mapping for each identical chunk. <code>chunk--&gt; digit/array</code>.</p>
<p>Lets get the first case</p>
<p>It's literally no. As I mentioned above Tokenizers also were trained. It tokenizes sentences/words according to its own rules and assign numbers according to it's own mapping. The same sentence/word can be broken into different no of chunks at different places by different tokenizers. So , It's not possible to do something like finding french meaning of an English word using dictionary.</p>
<p>The second case</p>
<p>When training the transformer (BART/BERT or any transformer derivative) we pass the result into the transformer. Because of transformers only accept vectors/tensors/matrices , not strings. And then transformer is trained on that input. So you must remember five things.</p>
<ol>
<li>Transformer's output/training depends on input</li>
<li>Input depends on Tokenizer's output</li>
<li>So, Transformer's output/training depends on Tokenizer</li>
<li>Each tokenizer has different mappings. (Output is different for same text)</li>
<li>Each tokenizer has different output vertor size.</li>
<li>So Once a Transformer has trained along with a specific tokenizer, It can only use that tokenizer.</li>
</ol>
<p>Can you use same tokenizer, It depends on are you using pre-trained bart and bert or train them from scratch. If you use pretrained ones, you have to use specific tokenizer with it. (If you are using huggingface models, the compatible tokenizer name has been given). Otherwise you can use same tokenizer without any problem. You just have to use same tokenizer for transformers training session only if both transformers have input size equal to output vector of the tokenizer. But after that you can't use other tokenizers.</p>
<p>Lets move to the third case.</p>
<p>Of course you can train both at once. But you have to build an ML pipeline first. It's not very difficult. But you have to learn how to build pipelines first. Many libraries provide facilities to build pipelines easily..</p>
",3,3,2525,2022-06-15 03:12:40,https://stackoverflow.com/questions/72625528/translation-between-different-tokenizers
&#39;MaskedLMOutput&#39; object has no attribute &#39;view&#39;,"<p>I wrote this:</p>
<pre><code>def forward(self, x):
    x = self.bert(x)
    
    x = x.view(x.shape[0], -1)
    x = self.fc(self.dropout(self.bn(x)))
    return x
</code></pre>
<p>but it doesn't work well, and the error is 'MaskedLMOutput' object has no attribute 'view'.
I'm considering the input might not be 'tensor' type, so I change it as below:</p>
<pre><code>def forward(self, x):
        x = torch.tensor(x)     # this part
        x = self.bert(x)
        
        x = x.view(x.shape[0], -1)
        x = self.fc(self.dropout(self.bn(x)))
        return x
</code></pre>
<p>but it still gets wrong, same error 'MaskedLMOutput' object has no attribute 'view'.</p>
<p>Could someone tell me how to fix this?  Much thanks.</p>
<p>Whole error information here:</p>
<pre><code> ------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    Input In [5], in &lt;cell line: 8&gt;()
          6 optimizer = optim.Adam(bert_punc.parameters(), lr=learning_rate_top)
          7 criterion = nn.CrossEntropyLoss()
    ----&gt; 8 bert_punc, optimizer, best_val_loss = train(bert_punc, optimizer, criterion, epochs_top, 
          9     data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations_top, best_val_loss=1e9)
    
    Input In [3], in train(model, optimizer, criterion, epochs, data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations, best_val_loss)
         17 inputs.requires_grad = False
         18 labels.requires_grad = False
    ---&gt; 19 output = model(inputs)
         20 loss = criterion(output, labels)
         21 loss.backward()
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File ~\anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py:166, in DataParallel.forward(self, *inputs, **kwargs)
        163     kwargs = ({},)
        165 if len(self.device_ids) == 1:
    --&gt; 166     return self.module(*inputs[0], **kwargs[0])
        167 replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        168 outputs = self.parallel_apply(replicas, inputs, kwargs)
    
    File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in Module._call_impl(self, *input, **kwargs)
       1106 # If we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -&gt; 1110     return forward_call(*input, **kwargs)
       1111 # Do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    File D:\BertPunc-original\model.py:21, in BertPunc.forward(self, x)
         18 x = torch.tensor(x)
         19 x = self.bert(x)
    ---&gt; 21 x = x.view(x.shape[0], -1)
         22 x = self.fc(self.dropout(self.bn(x)))
         23 return x
    
    AttributeError: 'MaskedLMOutput' object has no attribute 'view'
</code></pre>
","python, tensorflow, pytorch, huggingface-transformers, bert-language-model","<p>I think so this should help you solve the error. <a href=""https://stackoverflow.com/a/72601533/13748930"">https://stackoverflow.com/a/72601533/13748930</a>
The output after self.bert(x) is an object of the class MaskedLMOutput.</p>
",0,0,1882,2022-06-15 08:59:48,https://stackoverflow.com/questions/72628556/maskedlmoutput-object-has-no-attribute-view
Evaluate BERT Model param.requires_grad,"<p>I have a doubt regarding the evaluation on the test set of my bert model. During the eval part param.requires_grad is suppose to be True or False? indipendently if I did a full fine tuning during training or not. My model is in model.eval() mode but I want to be sure to not force nothing wrong in the Model() class when i call it for evaluation. Thanks !</p>
<pre><code>  if freeze_bert == 'True':
        for param in self.bert.parameters():
            param.requires_grad = False
            #logging.info('freeze_bert: {}'.format(freeze_bert)) 
            #logging.info('param.requires_grad: {}'.format(param.requires_grad))
    if freeze_bert == 'False':
        for param in self.bert.parameters():
            param.requires_grad = True
</code></pre>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>If you freeze your model then the parameter of the corresponding modules must not be updated, <em>i.e.</em> they should not require gradient computation: <code>requires_grad=False</code>.</p>
<p>Note <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html"" rel=""nofollow noreferrer""><code>nn.Module</code></a> also has a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_"" rel=""nofollow noreferrer""><code>requires_grad_</code></a> method:</p>
<pre><code>if freeze_bert == 'True':
    self.bert.requires_grad_(False)

elif freeze_bert == 'False:
    self.bert.requires_grad_(True)
</code></pre>
<p>Ideally <code>freeze_bert</code> would be a boolean and you would simply do:</p>
<pre><code>self.bert.requires_grad_(not freeze_bert)
</code></pre>
",1,-1,902,2022-06-20 12:49:01,https://stackoverflow.com/questions/72687276/evaluate-bert-model-param-requires-grad
Formatting our data into PyTorch Dataset object for fine-tuning BERT,"<p>I'm using an already existing code from <em>Towards Data Science</em> for fine-tuning a BERT Model.
The problem I'm facing belongs to this part of the code which where try to format our data into a PyTorch <code>data.Dataset</code> object:</p>
<pre><code>class MeditationsDataset(torch.utils.data.Dataset):
    def _init_(self, encodings, *args, **kwargs):
        self.encodings = encodings
    def _getitem_(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def _len_(self):
        return len(self.encodings.input_ids)


dataset = MeditationsDataset(inputs)
</code></pre>
<p>When I run the code, I face this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-144-41fc3213bc25&gt; in &lt;module&gt;()
----&gt; 1 dataset = MeditationsDataset(inputs)

/usr/lib/python3.7/typing.py in __new__(cls, *args, **kwds)
    819             obj = super().__new__(cls)
    820         else:
--&gt; 821             obj = super().__new__(cls, *args, **kwds)
    822         return obj
    823 

TypeError: object.__new__() takes exactly one argument (the type to instantiate)
</code></pre>
<p>I already searched for this error but the problem here is that sadly I'm not familiar with either PyTorch or OOP so I couldn't fix this problem. Could you please let me know what should I add or remove from this code so I can run it? Thanks a lot in advance.</p>
<p>Also if needed, our data is as below:</p>
<pre><code>{'input_ids': tensor([[   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      ...,
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2,  103, 1005,  ...,    0,    0,    0],
                      [   2,    4,    0,  ...,    0,    0,    0]]), 
 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           ...,
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           ...,
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 0,  ..., 0, 0, 0]]), 
 'labels': tensor([[   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   ...,
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2,    4,    0,  ...,    0,    0,    0]])}
</code></pre>
","python, oop, pytorch, bert-language-model","<p>Special functions in Python use <em><strong>double underscores</strong></em> prefix and suffix. In your case, to implement a <a href=""https://pytorch.org/docs/stable/data.html#map-style-datasets"" rel=""nofollow noreferrer""><code>data.Dataset</code></a>, you must have <a href=""https://docs.python.org/3/reference/datamodel.html#object.__init__"" rel=""nofollow noreferrer""><code>__init__</code></a>, <a href=""https://docs.python.org/3/reference/datamodel.html#object.__getitem__"" rel=""nofollow noreferrer""><code>__getitem__</code></a>, and <a href=""https://docs.python.org/3/reference/datamodel.html#object.__len__"" rel=""nofollow noreferrer""><code>__len__</code></a>:</p>
<pre><code>class MeditationsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, *args, **kwargs):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
</code></pre>
",1,0,170,2022-06-26 13:07:28,https://stackoverflow.com/questions/72761858/formatting-our-data-into-pytorch-dataset-object-for-fine-tuning-bert
resize_token_embeddings on the a pertrained model with different embedding size,"<p>I would like to ask about the way to change the embedding size of the trained model.</p>
<p>I have a trained model <code>models/BERT-pretrain-1-step-5000.pkl</code>.
Now I am adding a new token <code>[TRA]</code>to the tokeniser and try to use the <code>resize_token_embeddings</code> to the pertained one.</p>
<pre class=""lang-py prettyprint-override""><code>from pytorch_pretrained_bert_inset import BertModel #BertTokenizer 
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence
import tqdm

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model_bert = BertModel.from_pretrained('bert-base-uncased', state_dict=torch.load('models/BERT-pretrain-1-step-5000.pkl', map_location=torch.device('cpu')))

#print(tokenizer.all_special_tokens) #--&gt; ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
#print(tokenizer.all_special_ids)    #--&gt; [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[TRA]'], special_tokens=True)
model_bert.resize_token_embeddings(len(tokenizer))  # --&gt; Embedding(30523, 768)
print('[TRA] token id: ', tokenizer.convert_tokens_to_ids('[TRA]'))  # --&gt; 30522
</code></pre>
<p>But I encountered the error:</p>
<pre><code>AttributeError: 'BertModel' object has no attribute 'resize_token_embeddings'
</code></pre>
<p>I assume that it is because the <code>model_bert(BERT-pretrain-1-step-5000.pkl)</code> I had has the different embedding size.
I would like to know if there is any way to fit the embedding size of my modified tokeniser and the model I would like to use as the initial weights.</p>
<p>Thanks a lot!!</p>
","pytorch, huggingface-transformers, bert-language-model, word-embedding, huggingface-tokenizers","<p><a href=""https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings"" rel=""noreferrer"">resize_token_embeddings</a> is a huggingface transformer method. You are using the BERTModel class from <code>pytorch_pretrained_bert_inset</code> which does not provide such a method. Looking at the <a href=""https://github.com/dreasysnail/INSET/blob/2dee48ef6045c8987cef399ecbb3d443b69c1c5e/pytorch_pretrained_bert_inset/modeling.py"" rel=""noreferrer"">code</a>, it seems like they have copied the BERT code from huggingface some time ago.</p>
<p>You can either wait for an update from INSET (maybe create a github issue) or write your own code to extend the word_embedding layer:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn 

embedding_layer = model.embeddings.word_embeddings

old_num_tokens, old_embedding_dim = embedding_layer.weight.shape

num_new_tokens = 1

# Creating new embedding layer with more entries
new_embeddings = nn.Embedding(
        old_num_tokens + num_new_tokens, old_embedding_dim
)

# Setting device and type accordingly
new_embeddings.to(
    embedding_layer.weight.device,
    dtype=embedding_layer.weight.dtype,
)

# Copying the old entries
new_embeddings.weight.data[:old_num_tokens, :] = embedding_layer.weight.data[
    :old_num_tokens, :
]

model.embeddings.word_embeddings = new_embeddings
</code></pre>
",7,3,14272,2022-06-27 16:38:00,https://stackoverflow.com/questions/72775559/resize-token-embeddings-on-the-a-pertrained-model-with-different-embedding-size
bert sentence_transformers list index out of range,"<p>I'm trying to use sentence_transformers to get bert embeddings, but it can't process for example 300 documents, i keep getting error <strong>IndexError: list index out of range</strong>. How to fix that?</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')
embeddings = model.encode(tokenized_docs_smaller, show_progress_bar=True)
</code></pre>
","python, nlp, data-science, bert-language-model, sentence-transformers","<p>Had to tokenize texts with BertTokenizer and not just use split()</p>
",1,0,1099,2022-06-27 18:38:45,https://stackoverflow.com/questions/72776921/bert-sentence-transformers-list-index-out-of-range
"TypeError: Expected `trainable` argument to be a boolean, but got: bert","<p>I got this error when implementing my model. I think the erros come from the bert model which i have imported.</p>
<pre class=""lang-py prettyprint-override""><code>def create_text_encoder(
    num_projection_layers, projection_dims, dropout_rate, trainable=False
):
    # Load the BERT preprocessing module.
    preprocess = hub.KerasLayer(
        &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&quot;,
        name=&quot;text_preprocessing&quot;,
    )
    # Load the pre-trained BERT model to be used as the base encoder.
    bert = hub.KerasLayer(
        &quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;,
        &quot;bert&quot;,
    )
    # Set the trainability of the base encoder.
    bert.trainable = trainable
    # Receive the text as inputs.
    inputs = layers.Input(shape=(), dtype=tf.string, name=&quot;text_input&quot;)
    # Preprocess the text.
    bert_inputs = preprocess(inputs)
    # Generate embeddings for the preprocessed text using the BERT model.
    embeddings = bert(bert_inputs)[&quot;pooled_output&quot;]
    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the text encoder model.
    return keras.Model(inputs, outputs, name=&quot;text_encoder&quot;)
</code></pre>
<p>The error is showing in below code but I think problem is in above part.</p>
<pre class=""lang-py prettyprint-override""><code>num_epochs = 5  # In practice, train for at least 30 epochs
batch_size = 256

vision_encoder = create_vision_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
text_encoder = create_text_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)
dual_encoder.compile(
    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)
)
</code></pre>
<p>Thanks.</p>
","machine-learning, deep-learning, nlp, data-science, bert-language-model","<p>I saw this question again after facing this issue in same code, Now I am writing an answer as I have solved it.</p>
<p>There is a keyword <code>name</code> missing in the above code</p>
<pre><code> bert = hub.KerasLayer(
    &quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;,
    &quot;bert&quot;,
)
</code></pre>
<p>I changed it to</p>
<pre><code>bert = hub.KerasLayer(
    &quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;,
    name = &quot;bert&quot;,
)
</code></pre>
<p>by just putting <code>name = &quot;bert&quot;</code> and now it works.</p>
",2,1,1833,2022-06-29 12:37:03,https://stackoverflow.com/questions/72801555/typeerror-expected-trainable-argument-to-be-a-boolean-but-got-bert
Does HuggingFace&#39;s Trainer automatically ignore features not required by the model?,"<p>I am a new user of Transformers and I have successfully fine-tuned a BERT model following the tutorial.</p>
<p>However, I have one question about the features I send to the Trainer and those accepted by the BERT model.</p>
<p>Specifically, my original dataset contains two columns named “text” and “label”. After tokenizing the “text”, the dataset object now has three more columns named “input_ids”, “token_type_ids”, and “attention_mask”. I understand that these three columns are required by the BERT model, but I didn’t drop the original “text” column when I fed the dataset to the Trainer API.</p>
<p>So my question is, does BERT automatically ignore non-relevant features? (maybe this is achieved quietly by the Trainer API) Or should I remove these columns, leaving only “input_ids”, “token_type_ids”, and “attention_mask”?</p>
<p>For example, below is my dataset object:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 6851
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 762
    })
})
</code></pre>
<p>And I fed it and my model to the Trainer:</p>
<pre><code>trainer = Trainer(
    model,
    training_args,
    train_dataset=data[&quot;train&quot;],
    eval_dataset=data[&quot;test&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<p>What happened to the &quot;text&quot; feature?</p>
","deep-learning, huggingface-transformers, bert-language-model, transformer-model","<p>As you guessed, these columns are ignored by the Trainer (see also the comments on this <a href=""https://stackoverflow.com/questions/70263251/valueerror-when-pre-training-bert-model-using-trainer-api"">answer</a>).</p>
<p>However, when a column is ignored during training, you should get the following warning message:
<code>The following columns in the training set don't have a corresponding argument in MyModel.forward and have been ignored: column1, column2. If column1, column2 are not expected by MyModel.forward,  you can safely ignore this message.</code> (triggered <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L662"" rel=""nofollow noreferrer"">here</a> in the code).</p>
<p>Did you disable warnings?</p>
",1,0,1721,2022-06-30 11:44:40,https://stackoverflow.com/questions/72815200/does-huggingfaces-trainer-automatically-ignore-features-not-required-by-the-mod
"bert-base-uncased: TypeError: tuple indices must be integers or slices, not tuple","<p>I want to see embeddings for the input text I give to the model, and then feed it to the rest of the BERT. To do so, I partitioned the model into two sequential models, but I must have done it wrong because rest_of_bert model raises TypeError. Original model does not raise any error with the input_ids as input processed with text_to_input function.</p>
<p>Input[0]:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
cls_token_id = tokenizer.cls_token_id
sep_token_id = tokenizer.sep_token_id
pad_token_id = tokenizer.pad_token_id

model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
model.eval()
</code></pre>
<p>Output[0]:</p>
<pre class=""lang-py prettyprint-override""><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</code></pre>
<p>Input[1]:</p>
<pre class=""lang-py prettyprint-override""><code>def text_to_input(text):
  x = tokenizer.encode(text, add_special_tokens=False) # returns python list
  x = [cls_token_id] + x + [sep_token_id]
  token_count = len(x)
  pad_count = 512 - token_count
  x = x + [pad_token_id for i in range(pad_count)]
  return torch.tensor([x])

extract_embeddings = torch.nn.Sequential(list(model.children())[0])
rest_of_bert = torch.nn.Sequential(*list(model.children())[1:])

input_ids = text_to_input('A sentence.')
x_embedding = extract_embeddings(input_ids)
output = rest_of_bert(x_embedding)
</code></pre>
<p>Output[1]:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-d371d8a2fb3c&gt; in &lt;module&gt;()
     12 input_ids = text_to_input('A sentence.')
     13 x_embedding = extract_embeddings(input_ids)
---&gt; 14 output = rest_of_bert(x_embedding)

4 frames
/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in __getitem__(self, k)
    220             return inner_dict[k]
    221         else:
--&gt; 222             return self.to_tuple()[k]
    223 
    224     def __setattr__(self, name, value):

TypeError: tuple indices must be integers or slices, not tuple
</code></pre>
","python, machine-learning, pytorch, huggingface-transformers, bert-language-model","<p>Each <code>BertLayer</code> returns a tuple that contains at least one tensor (depending on what output you requested). The first element of the tuple is the tensor you want to feed to the next BertLayer.</p>
<p>A more huggingface-like approach would be calling the model with <a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward.output_hidden_states"" rel=""nofollow noreferrer"">output_hidden_states</a>:</p>
<pre class=""lang-py prettyprint-override""><code>o = model(input_ids, output_hidden_states=True)
print(len(o.hidden_states))
</code></pre>
<p>Output:</p>
<pre><code>13
</code></pre>
<p>The first tensor of the hidden_states tuple is the output of your <code>extract_embeddings</code> object (token embeddings). The other 12 tensors are the contextualized embeddings that are the output of each <code>BertLayer</code>.</p>
<p>You should, by the way, provide an attention mask, because otherwise, your padding tokens will affect your output. The tokenizer is able to do that for you and you can replace your whole <code>text_to_input</code> method with:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer('A sentence.', return_tensors='pt', padding='max_length', max_length=512)
</code></pre>
",2,0,1444,2022-07-03 10:43:50,https://stackoverflow.com/questions/72845812/bert-base-uncased-typeerror-tuple-indices-must-be-integers-or-slices-not-tupl
Apply Bert encoding on all values of pandas dataframe,"<p>I am trying to get the bert embeddings for all the values present in dataframe.</p>
<p>My code looks like:</p>
<pre><code>
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
import pandas as pd

sentences =[ [&quot;I'm happy&quot;, &quot;I'm full of happiness&quot;], [&quot;I am sam&quot;, &quot;I am good&quot;]]

df = pd.DataFrame(sentences)

encoded_df = df.applymap(model.encode(convert_to_tensor=True))

</code></pre>
<p>Facing the below error:</p>
<pre><code>encode() missing 1 required positional argument: 'sentences'
</code></pre>
<p>Is there any method to achieve this?</p>
","python, pandas, dataframe, bert-language-model","<p>You can use</p>
<pre class=""lang-py prettyprint-override""><code>encoded_df = df.applymap(lambda x: model.encode(x, convert_to_tensor=True))
</code></pre>
",1,0,1461,2022-07-03 13:58:56,https://stackoverflow.com/questions/72847085/apply-bert-encoding-on-all-values-of-pandas-dataframe
Are the pre-trained layers of the Huggingface BERT models frozen?,"<p>I use the following classification model from Huggingface:</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;, num_labels=2).to(device)
</code></pre>
<p>As I understand, this adds a dense layer at the end of the pre-trained model which has 2 output nodes. But are all the pre-trained layers before that frozen? Or are they also updated when fine-tuning? I can't find information about that in the docs...</p>
<p>So do I still have to do something like this?:</p>
<pre class=""lang-py prettyprint-override""><code>for param in model.bert.parameters():
    param.requires_grad = False
</code></pre>
","nlp, pytorch, huggingface-transformers, bert-language-model","<p>They are not frozen. All parameters are trainable by default. You can also check that with:</p>
<pre class=""lang-py prettyprint-override""><code>for name, param in model.named_parameters():
    print(name, param.requires_grad)
</code></pre>
<p>Output:</p>
<pre><code>bert.embeddings.word_embeddings.weight True
bert.embeddings.position_embeddings.weight True
bert.embeddings.token_type_embeddings.weight True
bert.embeddings.LayerNorm.weight True
bert.embeddings.LayerNorm.bias True
bert.encoder.layer.0.attention.self.query.weight True
bert.encoder.layer.0.attention.self.query.bias True
bert.encoder.layer.0.attention.self.key.weight True
bert.encoder.layer.0.attention.self.key.bias True
bert.encoder.layer.0.attention.self.value.weight True
bert.encoder.layer.0.attention.self.value.bias True
bert.encoder.layer.0.attention.output.dense.weight True
bert.encoder.layer.0.attention.output.dense.bias True
bert.encoder.layer.0.attention.output.LayerNorm.weight True
bert.encoder.layer.0.attention.output.LayerNorm.bias True
bert.encoder.layer.0.intermediate.dense.weight True
bert.encoder.layer.0.intermediate.dense.bias True
bert.encoder.layer.0.output.dense.weight True
bert.encoder.layer.0.output.dense.bias True
bert.encoder.layer.0.output.LayerNorm.weight True
bert.encoder.layer.0.output.LayerNorm.bias True
bert.encoder.layer.1.attention.self.query.weight True
bert.encoder.layer.1.attention.self.query.bias True
bert.encoder.layer.1.attention.self.key.weight True
bert.encoder.layer.1.attention.self.key.bias True
bert.encoder.layer.1.attention.self.value.weight True
bert.encoder.layer.1.attention.self.value.bias True
bert.encoder.layer.1.attention.output.dense.weight True
bert.encoder.layer.1.attention.output.dense.bias True
bert.encoder.layer.1.attention.output.LayerNorm.weight True
bert.encoder.layer.1.attention.output.LayerNorm.bias True
bert.encoder.layer.1.intermediate.dense.weight True
bert.encoder.layer.1.intermediate.dense.bias True
bert.encoder.layer.1.output.dense.weight True
bert.encoder.layer.1.output.dense.bias True
bert.encoder.layer.1.output.LayerNorm.weight True
bert.encoder.layer.1.output.LayerNorm.bias True
bert.encoder.layer.2.attention.self.query.weight True
bert.encoder.layer.2.attention.self.query.bias True
bert.encoder.layer.2.attention.self.key.weight True
bert.encoder.layer.2.attention.self.key.bias True
bert.encoder.layer.2.attention.self.value.weight True
bert.encoder.layer.2.attention.self.value.bias True
bert.encoder.layer.2.attention.output.dense.weight True
bert.encoder.layer.2.attention.output.dense.bias True
bert.encoder.layer.2.attention.output.LayerNorm.weight True
bert.encoder.layer.2.attention.output.LayerNorm.bias True
bert.encoder.layer.2.intermediate.dense.weight True
bert.encoder.layer.2.intermediate.dense.bias True
bert.encoder.layer.2.output.dense.weight True
bert.encoder.layer.2.output.dense.bias True
bert.encoder.layer.2.output.LayerNorm.weight True
bert.encoder.layer.2.output.LayerNorm.bias True
bert.encoder.layer.3.attention.self.query.weight True
bert.encoder.layer.3.attention.self.query.bias True
bert.encoder.layer.3.attention.self.key.weight True
bert.encoder.layer.3.attention.self.key.bias True
bert.encoder.layer.3.attention.self.value.weight True
bert.encoder.layer.3.attention.self.value.bias True
bert.encoder.layer.3.attention.output.dense.weight True
bert.encoder.layer.3.attention.output.dense.bias True
bert.encoder.layer.3.attention.output.LayerNorm.weight True
bert.encoder.layer.3.attention.output.LayerNorm.bias True
bert.encoder.layer.3.intermediate.dense.weight True
bert.encoder.layer.3.intermediate.dense.bias True
bert.encoder.layer.3.output.dense.weight True
bert.encoder.layer.3.output.dense.bias True
bert.encoder.layer.3.output.LayerNorm.weight True
bert.encoder.layer.3.output.LayerNorm.bias True
bert.encoder.layer.4.attention.self.query.weight True
bert.encoder.layer.4.attention.self.query.bias True
bert.encoder.layer.4.attention.self.key.weight True
bert.encoder.layer.4.attention.self.key.bias True
bert.encoder.layer.4.attention.self.value.weight True
bert.encoder.layer.4.attention.self.value.bias True
bert.encoder.layer.4.attention.output.dense.weight True
bert.encoder.layer.4.attention.output.dense.bias True
bert.encoder.layer.4.attention.output.LayerNorm.weight True
bert.encoder.layer.4.attention.output.LayerNorm.bias True
bert.encoder.layer.4.intermediate.dense.weight True
bert.encoder.layer.4.intermediate.dense.bias True
bert.encoder.layer.4.output.dense.weight True
bert.encoder.layer.4.output.dense.bias True
bert.encoder.layer.4.output.LayerNorm.weight True
bert.encoder.layer.4.output.LayerNorm.bias True
bert.encoder.layer.5.attention.self.query.weight True
bert.encoder.layer.5.attention.self.query.bias True
bert.encoder.layer.5.attention.self.key.weight True
bert.encoder.layer.5.attention.self.key.bias True
bert.encoder.layer.5.attention.self.value.weight True
bert.encoder.layer.5.attention.self.value.bias True
bert.encoder.layer.5.attention.output.dense.weight True
bert.encoder.layer.5.attention.output.dense.bias True
bert.encoder.layer.5.attention.output.LayerNorm.weight True
bert.encoder.layer.5.attention.output.LayerNorm.bias True
bert.encoder.layer.5.intermediate.dense.weight True
bert.encoder.layer.5.intermediate.dense.bias True
bert.encoder.layer.5.output.dense.weight True
bert.encoder.layer.5.output.dense.bias True
bert.encoder.layer.5.output.LayerNorm.weight True
bert.encoder.layer.5.output.LayerNorm.bias True
bert.encoder.layer.6.attention.self.query.weight True
bert.encoder.layer.6.attention.self.query.bias True
bert.encoder.layer.6.attention.self.key.weight True
bert.encoder.layer.6.attention.self.key.bias True
bert.encoder.layer.6.attention.self.value.weight True
bert.encoder.layer.6.attention.self.value.bias True
bert.encoder.layer.6.attention.output.dense.weight True
bert.encoder.layer.6.attention.output.dense.bias True
bert.encoder.layer.6.attention.output.LayerNorm.weight True
bert.encoder.layer.6.attention.output.LayerNorm.bias True
bert.encoder.layer.6.intermediate.dense.weight True
bert.encoder.layer.6.intermediate.dense.bias True
bert.encoder.layer.6.output.dense.weight True
bert.encoder.layer.6.output.dense.bias True
bert.encoder.layer.6.output.LayerNorm.weight True
bert.encoder.layer.6.output.LayerNorm.bias True
bert.encoder.layer.7.attention.self.query.weight True
bert.encoder.layer.7.attention.self.query.bias True
bert.encoder.layer.7.attention.self.key.weight True
bert.encoder.layer.7.attention.self.key.bias True
bert.encoder.layer.7.attention.self.value.weight True
bert.encoder.layer.7.attention.self.value.bias True
bert.encoder.layer.7.attention.output.dense.weight True
bert.encoder.layer.7.attention.output.dense.bias True
bert.encoder.layer.7.attention.output.LayerNorm.weight True
bert.encoder.layer.7.attention.output.LayerNorm.bias True
bert.encoder.layer.7.intermediate.dense.weight True
bert.encoder.layer.7.intermediate.dense.bias True
bert.encoder.layer.7.output.dense.weight True
bert.encoder.layer.7.output.dense.bias True
bert.encoder.layer.7.output.LayerNorm.weight True
bert.encoder.layer.7.output.LayerNorm.bias True
bert.encoder.layer.8.attention.self.query.weight True
bert.encoder.layer.8.attention.self.query.bias True
bert.encoder.layer.8.attention.self.key.weight True
bert.encoder.layer.8.attention.self.key.bias True
bert.encoder.layer.8.attention.self.value.weight True
bert.encoder.layer.8.attention.self.value.bias True
bert.encoder.layer.8.attention.output.dense.weight True
bert.encoder.layer.8.attention.output.dense.bias True
bert.encoder.layer.8.attention.output.LayerNorm.weight True
bert.encoder.layer.8.attention.output.LayerNorm.bias True
bert.encoder.layer.8.intermediate.dense.weight True
bert.encoder.layer.8.intermediate.dense.bias True
bert.encoder.layer.8.output.dense.weight True
bert.encoder.layer.8.output.dense.bias True
bert.encoder.layer.8.output.LayerNorm.weight True
bert.encoder.layer.8.output.LayerNorm.bias True
bert.encoder.layer.9.attention.self.query.weight True
bert.encoder.layer.9.attention.self.query.bias True
bert.encoder.layer.9.attention.self.key.weight True
bert.encoder.layer.9.attention.self.key.bias True
bert.encoder.layer.9.attention.self.value.weight True
bert.encoder.layer.9.attention.self.value.bias True
bert.encoder.layer.9.attention.output.dense.weight True
bert.encoder.layer.9.attention.output.dense.bias True
bert.encoder.layer.9.attention.output.LayerNorm.weight True
bert.encoder.layer.9.attention.output.LayerNorm.bias True
bert.encoder.layer.9.intermediate.dense.weight True
bert.encoder.layer.9.intermediate.dense.bias True
bert.encoder.layer.9.output.dense.weight True
bert.encoder.layer.9.output.dense.bias True
bert.encoder.layer.9.output.LayerNorm.weight True
bert.encoder.layer.9.output.LayerNorm.bias True
bert.encoder.layer.10.attention.self.query.weight True
bert.encoder.layer.10.attention.self.query.bias True
bert.encoder.layer.10.attention.self.key.weight True
bert.encoder.layer.10.attention.self.key.bias True
bert.encoder.layer.10.attention.self.value.weight True
bert.encoder.layer.10.attention.self.value.bias True
bert.encoder.layer.10.attention.output.dense.weight True
bert.encoder.layer.10.attention.output.dense.bias True
bert.encoder.layer.10.attention.output.LayerNorm.weight True
bert.encoder.layer.10.attention.output.LayerNorm.bias True
bert.encoder.layer.10.intermediate.dense.weight True
bert.encoder.layer.10.intermediate.dense.bias True
bert.encoder.layer.10.output.dense.weight True
bert.encoder.layer.10.output.dense.bias True
bert.encoder.layer.10.output.LayerNorm.weight True
bert.encoder.layer.10.output.LayerNorm.bias True
bert.encoder.layer.11.attention.self.query.weight True
bert.encoder.layer.11.attention.self.query.bias True
bert.encoder.layer.11.attention.self.key.weight True
bert.encoder.layer.11.attention.self.key.bias True
bert.encoder.layer.11.attention.self.value.weight True
bert.encoder.layer.11.attention.self.value.bias True
bert.encoder.layer.11.attention.output.dense.weight True
bert.encoder.layer.11.attention.output.dense.bias True
bert.encoder.layer.11.attention.output.LayerNorm.weight True
bert.encoder.layer.11.attention.output.LayerNorm.bias True
bert.encoder.layer.11.intermediate.dense.weight True
bert.encoder.layer.11.intermediate.dense.bias True
bert.encoder.layer.11.output.dense.weight True
bert.encoder.layer.11.output.dense.bias True
bert.encoder.layer.11.output.LayerNorm.weight True
bert.encoder.layer.11.output.LayerNorm.bias True
bert.pooler.dense.weight True
bert.pooler.dense.bias True
classifier.weight True
classifier.bias True
</code></pre>
",8,4,2122,2022-07-04 09:09:59,https://stackoverflow.com/questions/72854302/are-the-pre-trained-layers-of-the-huggingface-bert-models-frozen
Top2Vec Model Failing To Train (Following Simple PyPi Tutorial),"<p>I am trying to follow this tutorial on PyPi (See Example -&gt; Train Model): <a href=""https://pypi.org/project/top2vec/"" rel=""nofollow noreferrer"">https://pypi.org/project/top2vec/</a></p>
<p>Very short amount of code, following it line by line:</p>
<pre><code>from top2vec import Top2Vec
from sklearn.datasets import fetch_20newsgroups

newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

model = Top2Vec(documents=newsgroups.data, speed=&quot;learn&quot;, workers=8)
</code></pre>
<p>I've tried running multiple times on different datasets, yet I keep running into the following error when training/building the model:</p>
<pre><code>UFuncTypeError: ufunc 'correct_alternative_cosine' did not contain a loop with signature matching types &lt;class 'numpy.dtype[float32]'&gt; -&gt; None
</code></pre>
<p>Has anyone encountered this error before and if so how have you fixed it? Otherwise, if anyone can run this same code please let me know if you run into the same error.</p>
<p>Thanks</p>
","python, nlp, bert-language-model, lda","<p>Solved this by moving from a Jupyter notebook in favor for a typical .py file, as well as cloning the library, installing the requirements to a fresh virtualenv and running the setup.py file.</p>
",0,0,260,2022-07-08 16:14:48,https://stackoverflow.com/questions/72914229/top2vec-model-failing-to-train-following-simple-pypi-tutorial
Why ML models installed with pip need to download something else after installation?,"<p>Why does this statement download the model? Why isn't it downloaded when I install the package with <code>pip3 install keybert</code>? How can I pre-load it to the docker image so it wouldn't be downloaded every time?</p>
<pre><code>from keybert import KeyBERT
kw_model = KeyBERT()
</code></pre>
<p>Right now my dockerfile does the following:</p>
<pre><code>RUN pip install --user -r requirements.txt
</code></pre>
<p>requirements.txt:</p>
<pre><code>google-cloud-pubsub==2.8.0
google-cloud-logging==2.6.0
requests==2.28.0
keybert==0.5.1
</code></pre>
","python, docker, pip, bert-language-model","<p>One potential solution is</p>
<ol>
<li>Run this code on your local computer to save a copy of the model to a local directory. e.g. save to a directory &quot;keybert&quot;</li>
</ol>
<pre><code>from keybert import KeyBERT
kw_model = KeyBERT()
kw_model.model.embedding_model.save(&quot;keybert&quot;)
</code></pre>
<ol start=""2"">
<li>Add the local copy of the model to the Docker image using the <code>COPY</code> command in the Dockerfile</li>
</ol>
<pre><code># Copy local code to the container image.
COPY ./keybert/ ./keybert/
</code></pre>
<ol start=""3"">
<li>In your script running in the Docker container, load the model from the directory</li>
</ol>
<pre><code>from keybert import KeyBERT
new_kw_model = KeyBERT(&quot;./keybert&quot;)
</code></pre>
<p>The reason for this behavior is that <code>KeyBERT</code> uses other SBERT models, and you can use KeyBERT with more than one model: <a href=""https://maartengr.github.io/KeyBERT/guides/embeddings.html"" rel=""nofollow noreferrer"">https://maartengr.github.io/KeyBERT/guides/embeddings.html</a></p>
<p>So you'd add a copy of whichever model best suits your purposes to the Docker image</p>
",2,1,320,2022-07-12 06:36:21,https://stackoverflow.com/questions/72947973/why-ml-models-installed-with-pip-need-to-download-something-else-after-installat
Finding the scores for each tweet with a BERT-based sentiment analysis model,"<p>I am doing a sentiment analysis of twitter posts and I have a question regarding “German Sentiment Classification with Bert”:</p>
<p>I would like to display the sentiment score (positive, negative, neutral) for each tweet like it is shown on the models card on <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">huggingface</a>(screenshot)
I tried to go through the implementation of the mode by stepping into every line of code but could not figure out how to find the scores.</p>
<p><a href=""https://i.sstatic.net/PPa99.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PPa99.png"" alt=""enter image description here"" /></a></p>
<p>My code is based on the following code:</p>
<pre><code>
model = SentimentModel()

texts = [
    &quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]
       
result = model.predict_sentiment(texts)
print(result)
</code></pre>
","python, twitter, sentiment-analysis, huggingface-transformers, bert-language-model","<p>you can inherit from the model's class and define a function to output the scores:</p>
<pre><code>from typing import List
import torch
from germansentiment import SentimentModel


class SentimentModel(SentimentModel):
    def __init__(self):
        super().__init__()
        
    def predict_sentiment_proba(self, texts: List[str])-&gt; List[str]:
        texts = [self.clean_text(text) for text in texts]
        # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
        # truncation=True limits number of tokens to model's limitations (512)
        encoded = self.tokenizer.batch_encode_plus(texts, padding=True, add_special_tokens=True,truncation=True, return_tensors=&quot;pt&quot;)
        
        encoded = encoded.to(self.device)
        with torch.no_grad():
                logits = self.model(**encoded)
        
        #label_ids = torch.argmax(logits[0], axis=1)
        return torch.nn.Softmax(dim=1)(logits[0]), self.model.config.id2label
   
texts = [&quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]

model = SentimentModel()
scores, ids = model.predict_sentiment_proba(texts)

scores
&gt;tensor([[1.1602e-03, 9.9877e-01, 6.8676e-05],
        [8.8440e-04, 9.9909e-01, 2.3437e-05],
        [9.8738e-01, 1.2542e-02, 7.6997e-05],
        [9.7940e-01, 2.0516e-02, 8.2444e-05],
        [4.1755e-04, 4.6088e-04, 9.9912e-01],
        [2.1236e-05, 5.3932e-05, 9.9992e-01]])
ids
&gt;{0: 'positive', 1: 'negative', 2: 'neutral'}

scores.argmax(dim=-1)
&gt;tensor([1, 1, 0, 0, 2, 2]) #negative, negative, positive, positive, neutral, neutral
</code></pre>
",2,2,1232,2022-07-12 16:46:08,https://stackoverflow.com/questions/72955752/finding-the-scores-for-each-tweet-with-a-bert-based-sentiment-analysis-model
Can I remove some topics from a BERTopic model?,"<p>I trained a BERTopic model and analysed the resulting topics. About half of them are good, but the others I don't need. Can I remove them from the model, to get faster predictions?</p>
","nlp, bert-language-model, topic-modeling","<p>I had the same question and I asked in github discussions for the package. If you ask there the package author answers very quickly.</p>
<p>Here is his answer to our question:</p>
<p>&quot;Deleting topics is unlikely to help with speeding up the model in .transform as it is not possible to do that easily in the underlying models. Instead, I would either advise using the slower model and use .merge_topics to merge all unwanted topics into a single topic, so that it is easier to identify those. Or you can adjust the min_topic_size a bit lower to get a balance between helpful topics and speed of the transform function.</p>
<p>Do note that the transform function can be speed up by a number of different ways. For example, if you have an older GPU then embedding the documents can be much slower. In practice, it is helpful to identify which steps of the algorithm are relatively slow for you. By setting verbose=True, you have some indication of the time spent at each of those steps. If UMAP is too slow for you, then you can consider using PCA instead.&quot; -c Maarten Grootendorst</p>
<p>Also note that you could improve the speed of .transform by enabling the gpu acceleration for the latter two stages (by default only the first stage is gpu accelerated). You will find the info on that here <a href=""https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#gpu-acceleration"" rel=""nofollow noreferrer"">https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#gpu-acceleration</a></p>
",2,1,1584,2022-07-14 15:29:16,https://stackoverflow.com/questions/72983056/can-i-remove-some-topics-from-a-bertopic-model
How to understand the answer_start parameter of Squad dataset for training BERT-QA model + practical implications for creating custom dataset?,"<p>I am in the process of creating a custom dataset to benchmark the accuracy of the <a href=""https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad"" rel=""nofollow noreferrer"">'bert-large-uncased-whole-word-masking-finetuned-squad'</a> model for my domain, to understand if I need to fine-tune further, etc.</p>
<p>When looking at the different Question Answering datasets on the Hugging Face site (<a href=""https://huggingface.co/datasets/squad"" rel=""nofollow noreferrer"">squad</a>, <a href=""https://huggingface.co/datasets/adversarial_qa"" rel=""nofollow noreferrer"">adversarial_qa</a>, etc. ), I see that the answer is commonly formatted as a dictionary with keys: answer (the text) and answer_start (char index where answer starts).</p>
<p>I'm trying to understand:</p>
<ul>
<li>The intuition behind how the model uses the answer_start when calculating the loss, accuracy, etc.</li>
<li>If I need to go through the process of adding this to my custom dataset (easier to run model evaluation code, etc?)</li>
<li>If so, is there a programmatic way to do this to avoid manual effort?</li>
</ul>
<p>Any help or direction would be greatly appreciated!</p>
<p><strong>Code example to show format:</strong></p>
<pre><code>import datasets
ds = datasets.load_dataset('squad')
train = ds['train']
print('Example: \n')
print(train['answers'][0])
</code></pre>
","nlp, huggingface-transformers, bert-language-model, huggingface-datasets, squad","<p>Your question is a bit broad to give you a specific answer, but I will try my best to point you in some directions.</p>
<blockquote>
<p>The intuition behind how the model uses the answer_start when
calculating the loss, accuracy, etc.</p>
</blockquote>
<p>There are different types of QA tasks/datasets. The ones you mentioned (SQuAD and adversarial_qa) belong to the field of extractive question answering. There, a model must select a span from a given context that answers the given question. For example:</p>
<pre class=""lang-py prettyprint-override""><code>context = 'Second, Democrats have always elevated their minority floor leader to the speakership upon reclaiming majority status. Republicans have not always followed this leadership succession pattern. In 1919, for instance, Republicans bypassed James R. Mann, R-IL, who had been minority leader for eight years, and elected Frederick Gillett, R-MA, to be Speaker. Mann &quot;had angered many Republicans by objecting to their private bills on the floor;&quot; also he was a protégé of autocratic Speaker Joseph Cannon, R-IL (1903–1911), and many Members &quot;suspected that he would try to re-centralize power in his hands if elected Speaker.&quot; More recently, although Robert H. Michel was the Minority Leader in 1994 when the Republicans regained control of the House in the 1994 midterm elections, he had already announced his retirement and had little or no involvement in the campaign, including the Contract with America which was unveiled six weeks before voting day.'
question='How did Republicans feel about Mann in 1919?' 
answer='angered' #-&gt; starting at character 365
</code></pre>
<p>A simple approach that is often used today, is a linear layer that predicts the answer start and answer end from the last hidden state of a transformer encoder (<a href=""https://github.com/huggingface/transformers/blob/d0acc9537829e7d067edbb791473bbceb2ecf056/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L1469-L1482"" rel=""nofollow noreferrer"">code example</a>). The last hidden state holds one vector for each input token (token!= words) and the linear layer is trained to assign high probabilities to tokens that could potentially be the start and end of the answer span. To train a model with your data, the loss function needs to know which tokens should get a high probability (i.e. the answer and the start token).</p>
<blockquote>
<p>If I need to go through the process of adding this to my custom
dataset (easier to run model evaluation code, etc?)</p>
</blockquote>
<p>You should go through this process, otherwise, how should someone know where the answer starts in your context? They can of course interfere with it programmatically, but what if your answer string appears twice in the context? Providing an answer start position avoids confusion and allows your users to use it right away with one of the many extractive questions answering scripts that are already available out there.</p>
<blockquote>
<p>If so, is there a programmatic way to do this to avoid manual effort?</p>
</blockquote>
<p>You could simply loop through your dataset and use <a href=""https://docs.python.org/3/library/stdtypes.html#str.find"" rel=""nofollow noreferrer"">str.find</a>:</p>
<pre class=""lang-py prettyprint-override""><code>context.find(answer)
</code></pre>
<p>Output:</p>
<pre><code>365
</code></pre>
",3,1,1577,2022-07-15 16:25:12,https://stackoverflow.com/questions/72997028/how-to-understand-the-answer-start-parameter-of-squad-dataset-for-training-bert
prediction logits using lxmert with hugging face library,"<p>how can we get the prediction logits in the lxmert model using hugging face library? It's fairly easy to get in visualbert, but I'm not able to get it with the lxmert model. In case of visualbert model, the keys I'm getting are :</p>
<pre class=""lang-py prettyprint-override""><code>['prediction_logits', 'seq_relationship_logits', 'attentions']
</code></pre>
<p>and with the help of lxmert mode, the keys are :</p>
<pre class=""lang-py prettyprint-override""><code>['language_output', 'vision_output', 'pooled_output', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions']
</code></pre>
<p>Even though there's a mention of prediction logits in the documentation I am not able to get them, if someone can help that would be great.</p>
<p>EDIT : <a href=""https://colab.research.google.com/drive/103TRUWj6mXPIERuvdKgLBbSF8N_SfEe1?usp=sharing"" rel=""nofollow noreferrer"">Link</a> to colab notebook for lxmert.</p>
","python, image-processing, huggingface-transformers, bert-language-model, multimodal","<p>Use <a href=""https://huggingface.co/docs/transformers/model_doc/lxmert#transformers.LxmertForPreTraining"" rel=""nofollow noreferrer"">LxmertForPreTraining</a> instead of <a href=""https://huggingface.co/docs/transformers/model_doc/lxmert#transformers.LxmertModel"" rel=""nofollow noreferrer"">LxmertModel</a>:</p>
<pre class=""lang-py prettyprint-override""><code>###Colab commands
#pip install transformers
#!git clone https://github.com/huggingface/transformers
#cd transformers
#cd examples/research_projects/lxmert
#pip install wget

from IPython.display import clear_output, Image, display
import PIL.Image
import io
import json
import torch
import numpy as np
from processing_image import Preprocess
from visualizing_image import SingleImageViz
from modeling_frcnn import GeneralizedRCNN
from utils import Config
import utils
import wget
import pickle
import os
import cv2
from copy import deepcopy

torch.cuda.is_available()

URL = &quot;https://github.com/jacobgil/vit-explain/raw/main/examples/both.png&quot;

frcnn_cfg = Config.from_pretrained(&quot;unc-nlp/frcnn-vg-finetuned&quot;)
frcnn = GeneralizedRCNN.from_pretrained(&quot;unc-nlp/frcnn-vg-finetuned&quot;, config=frcnn_cfg)
image_preprocess = Preprocess(frcnn_cfg)

# run frcnn
images, sizes, scales_yx = image_preprocess(URL)
output_dict = frcnn(
    images,
    sizes,
    scales_yx=scales_yx,
    padding=&quot;max_detections&quot;,
    max_detections=frcnn_cfg.max_detections,
    return_tensors=&quot;pt&quot;,
)

# Very important that the boxes are normalized
normalized_boxes = output_dict.get(&quot;normalized_boxes&quot;)
features = output_dict.get(&quot;roi_features&quot;)

from transformers import LxmertTokenizer, LxmertForPreTraining
import torch

tokenizer = LxmertTokenizer.from_pretrained(&quot;unc-nlp/lxmert-base-uncased&quot;)
model = LxmertForPreTraining.from_pretrained(&quot;unc-nlp/lxmert-base-uncased&quot;)

text_sentence = &quot;dog and cat are in the room and &quot; + tokenizer.mask_token + &quot; is laying on the ground&quot;

inputs = tokenizer(text_sentence, return_token_type_ids=True, return_attention_mask=True, add_special_tokens=True, return_tensors=&quot;pt&quot;)

visual_feats = features
visual_attention_mask = torch.ones(features.shape[:-1], dtype=torch.long)
visual_pos=normalized_boxes

inputs.update(
    {
        &quot;visual_feats&quot;: visual_feats,
        &quot;visual_pos&quot;: visual_pos,
        &quot;visual_attention_mask&quot;: visual_attention_mask,
    }
)

model_outputs = model(**inputs, output_attentions=True)

model_outputs.keys()
</code></pre>
<p>Output:</p>
<pre><code>odict_keys(['prediction_logits', 'cross_relationship_score', 'question_answering_score', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions'])
</code></pre>
<p>P.S.: You can control the pertaining task heads via the configuration fields <code>task_matched</code>, <code>task_mask_lm</code>, <code>task_obj_predict</code>, and <code>task_qa</code>. I assume you are only interested in <code>mask_lm</code> following your comment. That means you should initialize your model as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import LxmertConfig, LxmertForPreTraining

config = LxmertConfig.from_pretrained(&quot;unc-nlp/lxmert-base-uncased&quot;)
config.task_matched = False
config.task_obj_predict=False
config.task_qa= False
model = LxmertForPreTraining.from_pretrained(&quot;unc-nlp/lxmert-base-uncased&quot;, config=config)
</code></pre>
",1,1,337,2022-07-22 14:46:32,https://stackoverflow.com/questions/73082185/prediction-logits-using-lxmert-with-hugging-face-library
BERT pre-training from scratch with tensorflow version 2.x,"<p>i used <code>run_pretraining.py</code> (<a href=""https://github.com/google-research/bert/blob/master/run_pretraining.py"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/master/run_pretraining.py</a>) python script in <code>tensorflow version 1.15.5</code> version before. I use Google cloud TPU, as well.
Is it possible or any python script for BERT pre-training from scratch on TPU using tensorflow version 2.x ?</p>
","python, tensorflow, bert-language-model, tpu","<p>Yes you can use <a href=""https://github.com/tensorflow/models/tree/master/official/nlp"" rel=""nofollow noreferrer"">NPL library</a> from <a href=""https://github.com/tensorflow/models/tree/master/official"" rel=""nofollow noreferrer"">TF2 model garden</a>.</p>
<p>The instructions for creating training data and running pretraining are here:
<a href=""https://github.com/tensorflow/models/blob/master/official/nlp/docs/train.md#pre-train-a-bert-from-scratch"" rel=""nofollow noreferrer"">nlp/docs/train.md#pre-train-a-bert-from-scratch</a>.</p>
<p>You can also follow <a href=""https://cloud.google.com/tpu/docs/tutorials/bert-2.x"" rel=""nofollow noreferrer"">BERT Fine Tuning with Cloud TPU</a> tutorial with some changes to run pretraining script instead of fine tuning.</p>
",2,2,1021,2022-07-25 10:34:23,https://stackoverflow.com/questions/73107793/bert-pre-training-from-scratch-with-tensorflow-version-2-x
The essence of learnable positional embedding? Does embedding improve outcomes better?,"<p>I was recently reading the bert source code from the hugging face project. I noticed that the so-called &quot;learnable position encoding&quot; seems to refer to a specific nn.Parameter layer when it comes to implementation.</p>
<pre class=""lang-py prettyprint-override""><code>def __init__(self):
    super()
    positional_encoding = nn.Parameter()
def forward(self, x):
    x += positional_encoding
</code></pre>
<p>↑ Could be this feeling, then performed the learnable position encoding. Whether that means it's that simple or not, I'm not sure I understand it correctly, I want to ask someone with experience.</p>
<p>In addition, I noticed a classic bert structure whose location is actually coded only once at the initial input. Does this mean that the subsequent bert layers, for each other, lose the ability to capture location information?</p>
<pre class=""lang-py prettyprint-override""><code>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(...)
      ...
  (pooler): BertPooler(...)
</code></pre>
<p>Would I get better results if the results of the previous layer were re-positional encoded before the next BERT layer?</p>
","deep-learning, pytorch, bert-language-model, transformer-model","<p><strong>What is the purpose of positional embeddings?</strong></p>
<p>In transformers (BERT included) the only interaction between the different tokens is done via self-attention layers. If you look closely at the mathematical operation implemented by these layers you will notice that these layers are <strong>permutation <a href=""https://en.wikipedia.org/wiki/Equivariant_map"" rel=""noreferrer"">equivariant</a></strong>: That is, the representation of<br />
<em>&quot;I do like coding&quot;</em><br />
and<br />
<em>&quot;Do I like coding&quot;</em><br />
is the same, because the words (=tokens) are the same in both sentences, only their order is different.<br />
As you can see, this &quot;permutation equivariance&quot; is not a desired property in many cases.<br />
To break this symmetry/equivariance one can simply &quot;code&quot; the actual position of each word/token in the sentence. For example:<br />
<em>&quot;I_1 do_2 like_3 coding_4&quot;</em><br />
is no longer identical to<br />
<em>&quot;Do_1 I_2 like_3 coding_4&quot;</em></p>
<p>This is the purpose of positional encoding/embeddings -- to make self-attention layers sensitive to the order of the tokens.</p>
<p>Now to your questions:</p>
<ol>
<li>learnable position encoding is indeed implemented with a simple single <code>nn.Parameter</code>. The position encoding is just a &quot;code&quot; added to each token marking its position in the sequence. Therefore, all it requires is a tensor of the same size as the input sequence with different values per position.</li>
<li><em>Is it enough to introduce position encoding once in a transformer architecture?</em>  Yes! Since transformers stack multiple self-attention layers it is enough to add positional embeddings once at the beginning of the processing. The position information is &quot;fused&quot; into the semantic representation learned per token.<br />
A nice visualization of this effect in Vision Transformers (ViT) can be found in this work:<br />
<em>Shir Amir, Yossi Gandelsman, Shai Bagon and Tali Dekel</em> <a href=""https://arxiv.org/abs/2112.05814"" rel=""noreferrer""><strong>Deep ViT Features as Dense Visual Descriptors</strong></a> (arXiv 2021).<br />
In sec. 3.1 and fig. 3 they show how the position information dominates the representation of tokens at early layers, but as you go deeper in a transformer, semantic information takes over.</li>
</ol>
",16,7,10886,2022-07-25 17:37:50,https://stackoverflow.com/questions/73113261/the-essence-of-learnable-positional-embedding-does-embedding-improve-outcomes-b
Equivalent to tokenizer() in Transformers 2.5.0?,"<p>I am trying to convert the following code to work with Transformers 2.5.0. As written, it works in version 4.18.0, but not 2.5.0.</p>
<pre><code># Converting pretrained BERT classification model to regression model
# i.e. extracting base model and swapping out heads

from transformers import BertTokenizer, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification, AutoConfig, AutoModelForTokenClassification
import torch
import numpy as np

old_model = BertForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-yelp-polarity&quot;)
model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=1) 
model.bert = old_model.bert

# Ensure that model parameters are equivalent except for classifier head layer
for param_name in model.state_dict():
    if 'classifier' not in param_name:
        sub_param, full_param = model.state_dict()[param_name], old_model.state_dict()[param_name] # type: torch.Tensor, torch.Tensor
        assert (sub_param.cpu().numpy() == full_param.cpu().numpy()).all(), param_name


tokenizer = BertTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-yelp-polarity&quot;)
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**inputs).logits

output_value = np.array(logits)[0][0]
print(output_value)
</code></pre>
<p>tokenizer is not callable with transformers 2.5.0, resulting the following:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-d83f0d613f4b&gt; in &lt;module&gt;
     19 
     20 
---&gt; 21 inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
     22 
     23 with torch.no_grad():

TypeError: 'BertTokenizer' object is not callable
</code></pre>
<p>However, attempting to replace tokenizer() with tokenizer.tokenize() results in the following:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-1d431131eb87&gt; in &lt;module&gt;
     21 
     22 with torch.no_grad():
---&gt; 23     logits = model(**inputs).logits
     24 
     25 output_value = np.array(logits)[0][0]

TypeError: BertForSequenceClassification object argument after ** must be a mapping, not list
</code></pre>
<p>Any help would be greatly appreciated.</p>
<hr />
<h2>Solution</h2>
<p>Using tokenizer.encode_plus() as suggested by @cronoik:</p>
<pre><code>tokenized = tokenizer.encode_plus(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**tokenized)

output_value = np.array(logits)[0]
print(output_value)
</code></pre>
","pytorch, tokenize, huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>Sadly their documentation for the old versions is broken, but you can use encode_plus as shown in the following (he oldest available documentation of encode_plus is from <a href=""https://huggingface.co/transformers/v2.10.0/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus"" rel=""nofollow noreferrer"">2.10.0</a>):</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizer


t = BertTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-yelp-polarity&quot;)
tokenized = t.encode_plus(&quot;Hello, my dog is cute&quot;, return_tensors='pt')
print(tokenized)
</code></pre>
<p>Output:</p>
<pre><code>{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}
</code></pre>
",0,1,394,2022-07-26 16:55:11,https://stackoverflow.com/questions/73127139/equivalent-to-tokenizer-in-transformers-2-5-0
How to get the word on which the text classification has been made?,"<p>I am doing a multi-label text classification using a pre-trained model of BERT. Here is an example of the prediction that has been made for one sentence-
<a href=""https://i.sstatic.net/phwu9.jpg"" rel=""nofollow noreferrer"">pred_image</a></p>
<p>I want to get those words from the sentence on which the prediction has been made. Like this one - <a href=""https://i.sstatic.net/Xc4ij.jpg"" rel=""nofollow noreferrer"">right_one</a></p>
<p>If anyone has any idea, Please enlighten me.</p>
","nlp, text-classification, bert-language-model, multilabel-classification","<p>Multi-Label <a href=""https://huggingface.co/tasks/text-classification"" rel=""nofollow noreferrer"">Text Classification</a> (first image) and <a href=""https://huggingface.co/tasks/token-classification"" rel=""nofollow noreferrer"">Token Classification</a> (second image) are two different tasks for each which the model needs to be specifally trained for.</p>
<p>The first one returns a probability for each label considering the entire sentence. The second returns such predictions for each single word in the sentence while usually considering the rest of the sentence as context.</p>
<p>So you can not really use the output from a Text Classifier and use it for Token Classification because the information you get is not detailed enough.</p>
<p>What you can and should do is train a Token Classification model, although you obviously will need token-level-annotated data to do so.</p>
",0,0,261,2022-07-29 04:29:13,https://stackoverflow.com/questions/73161662/how-to-get-the-word-on-which-the-text-classification-has-been-made
How does NLP model know the output length during translation tasks?,"<p>Translating English to French, we may have this:</p>
<p>Input: &quot;Please help me translate this sentence&quot;            6 tokens
Output: &quot;Merci de m'aider à traduire cette phrase&quot;         7 tokens</p>
<p>We have 7 tokens in the output. How does Bert model know this length during the network processing? Which hyperparameters are involved?</p>
","nlp, huggingface-transformers, bert-language-model, transformer-model, sentence-transformers","<p>A sequence to sequence model (for translation) does not usually &quot;know&quot; the length of the output it predicts, but rather:</p>
<ul>
<li>encodes a full sentence to a (often fixed size) representation</li>
<li>then, from said representation, predicts one output item at a time, until an &quot;EOS&quot; (end of sequence) item (token) is predicted, which marks the end of the prediction.</li>
</ul>
<p>BERT specifically only does the first part, and creates a mathematical sentence representation from an input.</p>
",0,-1,139,2022-08-03 03:36:49,https://stackoverflow.com/questions/73216050/how-does-nlp-model-know-the-output-length-during-translation-tasks
Load estimator from model artifact in s3 bucket aws,"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code</p>
<pre><code>estimator = PyTorch(
    entry_point=&quot;train_deploy.py&quot;,
    source_dir=&quot;code&quot;,
    role=role,
    framework_version=&quot;1.3.1&quot;,
    py_version=&quot;py3&quot;,
    instance_count=1,  # this script only support distributed training for GPU instances.
    instance_type=&quot;ml.m5.12xlarge&quot;,
    output_path=output_path,
    hyperparameters={
        &quot;epochs&quot;: 1,
        &quot;num_labels&quot;: 7,
        &quot;backend&quot;: &quot;gloo&quot;,
    },
    disable_profiler=False, # disable debugger
)
estimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})
</code></pre>
<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below</p>
<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)
</code></pre>
","amazon-web-services, amazon-s3, pytorch, amazon-sagemaker, bert-language-model","<p>I was able to solve this by the following steps</p>
<pre><code>model_data=output_path
from sagemaker.pytorch.model import PyTorchModel 

pytorch_model = PyTorchModel(model_data=model_data,
                             role=role,
                             framework_version=&quot;1.3.1&quot;,
                             source_dir=&quot;code&quot;,
                             py_version=&quot;py3&quot;,
                             entry_point=&quot;train_deploy.py&quot;)

predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)
predictor.serializer = sagemaker.serializers.JSONSerializer()
predictor.deserializer = sagemaker.deserializers.JSONDeserializer()
result = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)
print(&quot;predicted class: &quot;, np.argmax(result, axis=1))
</code></pre>
",1,0,362,2022-08-03 06:06:27,https://stackoverflow.com/questions/73216926/load-estimator-from-model-artifact-in-s3-bucket-aws
Why was BERT&#39;s default vocabulary size set to 30522?,"<p>I have been trying to build a BERT model for a specific domain. However, my model is trained on non-English text, so I'm worried that the default token size, 30522, won't fit my model.</p>
<p>Does anyone know where the number 30522 came from?</p>
<p>I expect that researchers were fine-tuning their model by focusing on training time and vocabulary coverage, but a more clear explanation will be appreciated.</p>
","tokenize, bert-language-model","<p>The number of 30522 is not &quot;token size.&quot; It's the size of WordPiece vocabulary BERT was trained on. See this <a href=""https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble"">link</a> for an explanation of WordPiece. The number 30522 likely means the base character set was 522 characters in size and the WordPiece algorithm was trained on 30,000 iterations.</p>
",1,2,2652,2022-08-04 08:06:45,https://stackoverflow.com/questions/73232413/why-was-berts-default-vocabulary-size-set-to-30522
get contrastive_logits_per_image with flava model using huggingface library,"<p>I have used a code of Flava model from this link:</p>
<pre><code>https://huggingface.co/docs/transformers/model_doc/flava#transformers.FlavaModel.forward.example
</code></pre>
<p>But I am getting the following error:</p>
<pre><code>'FlavaModelOutput' object has no attribute 'contrastive_logits_per_image'
</code></pre>
<p>I tried using <code>FlavaForPreTraining</code> model instead, so updated code was :</p>
<pre class=""lang-py prettyprint-override""><code>from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaForPreTraining

model = FlavaForPreTraining.from_pretrained(&quot;facebook/flava-full&quot;)
processor = FlavaProcessor.from_pretrained(&quot;facebook/flava-full&quot;)

url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[&quot;a photo of a cat&quot;], images=image, return_tensors=&quot;pt&quot;, padding=True, return_codebook_pixels = True)

inputs.update(
    {
        &quot;input_ids_masked&quot;: inputs.input_ids,
    }
)

outputs = model(**inputs)

logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
</code></pre>
<p>but I'm still getting this as error:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:714: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  &quot;The `device` argument is deprecated and will be removed in v5 of Transformers.&quot;, FutureWarning

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-44-bdb428b8184a&gt; in &lt;module&gt;()
----&gt; 1 outputs = model(**inputs)

2 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130             return forward_call(*input, **kwargs)
   1131         # Do not call functions when jit is used
   1132         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in forward(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)
   1968             if mim_labels is not None:
   1969                 mim_labels = self._resize_to_2d(mim_labels)
-&gt; 1970                 bool_masked_pos = self._resize_to_2d(bool_masked_pos)
   1971                 mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index
   1972 

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in _resize_to_2d(self, x)
   1765 
   1766     def _resize_to_2d(self, x: torch.Tensor):
-&gt; 1767         if x.dim() &gt; 2:
   1768             x = x.view(x.size(0), -1)
   1769         return x

AttributeError: 'NoneType' object has no attribute 'dim'
</code></pre>
<p>Can anyone provide suggestions with what's going wrong?</p>
","python-3.x, image-processing, huggingface-transformers, bert-language-model, multimodal","<p>FLAVA's author here.</p>
<p>Can you please add the following arguments to your processor call:</p>
<pre><code>return_codebook_pixels=True, return_image_mask=True
</code></pre>
<p>Here is an example colab if you want to see how to call FLAVA model: <a href=""https://colab.research.google.com/drive/1c3l4r4cEA5oXfq9uXhrJibddwRkcBxzP?usp=sharing#scrollTo=xtkrSjfhCdv-"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1c3l4r4cEA5oXfq9uXhrJibddwRkcBxzP?usp=sharing#scrollTo=xtkrSjfhCdv-</a></p>
",2,1,347,2022-08-06 06:40:35,https://stackoverflow.com/questions/73257704/get-contrastive-logits-per-image-with-flava-model-using-huggingface-library
BerTopic Model - Visualization ignores 0th index,"<p>The <a href=""https://maartengr.github.io/BERTopic/"" rel=""nofollow noreferrer"">BerTopic</a> model resulted the below Topics:</p>
<p><a href=""https://i.sstatic.net/CNkp6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CNkp6.png"" alt=""enter image description here"" /></a></p>
<p>As you can see from the above, the model is finetuned to generate lesser outliers '-1' which has the count of 3 and it appears in the last.</p>
<p>While visualizing the <a href=""https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html"" rel=""nofollow noreferrer"">Topics per class</a>,</p>
<p><code>topic_model.visualize_topics_per_class(topics_per_class)</code></p>
<p>the below interactive visual is generated, and however it ignored the <code>0th</code> index, to be precise the Topic 0. The Global Topic Representations are displayed from <code>1, 2, 3, 4, 5, 6, -1</code></p>
<p><a href=""https://i.sstatic.net/fAdtt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fAdtt.png"" alt=""enter image description here"" /></a></p>
<p>Is the BerTopic designed in a way that it always assumes the very first index will be an outlier (<code>-1</code>), and eliminates it blindly?</p>
<p>Are the generated topics always accessed based on the count size, may be in descending order?</p>
","python, data-science, artificial-intelligence, bert-language-model, topic-modeling","<p>This issue is posted in the BerTopic <a href=""https://github.com/MaartenGr/BERTopic/issues/667"" rel=""nofollow noreferrer"">github</a> forum as well, and the response from the Author himself,</p>
<p><a href=""https://i.sstatic.net/xMlqY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xMlqY.png"" alt=""enter image description here"" /></a></p>
<p>by setting <code>top_n_topics=None</code>, all the topics along with the <code>0th</code> index can be viewed while visualizing,</p>
<p><code>topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=None)</code></p>
",0,0,330,2022-08-10 06:06:55,https://stackoverflow.com/questions/73301384/bertopic-model-visualization-ignores-0th-index
How to calculate perplexity of BERTopic?,"<p>Is there a way to calculate the perplexity of <strong>BERTopic</strong>? I am unable to find any such thing in the BERTopic library and in other places.</p>
","bert-language-model, topic-modeling, perplexity","<p>I managed to figure it out how to get the log perplexity, and then convert it back</p>
<pre><code>import numpy as np
model = BERTopic(top_n_words =15,
                   calculate_probabilities=True)

topics, probs = model.fit_transform(docs) # docs = dataset
log_perplexity = -1 * np.mean(np.log(np.sum(probs, axis=1)))
perplexity = np.exp(log_perplexity)
</code></pre>
",4,2,1323,2022-08-16 06:29:21,https://stackoverflow.com/questions/73369640/how-to-calculate-perplexity-of-bertopic
How can we pass a list of strings to a fine tuned bert model?,"<p>I want  to pass a list of strings instead of a single string input to my fine tuned bert question classification model.
This is my code which accept a single string input.</p>
<pre><code>questionclassification_model = tf.keras.models.load_model('/content/drive/MyDrive/questionclassification_model')
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text):
    token = tokenizer.encode_plus(
        input_text,
        max_length=256, 
        truncation=True, 
        padding='max_length', 
        add_special_tokens=True,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token['input_ids'], tf.float64),
        'attention_mask': tf.cast(token['attention_mask'], tf.float64)
    }

def make_prediction(model, processed_data, classes=['Easy', 'Medium', 'Hard']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)],probs;
</code></pre>
<p>I don't want to use a for loop over the list as it takes more execution time.
when I tried to pass a list as input to the tokenizer it was returning same output for every input.</p>
<pre><code>input_text = [&quot;What is gandhi commonly considered to be?,Father of the nation in india&quot;,&quot;What is the long-term warming of the planets overall temperature called?, Global Warming&quot;]
processed_data = prepare_data(input_text)
</code></pre>
<hr />
<p>{'input_ids': &lt;tf.Tensor: shape=(1, 256), dtype=float64, numpy=
array([[101., 100., 100., 102.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.]])&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1, 256), dtype=float64, numpy=
array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])&gt;}</p>
<hr />
<p>and that is not the right tokens for the input text.</p>
<p>Thanks in advance...</p>
","python, nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers","<h2>Different methods for one sentence vs batches</h2>
<p>There are different methods for encoding one sentence versus encoding a batch of sentences
According to the documentation (<a href=""https://huggingface.co/docs/transformers/v4.21.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/v4.21.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus</a>) the <code>encode_plus</code> method expects the first parameter to be &quot;This can be a string, a list of strings (<em><strong>tokenized string using the tokenize method</strong></em>) or a list of integers (tokenized string ids using the convert_tokens_to_ids method).&quot;
(emphasis mine) - so that if you're passing a list of strings to this particular method, they are interpreted as <em>a list of tokens</em>, not sentences, and obviously all those very long &quot;tokens&quot; like &quot;What is gandhi commonly considered to be?,Father of the nation in india&quot; do not match anything in the vocabulary so they get mapped to the out-of-vocabulary id.</p>
<p>If you want to encode a batch of sentences, then you need to pass your list of strings to the <code>batch_encode_plus</code> method (<a href=""https://huggingface.co/docs/transformers/v4.21.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/v4.21.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus</a>)</p>
",1,1,2875,2022-08-17 05:46:48,https://stackoverflow.com/questions/73383418/how-can-we-pass-a-list-of-strings-to-a-fine-tuned-bert-model
Is it possible to get the meaning of each word using BERT?,"<p>I'm a linguist, I'm new to AI and</p>
<p>I would like to know if BERT is able to get the meaning of each word based on context.</p>
<p>I've done some searches and found that BERT is able to do that and that if I'm not wrong, it recognizes them/ converts them into unique vectors, but that's not the output I want.</p>
<p>What I want is to get the meaning/ or the components that constitute the meaning of each word, written in plain English, is this possible?</p>
","huggingface-transformers, bert-language-model","<p>No, you can not get the meaning of the word in plain english. The whole idea of the BERT is to convert plain english into meaningful numerical representations.</p>
<p>Unfortunately, these vectors are <strong>not interpretable</strong>. It is a general limitation of Deep Learning compared to other traditional ML models that use self-extracted features.</p>
<p>But note that you can use these representation to find out <strong>certain relationships between words</strong>. For example the words that are close to each other (in terms of some distance measure), have similar meanings. Have a look at this link for more information.</p>
<p><a href=""https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html"" rel=""nofollow noreferrer"">https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html</a></p>
",2,1,211,2022-08-18 22:02:18,https://stackoverflow.com/questions/73409904/is-it-possible-to-get-the-meaning-of-each-word-using-bert
BART Tokenizer tokenises same word differently?,"<p>I have noticed that if I tokenize a full text with many sentences, I sometimes get a different number of tokens than if I tokenise each sentence individually and add up the tokens. I have done some debugging and have this small reproducible example to show the issue</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')

print(tokenizer.tokenize(&quot;Thames is a river&quot;))
print(tokenizer.tokenize(&quot;We are in London. Thames is a river&quot;))
</code></pre>
<p>I get the following output</p>
<pre><code>['Th', 'ames', 'Ġis', 'Ġa', 'Ġriver']
['We', 'Ġare', 'Ġin', 'ĠLondon', '.', 'ĠThames', 'Ġis', 'Ġa', 'Ġriver']
</code></pre>
<p>I would like to understand why the word Thames has been split into two tokens when it’s at the start of sequence, whereas it’s a single word if it’s not at the start of sequence. I have noticed this behaviour is very frequent and, assuming it’s not a bug, I would like to understand why the BART tokeniser behaves like this.</p>
","nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers, bart","<p>According to <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/tokenization_bart.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/tokenization_bart.py</a>:</p>
<p>This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not. You can get around that behavior by passing <code>add_prefix_space=True</code> when instantiating this tokenizer or when you call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.</p>
<p>Trying</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn', add_prefix_space=True)

print(tokenizer.tokenize(&quot;Thames is a river&quot;))
print(tokenizer.tokenize(&quot;We are in London. Thames is a river&quot;))
</code></pre>
<p>yields the 'correct' result to me.</p>
",3,2,993,2022-08-23 13:28:24,https://stackoverflow.com/questions/73459649/bart-tokenizer-tokenises-same-word-differently
"Getting &quot;ValueError: Shapes (None, 2) and (None, 1) are incompatible&quot; for binary classification when I am using &quot;tf.keras.metrics.Recall()&quot;","<p>I am working DistillBert project for binary classification. I am trying to run the following code using the Spam SMS data set (You can also use the IMDB dataset, it is also giving the same issue), which runs fine using 'accuracy' and 'sparse_categorical_accuracy' but gives an error when I am using tf.keras.metrics.Recall() or tf.keras.metrics.AUC(). Here I am using the SparseCategoricalCrossentropy loss function and Ada optimizer.</p>
<p>Dataset - The dataset used here is the spam SMS dataset which has binary labels 0 for normal SMS, and 1 for spam SMS. The same error can be reproduced using the IMDB data set for this code.</p>
<p>error -</p>
<pre><code>ValueError: Shapes (None, 2) and (None, 1) are incompatible
</code></pre>
<p>Code -</p>
<pre><code>import pandas as pd
import tensorflow as tf
import transformers
from transformers import DistilBertTokenizer
from transformers import TFAutoModelForSequenceClassification
pd.set_option('display.max_colwidth', None)
MODEL_NAME = 'distilbert-base-uncased'
BATCH_SIZE = 8
N_EPOCHS = 3

train = pd.read_csv(&quot;train_set.csv&quot;, error_bad_lines=False)
test = pd.read_csv(&quot;test_set.csv&quot;, error_bad_lines=False)

X_train = train.text
X_test = test.text
y_train = train.label
y_test = test.label

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(list(X_train.values),
                        truncation=True, 
                        padding=True)
test_encodings = tokenizer(list(X_test.values),
                       truncation=True, 
                       padding=True)

train_dataset = 
tf.data.Dataset.from_tensor_slices((dict(train_encodings),list(y_train.values)))

test_dataset = 
tf.data.Dataset.from_tensor_slices((dict(test_encodings),list(y_test.values)))
test_dataset2 = test_dataset.shuffle(buffer_size=1024).take(1000).batch(16)

model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)

losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer=optimizerr,
          loss=losss,
          metrics= 
['accuracy','sparse_categorical_accuracy',tf.keras.metrics.Recall()])

print(&quot;Evaluate Base model on test data&quot;)
results = model.evaluate(test_dataset2)
print(&quot;test loss, test acc:&quot;, results)
</code></pre>
<p>How can I get the recall, precision, AUC and other metrics scores for this code?</p>
<p>Edit - When I am using loss function</p>
<pre><code>loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)
</code></pre>
<p>The error that I am getting is -</p>
<pre><code>ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))
</code></pre>
","python, tensorflow, machine-learning, keras, bert-language-model","<p>You typically don't use sparse categorical cross entropy for a binary classification problem. Just use binary cross entropy. That is most likely why the tensor sizes don't match up.</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy</a></p>
",2,1,344,2022-08-28 01:54:53,https://stackoverflow.com/questions/73515496/getting-valueerror-shapes-none-2-and-none-1-are-incompatible-for-binary
Extend BERT or any transformer model using manual features,"<p>I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task.</p>
<p>You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers?</p>
<p>I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical.</p>
","huggingface-transformers, text-classification, bert-language-model","<p>The are several ways to achieve that. I will explain just two in the following answer:</p>
<ol>
<li>Add category as a token:<br />
The idea of this approach is rather simple when transformer models like BERT are able to produce contextualized embeddings for a given sentence, why can't we incorporate categorical features as text as well? For example, you use the title of a cited paper as input and also want to incorporate the research area of the paper to provide more context:</li>
</ol>
<pre><code>&quot;Attention is all you need. [Computer Science] [Machine Translation]&quot; -&gt; BERT
</code></pre>
<p>To do that, I would add the categories of your new feature as separate tokens to BERT (that is not required but reduces the sequence length) and fine-tune it for a few epochs:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForSequenceClassification

my_categories = [&quot;[Computer Science]&quot;, &quot;[Machine Translation]&quot;]
sentence=&quot;Attention is all you need. [Computer Science] [Machine Translation]&quot;

t= BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
m=BertForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)
# tokenized without separate tokens
print(len(t(sentence)[&quot;input_ids&quot;]))

# tokenized without separate tokens
t.add_tokens(my_categories)
print(len(t(sentence)[&quot;input_ids&quot;]))

# Extend embedding layer of model
m.resize_token_embeddings(len(t.get_vocab()))

# Training...
</code></pre>
<p>Output:</p>
<pre><code>18
12
Embedding(28998, 768, padding_idx=0)
</code></pre>
<ol start=""2"">
<li>Separate Embedding layer:<br />
A more traditional way is to hold an embedding for each category and concatenate (or any other method to combine features) it with the contextualized output of BERT before you feed it to the classification layer. For this approach, you can simply copy the code from huggingfaces <a href=""https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/bert/modeling_bert.py#L1510"" rel=""noreferrer"">BertForSequenceClassification</a> class (or whatever class you are using) and make the required changes:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
from transformers import BertPreTrainedModel, BertModel
from typing import Optional

class MyBertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = BertModel(config)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = torch.nn.Dropout(classifier_dropout)
        
        # Modified +20
        self.classifier = torch.nn.Linear(config.hidden_size +20, config.num_labels)

        # Modified 50 different categories embedding dimension 20 
        self.my_categorical_feature = torch.nn.Embedding(50,20)

        # Initialize weights and apply final processing
        self.post_init()

    # Modified new parameter categorical_feature_ids
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        categorical_feature_ids = None,
    ):

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        # Modified get embeddings
        my_categorical_embedding = self.my_categorical_feature(categorical_feature_ids)
        my_categorical_embedding = self.dropout(my_categorical_embedding)

        pooled_output = self.dropout(pooled_output)
        
        # Modified concatenate contextualized embeddings from BERT and your categorical embedding
        pooled_output = torch.cat((pooled_output, my_categorical_embedding), dim=-1)

        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = &quot;regression&quot;
                elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = &quot;single_label_classification&quot;
                else:
                    self.config.problem_type = &quot;multi_label_classification&quot;

            if self.config.problem_type == &quot;regression&quot;:
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == &quot;single_label_classification&quot;:
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == &quot;multi_label_classification&quot;:
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return {
            &quot;loss&quot;:loss,
            &quot;logits&quot;:logits,
            &quot;hidden_states&quot;:outputs.hidden_states,
            &quot;attentions&quot;:outputs.attentions,
        }
</code></pre>
<p>You can use this class just as the BertForSerquenceClassification class, the only difference is, that it expects <code>categorical_feature_ids</code> as additional input:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForSequenceClassification

t= BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
m= MyBertForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)

# batch with two sentences (i.e. the citation text you have already used) 
i = t([&quot;paper title 1&quot;, &quot;paper title 2&quot;], padding=True, return_tensors=&quot;pt&quot;)

# We assume that the first sentence (i.e. paper title 1) belongs to category 23 and the second sentence to category 42
# You probably want to use a dictionary in your own code 
i[&quot;categorical_feature_ids&quot;] = torch.tensor([23,42])

print(m(**i))
</code></pre>
<p>Output:</p>
<pre><code>{'loss': None, 
'logits': tensor([[ 0.6069, -0.1878], [ 0.6347, -0.2608]], grad_fn=&lt;AddmmBackward0&gt;), 
'hidden_states': None, 
'attentions': None}
</code></pre>
",6,6,2747,2022-09-01 09:25:45,https://stackoverflow.com/questions/73567055/extend-bert-or-any-transformer-model-using-manual-features
How to get hidden layer/state outputs from a Bert model?,"<p>Based on the documentation provided here, <a href=""https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/modeling_outputs.py#L101"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/modeling_outputs.py#L101</a>, how can i read all the outputs, last_hidden_state (), pooler_output and hidden_state. in my sample code below, i get the outputs</p>
<pre><code>from transformers import BertModel, BertConfig

config = BertConfig.from_pretrained(&quot;xxx&quot;, output_hidden_states=True)
model = BertModel.from_pretrained(&quot;xxx&quot;, config=config)

outputs = model(inputs)

</code></pre>
<p>when i print one of the output (sample below) . i looked through the documentation to see if i can use some functions of this class to just get the last_hidden_state values , but i'm not sure of the type here.</p>
<p>the value for the last_hidden_state =</p>
<pre><code>tensor([[...
</code></pre>
<p>is it some class or tuple or array .
how can i get the values or array of values such as</p>
<pre><code>[0, 1, 2, 3 , ...]
</code></pre>
<pre><code>BaseModelOutputWithPoolingAndNoAttention(
last_hidden_state=tensor([
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 
         11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 
         15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
        ...
        hidden_states= ...
        
</code></pre>
","huggingface-transformers, bert-language-model","<p>The <code>BaseModelOutputWithPoolingAndCrossAttentions</code> you retrieve is class that inherits from OrderedDict (<a href=""https://github.com/huggingface/transformers/blob/90f6fe9155d2f477588d9ba2d7c697a1933e205a/src/transformers/utils/generic.py#L148"" rel=""nofollow noreferrer"">code</a>) that holds <a href=""https://pytorch.org/docs/stable/tensors.html"" rel=""nofollow noreferrer"">pytorch tensors</a>. You can access the keys of the <code>OrderedDict</code> like properties of a class and, in case you do not want to work with Tensors, you can them to python lists or numpy. Please have a look at the example below:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel

t = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
m = BertModel.from_pretrained(&quot;bert-base-cased&quot;)

i = t(&quot;This is a test&quot;, return_tensors=&quot;pt&quot;)
o = m(**i, output_hidden_states=True)

print(o.keys())
print(type(o.last_hidden_state))
print(o.last_hidden_state.tolist())
print(o.last_hidden_state.detach().numpy())
</code></pre>
<p>Output:</p>
<pre><code>odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])
&lt;class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'&gt;
&lt;class 'torch.Tensor'&gt;
[[[0.36328405141830444, 0.018902940675616264, 0.1893523931503296, ..., 0.09052444249391556, 1.4617693424224854, 0.0774402841925621]]]
[[[ 0.36328405  0.01890294  0.1893524  ... -0.0259465   0.38701165
    0.19099694]
  [ 0.30656984 -0.25377586  0.76075834 ...  0.2055152   0.29494798
    0.4561815 ]
  [ 0.32563183  0.02308523  0.665546   ...  0.34597045 -0.0644953
    0.5391255 ]
  [ 0.3346715  -0.02526359  0.12209094 ...  0.50101244  0.36993945
    0.3237842 ]
  [ 0.18683438  0.03102166  0.25582778 ...  0.5166369  -0.1238729
    0.4419385 ]
  [ 0.81130844  0.4746894  -0.03862225 ...  0.09052444  1.4617693
    0.07744028]]]
</code></pre>
",4,1,3104,2022-09-08 02:06:57,https://stackoverflow.com/questions/73643066/how-to-get-hidden-layer-state-outputs-from-a-bert-model
Can I feed categorical data in Keras embedding layer without encoding the data?,"<p>I am trying to feed multicolumn categorical data into Keras embedding layer. Can I feed categorical data in Keras embedding layer without encoding ?</p>
<p>If not then which encoding method is preferable to retrieve contextual information from the categorical data ?</p>
","tensorflow, keras, bert-language-model, embedding","<p><strong>No</strong> you cannot feed categorical data into Keras embedding layer without encoding the data.</p>
<p>There are couple of ways to encode the data:</p>
<ol>
<li>Integer Encoding: Where each unique label is mapped to an integer.</li>
<li>One Hot Encoding: Where each label is mapped to a binary vector.</li>
<li>Learned Embedding: Where a distributed representation of the categories is learned.</li>
</ol>
<p>The most preferred method to retrieve contextual information from the categorical data is Learned Embedding method.
You could use any pertained embeddings from below:</p>
<ol>
<li>Glove Embeddings (<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a>)</li>
<li>Word2Vec.</li>
<li>ConceptNet (<a href=""https://github.com/commonsense/conceptnet-numberbatch"" rel=""nofollow noreferrer"">https://github.com/commonsense/conceptnet-numberbatch</a>)</li>
<li>ELMo embeddings (<a href=""https://github.com/yuanxiaosc/ELMo"" rel=""nofollow noreferrer"">https://github.com/yuanxiaosc/ELMo</a>)</li>
</ol>
<p>ELMo embeddings code usage example:</p>
<pre><code>import tensorflow_hub as hub
import tensorflow as tf

elmo = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True))
</code></pre>
",0,0,702,2022-09-10 11:47:46,https://stackoverflow.com/questions/73671560/can-i-feed-categorical-data-in-keras-embedding-layer-without-encoding-the-data
How to model with NLP when the token is not relevant (by itself) but its type is?,"<p>I would like to build an NLP classification model.
My input is a paragraph or a sentence. Ideally, my output is a score or probability (between 0 and 1).</p>
<p>I have defined specific entities ex-ante, each entity belongs to a single group.</p>
<p>Based on business insights, we know that the output to predict does not depend on the entities by themselves, but depends on their groups. For example, the phrase “Max barks” would return 1 because “Max” belongs to the group “Dogs”, but “Kitty barks” would return 0 (because Kitty is not a dog). If “Max” was a cat, the phrase would return 0.
One way to do so would be to generate all the sentences with all the permutations of dogs and cats (in my example) but that is very cumbersome!
Another way would be to replace the entity with the name of the group (the phrase “Max” becomes “” for example) but that looks weird to me!</p>
<p>I don't have any other idea how to tackle this problem.</p>
<p>Could you please help me, ideally with code?</p>
<p>Thanks a lot.</p>
","nlp, tokenize, bert-language-model, word-embedding","<p>If I understand your question correctly, you are to classify the text into &quot;dog activities&quot; vs. &quot;non-dog activities&quot; and in the text you are referencing dogs, cats (and maybe other animals) by their names but you know which name is related with which species.</p>
<p>In such a case I would suggest introducing a named entity token replacing each name of an animal with its species. In your example <code>&quot;Max barks&quot;</code> could be replaced with <code>&quot;%DOG% barks&quot;</code> and <code>&quot;Kitty barks&quot;</code> with <code>&quot;%CAT% barks&quot;</code>.</p>
<p>This would form a strong signal for the model to pick up and train correctly.</p>
<p>Otherwise, you could also go with your approach of generating all of the potential examples of dogs and cats where the name would be loosely linked with a one or the other group by the label of the training / testing example. Even though it is a bit cumbersome it can be more practical that introducing another step to the processing pipeline - Name Entity Recognition - which translates the names of the animals to their species. And such a step would be necessary both in the training and during inference.</p>
",0,0,50,2022-09-17 21:54:59,https://stackoverflow.com/questions/73758805/how-to-model-with-nlp-when-the-token-is-not-relevant-by-itself-but-its-type-is
How to set output_shape of BERT preprocessing layer from tensorflow hub?,"<p>I am building a simple BERT model for text classification, using the tensorflow hub.</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as tf_hub

bert_preprocess = tf_hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = tf_hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)


text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
encoded_input = bert_encoder(preprocessed_text)

l1 = tf.keras.layers.Dropout(0.3, name=&quot;dropout1&quot;)(encoded_input['pooled_output'])
l2 = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l1)

model = tf.keras.Model(inputs=[text_input], outputs = [l2])

model.summary()
</code></pre>
<p>Upon analyzing the output of the <code>bert_preprocess</code> step, I noticed that they are arrays of length 128. My texts are much shorter on average than 128 tokens and as such, my intention would be to decrease this length parameter, so that the preprocessing yields, say, arrays of length 40 only. However, I cannot figure out how to pass this <code>max_length</code> or <code>output_shape</code> parameter to the <code>bert_preprocess</code>.</p>
<p>Printed model summary:</p>
<pre><code>__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 text (InputLayer)              [(None,)]            0           []                               
                                                                                                  
 keras_layer_16 (KerasLayer)    {'input_word_ids':   0           ['text[0][0]']                   
                                (None, 128),                                                      
                                 'input_type_ids':                                                
                                (None, 128),                                                      
                                 'input_mask': (Non                                               
                                e, 128)}                                                          
                                                                                                  
 keras_layer_17 (KerasLayer)    {'sequence_output':  109482241   ['keras_layer_16[0][0]',         
                                 (None, 128, 768),                'keras_layer_16[0][1]',         
                                 'default': (None,                'keras_layer_16[0][2]']         
                                768),                                                             
                                 'encoder_outputs':                                               
                                 [(None, 128, 768),                                               
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
                                 (None, 128, 768),                                                
...
Total params: 109,483,010
Trainable params: 769
Non-trainable params: 109,482,241
</code></pre>
<p>Checking the documentation, I found there is a <code>output_shape</code> argument for <code>tf_hub.KerasLayer</code>, so I tried passing the following arguments:</p>
<pre><code>bert_preprocess = tf_hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;, output_shape=(64,))
</code></pre>
<pre><code>bert_preprocess = tf_hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;, output_shape=[64])
</code></pre>
<p>However, in both of these cases, the following line throws an error:</p>
<pre><code>bert_preprocess([&quot;we have a very sunny day today don't you think so?&quot;])
</code></pre>
<p>Error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_23952\4048288771.py in &lt;module&gt;
----&gt; 1 bert_preprocess(&quot;we have a very sunny day today don't you think so?&quot;)

~\AppData\Roaming\Python\Python37\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

c:\Users\username\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_hub\keras_layer.py in call(self, inputs, training)
    237       result = smart_cond.smart_cond(training,
    238                                      lambda: f(training=True),
--&gt; 239                                      lambda: f(training=False))
    240 
    241     # Unwrap dicts returned by signatures.

c:\Users\username\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_hub\keras_layer.py in &lt;lambda&gt;()
    237       result = smart_cond.smart_cond(training,
    238                                      lambda: f(training=True),
--&gt; 239                                      lambda: f(training=False))
    240 
    241     # Unwrap dicts returned by signatures.
...
  Keyword arguments: {}

Call arguments received:
  • inputs=&quot;we have a very sunny day today don't you think so?&quot;
  • training=False
</code></pre>
","tensorflow, keras, deep-learning, huggingface-transformers, bert-language-model","<p>You need to go lower levels in order to achieve this. Your goal was shown in the page of preprocess layer, however, not properly introduced.</p>
<p>You can wrap your intention into a custom TF layer:</p>
<pre><code>class ModifiedBertPreprocess(tf.keras.layers.Layer):
    def __init__(self, max_len):
        super(ModifiedBertPreprocess, self).__init__()
        
        preprocessor = tf_hub.load(
                    &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
        
        self.tokenizer = tf_hub.KerasLayer(preprocessor.tokenize, name=&quot;tokenizer&quot;)
        
        self.prep_layer = tf_hub.KerasLayer(
                             preprocessor.bert_pack_inputs,
                             arguments={&quot;seq_length&quot;:max_len})
        
    def call(self, inputs, training):
        tokenized = [self.tokenizer(seq) for seq in inputs]
        return self.prep_layer(tokenized)
</code></pre>
<p>Basically, you will tokenize and prepare your inputs by yourself. Preprocessor has a method named <code>bert_pack_inputs</code> which will let you the specify <code>max_len</code> of the inputs.</p>
<p>For some reason, <code>self.tokenizer</code> expects the inputs in a list format. Mostly likely this will allow it to accept multiple inputs.</p>
<p>Your model should look like this:</p>
<pre><code>bert_encoder = tf_hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)

text_input = [tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')]

bert_seq_changed = ModifiedBertPreprocess(max_len=40)

encoder_inputs = bert_seq_changed(text_input)

encoded_input = bert_encoder(encoder_inputs)

l1 = tf.keras.layers.Dropout(0.3, name=&quot;dropout1&quot;)(encoded_input['pooled_output'])
l2 = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l1)

model = tf.keras.Model(inputs=[text_input], outputs = [l2])
</code></pre>
<p>Note that <code>text_input</code> layer is now inside in a list as <code>self.tokenizer's</code> input signatures expects a list.</p>
<p>Here's the model summary:</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 text (InputLayer)              [(None,)]            0           []                               
                                                                                                  
 modified_bert_preprocess (Modi  {'input_type_ids':   0          ['text[0][0]']                   
 fiedBertPreprocess)            (None, 40),                                                       
                                 'input_word_ids':                                                
                                (None, 40),                                                       
                                 'input_mask': (Non                                               
                                e, 40)}                                                           
                                                                                                  
 keras_layer (KerasLayer)       {'encoder_outputs':  109482241   ['modified_bert_preprocess[0][0]'
                                 [(None, 40, 768),               , 'modified_bert_preprocess[0][1]
                                 (None, 40, 768),                ',                               
                                 (None, 40, 768),                 'modified_bert_preprocess[0][2]'
                                 (None, 40, 768),                ]                                
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768),                                                 
                                 (None, 40, 768)],                                                
                                 'default': (None,                                                
                                768),                                                             
                                 'pooled_output': (                                               
                                None, 768),                                                       
                                 'sequence_output':                                               
                                 (None, 40, 768)}                                                 
                                                                                                  
 dropout1 (Dropout)             (None, 768)          0           ['keras_layer[0][13]']           
                                                                                                  
 output (Dense)                 (None, 1)            769         ['dropout1[0][0]']               
                                                                                                  
==================================================================================================
Total params: 109,483,010
Trainable params: 769
Non-trainable params: 109,482,241
</code></pre>
<p>When calling the custom preprocessing layer:</p>
<pre><code>bert_seq_changed([tf.convert_to_tensor([&quot;we have a very sunny day today don't you think so?&quot;], dtype=tf.string)])
</code></pre>
<p>Notice, the inputs should be in a list. Calling the model can be done with both ways:</p>
<pre><code>model([tf.convert_to_tensor([&quot;we have a very sunny day today don't you think so?&quot;], dtype=tf.string)])
</code></pre>
<p>or</p>
<pre><code>model(tf.convert_to_tensor([&quot;we have a very sunny day today don't you think so?&quot;], dtype=tf.string))
</code></pre>
",1,3,1033,2022-09-18 14:26:26,https://stackoverflow.com/questions/73763538/how-to-set-output-shape-of-bert-preprocessing-layer-from-tensorflow-hub
Problem completing BERT model for sentiment classification,"<p>I am trying to figure out sentiment classification on movie reviews using BERT, transformers and tensorflow. This is the code I currently have:</p>
<pre><code>def read_dataset(filename, model_name=&quot;bert-base-uncased&quot;):
    &quot;&quot;&quot;Reads a dataset from the specified path and returns sentences and labels&quot;&quot;&quot;

    tokenizer = BertTokenizer.from_pretrained(model_name)
    with open(filename, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        lines = f.readlines()
        # preallocate memory for the data
        sents, labels = list(), np.empty((len(lines), 1), dtype=int)

        for i, line in enumerate(lines):
            text, str_label, _ = line.split(&quot;\t&quot;)
            labels[i] = int(str_label.split(&quot;=&quot;)[1] == &quot;POS&quot;)
            sents.append(text)
    return dict(tokenizer(sents, padding=True, truncation=True, return_tensors=&quot;tf&quot;)), labels


class BertMLP(tf.keras.Model):
    def __init__(self, embed_batch_size=100, model_name=&quot;bert-base-cased&quot;):
        super(BertMLP, self).__init__()
        self.bs = embed_batch_size
        self.model = TFBertModel.from_pretrained(model_name)
        self.classification_head = tf.keras.models.Sequential(
            layers = [
                tf.keras.Input(shape=(self.model.config.hidden_size,)),
                tf.keras.layers.Dense(350, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(200, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(50, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, use_bias=False)
            ]
        )

    def call(self, inputs):
        outputs = self.model(inputs)
        return outputs

def evaluate(model, inputs, labels, loss_func):
    mean_loss = tf.keras.metrics.Mean(name=&quot;train_loss&quot;)
    accuracy = tf.keras.metrics.BinaryAccuracy(name=&quot;train_accuracy&quot;)

    predictions = model(inputs)
    mean_loss(loss_func(labels, predictions))
    accuracy(labels, predictions)

    return mean_loss.result(), accuracy.result() * 100


if __name__ == &quot;__main__&quot;:
    train = read_dataset(&quot;datasets/rt-polarity.train.vecs&quot;)
    dev = read_dataset(&quot;datasets/rt-polarity.dev.vecs&quot;)
    test = read_dataset(&quot;datasets/rt-polarity.test.vecs&quot;)

    mlp = BertMLP()
    mlp.compile(tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())
    print(&quot;Before training:&quot;, f&quot;Dev Loss: {dev_loss}, Dev Acc: {dev_acc}&quot;)
    mlp.fit(*train, epochs=10, batch_size=10)
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())
    print(&quot;After training:&quot;, f&quot;Dev Loss: {dev_loss}, Dev Acc: {dev_acc}&quot;)
</code></pre>
<p>However, when I run this code, I get an error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;C:\Users\home\anaconda3\lib\site-packages\spyder_kernels\py3compat.py&quot;, line 356, in compat_exec
    exec(code, globals, locals)

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 60, in &lt;module&gt;
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 46, in evaluate
    predictions = model(inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 39, in call
    outputs = self.model(inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py&quot;, line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 1108, in call
    outputs = self.bert(

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py&quot;, line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 781, in call
    embedding_output = self.embeddings(

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 203, in call
    inputs_embeds = tf.gather(params=self.weight, indices=input_ids)

InvalidArgumentError: Exception encountered when calling layer &quot;embeddings&quot; (type TFBertEmbeddings).

indices[1174,8] = 29550 is not in [0, 28996) [Op:ResourceGather]

Call arguments received:
  • input_ids=tf.Tensor(shape=(1599, 73), dtype=int32)
  • position_ids=None
  • token_type_ids=tf.Tensor(shape=(1599, 73), dtype=int32)
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>I googled for a while, and I can't find anything conclusive. I am pretty sure it has something to do with this part:</p>
<pre><code>def call(self, inputs):
        outputs = self.model(inputs)
        return outputs
</code></pre>
<p>But again, I have tried a lot of different things, including limiting dataset size and installing different versions of transformers and tensorflow, but to no avail. Please let me know what I'm doing wrong. Thank you!</p>
","python, tensorflow, keras, sentiment-analysis, bert-language-model","<p>OP was using <code>bert-base-cased</code> for their model, and <code>bert-base-uncased</code> for their tokenizer, causing issues during training when the vocab size of the model and the tokenized data differed.</p>
",1,0,566,2022-10-05 15:46:48,https://stackoverflow.com/questions/73963008/problem-completing-bert-model-for-sentiment-classification
BERT without positional embeddings,"<p>I am trying to build a pipeline in HuggingFace which will not use the positional embeddings in BERT, in order to study the role of the embeddings for a particular use case. I have looked through the documentation and the code, but I have not been able to find a way to implement a model like that. Will I need to modify BERT source code, or is there a configuration I can fiddle around with?</p>
","huggingface-transformers, bert-language-model, word-embedding","<p>You can do a workaround by setting the position embedding layer to zeros. When you check, the embeddings part of BERT, you can see that the position embeddings are there as a separate PyTorch module:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel
bert = AutoModel.from_pretrained(&quot;bert-base-cased&quot;)
print(bert.embeddings)
</code></pre>
<pre><code>BertEmbeddings(
  (word_embeddings): Embedding(28996, 768, padding_idx=0)
  (position_embeddings): Embedding(512, 768)
  (token_type_embeddings): Embedding(2, 768)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
</code></pre>
<p>You can assign the position embedding parameters whatever value you want, including zeros, which will effectively disable the position embeddings:</p>
<pre class=""lang-py prettyprint-override""><code>bert.embeddings.position_embeddings.weight.data = torch.zeros((512, 768))
</code></pre>
<p>If you plan to fine-tune the modified model, make sure the zeroed parameters do not get updated by setting:</p>
<pre class=""lang-py prettyprint-override""><code>bert.embeddings.position_embeddings.requires_grad_ = False
</code></pre>
<p>This sort of bypassing the position embeddings might work well when you train a model from scratch. When you work with a pre-trained model, such removal of some parameters might confuse the models quite a bit, so more fine-tuning data might be needed. In this case, there might be better strategies on how to replace the position embeddings, e.g., using the average value for all positions.</p>
",2,0,1259,2022-10-10 23:25:43,https://stackoverflow.com/questions/74021562/bert-without-positional-embeddings
Using Arabert model with SpaCy,"<p>SpaCy doesn't support the Arabic language, but Can I use SpaCy with the pretrained Arabert model?</p>
<p>Is it possible to modify this code so it can accept bert-large-arabertv02 instead of en_core_web_lg?</p>
<pre><code>!python -m spacy download en_core_web_lg
import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;)
</code></pre>
<p>Here How we can call AraBertV.02</p>
<pre><code>from arabert.preprocess import ArabertPreprocessor
from transformers import AutoTokenizer, AutoModelForMaskedLM

model_name=&quot;aubmindlab/bert-large-arabertv02&quot;
arabert_prep = ArabertPreprocessor(model_name=model_name)  
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)
</code></pre>
","nlp, spacy, bert-language-model","<p>spaCy actually does support Arabic, though only at an alpha level, which basically just means tokenization support (see <a href=""https://spacy.io/usage/models#languages"" rel=""noreferrer"">here</a>). That's enough for loading external models or training your own, though, so in this case you should be able to load this like any HuggingFace model - see <a href=""https://github.com/explosion/spaCy/discussions/10768"" rel=""noreferrer"">this FAQ</a>.</p>
<p>In this case this would look like:</p>
<pre><code>import spacy
nlp = spacy.blank(&quot;ar&quot;) # empty English pipeline
# create the config with the name of your model
# values omitted will get default values
config = {
    &quot;model&quot;: {
        &quot;@architectures&quot;: &quot;spacy-transformers.TransformerModel.v3&quot;,
        &quot;name&quot;: &quot;aubmindlab/bert-large-arabertv02&quot;
    }
}
nlp.add_pipe(&quot;transformer&quot;, config=config)
nlp.initialize() # XXX don't forget this step!
doc = nlp(&quot;فريك الذرة لذيذة&quot;)
print(doc._.trf_data) # all the Transformer output is stored here
</code></pre>
<p>I don't speak Arabic, so I can't check the output thoroughly, but that code ran and produced an embedding for me.</p>
",6,5,2994,2022-10-13 22:00:12,https://stackoverflow.com/questions/74062240/using-arabert-model-with-spacy
NLP BERT in R with tensorflow/Keras setup,"<p>I am trying to get BERT to run in R.</p>
<p>I got other NLP tasks (e.g. word2vec) done with Keras, so the general setup should be ok.</p>
<p>I adapted the model code from here: <a href=""https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"" rel=""nofollow noreferrer"">https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379</a></p>
<p>The problem is how to insert the inputs (tokens) correctly. I have tried a lot of different ways to transform them (as tensors, various forms of arrays etc), but can't seem to figure out what kind of data structure/type/shape is expected as input.</p>
<p>Here is a simplified, replicable example:</p>
<pre><code>#rm(list=ls())
packages &lt;- c(&quot;reticulate&quot;, &quot;keras&quot;, &quot;tensorflow&quot;, &quot;tfdatasets&quot;, &quot;tidyverse&quot;, &quot;data.table&quot;)
for (p in packages) if (!(p %in% installed.packages()[,1])) install.packages(p, character.only = TRUE) else require(p, character.only = TRUE)
rm(packages, p)

#reticulate::install_miniconda(force = TRUE) # 1time
reticulate::use_condaenv(&quot;~/.local/share/r-miniconda&quot;) # win? reticulate::use_condaenv(&quot;r-miniconda&quot;)

Sys.setenv(TF_KERAS=1) 
tensorflow::tf_version() # install_tensorflow() if NULL
reticulate::py_config()

#reticulate::py_install('transformers', pip = TRUE)
#reticulate::py_install('torch', pip = TRUE)
transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
builtins &lt;- import_builtins() #built in python methods

set.tf.repos &lt;- &quot;distilbert-base-german-cased&quot;

tokenizer &lt;- transformer$AutoTokenizer$from_pretrained(set.tf.repos)  # 
tokenizer_vocab_size &lt;- length(tokenizer$vocab)

###### load model
model_tf = transformer$TFDistilBertModel$from_pretrained(set.tf.repos, from_pt = T, trainable = FALSE)
model_tf$config

# set configs
model_tf$config$output_hidden_states = TRUE
summary(model_tf)

###### data &amp; tokens #####
data &lt;- data.table::fread(&quot;https://raw.githubusercontent.com/michael-eble/nlp-dataset-health-german-language/master/nlp-health-data-set-german-language.txt&quot;, encoding = &quot;Latin-1&quot;)
txt &lt;- data$V1
y &lt;- data$V2
table(y, exclude = NULL)

set.max_length = 100
tokens &lt;- tokenizer(
  txt,
  max_length = set.max_length %&gt;% as.integer(),
  padding = 'max_length', #'longest' #implements dynamic padding
  truncation = TRUE,
  return_attention_mask = TRUE,
  return_token_type_ids = FALSE
)
#tokens[[&quot;input_ids&quot;]] %&gt;% str()
#tokens[[&quot;attention_mask&quot;]] %&gt;% str()

tokens &lt;- list(tokens[[&quot;input_ids&quot;]], tokens[[&quot;attention_mask&quot;]])
str(tokens)



####### model ########
input_word_ids &lt;- layer_input(shape = c(set.max_length), dtype = 'int32', name = &quot;input_word_ids&quot;)
input_mask &lt;- layer_input(shape = c(set.max_length), dtype = 'int32', name = &quot;input_attention_mask&quot;)
#input_segment_ids &lt;- layer_input(shape = c(max_len), dtype = 'int32', name=&quot;input_segment_ids&quot;)

last_hidden_state &lt;- model_tf(input_word_ids, attention_mask = input_mask)[[1]]
cls_token &lt;- last_hidden_state[, 1,]

output &lt;- cls_token %&gt;%
  layer_dense(units = 32, input_shape = c(set.max_length, 768), activation = 'relu') %&gt;%
  layer_dense(units = 1, activation = 'sigmoid')

model &lt;- keras_model(inputs = list(input_word_ids, input_mask), outputs = output)

model %&gt;% compile(optimizer = &quot;adam&quot;,
                  loss = &quot;binary_crossentropy&quot;
)

history = model %&gt;%
  keras::fit(
    x = list(input_word_ids = tokens$input_ids, input_mask = tokens$attention_mask),
    y = y,
    epochs = 2,
    batch_size = 256,
    #metrics = &quot;accuracy&quot;,
    validation_split = .2
  )
</code></pre>
<p>Error message:</p>
<pre><code>Error in py_call_impl(callable, dots$args, dots$keywords) : 
  ValueError: Failed to find data adapter that can handle input: (&lt;class 'dict'&gt; containing {&quot;&lt;class 'str'&gt;&quot;} keys and {&quot;&lt;class 'NoneType'&gt;&quot;} values), &lt;class 'numpy.ndarray'&gt;

Detailed traceback:
  File &quot;/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 984, in select_data_adapter
    raise ValueError(
</code></pre>
<p>Many thanks in advance!</p>
","r, keras, bert-language-model, reticulate","<p>Your <code>model$inputs</code> shapes don't match the inputs you're feeding it in <code>fit()</code>.</p>
<p>It's helpful to create a TF Dataset, so you can be explicit about your training dataset tensor shapes, and make sure that those tensor shapes match <code>model$inputs</code>.</p>
<p>Changing your <code>fit()</code> call to this makes it work:</p>
<pre><code>x_ds &lt;- tensor_slices_dataset(tokens) 
y_ds &lt;- tensor_slices_dataset(y)

ds &lt;- zip_datasets(x_ds, y_ds) %&gt;% 
  dataset_batch(256)

history = model %&gt;% fit(ds, epochs = 2)
</code></pre>
",2,1,1028,2022-10-14 07:14:25,https://stackoverflow.com/questions/74065619/nlp-bert-in-r-with-tensorflow-keras-setup
Tensorflow expected 2 inputs but received 1 input tensor,"<p>Hey guys so I'm building a model based on the Roberta-Base and at the end when I try to fit the model I get a error saying: <code>ValueError: Layer model_39 expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=(16, 128) dtype=float64&gt;]</code></p>
<p>I'm using <code>tf.data.Dataset</code> to make the dataset:</p>
<pre><code>def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# Create dataset
dataset = tf.data.Dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)
</code></pre>
<p>Supposedly dataset is generating 2 inputs properly but for some reason fit is refusing to work and I'm not sure why.</p>
<p>Full code:</p>
<pre><code>LEN_SEQ = 128
BATCH_SIZE = 16
TEST_TRAIN_SPLIT = 0.9
TRANSFORMER = 'roberta-base'

# Load roberta model
base_model = TFAutoModel.from_pretrained('roberta-base')
for layer in base_model.layers:
    layer.trainable = False

# Define input layers
input_ids = tf.keras.layers.Input(shape=(LEN_SEQ,), name='input_ids', dtype='int32')
input_mask = tf.keras.layers.Input(shape=(LEN_SEQ,), name='input_mask', dtype='int32')

# Define hidden layers
embedding = base_model([input_ids, input_mask])[1]
layer = tf.keras.layers.Dense(LEN_SEQ * 2, activation='relu')(embedding)
layer = tf.keras.layers.Dense(LEN_SEQ, activation='relu')(layer)

# Define output
output = tf.keras.layers.Dense(1, activation='softmax', name='output')(layer)

model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=[output])

model.compile(
    optimizer = Adam(learning_rate=1e-3, decay=1e-4),
    loss = CategoricalCrossentropy(),
    metrics = [
        CategoricalAccuracy('accuracy')
    ]
)

# Load data
df = pd.read_csv('train-processed.csv')
df = df.head(100)
samples_count = len(df)

# Tokenize data
tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)
tokens = tokenizer(
    df['first_Phrase'].tolist(),
    max_length=LEN_SEQ,
    truncation=True,
    padding='max_length',
    add_special_tokens=True,
    return_tensors='tf'
)
ids = tokens['input_ids']
mask = tokens['attention_mask']

def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# Create dataset
dataset = tf.data.Dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)

# Split data intro train and test
train_size = int((samples_count / BATCH_SIZE) * TEST_TRAIN_SPLIT)
train = dataset.take(train_size)
test = dataset.skip(train_size)

# Train model
history = model.fit(
    train,
    validation_data=test,
    epochs=2
)
</code></pre>
<p>Inside dataset -&gt; <code>&lt;BatchDataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)&gt;</code></p>
<p>Inside train -&gt; <code>&lt;TakeDataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)&gt;</code></p>
<p>Data example:</p>
<p><a href=""https://i.sstatic.net/Y2jwG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y2jwG.png"" alt=""enter image description here"" /></a></p>
<p>Any help appreciated. I'm new to transformers so please feel free to point any extra considerations.</p>
","python, tensorflow, bert-language-model","<p>So I managed to fix this as far as I know with the help of @Djinn.
I did remove dataset API and instead built my own datasets manually using the following code:</p>
<pre><code># Split into training and validation sets
train_size = int(samples_count * TEST_TRAIN_SPLIT)
train = [ids[:train_size], mask[:train_size]]
train_labels = labels[:train_size]
test = [ids[train_size:], mask[train_size:]], labels[train_size:]
# Train model
history = model.fit(
    train, train_labels,
    validation_data=test,
    epochs=10
)
</code></pre>
<p>This seems to be working and <code>fit()</code> accepted this data, but feel free to point out if this is wrong or could be made differently.</p>
",0,0,969,2022-10-14 20:03:57,https://stackoverflow.com/questions/74074350/tensorflow-expected-2-inputs-but-received-1-input-tensor
"When I run the code, it stops when the sanity check dataloader, but no error is prompted","<p>Through debugging, I found that the problem occurred when I ran to the line of trainer. fit (model).It seems that there are some problems when loading data.</p>
<p>Here's my code</p>
<pre><code>WEIGHT = &quot;bert-base-uncased&quot;

class Classifier(pl.LightningModule): 
    
    def __init__(self, 
                 num_classes: int,
                 train_dataloader_: DataLoader,
                 val_dataloader_: DataLoader,
                 weights: str = WEIGHT):
        
        super(Classifier, self).__init__()
        self.train_dataloader_ = train_dataloader_
        self.val_dataloader_ = val_dataloader_
        
        self.bert = AutoModel.from_pretrained(weights)
        self.num_classes = num_classes
        self.classifier = nn.Linear(self.bert.config.hidden_size, self.num_classes)
    
    def forward(self, input_ids: torch.tensor):
        bert_logits, bert_pooled = self.bert(input_ids = input_ids)
        out = self.classifier(bert_pooled)
        return out
    
    def training_step(self, batch, batch_idx):
        # batch
        input_ids, labels = batch
    
        # predict
        y_hat = self.forward(input_ids=input_ids)
        
        # loss 
        loss = F.cross_entropy(y_hat, labels)

        # logs
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}
    
    def validation_step(self, batch, batch_idx):
        input_ids, labels = batch
        
        y_hat = self.forward(input_ids = input_ids)
        
        loss = F.cross_entropy(y_hat, labels)
        
        a, y_hat = torch.max(y_hat, dim=1)
        y_hat = y_hat.cpu()
        labels = labels.cpu()

        val_acc = accuracy_score(labels, y_hat)
        val_acc = torch.tensor(val_acc)
        
        val_f1 = f1_score(labels, y_hat, average='micro')
        val_f1 = torch.tensor(val_f1)

        return {'val_loss': loss, 'val_acc': val_acc, 'val_f1': val_f1}
    
    def validation_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()
        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()
        
        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc, 'avg_val_f1': avg_val_f1}
        
        return {'avg_val_loss': avg_loss, 'avg_val_f1':avg_val_f1 ,'progress_bar': tensorboard_logs}
    
    def configure_optimizers(self):
        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], 
                                lr=2e-05, eps=1e-08)
    
    def train_dataloader(self):
        return self.train_dataloader_
    
    def val_dataloader(self):
        return self.val_dataloader_

train  = pd.read_csv(&quot;data/practice/task1.csv&quot;, names =[&quot;index&quot;, &quot;text&quot;, &quot;gold&quot;], sep=&quot;;&quot;, header=0)
test   = pd.read_csv(&quot;data/trial/task1.csv&quot;, names =[&quot;index&quot;, &quot;text&quot;, &quot;gold&quot;], sep=&quot;;&quot;, header=0)

WEIGHTS = [&quot;distilroberta-base&quot;, &quot;bert-base-uncased&quot;, &quot;roberta-base&quot;, &quot;t5-base&quot;]
BATCH_SIZE = 12

random_seed = 1988
train, val = train_test_split(train, stratify=train[&quot;gold&quot;], random_state=random_seed)
# from transformers import logging

# logging.set_verbosity_warning()
# logging.set_verbosity_error()
for weight in WEIGHTS:
    try:
        tokenizer = AutoTokenizer.from_pretrained(weight)
        X_train = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=True)) for text in train[&quot;text&quot;]]
        X_train = pad_sequence(X_train, batch_first=True, padding_value=0)
        y_train = torch.tensor(train[&quot;gold&quot;].tolist())

        X_val = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=True)) for text in val[&quot;text&quot;]]
        X_val = pad_sequence(X_val, batch_first=True, padding_value=0)
        y_val = torch.tensor(val[&quot;gold&quot;].tolist())

        ros = RandomOverSampler(random_state=random_seed)
        X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

        X_train_resampled = torch.tensor(X_train_resampled)
        y_train_resampled = torch.tensor(y_train_resampled)

        train_dataset = TensorDataset(X_train_resampled, y_train_resampled)
        train_dataloader_ = DataLoader(train_dataset,
                                    sampler=RandomSampler(train_dataset),
                                    batch_size=BATCH_SIZE,
                                    num_workers=24,
                                    pin_memory=True)

        val_dataset = TensorDataset(X_val, y_val)
        val_dataloader_ = DataLoader(val_dataset,
                                    batch_size=BATCH_SIZE,
                                    num_workers=24,
                                    pin_memory=True)
        
        model = Classifier(num_classes=2,
                            train_dataloader_=train_dataloader_,
                            val_dataloader_ = val_dataloader_,
                            weights=weight)

        trainer = pl.Trainer(devices=1,accelerator=&quot;gpu&quot;,
                            max_epochs=30)
        
        trainer.fit(model)
        
        X_test = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=True)) for text in test[&quot;text&quot;].tolist()]
        X_test = pad_sequence(X_test, batch_first=True, padding_value=0)
        y_test = torch.tensor(test[&quot;gold&quot;].tolist())

        test_dataset = TensorDataset(X_test, y_test)
        test_dataloader_ = DataLoader(test_dataset, batch_size=16, num_workers=4)

        device = &quot;cuda:0&quot;
        model.eval()
        model = model.to(device)

        test_preds = []
        for batch in tqdm(test_dataloader_, total=len(list(test_dataloader_))):
            ii, _ = batch
            ii = ii.to(device)
            preds = model(input_ids = ii)
            preds = torch.argmax(preds, axis=1).detach().cpu().tolist()
            test_preds.extend(preds)    

        from sklearn.metrics import classification_report

        report = classification_report(test[&quot;gold&quot;].tolist(), test_preds)

        with open(&quot;task1_experiments/&quot;+weight+&quot;_baseline.txt&quot;, &quot;w&quot;) as f:
            f.write(report)
    except:
        continue
</code></pre>
<p>When the code stops running, the output of the terminal is shown in the following.I don't know what caused this problem. I hope someone can help me solve this problem.</p>
<p>How can I solve this problem.
Thanks in advance for helping me</p>
<p>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]</p>
<h2>| Name       | Type         | Params</h2>
<h2>0 | bert       | RobertaModel | 124 M
1 | classifier | Linear       | 1.5 K</h2>
<p>124 M     Trainable params
0         Non-trainable params
124 M     Total params
498.589   Total estimated model params size (MB)
Sanity Checking DataLoader 0:   0%|                                                                                                                                     | 0/2 [00:00&lt;?, ?it/s]<br />
<a href=""https://i.sstatic.net/kwJQ4.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","python, pytorch, bert-language-model, pytorch-lightning","<p>打印出异常信息后发现。forward方法中调用的classfier方法需要传入tensor，但是传入了字符串。
print(self.bert(input_ids = input_ids)输出一个字典，print(bert_logits, bert_pooled)得到这两个变量对应的key，通过bert_pooled = self.bert(input_ids = input_ids)['pooler_output']重新赋值，问题解决</p>
",-3,0,2169,2022-10-22 09:47:25,https://stackoverflow.com/questions/74162510/when-i-run-the-code-it-stops-when-the-sanity-check-dataloader-but-no-error-is
BERT Transformer model gives an error for multiclass classification,"<p>I am trying to train a sentiment analysis model with 5 classes (1-Very Negative, 2-Negative, 3-Neutral, 4-Positive, 5-Very Positive) with the BERT model.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures
        
model = TFBertForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
        
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])
    
model.fit(train_data, epochs=2, validation_data=validation_data)
</code></pre>
<p>But I get the following error (Just the last part of the error message)</p>
<pre><code>Node: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'
Received a label value of 5 which is outside the valid range of [0, 2).  Label values: 3 4 5 2 2 4 4 3 4 5 5 4 5 5 4 4 4 3 4 4 5 5 5 4 4 5 3 5 4 4 3 5
         [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_31614]
</code></pre>
<p>Can somebody tell me what I am doing wrong here?</p>
","bert-language-model, softmax, cross-entropy","<p>The <code>TFBertForSequenceClassification</code> object needs to create a so-called classification head. The classification head is a cool name for a single NN layer that projects the <code>[CLS]</code> token representation into a vector with one item for each possible target class.</p>
<p>When you initialize the model by calling <code>from_pretrained</code>, you can specify <code>num_labels</code>, which is a number of target labels (see <a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForSequenceClassification.call.example-2"" rel=""nofollow noreferrer"">an example in Transformers documentation</a>). If you do not specify it, the number of target classes will be inferred from the first training batch by taking the maximum class ID in the batch. If you are not lucky and the first batch only contains lower label IDs, it initializes a smaller classification head and fails when a batch with higher IDs comes.</p>
<p>Note also, that the class numbers start from zero. If you use labels 1-5, the model will have an additional 0th class that will not be used. If you want to keep the numbers 1-5, your <code>num_labels</code> will be 6.</p>
",0,0,169,2022-10-23 18:47:54,https://stackoverflow.com/questions/74173869/bert-transformer-model-gives-an-error-for-multiclass-classification
AttributeError: &#39;list&#39; object has no attribute &#39;ents&#39; in building NER using BERT,"<p>I'm trying to build a <code>NER</code> model using <code>Bert-base-NER</code> for a <code>tweets dataset</code> and ending up getting this error . Please help</p>
<p>This is what I have done</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

# ---------

def all_ents(v):
        return [(ent.text, ent.label_) for ent in nlp(v).ents]

df1['Entities'] = df['text'].apply(lambda v: all_ents(v))

df1.head()
</code></pre>
<pre><code>AttributeError: 'list' object has no attribute 'ents'
</code></pre>
<p>Thank you for the help</p>
","python, pandas, bert-language-model, named-entity-recognition","<p>It seems you mix code from different modules.</p>
<p><code>.ents</code> exists in module <code>spacy</code> but not in <code>transformers</code></p>
<pre><code>#import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()

doc = nlp('Hello World of Python. Have a nice day')

print([(x.text, x.label_) for x in doc.ents])
</code></pre>
<p>In <code>transformers</code> you should use directly <code>nlp(v)</code> but it gives directory with <code>ent[&quot;entity&quot;], ent[&quot;score&quot;], ent[&quot;index&quot;], ent[&quot;word&quot;], ent[&quot;start&quot;], ent[&quot;end&quot;]</code></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)

nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)

# ---------

import pandas as pd

df = pd.DataFrame({
    'text': ['Hello World of Python. Have a nice day']
})

# ---------

def all_ents(v):
    #print(nlp(v))
    return [(ent['word'], ent['entity']) for ent in nlp(v)]

df['Entities'] = df['text'].apply(all_ents)

#df1['Entities'] = df['text'].apply(lambda v: [(ent['word'], ent['entity']) for ent in nlp(v)])

print(df['Entities'].head())
</code></pre>
",1,0,301,2022-10-25 10:53:06,https://stackoverflow.com/questions/74192948/attributeerror-list-object-has-no-attribute-ents-in-building-ner-using-bert
Error while using bert-base-nli-mean-tokens bert model,"<p>I am using this code:</p>
<pre><code>model = SentenceTransformer('bert-base-nli-mean-tokens')
body = list(data['preprocessedBody'])
bodyEmbedding = model.encode(body, show_progress_bar = True)
</code></pre>
<p>However, I am getting this error:</p>
<pre><code>ProxyError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/bert-base-nli-mean-tokens (Caused by ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1129)'))))
​
</code></pre>
<p>Is there any solution to it?</p>
<p>Thank you for your time</p>
","python, nlp, bert-language-model, sentence-transformers, sslerrorhandler","<p>It was simply a proxy issue. I just added <code>https</code> and <code>http</code> and their relative proxy values into system environment in windows.</p>
",1,1,348,2022-10-27 21:35:41,https://stackoverflow.com/questions/74228567/error-while-using-bert-base-nli-mean-tokens-bert-model
Map BERTopic topic IDs back to the training dataframe,"<p>I have trained a BERTopic model on a dataframe of length of 400k. I want to map the topics of each document in a new column inside the dataframe. I could do that by running a for loop on all the documents and do <code>topic_model.transform(doc)</code> on them. The only problem is, it takes more than a second to transform each document into its topic and it would take days for the whole dataset.</p>
<p>Is there a way to achieve this faster since I want to map the topics on the training data.</p>
<p>I tried:</p>
<pre><code>topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)
topic_model.reduce_topics(docs, nr_topics=200)

topics = []
for text in df.texts:
    tops = topic_model.transform(text)
    topics.append(tops)
df['topics'] = topics
</code></pre>
","python-3.x, nlp, bert-language-model, topic-modeling","<p>There is no need to recalculate the topics as you already retrieved them when using <code>.fit_transform</code>. There, the <code>topics</code> that you retrieve are in the exact same order as the input documents. Therefore, you can perform the following:</p>
<pre class=""lang-py prettyprint-override""><code># The `topics` that you get here are in the exact same order as `docs`
# `topics[0]` belongs to `docs[0]`, `topics[1]` to `docs[1]`, etc.
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)
topic_model.reduce_topics(docs, nr_topics=200)

# When you used `.fit_transform`:
df = pd.DataFrame({&quot;Document&quot;: docs, &quot;Topic&quot;: topic})
</code></pre>
<p>For those using <code>.fit</code> instead of <code>.fit_transform</code>, you can also access the topics and their documents as follows:</p>
<pre class=""lang-py prettyprint-override""><code># When you used `.fit`:
df = pd.DataFrame({&quot;Document&quot;: docs, &quot;Topic&quot;: topic_model.topics_})
</code></pre>
",3,2,2112,2022-11-15 15:36:42,https://stackoverflow.com/questions/74448344/map-bertopic-topic-ids-back-to-the-training-dataframe
AttributeError: &#39;numpy.float64&#39; object has no attribute &#39;cpu&#39;,"<p>I am trying to run BERT and train a model using pytorch.
I am not sure why I am getting this error after finishing the first Epoch.
I am using this code <a href=""https://www.kaggle.com/code/prakharrathi25/sentiment-analysis-using-bert/notebook"" rel=""nofollow noreferrer"">link</a></p>
<pre><code>history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    
    # Show details 
    print(f&quot;Epoch {epoch + 1}/{EPOCHS}&quot;)
    print(&quot;-&quot; * 10)
    
    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(df_train)
    )
    
    print(f&quot;Train loss {train_loss} accuracy {train_acc}&quot;)
    
    # Get model performance (accuracy and loss)
    val_acc, val_loss = eval_model(
        model,
        val_data_loader,
        loss_fn,
        device,
        len(df_val)
    )
    
    print(f&quot;Val   loss {val_loss} accuracy {val_acc}&quot;)
    print()
    
    history['train_acc'].append(train_acc.cpu())
    history['train_loss'].append(train_loss.cpu())
    history['val_acc'].append(val_acc.cpu())
    history['val_loss'].append(val_loss.cpu())
    
    # If we beat prev performance
    if val_acc &gt; best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc
</code></pre>
<p>Here is the output and the error message
<a href=""https://i.sstatic.net/WqSdQ.png"" rel=""nofollow noreferrer"">Image</a></p>
<p>It is a first time for me to work with pytorch. Any ideas how to fix the error&gt;</p>
","python, pytorch, bert-language-model","<p>I checked kaggle link and I see that there is no <code>cpu()</code> reference as you have posted in your code. It should simply be:</p>
<pre class=""lang-py prettyprint-override""><code>history['train_acc'].append(train_acc)
history['train_loss'].append(train_loss)
history['val_acc'].append(val_acc)
history['val_loss'].append(val_loss)
</code></pre>
",1,1,1387,2022-11-17 09:47:32,https://stackoverflow.com/questions/74473271/attributeerror-numpy-float64-object-has-no-attribute-cpu
BERTopic Embeddings ValueError when transform a new text,"<p>I have created embeddings using SentenceTransformer and trained a BERTopic model on those embeddings.</p>
<pre><code>sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = sentence_model.encode(training_docs, show_progress_bar=True)
topic_model = BERTopic().fit_transform(training_docs, embeddings)
topic_model.reduce_topics(training_docs, nr_topics=5)
</code></pre>
<p>I have then saved the <code>embeddings</code> using pickle and topic_model using <code>topic_model.save()</code>. I can also load them both but when I try to use it on a new text such as:</p>
<pre><code>with open('embeddings.pickle', 'rb') as pkl:
    embeddings = pickle.load(pkl)

topic_model = BERTopic.load('mybertopic')

sentence = 'I have found my car.'

topics, probs = topic_model.transform(sentence, embeddings)
</code></pre>
<p>I get the following error:</p>
<pre><code>ValueError: Make sure that the embeddings are a numpy array with shape: (len(docs), vector_dim) where vector_dim is the dimensionality of the vector embeddings. 
</code></pre>
<p>The embeddings are a numpy array. How do I solve this?</p>
","python-3.x, bert-language-model, topic-modeling, sentence-transformers","<p>Okay I solved it. I have to encode my text using the same SentenceTransformer and not use the entire embeddings in <code>transform</code> method.</p>
<pre><code>embeddings = sentence_model.encode(sentence)
topics, probs = topic_model.transform(sentence, embeddings)
print(topics)
[-1]
</code></pre>
",4,1,1540,2022-11-18 16:33:21,https://stackoverflow.com/questions/74492719/bertopic-embeddings-valueerror-when-transform-a-new-text
How to get Non-contextual Word Embeddings in BERT?,"<p>I am already installed BERT, But I don't know how to get Non-contextual word embeddings.</p>
<p>For example:</p>
<pre><code>
input: 'Apple'
output: [1,2,23,2,13,...] #embedding of 'Apple'


</code></pre>
<p>How can i get these word embeddings?</p>
<p>Thank you.</p>
<p>I search some method, but no blogs have written the way.</p>
","python, pytorch, nlp, bert-language-model","<p>Sloved.</p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

# get the word embedding from BERT
def get_word_embedding(word:str):
    input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)  # Batch size 1
    # print(input_ids)
    outputs = model(input_ids)
    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
    # output[0] is token vector
    # output[1] is the mean pooling of all hidden states
    return last_hidden_states[0][1]


</code></pre>
",1,0,858,2022-11-22 05:35:09,https://stackoverflow.com/questions/74527928/how-to-get-non-contextual-word-embeddings-in-bert
NotFoundError using BERT Preprocessing from TFHub,"<p>I'm trying to use the pre-trained BERT models on TensorFlow Hub to do some simple NLP. I'm on a 2021 MacBook Pro (Apple Silicon) with Python 3.9.13 and TensorFlow v2.9.2. However, preprocessing any amount of text returns a &quot;NotFoundError&quot; that I can't seem to resolve. The link to the preprocessing model is here: (<a href=""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"" rel=""nofollow noreferrer"">https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3</a>) and I have pasted my code/error messages below. Does anyone know why this is happening and how I can fix it? Thanks in advance.</p>
<h1>Code</h1>
<pre><code>bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)
print(bert_preprocess([&quot;test&quot;]))
</code></pre>
<h1>Output</h1>
<pre><code>Output exceeds the size limit. Open the full output data in a text editor
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
Cell In [42], line 3
      1 bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
      2 bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)
----&gt; 3 print(bert_preprocess([&quot;test&quot;]))

File ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:237, in KerasLayer.call(self, inputs, training)
    234   else:
    235     # Behave like BatchNormalization. (Dropout is different, b/181839368.)
    236     training = False
--&gt; 237   result = smart_cond.smart_cond(training,
    238                                  lambda: f(training=True),
    239                                  lambda: f(training=False))
    241 # Unwrap dicts returned by signatures.
    242 if self._output_key:

File ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:239, in KerasLayer.call.&lt;locals&gt;.&lt;lambda&gt;()
...
     [[StatefulPartitionedCall/StatefulPartitionedCall/bert_pack_inputs/PartitionedCall/RaggedConcat/ArithmeticOptimizer/AddOpsRewrite_Leaf_0_add_2]] [Op:__inference_restored_function_body_209194]

Call arguments received by layer &quot;keras_layer_6&quot; (type KerasLayer):
  • inputs=[&quot;'test'&quot;]
  • training=None
</code></pre>
","python, tensorflow, bert-language-model, data-preprocessing, tensorflow-hub","<p><strong>Update</strong>: While using BERT preprocessing from TFHub, <code>Tensorflow</code> and <code>tensorflow_text</code> versions should be same so please make sure that installed both versions are same. It happens because you're using latest version for <code>tensorflow_text</code> but you're using other versions for python and tensorflow but there is internal dependancy with versions for <code>Tensorflow</code> and <code>tensorflow_text</code> which should be same.</p>
<pre><code>!pip install -U tensorflow
!pip install -U tensorflow-text
import tensorflow as tf
import tensorflow_text as text

# Or install with a specific Version
!pip install -U tensorflow==2.11.*
!pip install -U tensorflow-text==2.11.*
import tensorflow as tf
import tensorflow_text as text
</code></pre>
<p>I have executed below lines of code in Google Colab and It's working fine,</p>
<pre><code>bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)
print(bert_preprocess([&quot;test&quot;])) 
</code></pre>
<p>Here is output:</p>
<pre><code>{'input_type_ids': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)&gt;, 'input_mask': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=
array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)&gt;, 'input_word_ids': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=
array([[ 101, 3231,  102,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0]], dtype=int32)&gt;}
</code></pre>
<p>I hope it will help you to resolve your issue, Thank You!</p>
",0,0,400,2022-11-24 01:51:56,https://stackoverflow.com/questions/74554805/notfounderror-using-bert-preprocessing-from-tfhub
SBERT gives same result no matter what,"<p>I have a test script for SBERT:</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel
from sklearn.cluster import KMeans

# 1. Use SBERT to compare two sentences for semantic similarity.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

input_ids_1 = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
input_ids_2 = torch.tensor(tokenizer.encode(&quot;Hello, my cat is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
outputs_1 = model(input_ids_1)
outputs_2 = model(input_ids_2)
last_hidden_states_1 = outputs_1[0]  # The last hidden-state is the first element of the output tuple
last_hidden_states_2 = outputs_2[0]  # The last hidden-state is the first element of the output tuple

# 2. Take SBERT embeddings for both sentences and cluster them.
kmeans = KMeans(n_clusters=2, random_state=0).fit(last_hidden_states_1.detach().numpy()[0], last_hidden_states_2.detach().numpy()[0])

# 3. Print the clusters.
print(kmeans.labels_)
print(kmeans.cluster_centers_)
</code></pre>
<p>The output is:</p>
<pre><code>[0 0 0 0 0 0 0 1]
[[-0.2281394   0.29968688  0.3390873  ... -0.40648264  0.2930719
   0.41721284]
 [ 0.6079925   0.26097086 -0.3130729  ...  0.03109726 -0.6282735
  -0.19942412]]
</code></pre>
<p>This happens no matter what the second sentence is. I changed it to &quot;The capital of France is Paris&quot; and it still gave me the same output, so clearly I am not passing/transforming the data correctly.</p>
<p>What am I doing wrong?</p>
","python, nlp, bert-language-model","<p>There was a couple of tiny modification to sort things out. Please bear in mind in order to cluster sentences you need to catch only the first/last embedding for the sentence. In addition <code>KMeans</code> expects to receive a 2D array for clustering.</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel
from sklearn.cluster import KMeans
import numpy as np 

# 1. Use SBERT to compare two sentences for semantic similarity.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

input_ids_1 = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
input_ids_2 = torch.tensor(tokenizer.encode(&quot;The capital of France is Paris&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
outputs_1 = model(input_ids_1)
outputs_2 = model(input_ids_2)
last_hidden_states_1 = outputs_1[0][0, 0, :]  # The last hidden-state is the first element of the output tuple
last_hidden_states_2 = outputs_2[0][0, 0, :]  # The last hidden-state is the first element of the output tuple

# 2. Take SBERT embeddings for both sentences and cluster them.
kmeans = KMeans(n_clusters=2, random_state=0).fit([last_hidden_states_1.detach().numpy(), last_hidden_states_2.detach().numpy()])

# 3. Print the clusters.
print(kmeans.labels_)
print(kmeans.cluster_centers_)
</code></pre>
<p>output:</p>
<pre><code>[0 1]
[[-0.11437159  0.19371444  0.1249602  ... -0.38269117  0.21065859
   0.54070717]
 [-0.06510071  0.06050608 -0.10048206 ... -0.27256876  0.36847278
   0.57706201]]
</code></pre>
",1,0,171,2022-11-27 16:47:30,https://stackoverflow.com/questions/74591917/sbert-gives-same-result-no-matter-what
Calculating embedding overload problems with BERT,"<p>I'm trying to calculate the embedding of a sentence using BERT. After I input the sentence into BERT, I calculate the Mean-pooling, which is used as the embedding of the sentence.</p>
<h2>Problem</h2>
<p>My code can calculate the embedding of sentences, but the computational cost is very high. I don't know what's wrong and I hope someone can help me.</p>
<h3>Install BERT</h3>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<h3>Get Embedding Function</h3>
<pre><code># get the word embedding from BERT
def get_word_embedding(text:str):
    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[1]  
    # The last hidden-state is the first element of the output tuple
    return last_hidden_states[0]
</code></pre>
<h3>Data</h3>
<p>The maximum number of words in the text is 50. I calculate the entity+text embedding</p>
<p><a href=""https://i.sstatic.net/hvw4Y.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<h3>Run code</h3>
<p><code>entity_desc</code> is my data.
It's this step that overloads my computer every time I run it.
Please help me!!!</p>
<p>I was use RAM 80GB machine in Colab.</p>
<pre><code>entity_embedding = {}
for i in range(len(entity_desc)):
    entity = entity_desc['entity'][i]
    text = entity_desc['text'][i]
    entity += ' ' + text
    entity_embedding[entity_desc['entity_id'][i]] = get_word_embedding(entity)
</code></pre>
","python, pytorch, nlp, bert-language-model, embedding","<p>I fixed the problem.
The reason for the memory overload was that I wasn't saving the tensor to the GPU, so I made the following changes to the code.</p>
<pre><code>model = model.to(device)


import torch
# get the word embedding from BERT
def get_word_embedding(text:str):
    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1
    input_ids = input_ids.to(device)

    outputs = model(input_ids)
    last_hidden_states = outputs[1]
    last_hidden_states = last_hidden_states.to(device)  
    # The last hidden-state is the first element of the output tuple
    return last_hidden_states[0].detach().to(device)

</code></pre>
",1,1,132,2022-11-28 02:39:55,https://stackoverflow.com/questions/74595449/calculating-embedding-overload-problems-with-bert
Translation with multi-lingual BERT model,"<p>I want to translate my dataframe using multi-lingual BERT.
I have copied this code but in place of <code>text</code>, I want to use my own dataframe.</p>
<pre><code>from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = TFBertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
</code></pre>
<p>However, I get some errors when using it like below.</p>
<pre><code>df  =pd.read_csv(&quot;/content/drive/text.csv&quot;)
encoded_input = tokenizer(df, return_tensors='tf')
</code></pre>
<p>Error</p>
<pre><code>ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
<p>My dataframe looks like this</p>
<pre><code>0    There is XXXX increased opacity within the rig...
1    There is XXXX increased opacity within the rig...
2    There is XXXX increased opacity within the rig...
3    Interstitial markings are diffusely prominent ...
4    Interstitial markings are diffusely prominent ...
Name: findings, dtype: object
</code></pre>
","pandas, nlp, bert-language-model, generative-pretrained-transformer","<p>The first one is using a <strong>string</strong> to <strong>tokenizer</strong>.
The second one you are trying to tokenizer an entire <strong>dataframe</strong>, not a string.</p>
",1,0,404,2022-11-29 07:13:36,https://stackoverflow.com/questions/74610298/translation-with-multi-lingual-bert-model
How to add simple custom pytorch-crf layer on top of TokenClassification model using pytorch and Trainer,"<p>I followed this link, but its implemented in Keras.</p>
<p><a href=""https://stackoverflow.com/questions/67095045/cannot-add-crf-layer-on-top-of-bert-in-keras-for-ner"">Cannot add CRF layer on top of BERT in keras for NER</a></p>
<h3>Model description</h3>
<p>Is it possible to add simple custom <code>pytorch-crf</code> layer on top of <code>TokenClassification model</code>. It will make the model more robust.</p>
<pre><code>from torchcrf import CRF

model_checkpoint = &quot;dslim/bert-base-NER&quot;
tokenizer = BertTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)
config = BertConfig.from_pretrained(model_checkpoint, output_hidden_states=True)
bert_model = BertForTokenClassification.from_pretrained(model_checkpoint,id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)


class BERT_CRF(nn.Module):
    
    def __init__(self, bert_model, num_labels):
        super(BERT_CRF, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.25)
        
        self.classifier = nn.Linear(4*768, num_labels)

        self.crf = CRF(num_labels, batch_first = True)
    
    def forward(self, input_ids, attention_mask,  labels=None, token_type_ids=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)**
        sequence_output = self.dropout(sequence_output)
        
        emission = self.classifier(sequence_output) # [32,256,17]
        labels=labels.reshape(attention_mask.size()[0],attention_mask.size()[1])
        
        if labels is not None:    
            loss = -self.crf(log_soft(emission, 2), labels, mask=attention_mask.type(torch.uint8), reduction='mean')
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return [loss, prediction]
                
        else:         
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return prediction

</code></pre>
<pre><code>args = TrainingArguments(
    &quot;spanbert_crf_ner-pos2&quot;,
    # evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    per_device_train_batch_size=8,
    # per_device_eval_batch_size=32
    fp16=True
    # bf16=True #Ampere GPU
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    # eval_dataset=train_data,
    # data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)
</code></pre>
<p>I get error on line <code>   **sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)**</code></p>
<p>As <code>outputs = self.bert(input_ids, attention_mask=attention_mask)</code> gives the logits for tokenclassification<code>. How can we get hidden states so that I can concate last 4 hidden states. so that I can do</code>outputs[1][-1]`?</p>
<p>Or is their easier way to implement <code>BERT-CRF</code> model?</p>
","python-3.x, pytorch, bert-language-model, named-entity-recognition, crf","<p>i know it's 10 months later, but maybe it helps other guys</p>
<p>Here is what I used for Trainer and it works in hyperparameter_search too:</p>
<pre><code>class BERT_CRF_Config(PretrainedConfig):
    model_type = &quot;BERT_CRF&quot;

    def __init__(self, **kwarg):
        super().__init__(**kwarg)
        self.model_name = &quot;BERT_CRF&quot;
        self.use_last_n_hidden_states = 1
        self.dropout = 0.5

class BERT_CRF(PreTrainedModel):
    config_class = BERT_CRF_Config

    def __init__(self, config):
        super().__init__(config)

        bert_config = BertConfig.from_pretrained(config.bert_name)

        bert_config.output_attentions = True
        bert_config.output_hidden_states = True

        self.bert = AutoModel.from_pretrained(config.bert_name, config=bert_config)

        self.dropout = nn.Dropout(p=config.dropout)

        self.linear = nn.Linear(
            self.bert.config.hidden_size*config.use_last_n_hidden_states, config.num_labels)
        
        self.crf = CRF(config.num_labels, batch_first=True)

    def forward(self,  input_ids = None, attention_mask = None, labels = None,
                labels_mask=None,  token_type_ids=None, return_dict = None, **kwargs):

        if not torch.is_tensor(input_ids):
          input_ids = torch.tensor(input_ids).to(self.device)

        if not torch.is_tensor(token_type_ids):
          token_type_ids = torch.tensor(token_type_ids).to(self.device)

        if not torch.is_tensor(attention_mask):
          attention_mask = torch.tensor(attention_mask).to(self.device)

        if not torch.is_tensor(labels):
          labels = torch.tensor(labels).to(self.device)

        if not torch.is_tensor(labels_mask):
          labels_mask = torch.tensor(labels_mask).to(self.device)

        bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, 
                                attention_mask=attention_mask)
        # last_hidden_layer = bert_output['last_hidden_state']
        # logits = self.linear(last_hidden_layer)

        last_hidden_layers = torch.cat(bert_output['hidden_states'][-self.config.use_last_n_hidden_states:], dim=2)
        last_hidden_layers = self.dropout(last_hidden_layers)
        logits = self.linear(last_hidden_layers)

        def to_tensor(x):
          x = list(map(lambda y: torch.as_tensor(y), x))
          x = torch.nested.as_nested_tensor(x)
          x = torch.nested.to_padded_tensor(x,padding=0)

          x = torch.clamp(x, min=0)

          return x

        if labels is not None:
          log_likelihood, outputs = (
                                     self.crf(logits, labels, mask=labels_mask.bool()), 
                                     self.crf.decode(logits, mask=labels_mask.bool())
                                    )
          outputs = to_tensor(outputs)
          loss = -log_likelihood
          if not return_dict:
            return loss, outputs
          else:
            return TokenClassifierOutput(
                loss=loss,
                logits=outputs,
                hidden_states=bert_output.hidden_states,
                attentions=bert_output.attentions,
            )
        
        outputs = self.crf.decode(logits, batch_first=True)
        outputs = to_tensor(outputs)

        return outputs

    @property
    def device(self):
        return next(self.parameters()).device
</code></pre>
<p>and for your hyperparameter search you can use something like this:</p>
<pre><code>def optuna_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_categorical(&quot;learning_rate&quot;, [1e-5, 2e-5, 2e-5, 4e-5, 5e-5, 6e-5]),
        &quot;warmup_ratio&quot;: trial.suggest_categorical(&quot;warmup_ratio&quot;, [0, 0.1, 0.2, 0.3]),
        &quot;weight_decay&quot;: trial.suggest_categorical(&quot;weight_decay&quot;, [1e-6, 1e-5, 1e-4]),
        &quot;max_grad_norm&quot;: trial.suggest_categorical(&quot;max_grad_norm&quot;, [8, 9,10,11]),
    }

def model_init_crf(trial):
    config = BERT_CRF_Config.from_pretrained(BERT_MODEL, num_labels=NR_LABELS, )
    config.bert_name = BERT_MODEL
    config.dropout = trial.suggest_categorical(&quot;dropout&quot;, [0, 0.10,  0.30,  0.50])
    config.use_last_n_hidden_states = trial.suggest_categorical(&quot;last_n_hidden_states&quot;,
                                                          range(1, config.num_hidden_layers+1))

    model = BERT_CRF(config).to('cuda')
    return model

best_trial = trainer.hyperparameter_search(
    direction=&quot;maximize&quot;,
    backend=&quot;optuna&quot;,
    hp_space=optuna_hp_space,
    n_trials=50,
    compute_objective=my_objective,
)
</code></pre>
",2,1,877,2022-12-06 06:21:53,https://stackoverflow.com/questions/74698116/how-to-add-simple-custom-pytorch-crf-layer-on-top-of-tokenclassification-model-u
How to save and load the custom Hugging face model including config.json file using pytorch,"<p>Model description</p>
<p>I add simple custom <code>pytorch-crf layer</code> on top of <code>TokenClassification model</code>. It will make the model more robust.</p>
<p>I train the model successfully but when I save the mode. The folder doesn't have <code>config.json</code> file inside it. How to save the config.json file for this custom model ?</p>
<p>When I load the custom trained model, the <code>last CRF layer</code> was not there?</p>
<pre><code>from torchcrf import CRF

model_checkpoint = &quot;dslim/bert-base-NER&quot;
tokenizer = BertTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)
bert_model = BertForTokenClassification.from_pretrained(
                        model_checkpoint,id2label=id2label,label2id=label2id)
bert_model.config.output_hidden_states=True


class BERT_CRF(nn.Module):
    
    def __init__(self, bert_model, num_labels):
        super(BERT_CRF, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.25)
        
        self.classifier = nn.Linear(768, num_labels)

        self.crf = CRF(num_labels, batch_first = True)
    
    def forward(self, input_ids, attention_mask,  labels=None, token_type_ids=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        sequence_output = torch.stack((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4])).mean(dim=0)
        sequence_output = self.dropout(sequence_output)
        
        emission = self.classifier(sequence_output) # [32,256,17]
        labels=labels.reshape(attention_mask.size()[0],attention_mask.size()[1])
        
        if labels is not None:    
            loss = -self.crf(log_soft(emission, 2), labels, mask=attention_mask.type(torch.uint8), reduction='mean')
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return [loss, prediction]
                
        else:         
            prediction = self.crf.decode(emission, mask=attention_mask.type(torch.uint8))
            return prediction


model = BERT_CRF(bert_model, num_labels=len(label2id))
model.to(device)

args = TrainingArguments(
    &quot;spanbert_crf_ner2&quot;,
    # evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    per_device_train_batch_size=8,
    # per_device_eval_batch_size=32
    fp16=True
    # bf16=True #Ampere GPU
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    # eval_dataset=train_data,
    # data_collator=data_collator,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer)

trainer.train()
trainer.save_model(&quot;model_spanbert_ner&quot;)
</code></pre>
<p><strong>Saved model</strong></p>
<pre><code>Saving model checkpoint to spanbert_crf_ner2/checkpoint-62500
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in spanbert_crf_ner2/checkpoint-62500/tokenizer_config.json
Special tokens file saved in spanbert_crf_ner2/checkpoint-62500/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 62500/62500 [15:30:27&lt;00:00,  1.12it/s]
Saving model checkpoint to model_spanbert_ner
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'train_runtime': 55837.6817, 'train_samples_per_second': 17.909, 'train_steps_per_second': 1.119, 'train_loss': 1.8942613859863282, 'epoch': 2.0}
tokenizer config file saved in model_spanbert_ner/tokenizer_config.json
Special tokens file saved in model_spanbert_ner/special_tokens_map.json
</code></pre>
<p><strong>Trained model last layer</strong></p>
<pre><code>(11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=21, bias=True)
  )
  (dropout): Dropout(p=0.25, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
  (crf): CRF(num_tags=21)
)
</code></pre>
<p><strong>When I loaded it after training:</strong></p>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(&quot;model_spanbert_ner&quot;,ignore_mismatched_sizes=True)
tokenizer = AutoTokenizer.from_pretrained(&quot;model_spanbert_ner&quot;,model_max_length=512)



(11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=21, bias=True)
</code></pre>
<p><strong>CRF layer was not there??</strong></p>
","python-3.x, pytorch, huggingface-transformers, bert-language-model","<p>You are loading from the original model architecture. Try loading the checkpoint with your own model class:</p>
<pre><code>model = BERT_CRF(bert_model, num_labels=len(label2id))
model.load_state_dict(torch.load(&quot;model_spanbert_ner/pytorch_model.bin&quot;))
model.eval()
</code></pre>
",1,0,2062,2022-12-08 09:09:18,https://stackoverflow.com/questions/74727866/how-to-save-and-load-the-custom-hugging-face-model-including-config-json-file-us
Cast topic modeling outcome to dataframe,"<p>I have used <code>BertTopic</code> with <code>KeyBERT</code> to extract some <code>topics</code> from some <code>docs</code></p>
<pre><code>from bertopic import BERTopic
topic_model = BERTopic(nr_topics=&quot;auto&quot;, verbose=True, n_gram_range=(1, 4), calculate_probabilities=True, embedding_model='paraphrase-MiniLM-L3-v2', min_topic_size= 3)
topics, probs = topic_model.fit_transform(docs)
</code></pre>
<p>Now I can access the <code>topic name</code></p>
<pre><code>freq = topic_model.get_topic_info()
print(&quot;Number of topics: {}&quot;.format( len(freq)))
freq.head(30)

   Topic    Count   Name
0   -1       1     -1_default_greenbone_gmp_manager
1    0      14      0_http_tls_ssl tls_ssl
2    1      8       1_jboss_console_web_application
</code></pre>
<p>and inspect the topics</p>
<pre><code>[('http', 0.0855701486234524),          
 ('tls', 0.061977919455444744),
 ('ssl tls', 0.061977919455444744),
 ('ssl', 0.061977919455444744),
 ('tcp', 0.04551718585531556),
 ('number', 0.04551718585531556)]

[('jboss', 0.14014705432060262),
 ('console', 0.09285308122803233),
 ('web', 0.07323749337563096),
 ('application', 0.0622930523123512),
 ('management', 0.0622930523123512),
 ('apache', 0.05032395169459188)]
</code></pre>
<p>What I want is to have a final data<code>frame</code> that has in one <code>column</code> the <code>topic name</code> and in another <code>column</code> the elements of the <code>topic</code></p>
<pre><code>expected outcome:

  class                         entities
o http_tls_ssl tls_ssl           HTTP...etc
1 jboss_console_web_application  JBoss, console, etc
</code></pre>
<p>and one dataframe with the topic name on different columns</p>
<pre><code>  http_tls_ssl tls_ssl           jboss_console_web_application
o http                           JBoss
1 tls                            console
2 etc                            etc
</code></pre>
<p>I did not find out how to do this. Is there a way?</p>
","python-3.x, pandas, nlp, bert-language-model, topic-modeling","<p>Here is one way to to it:</p>
<h3>Setup</h3>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups

docs = fetch_20newsgroups(subset=&quot;all&quot;, remove=(&quot;headers&quot;, &quot;footers&quot;, &quot;quotes&quot;))[&quot;data&quot;]

topic_model = BERTopic()
# To keep the example reproducible in a reasonable time, limit to 3,000 docs
topics, probs = topic_model.fit_transform(docs[:3_000])

df = topic_model.get_topic_info()
print(df)
# Output
   Topic  Count                    Name
0     -1     23         -1_the_of_in_to
1      0   2635         0_the_to_of_and
2      1    114          1_the_he_to_in
3      2    103         2_the_to_in_and
4      3     59           3_ditto_was__
5      4     34  4_pool_andy_table_tell
6      5     32       5_the_to_game_and
</code></pre>
<h3>First dataframe</h3>
<p>Using Pandas <a href=""https://pandas.pydata.org/docs/user_guide/text.html#string-methods"" rel=""nofollow noreferrer"">string methods</a>:</p>
<pre class=""lang-py prettyprint-override""><code>df = (
    df.rename(columns={&quot;Name&quot;: &quot;class&quot;})
    .drop(columns=[&quot;Topic&quot;, &quot;Count&quot;])
    .reset_index(drop=True)
)

df[&quot;entities&quot;] = [
    [item[0] if item[0] else pd.NA for item in topics]
    for topics in topic_model.get_topics().values()
]

df = df.loc[~df[&quot;class&quot;].str.startswith(&quot;-1&quot;), :]  # Remove -1 topic

df[&quot;class&quot;] = df[&quot;class&quot;].replace(
    &quot;^-?\d+_&quot;, &quot;&quot;, regex=True
)  # remove prefix '1_', '2_', ...
</code></pre>
<pre><code>print(df)
# Output
                  class                                                      entities
1         the_to_of_and                [the, to, of, and, is, in, that, it, for, you]
2          the_he_to_in               [the, he, to, in, and, that, is, of, his, year]
3         the_to_in_and             [the, to, in, and, of, he, team, that, was, game]
4           ditto_was__  [ditto, was, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;]
5  pool_andy_table_tell  [pool, andy, table, tell, us, well, your, about, &lt;NA&gt;, &lt;NA&gt;]
6       the_to_game_and           [the, to, game, and, games, espn, on, in, is, have]
</code></pre>
<h3>Second dataframe</h3>
<p>Using Pandas <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html"" rel=""nofollow noreferrer"">transpose</a>:</p>
<pre class=""lang-py prettyprint-override""><code>other_df = df.T.reset_index(drop=True)
new_col_labels = other_df.iloc[0]  # save first row
other_df = other_df[1:]  # remove first row
other_df.columns = new_col_labels
other_df = pd.DataFrame({col: other_df.loc[1, col] for col in other_df.columns})
</code></pre>
<pre><code>print(other_df)
# Output
  the_to_of_and the_he_to_in the_to_in_and ditto_was__ pool_andy_table_tell the_to_game_and
0           the          the           the       ditto                 pool             the
1            to           he            to         was                 andy              to
2            of           to            in        &lt;NA&gt;                table            game
3           and           in           and        &lt;NA&gt;                 tell             and
4            is          and            of        &lt;NA&gt;                   us           games
5            in         that            he        &lt;NA&gt;                 well            espn
6          that           is          team        &lt;NA&gt;                 your              on
7            it           of          that        &lt;NA&gt;                about              in
8           for          his           was        &lt;NA&gt;                 &lt;NA&gt;              is
9           you         year          game        &lt;NA&gt;                 &lt;NA&gt;            have
</code></pre>
",3,5,711,2022-12-13 12:58:11,https://stackoverflow.com/questions/74785255/cast-topic-modeling-outcome-to-dataframe
Text classification with a Language Model (LM) with class labels existing in text tokens,"<p>I have a multi-label text classification task. The train data labels are categories that might exist as tokens in the training data texts. For instance, some observations look like the following:</p>
<pre><code>Train=[[&quot;input&quot;: &quot;Dogs are animals. Dogs  ove humans.&quot;, class: [&quot;dog&quot;]],
       [&quot;input&quot;: &quot;Cats are running in the street.&quot;, class: [&quot;cat&quot;]],
       [&quot;input&quot;: &quot;Cats and dogs live with humans.&quot;, class: [&quot;cat&quot;, &quot;dog&quot;]],
       [&quot;input&quot;: &quot;These animals don't each chocolate.&quot;, class: [&quot;dog&quot;]]]
</code></pre>
<p>I want to train a classifier by fine-tuning a language model using  Pytorch. My question is if I must ensure that the class labels are masked in the training input text? If not, will the classifier overfit or lose generalizability?</p>
<p>If I must mask the labels in the inputs, how can I do it using Pytorch?</p>
","pytorch, nlp, classification, bert-language-model","<p>First, the label will be mapped to continuous integers, so the model does not know that the text contains the label.<br />
Secondly, you are right, the dog label high probability model will pay more attention to the dog word in the text, but not because the text contains the label, but a feature.<br />
Finally, if you want the model to learn more features that are not related to label words, do not use MASK and replace them with other label words. It is a good solution at present. Refer to <a href=""https://arxiv.org/abs/2210.14975"" rel=""nofollow noreferrer"">MABEL: Attenuating Gender Bias using Textual Entailment Data</a></p>
",0,0,114,2022-12-13 17:34:59,https://stackoverflow.com/questions/74788816/text-classification-with-a-language-model-lm-with-class-labels-existing-in-tex
Fine-tuning distilbert takes hours,"<p>I am fine tuning the distilbert pretrained model for sentiment analysis (multilabel with 6 labels) using Huggingface emotion dataset. I am new to this, but 1 epoch, 250 steps takes around 2 hours to train on Google Colab notebook, is this normal? The train dataset has 16.000 twitter text data which of course affects the performance but isn't this too long? What is the reason behind this?</p>
<p>Also after 3 epochs, the accuracy started to drop. What could be the reason for this?</p>
","machine-learning, nlp, huggingface-transformers, bert-language-model","<p>Are you using a GPU? If not, it's normal that it would take this much time. Also, I wouldn't be bothered by the accuracy if you're not able to run it for more epochs.</p>
",3,1,1156,2022-12-19 22:31:35,https://stackoverflow.com/questions/74856703/fine-tuning-distilbert-takes-hours
BERT embeddings in LSTM model error in fit function,"<p>I am novice in TensorFlow</p>
<p>I am traying to use BERT embeddings in LSTM model
this is my model function</p>
<pre><code>def bert_tweets_model():
    Bertmodel = TFAutoModel.from_pretrained(model_name,output_hidden_states=True)
    
    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=&quot;input_ids&quot;) 
    input_masks_in = tf.keras.Input(shape=(max_length,), name='masked_token', dtype='int32')
    
    with torch.no_grad():
        last_hidden_states = Bertmodel(input_word_ids, attention_mask=input_masks_in)[0]
        
    x = tf.keras.layers.LSTM(100, dropout=0.1, activation='relu',recurrent_dropout=0.3,return_sequences = True)(last_hidden_states)
    x = tf.keras.layers.LSTM(50, dropout=0.1,activation='relu', recurrent_dropout=0.3,return_sequences = True)(x)
    
    x=tf.keras.layers.Flatten()(x)
    
    output = tf.keras.layers.Dense(units = 2, activation='sigmoid')(x)
    
    model = tf.keras.Model(inputs=[input_word_ids, input_masks_in], outputs = output)
    
    return model

with strategy.scope():
    model = bert_tweets_model()
    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
    model.compile(loss='binary_crossentropy',optimizer=adam_optimizer,metrics=['accuracy'])
    model.summary()


validation_data=[dev_encoded, y_val]
train2=[input_id, attention_mask]

history = model.fit(
    x=train2, y=y_train, batch_size=batch_size,
    epochs=3,
    validation_data=validation_data,
    verbose=2)
</code></pre>
<p>I recieved this error in fit function when I tried to input data</p>
<p>&quot;ValueError: Layer &quot;model_1&quot; expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 512) dtype=int32&gt;]&quot;</p>
<p>also,I received these warning massages I do not know what is means.</p>
<p>WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.</p>
<p>can someone help me, thanks in advance.</p>
","python, tensorflow, keras, nlp, bert-language-model","<p><strong>Regenerating your error</strong></p>
<pre><code>_input1 = tf.random.uniform((1,100), 0 , 10)
_input2 = tf.random.uniform((1,100), 0 , 10)
model(_input1, _input2)
</code></pre>
<p>After running this code I am getting the same error...</p>
<pre><code>Layer &quot;model&quot; expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor: shape=(1, 100), ...
</code></pre>
<pre><code>#Now, the problem is you have to enclose the inputs in the set or list then you have to pass the inputs to the model like this

model((_input1, _input2))
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.5324366, 0.3743334]], dtype=float32)&gt;
</code></pre>
<p><strong>Remember:</strong> if you are using <code>tf.data.Dataset</code> then encolse it then while making the dataset enclose the dataset within the set like this
<code>tf.data.Dataset.from_tensor_slices((words_id, words_mask))</code></p>
<p><strong>Second Problem as you asked</strong></p>
<p>The warning you are getting because, you should be aware that LSTM doesn't run in <em>CUDA GPU</em> it uses the <em>CPU</em> only therefore it is slow, so <em>TensorFlow</em> is just telling you that LSTM will not run under GPU or parallel computing.</p>
",0,0,214,2022-12-21 12:32:45,https://stackoverflow.com/questions/74876117/bert-embeddings-in-lstm-model-error-in-fit-function
AttributeError: &#39;TokenClassifierOutput&#39; object has no attribute &#39;backward&#39;,"<p>I'm fine-tuning the BERT model in Named Entity Recognition task using json file dataset, but when I run that code always get an error:</p>
<pre><code>    loss = model(b_input_ids, token_type_ids=None,
                 attention_mask=b_input_mask, labels=b_labels)
    # backward pass
    loss.backward()
</code></pre>
<p>I think the library has been changed but I don't know where can I find 'backward' attribute</p>
","python, bert-language-model","<p>The <a href=""https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"" rel=""nofollow noreferrer""><code>TokenClassifierOutput</code></a> class contains an attribute called <code>loss</code>, which has the type <code>torch.FloatTensor</code>, from which <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"" rel=""nofollow noreferrer""><code>.backward()</code></a> can be called.</p>
<pre class=""lang-py prettyprint-override""><code>token_classifier_output = model(b_input_ids,
                                token_type_ids=None,
                                attention_mask=b_input_mask,
                                labels=b_labels)
token_classifier_output.loss.backward()
</code></pre>
",0,1,537,2022-12-26 19:00:48,https://stackoverflow.com/questions/74923250/attributeerror-tokenclassifieroutput-object-has-no-attribute-backward
Failed to connect to TensorFlow master: TPU worker may not be ready or TensorFlow master address is incorrect,"<p>I signed up for the Tensor Research Cloud (TRC) program for the third time in two years. Now I barely created a preemptible v3-8 TPU. Before that, I could efficiently allocate five non-preemptible v3-8 TPUs. Even with this allocation (preemptible and non-preemptible), the TPU is listed as <strong><code>READY</code></strong> and <strong><code>HEALTHY</code></strong>. However, when I try to access it from the pretraining script, I run into this error that I have never encountered before:</p>
<pre><code>Failed to connect to the Tensorflow master. The TPU worker may not be ready (still scheduling), or the Tensorflow master address is incorrect.
</code></pre>
<p>I know that the TensorFlow master address is correct, and I have checked that the TPU is healthy and ready. I have also double-checked that my code is correctly creating the TensorFlow session and specifying the TPU address.</p>
<p>What could be causing this error message, and how can I troubleshoot and fix it?</p>
<p>I also tried this code from <a href=""https://www.tensorflow.org/guide/tpu"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/tpu</a>. Note that I'm not using Colab but using Google Cloud Platform.</p>
<pre class=""lang-py prettyprint-override""><code>resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='pretrain-1')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(&quot;All devices: &quot;, tf.config.list_logical_devices('TPU'))
</code></pre>
<p>Any I'm stuck at:</p>
<pre><code>INFO:tensorflow:Initializing the TPU system: pretrain-1
</code></pre>
<p>However, I expected something like this:</p>
<pre><code>INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.
2022-12-20 13:08:56.187870: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.
INFO:tensorflow:Initializing the TPU system: grpc://10.99.59.162:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.99.59.162:8470
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]
</code></pre>
<p><strong>Edit:</strong> I successfully accessed the TPU with the same configurations from a new Tensor Research Cloud (TRC) account. However, the problem is still ongoing with the previous TRC account. I suspect it might be a problem with the Google Cloud Platform (GCP) configuration.</p>
","google-cloud-platform, google-compute-engine, bert-language-model, pre-trained-model, tpu","<p>I solved the problem by deleting all TPUs and VM instances and then disabling and reenabling all APIs.</p>
<p>The issue might be related to the VPN connection to a GPU cluster during enabling services.</p>
",0,0,547,2022-12-30 12:01:01,https://stackoverflow.com/questions/74961297/failed-to-connect-to-tensorflow-master-tpu-worker-may-not-be-ready-or-tensorflo
Do I need to retrain Bert for NER to create new labels?,"<p>I am very new to natural language processing and I was thinking about working on named entity recognition NER. A friend of mine who works with NLP advised me to check out BERT, which I did. When reading the documentation and checking out the CoNLL-2003 data set, I noticed that the only labels are person, organization, location, miscellanious and outside. What if instead of outside, I want the model to recognize date, time, and other labels. I get that I would need a dataset labelled as such so, assuming that I have that, do I need to retrain BERT from stratch or can I somehow fine tune the existing model without needing to restart the whole process?</p>
","nlp, bert-language-model, fine-tuning","<p>Yes, you would have to use a model trained using the specific labels you require. The <a href=""https://catalog.ldc.upenn.edu/LDC2013T19"" rel=""nofollow noreferrer"">OntoNotes dataset</a> may be better suited for what you are trying to do, as it includes the 18 entity names listed below (see <a href=""https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf"" rel=""nofollow noreferrer"">OntoNotes 5.0 Release Notes</a> for further info).</p>
<p>The HuggingFace <code>flair/ner-english-ontonotes-large</code> (<a href=""https://huggingface.co/flair/ner-english-ontonotes-large"" rel=""nofollow noreferrer"">here</a>) and <code>flair/ner-english-ontonotes-fast</code> (<a href=""https://huggingface.co/flair/ner-english-ontonotes-fast"" rel=""nofollow noreferrer"">here</a>) models are trained on this dataset and will likely produce results closer to what you desire. As a demo (make sure to <code>pip install flair</code> first)</p>
<pre class=""lang-py prettyprint-override""><code>from flair.data import Sentence
from flair.models import SequenceTagger

tagger = SequenceTagger.load(&quot;flair/ner-english-ontonotes-large&quot;)  # load tagger
sentence = Sentence(&quot;On September 1st George won 1 dollar while watching Game of Thrones.&quot;)  # example sentence
tagger.predict(sentence)  # predict NER tags

# Print sentence and NER spans
print(sentence)
print('The following NER tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('ner'):
    print(entity)

# Output
# Span [2,3]: &quot;September 1st&quot;   [− Labels: DATE (1.0)]
# Span [4]: &quot;George&quot;   [− Labels: PERSON (1.0)]
# Span [6,7]: &quot;1 dollar&quot;   [− Labels: MONEY (1.0)]
# Span [10,11,12]: &quot;Game of Thrones&quot;   [− Labels: WORK_OF_ART (1.0)
</code></pre>
<h2>OntoNotes 5.0 Named Entities</h2>
<ol>
<li><code>PERSON</code> (People, including fictional)</li>
<li><code>NORP</code> (Nationalities or religious or political groups)</li>
<li><code>FACILITY</code> (Buildings, airports, highways, bridges, etc.)</li>
<li><code>ORGANIZATION</code> (Companies, agencies, institutions, etc.)</li>
<li><code>GPE</code> (Countries, cities, states)</li>
<li><code>LOCATION</code> (Non-GPE locations, mountain ranges, bodies of water)</li>
<li><code>PRODUCT</code> (Vehicles, weapons, foods, etc. (Not services))</li>
<li><code>EVENT</code> (Named hurricanes, battles, wars, sports events, etc.)</li>
<li><code>WORK OF ART</code> (Titles of books, songs, etc.)</li>
<li><code>LAW</code> (Named documents made into laws)</li>
<li><code>LANGUAGE</code> (Any named language)</li>
<li><code>DATE</code> (Absolute or relative dates or periods)</li>
<li><code>TIME</code> (Times smaller than a day)</li>
<li><code>PERCENT</code> (Percentage (including “%”))</li>
<li><code>MONEY</code> (Monetary values, including unit)</li>
<li><code>QUANTITY</code> (Measurements, as of weight or distance)</li>
<li><code>ORDINAL</code> (“first”, “second”)</li>
<li><code>CARDINAL</code> (Numerals that do not fall under another type)</li>
</ol>
",0,1,643,2023-01-02 00:22:59,https://stackoverflow.com/questions/74978191/do-i-need-to-retrain-bert-for-ner-to-create-new-labels
Do BERT word embeddings change depending on context?,"<p>Before answering &quot;yes, of course&quot;, let me clarify what I mean:</p>
<p>After BERT has been trained, and I want to use the pretrained embeddings for some other NLP task, can I once-off extract all the word-level embeddings from BERT for all the words in my dictionary, and then have a set of static key-value word-embedding pairs, from where I retrieve the embedding for let's say &quot;bank&quot;, or will the embeddings for &quot;bank&quot; change depending on whether the sentence is &quot;Trees grow on the river bank&quot;, or &quot;I deposited money at the bank&quot; ?</p>
<p>And if the latter is the case, how do I practically use the BERT embeddings for another NLP task, do I need to run every input sentence through BERT before passing it into my own model?</p>
<p>Essentially - do embeddings stay the same for each word / token after the model has been trained, or are they dynamically adjusted by the model weights, based on the context?</p>
","nlp, huggingface-transformers, bert-language-model, embedding, transformer-model","<p>This is a great question (I had the same question but you asking it made me experiment a bit).</p>
<p>The answer is yes, it changes based on the context. You should not extract the embeddings and re-use them (at least for most of the problems).</p>
<p>I'm checking the embedding for word <strong>bank</strong> in two cases: (1) when it comes separately and when it comes with a context (river bank). The embeddings that I'm getting are different from each other (they have a cosine distance of ~0.4).</p>
<pre><code>from transformers import TFBertModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')

print('bank is the second word in tokenization (index=1):', tokenizer.decode([i for i in tokenizer.encode('bank')]))
print('bank is the third word in tokenization (index=2):', tokenizer.decode([i for i in tokenizer.encode('river bank')]))
###output: bank is the second word in tokenization (index=1): [CLS] bank [SEP]
###output: bank is the third word in tokenization (index=2): [CLS] river bank [SEP]

bank_bank = model(tf.constant(tokenizer.encode('bank'))[None,:])[0][0,1,:] #use the index based on the tokenizer output above
river_bank_bank = model(tf.constant(tokenizer.encode('river bank'))[None,:])[0][0,2,:] #use the index based on the tokenizer output above

are_equal = np.allclose(bank_bank, river_bank_bank)

print(are_equal)
### output: False
</code></pre>
",1,2,1111,2023-01-03 17:58:21,https://stackoverflow.com/questions/74996994/do-bert-word-embeddings-change-depending-on-context
Why is positional encoding needed while input ids already represent the order of words in Bert?,"<p>For example, in Huggingface's example:</p>
<pre><code>encoded_input = tokenizer(&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;)
print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>The input_ids vector already encode the order of each token in the original sentence. Why does it need positional encoding again with an extra vector to represent it?</p>
","huggingface-transformers, bert-language-model","<p>The reason is the design of the neural architecture. BERT consists of self-attention and feedforward sub-layers, and neither of them is sequential.</p>
<p>The feedforward layers process each token independently of others.</p>
<p>The self-attention views the input states as an unordered set of states. Attention can be interpreted as soft probabilistic retrieval from a set of values according to some keys. The position embeddings are there so the keys can contain information about their relative order.</p>
",0,0,534,2023-01-08 19:42:12,https://stackoverflow.com/questions/75050748/why-is-positional-encoding-needed-while-input-ids-already-represent-the-order-of
Having trouble understanding the predictions array in classification model evaluation,"<p>I'm working on a sarcasm detector with the BERT model (binary classification). Currently, I'm having trouble with the model evaluation as I don't really understand the predictions array. The model should output 1 for sarcastic and 0 for not, but the predictions don't output that. Please let me know if more code is needed. Thank you!</p>
<p>model:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
    attention_probs_dropout_prob=0.25,
    hidden_dropout_prob=0.25
)

# Tell pytorch to run this model on the GPU.
model.cuda()
</code></pre>
<p>evaluation:</p>
<pre><code>from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd

print('Predicting labels for {:,} test sentences...'.format(len(eval_input_ids)))

# Put model in evaluation mode
model.eval()

predictions , true_labels = [], []


# iterate over test data
for batch in eval_dataloader:
  batch = tuple(t.to(device) for t in batch)
  
  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch
  
  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions.
      result = model(b_input_ids, 
                     token_type_ids=None, 
                     attention_mask=b_input_mask,
                     return_dict=True)

  logits = result.logits

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)

true_labels[1]
predictions[1]
</code></pre>
<p>output:</p>
<pre><code>array([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
   0, 1, 1, 0, 0, 0, 0, 1, 1, 1]) &lt;-- true_labels[1]
array([[ 2.9316974 , -2.855342  ],
   [ 3.4540875 , -3.3177233 ],
   [ 2.7424026 , -2.6472614 ],
   [-3.4326897 ,  3.330751  ],
   [ 3.7238903 , -3.7757814 ],
   [-3.208891  ,  3.175109  ],
   [ 3.0500402 , -2.8103237 ],
   [ 3.8333693 , -3.9073608 ],
   [-3.2779126 ,  3.231213  ],
   [ 1.484127  , -1.2610332 ],
   [ 3.686339  , -3.7582958 ],
   [-2.1883147 ,  2.205132  ],
   [-3.274582  ,  3.2254982 ],
   [-1.606854  ,  1.6213335 ],
   [ 3.7080388 , -3.6854186 ],
   [-2.351147  ,  2.365543  ],
   [-3.7317555 ,  3.4833894 ],
   [ 3.2413306 , -3.2116275 ],
   [ 3.7413723 , -3.7767386 ],
   [-3.6293464 ,  3.4446163 ],
   [ 3.7779078 , -3.9025154 ],
   [-3.5576923 ,  3.403335  ],
   [ 3.6226897 , -3.6370063 ],
   [-3.7081888 ,  3.4720154 ],
   [ 1.1533121 , -0.8105195 ],
   [ 1.0573612 , -0.69238156],
   [ 3.4189024 , -3.4764926 ],
   [-0.13847755,  0.450572  ],
   [ 3.7248163 , -3.7781181 ],
   [-3.2015219 ,  3.1719215 ],
   [-2.1409311 ,  2.1202204 ],
   [-3.470693  ,  3.358798  ]], dtype=float32) &lt;-- predictions[1]
</code></pre>
","deep-learning, bert-language-model, text-classification","<p>There are two values because you have two classes (0=no, 1=yes). These values are logits, which when fed into a softmax function gives the probability of each class. If you want to know whether the sample is classified as sarcasm or not, just take the class with the highest logit:</p>
<pre><code>predictions = a.max(1)[1]
print(predictions)
</code></pre>
",1,0,62,2023-01-09 18:21:10,https://stackoverflow.com/questions/75061462/having-trouble-understanding-the-predictions-array-in-classification-model-evalu
Trouble in installing BERTopic&#39;s dependency &#39;&#39;bertopic.dimensionality&#39;&#39;,"<p>I'm trying to run the following code from the <a href=""https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html#skip-dimensionality-reduction"" rel=""nofollow noreferrer"">BERTopic documentation</a>:</p>
<pre><code>from bertopic import BERTopic
from bertopic.dimensionality import BaseDimensionalityReduction

# Fit BERTopic without actually performing any dimensionality reduction
empty_dimensionality_model = BaseDimensionalityReduction()
topic_model = BERTopic(umap_model=empty_dimensionality_model)
</code></pre>
<p>However, despite the fact that bertopic is installed and updated in my machine, I keep getting the output <code>ModuleNotFoundError: No module named 'bertopic.dimensionality' </code> . Is there any discontinuity? Or how can I assute to install <code>bertopic.dimensionality</code>?</p>
","python, bert-language-model, topic-modeling","<p>I would advise starting from a completely fresh environment. Since it cannot be found in your environment and since the feature was added in the newest version of BERTopic (v0.13), that version might not yet be installed. You can check your version with <code>import bertopic; bertopic.__version__</code>. When you start from a completely fresh environment any issues with caching, conflicting dependencies, or version issues are often fixed. For me, it is working using the most recent release of BERTopic.</p>
",0,0,3024,2023-01-11 13:31:47,https://stackoverflow.com/questions/75083854/trouble-in-installing-bertopics-dependency-bertopic-dimensionality
BERT model conversion from DeepPavlov to HuggingFace format,"<p>I have a folder with the ruBERT model, which was fine-tuned with the application of the Deeppavlov library.</p>
<p>The folder contains the following model files:</p>
<p><a href=""https://i.sstatic.net/mKdZE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mKdZE.png"" alt=""enter image description here"" /></a></p>
<p>How do I convert it to Huggingface format so that I can load it this way?</p>
<pre><code>from transformers import TFAutoModelForSequenceClassification

model_name = &quot;folder_with_ruBERT&quot;
auto_model_rubert = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case = False)
</code></pre>
","python-3.x, huggingface-transformers, bert-language-model, huggingface, deeppavlov","<p>There is no need to convert the tf checkpoints into huggingface format. The DeepPavlov's pretrained language models are already available as huggingface models.</p>
<p><a href=""https://huggingface.co/DeepPavlov"" rel=""nofollow noreferrer"">https://huggingface.co/DeepPavlov</a></p>
",2,1,111,2023-01-18 04:38:43,https://stackoverflow.com/questions/75154742/bert-model-conversion-from-deeppavlov-to-huggingface-format
Is splitting a long document of a dataset for BERT considered bad practice?,"<p>I am fine-tuning a BERT model on a labeled dataset with many documents longer than the 512 token limit set by the tokenizer.
Since truncating would lose a lot of data I would rather use, I started looking for a workaround. However I noticed that simply splitting the documents after 512 tokens (or another heuristic) and creating new entries in the dataset with the same label was never mentioned.</p>
<p>In <a href=""https://stackoverflow.com/a/58643108/11614170"">this</a> answer, someone mentioned that you would need to recombine the predictions, is that necessary when splitting the documents?</p>
<p>Is this generally considered bad practice or does it mess with the integrity of the results?</p>
","machine-learning, nlp, classification, bert-language-model, text-classification","<p>You have not mentioned if your intention is to classify, but given that you refer to an article on classification I will refer to an approach where you classify the whole text.</p>
<p>The main question is - which part of the text is the most informative for your purpose - or - in other words - does it make sense to use more than the first / last split of text?</p>
<p>When considering long passages of text, frequently, it is enough to consider the first (or last) 512 tokens to <em>correctly</em> predict the class in substantial majority of cases (say 90%). Even though you may loose some precision, you gain on speed and performance of the overall solution <em>and</em> you are getting rid of a nasty problem of figuring out the correct class out of a set of classifications. Why?</p>
<p>Consider an example of text 2100 tokens long. You split it by 512 tokens, obtaining pieces: 512, 512, 512, 512, 52 (notice the small last piece - should you even consider it?). Your target class for this text is, say, A, however you get the following predictions on the pieces: A, B, A, B, C. So you have now a headache to figure out the right method to determine the class. You can:</p>
<ul>
<li>use majority voting but it is not conclusive here.</li>
<li>weight the predictions by the length of the piece. Again non conclusive.</li>
<li>check that prediction of the last piece is class C but it is barely above the threshold and class C is kinda A. So you are leaning towards A.</li>
<li>re-classify starting the split from the end. In the same order as before you get: A, B, C, A, A. So, clearly A. You also get it when you majority vote combining all of the classifications (forward and backward splits).</li>
<li>consider the confidence of the classifications, e.g. A: 80, B: 70, A: 90, B: 60, C: 55% - avg. 85% for A vs. 65% for B.</li>
<li>reconfirm the correction of labelling of the last piece manually: if it turns out to be B, then it changes all of the above.</li>
<li>then you can train an additional network to classify out of the <em>raw</em> classifications of pieces. Getting again into trouble of figuring out what to do with particularly long sequences or non-conclusive combinations of predictions resulting in poor confidence of the additional classification layer.</li>
</ul>
<p>It turns out that there is no easy way. And you will notice that text is a strange classification material exhibiting all of the above (and more) issues while typically the difference in agreement between the first piece prediction and the annotation vs. the ultimate, perfect classifier is slim at best.</p>
<p><strong>So, spare the effort and strive for simplicity, performance, and heuristic... and clip it!</strong></p>
<p>On details of the best practices you should probably refer to the article from <a href=""https://stackoverflow.com/a/59778726/6573902"">this answer</a>.</p>
",3,1,1569,2023-01-19 23:20:18,https://stackoverflow.com/questions/75179250/is-splitting-a-long-document-of-a-dataset-for-bert-considered-bad-practice
How to access BERT&#39;s inter layer?,"<p>I want to put [batch_size, 768, text_length] tensor into
6th layer of BERT.</p>
<ol>
<li><p>How can I give input to 6th layer?</p>
</li>
<li><p>Can I take just 6~last layer of BERT then use it?</p>
</li>
</ol>
<p>Thank you.</p>
","nlp, bert-language-model, pre-trained-model","<p>If you want to use only the last 6 layers of BERT, you can create a new model that only consists of these layers, using the following code:</p>
<pre><code>import torch
from transformers import BertModel

model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
last_6_layers = torch.nn.ModuleList(model.bert.encoder.layer[-6:]).to(device)
</code></pre>
<p>This will create a new model last_6_layers that consists of only the last 6 layers of the BERT model. You can then input your [batch_size, 768, text_length] tensor into this new model and use it for your specific task.</p>
",1,1,350,2023-01-28 07:29:42,https://stackoverflow.com/questions/75266069/how-to-access-berts-inter-layer
How to use GPU for training instead of CPU?,"<p>I was replicating the code which is fine-tuned for Domain Adaptation. This is the main link to the post for more details:</p>
<p>(<a href=""https://towardsdatascience.com/fine-tuning-for-domain-adaptation-in-nlp-c47def356fd6"" rel=""nofollow noreferrer"">https://towardsdatascience.com/fine-tuning-for-domain-adaptation-in-nlp-c47def356fd6</a>)</p>
<p>The code is as such:</p>
<pre><code>!pip install -q transformers
!pip install -q datasets

import multiprocessing
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import transformers
from datasets import Dataset
from sklearn.model_selection import train_test_split

from transformers import AutoModelForMaskedLM
from transformers import AutoTokenizer, AutoConfig
from transformers import BertForMaskedLM, DistilBertForMaskedLM
from transformers import BertTokenizer, DistilBertTokenizer
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
from tokenizers import BertWordPieceTokenizer

# HYPERPARAMS
SEED_SPLIT = 0
SEED_TRAIN = 0

MAX_SEQ_LEN = 128
TRAIN_BATCH_SIZE = 16
EVAL_BATCH_SIZE = 16
LEARNING_RATE = 2e-5 
LR_WARMUP_STEPS = 100
WEIGHT_DECAY = 0.01

# load data
dtf_mlm = pd.read_csv('data/jigsaw_train.csv', nrows=1000)
dtf_mlm = dtf_mlm[dtf_mlm[&quot;target&quot;] &lt; 0.5]
dtf_mlm = dtf_mlm.rename(columns={&quot;comment_text&quot;: &quot;text&quot;})


# Train/Valid Split
df_train, df_valid = train_test_split(
    dtf_mlm, test_size=0.15, random_state=SEED_SPLIT
)

len(df_train), len(df_valid)

# Convert to Dataset object
train_dataset = Dataset.from_pandas(df_train[['text']].dropna())
valid_dataset = Dataset.from_pandas(df_valid[['text']].dropna())

#Model Selection Part
MODEL = 'bert'
bert_type = 'bert-base-cased'

TokenizerClass = BertTokenizer
ModelClass = BertForMaskedLM 


#Tokenization Part
tokenizer = TokenizerClass.from_pretrained(
            bert_type, use_fast=True, do_lower_case=False, max_len=MAX_SEQ_LEN
            )
model = ModelClass.from_pretrained(bert_type)


def tokenize_function(row):
    return tokenizer(
        row['text'],
        padding='max_length',
        truncation=True,
        max_length=MAX_SEQ_LEN,
        return_special_tokens_mask=True)
  
column_names = train_dataset.column_names

train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    num_proc=multiprocessing.cpu_count(),
    remove_columns=column_names,
)

valid_dataset = valid_dataset.map(
    tokenize_function,
    batched=True,
    num_proc=multiprocessing.cpu_count(),
    remove_columns=column_names,
)


#Training and Model Saving Part
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)


steps_per_epoch = int(len(train_dataset) / TRAIN_BATCH_SIZE)

training_args = TrainingArguments(
    output_dir='./bert-news',
    logging_dir='./LMlogs',             
    num_train_epochs=2,
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    warmup_steps=LR_WARMUP_STEPS,
    save_steps=steps_per_epoch,
    save_total_limit=3,
    weight_decay=WEIGHT_DECAY,
    learning_rate=LEARNING_RATE, 
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='loss', 
    greater_is_better=False,
    seed=SEED_TRAIN
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    tokenizer=tokenizer,
)

trainer.train()
trainer.save_model(&quot;SavedModel/TestModel&quot;) #save your custom model


</code></pre>
<p>And this is the GPU that I am using:
<img src=""https://i.sstatic.net/kbWXm.png"" alt=""NVDIA RTX A6000"" /></p>
<p>I want to use the GPU for training the model on about 1.5 million comments.</p>
<p>I tried doing this:</p>
<pre><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


#Setting the tokenizer and the model
tokenizer = TokenizerClass.from_pretrained(
            bert_type, use_fast=True, do_lower_case=False, max_len=MAX_SEQ_LEN
            )
model = ModelClass.from_pretrained(bert_type).to(device)

</code></pre>
<p>But I am unsure how to send the inputs and tokens to the GPU.</p>
<p>Feel free to give your advice, and <em>I don't owe this code</em>, shout out to <strong>Marcello Politi</strong>. Thanks!</p>
","pytorch, gpu, bert-language-model, pre-trained-model","<p>After you load the dataset you should add:</p>
<pre class=""lang-py prettyprint-override""><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'
train_dataset = train_dataset.to(device)
</code></pre>
",1,0,2213,2023-01-28 07:33:07,https://stackoverflow.com/questions/75266086/how-to-use-gpu-for-training-instead-of-cpu
&#39;tuple&#39; object does not support item assignment in torch.cat(),"<p>I am trying to use the torch.cat() to contenate the torch tensor. However, I face the error messagge with --&gt; 'tuple' object does not support item assignment.</p>
<p>Here are my code:</p>
<pre><code>inputs = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors=&quot;pt&quot;)
input_id_chunks = inputs[&quot;input_ids&quot;][0].split(510)
mask_chunks = inputs[&quot;attention_mask&quot;][0].split(510)

print(type(input_id_chunks))

for i in range(len(input_id_chunks)):
    print(type(input_id_chunks[i]))
    print(input_id_chunks[i])

    input_id_chunks[i] = torch.cat([
        torch.Tensor([101]), input_id_chunks[i], torch.Tensor([102])
    ])
</code></pre>
<p>The outputs looks fine, the inputs_id_chunks[i] is torch.Tensor:</p>
<p>`&lt;class 'tuple'&gt;</p>
<p>&lt;class 'torch.Tensor'&gt;`</p>
<p>But I got the following print and error message:</p>
<p>TypeError: 'tuple' object does not support item assignment</p>
<p>in torch.cat()</p>
<p>I have using the small testing code for torch.cat() and it works fine, but I don't know what is missing in my original codes.</p>
","python, pytorch, torch, bert-language-model","<p>you can't change tuple value, instead you can assign it to list, then append new value to it and then after all changes you want to implement, you should assign again it to tuple.</p>
<p>please check this <a href=""https://www.w3schools.com/python/gloss_python_change_tuple_item.asp"" rel=""nofollow noreferrer"">link</a></p>
",0,-1,617,2023-02-06 11:54:25,https://stackoverflow.com/questions/75360974/tuple-object-does-not-support-item-assignment-in-torch-cat
Bert Tokenizer punctuation for named entity recognition task,"<p>I'm working on a named entity recognition task, where I need to identify person names, books etc.</p>
<p>I am using Huggingface Transformer package and BERT with PyTorch. Generally it works very good, however, my issue is that for some first names a dot &quot;.&quot; is a part of the first name and shouldn't be separate it from it. For example, for the person name &quot;Paul Adam&quot;, the first name in the training data is shortened to one letter combined with dot &quot;P. Adam&quot;. The tokenizer tokenize it as [&quot;P&quot;, &quot;.&quot;, &quot;Adam&quot;] which later negatively impact the ner trained model performance as &quot;P.&quot; is presented in the training data and not only &quot;P&quot;. The model is capable to recognize the full names but fails in the shortened one. I used Spacy tokenizer before and I didn't face this issue. Here more details:</p>
<pre><code>from transformers import BertTokenizer, BertConfig, AutoTokenizer, AutoConfig, BertModel
path_pretrained_model='/model/bert/'
tokenizer = BertTokenizer.from_pretrained(path_pretrained_model)

print(tokenizer.tokenize(&quot;P. Adam is a scientist.&quot;))

Output:
['p', '.', 'adam', 'is', 'a', 'scientist', '.']

The helpful output would be 
['p.', 'adam', 'is', 'a', 'scientist', '.']
</code></pre>
","huggingface-transformers, bert-language-model, named-entity-recognition, punctuation","<p>Not sure whether this might be a viable solution for you, but here's a possible hack.</p>
<pre><code>from transformers import BertTokenizer, BertConfig, AutoTokenizer, AutoConfig, BertModel
import string

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenization=True, never_split=[f&quot;{letter}.&quot; for letter in list(string.ascii_lowercase)])

print(tokenizer.tokenize(&quot;P. Adam is a scientist.&quot;))   
# ['p.', 'adam', 'is', 'a', 'scientist', '.']
</code></pre>
<p>Indeed, from the <a href=""https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"" rel=""nofollow noreferrer"">documentation</a></p>
<blockquote>
<p>never_split (Iterable, optional) — Collection of tokens which will never be split during tokenization. Only has an effect when do_basic_tokenize=True</p>
</blockquote>
",0,1,477,2023-02-14 09:03:10,https://stackoverflow.com/questions/75445494/bert-tokenizer-punctuation-for-named-entity-recognition-task
What does the embedding elements stand for in huggingFace bert model?,"<p>Prior to passing my tokens through encoder in BERT model, I would like to perform some processing on their embeddings. I extracted the embedding weight using:</p>
<pre><code>from transformers import TFBertModel

# Load a pre-trained BERT model
model = TFBertModel.from_pretrained('bert-base-uncased')

# Get the embedding layer of the model
embedding_layer = model.get_layer('bert').get_input_embeddings()

# Extract the embedding weights
embedding_weights = embedding_layer.get_weights()
</code></pre>
<p>I found it contains 5 elements as shown in Figure.
<a href=""https://i.sstatic.net/sSCfy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>In my understanding, the first three elements are the word embedding weights, token type embedding weights, and positional embedding weights. My question is what does the last two elements stand for?</p>
<p>I dive deep into the source code of bert model. But I cannot figure out the meaning of the last two elements.</p>
","tensorflow, nlp, huggingface-transformers, bert-language-model, word-embedding","<p>In bert model, there is a post-processing of the embedding tensor that uses layer normalization followed by dropout ,
<a href=""https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L362"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L362</a></p>
<p>I think that those two arrays are the gamma and beta of the normalization layer, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization</a>
They are learned parameters, and will span the axes of inputs specified in param &quot;axis&quot; which defaults to -1 (corresponding to 768 in embedding tensor).</p>
",0,1,356,2023-02-18 06:06:46,https://stackoverflow.com/questions/75491528/what-does-the-embedding-elements-stand-for-in-huggingface-bert-model
How to use tapas table question answer model when table size is big like containing 50000 rows?,"<p>I am trying to build up a model in which I load the dataframe (an excel file from Kaggle) and I am using TAPAS-large-finetuned-wtq model to query this dataset. I tried to query 259 rows (the memory usage is 62.9 KB). I didn't have a problem, but then I tried to query 260 rows with memory usage 63.1KB, and I have the error which says: &quot;index out of range in self&quot;. I have attached a screenshot for the reference as well. The data I used here can be found from Kaggle <a href=""https://www.kaggle.com/datasets/joseventura21/turismsdataset"" rel=""nofollow noreferrer"">datasets</a>.</p>
<p><a href=""https://i.sstatic.net/BLQSY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BLQSY.png"" alt=""enter image description here"" /></a></p>
<p>The code I am using is:</p>
<pre><code>from transformers import pipeline
import pandas as pd
import torch

question = &quot;Which Country code has the quantity 30604?&quot;
tqa = pipeline(task=&quot;table-question-answering&quot;, model=&quot;google/tapas-large-finetuned-wtq&quot;)

c = tqa(table=df[:100], query=question)['cells']
</code></pre>
<p>In the last line, as you can see in the screenshot, I get the error.</p>
<p>Please let me know what can be the way I can work for a solution? Any tips would be welcome.</p>
","huggingface-transformers, bert-language-model, nlp-question-answering","<p>The way TAPAS works it needs to flatten the table into a sequence of word pieces.
This sequence needs to fit into the specified maximal sequence length (default is 512).
TAPAS has a pruning mechanism that will try to drop tokens but it will never drop cells.
Therefore at a sequence length of 512 there is no way to fit a table with more than 512 cells.</p>
<p>If you really want to run the model on 1.8M rows I would suggest that you split your data row-wise.
For your table for example you would need blocks with a maximum of ~8 rows.</p>
<p>Alternatively, you can increase the sequence size but that will also increase the cost of running the model.</p>
<p><a href=""https://github.com/google-research/tapas/issues/14"" rel=""nofollow noreferrer"">https://github.com/google-research/tapas/issues/14</a></p>
",1,1,2316,2023-02-18 18:02:43,https://stackoverflow.com/questions/75495337/how-to-use-tapas-table-question-answer-model-when-table-size-is-big-like-contain
Bert model for word similarity,"<p>I'm quite new to NLP, and I want to calculate the similarity between a given word and each word in a given list.
I have the following code</p>
<pre><code># Load the BERT model
model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Encode the target word and the list of words
target_word = &quot;apple&quot;
word_list = [&quot;blackberry&quot;, &quot;iphone&quot;, &quot;microsoft&quot;, &quot;blueberry&quot;, &quot;pineapple&quot;]

# Tokenization of the target word and  the list of words

target_word_encoded = tokenizer.encode(target_word, return_tensors='pt').unsqueeze(0)
word_list_encoded = [tokenizer.encode(word, return_tensors='pt').unsqueeze(0) for word in word_list]

# Pad each sequence so they have the same length
max_len = max(target_word_encoded.shape[1], max(word_encoded.shape[1] for word_encoded in word_list_encoded))
target_word_encoded = torch.nn.functional.pad(target_word_encoded, (0, 0, 0, max_len - target_word_encoded.shape[1]))
word_list_encoded = [torch.nn.functional.pad(word_encoded, (0, 0, 0, max_len - word_encoded.shape[1])) for word_encoded in word_list_encoded]

# Calculate the similarities
with torch.no_grad():
    target_word_encoded = target_word_encoded.squeeze(0)
    target_word_embedding = model(input_ids=target_word_encoded)[1]['last_hidden_state'][0]
    similarities = []
    for word_encoded in word_list_encoded:
        word_encoded = word_encoded.squeeze(0)
        word_embedding = model(input_ids=word_encoded)[1]['last_hidden_state'][0]
        similarity = torch.nn.functional.cosine_similarity(target_word_embedding, word_embedding).item()
        similarities.append(similarity)

# Print the similarities
for word, similarity in zip(word_list, similarities):
    print(f&quot;Similarity between '{target_word}' and '{word}': {similarity:.2f}&quot;)
</code></pre>
<p>with this code I got the following error <strong>too many indices for tensor of dimension 2</strong></p>
<p>what does it means and how to fix it to get the result</p>
<p>Thanks in advance</p>
<p>I want to calculate the similarity of a given list of words using transformers &quot;the bert model&quot;.</p>
","python, deep-learning, huggingface-transformers, bert-language-model","<p>First of all, the similarity is a tricky word because there are different types of similarities. Especially semantic and sentimental similarities are very different concepts. For example, while good and bad are sentimental opposite words, they are semantically similar words. The basic BERT model is trained to capture the semantic similarity of the language. Therefore if you want to measure sentimental similarity, you can use BERT models for sentiment analysis. I suggest other similarity techniques for your task, like glove-embedding.</p>
<p>Regarding of your question, there are a couple of errors in your implementation.</p>
<ol>
<li>Output of the models is a dict. When you accessed the first item, you already accessed the 'last_hidden_state'. You don't need the [1] before the 'last_hidden_state'.</li>
<li>Bert-type transformers use tokenizers that can split the word to multiple tokens. One solution for this issue, you can take the average of the tokens which is basically the average of the output except for first and last elements.</li>
<li>Your cosine similarity function will give an error when you run the code.</li>
</ol>
<pre><code>    # Calculate the similarities
    with torch.no_grad():
    target_word_encoded = target_word_encoded.squeeze(0)
    target_word_embedding = torch.mean(model(input_ids=target_word_encoded)['last_hidden_state'][0][1:-1],dim=0)
    similarities = []
    for word_encoded in word_list_encoded:
        word_encoded = word_encoded.squeeze(0)
        word_embedding = torch.mean(model(input_ids=word_encoded)['last_hidden_state'][0][1:-1],dim=0)
        similarity = torch.nn.functional.cosine_similarity(target_word_embedding.reshape(1,-1), word_embedding.reshape(1,-1)).item()
        similarities.append(similarity)
</code></pre>
",0,0,571,2023-02-22 10:05:32,https://stackoverflow.com/questions/75531115/bert-model-for-word-similarity
transformers refine-tune with different classes,"<p>I want to fine-tune a BERT-based already fine-tuned model for classification with 7 classes another time on a 16 class dataset:</p>
<pre><code>MODEL_NAME_OR_PATH = 'some pretrained model for 7 class classification on huggingface repo'
model = build_model(MODEL_NAME_OR_PATH, learning_rate=LEARNING_RATE)

def build_model(model_name, learning_rate=3e-5):
    model = TFBertForSequenceClassification.from_pretrained(model_name)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

   return model
r = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    steps_per_epoch=train_steps,
    validation_steps=valid_steps,
    epochs=EPOCHS,
    verbose=1)
</code></pre>
<p>As expected the model expects 7 class at the final layer, and produces the following error:</p>
<pre><code>Node: 
'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'
Received a label value of 9 which is outside the valid range of [0, 8).  Label values: 6 2 0 6 0 9 6 6 0 6 6 0 7 2 2 2
     [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_43224]
</code></pre>
<p>How should one edit the structure of the model?</p>
","python, classification, huggingface-transformers, bert-language-model","<p>For further references, you need to edit the final layer. In my case, as i was using tensorflow:</p>
<pre><code>model.classifier = tf.keras.layers.Dense(nunits)
</code></pre>
",0,0,38,2023-02-24 19:03:48,https://stackoverflow.com/questions/75560424/transformers-refine-tune-with-different-classes
Tensorflow 2.X Error - Op type not registered &#39;CaseFoldUTF8&#39; in binary running on Colab,"<p>I have been using BERT encoder from the Tensorflow hub for quite sometime now. Here are the syntaxes:</p>
<p><code>tfhub_handle_encoder = &quot;https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4&quot; tfhub_handle_preprocess = &quot;https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3&quot; bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)</code></p>
<p>All off a sudden I am encountering this error message:
<code>FileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on acb9309ebd87. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) </code>tf.contrib.resampler<code>should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. You may be trying to load on a different device from the computational device. Consider setting the</code>experimental_io_device<code>option in</code>tf.saved_model.LoadOptions<code> to the io_device such as '/job:localhost'.</code></p>
<p>The same code has been running well for years now. Tf Version - 2.9, Environment - Colab/ GCP Vertex AI</p>
","tensorflow2.0, bert-language-model","<p>This ambiguous error message is due to importing tensorflow_hub package after calling the encoder. The issue got resolved when I pushed the import statement before calling the encoder.</p>
",2,4,7735,2023-02-27 05:17:42,https://stackoverflow.com/questions/75576980/tensorflow-2-x-error-op-type-not-registered-casefoldutf8-in-binary-running-o
Why is BERT Storing cache even after Caching is disabled?,"<p>I am trying to extract hidden state features from a fine-tuned BERT model, but each text entry consumes memory and does not free it up after the next call. I can only run 20-30 sentences with 24 GB of ram memory.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel
import numpy as np

data = pd.read_csv('https://docs.google.com/spreadsheets/d/' + 
                   '1cFyUJdpFC3gpQsqjNc4D8ZCxBAMd_Pcpu8SlrsjAv-Q' +
                   '/export?gid=0&amp;format=csv',
                  )
data = data.MESSAGES


# I will be using my own fine-tuned model, but with
# bert-base-uncased, I get the same problem
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(
    &quot;bert-base-cased&quot;,
    from_tf=True,
    output_hidden_states=True,
    use_cache=False)

sentences = data[0:].tolist()
inputs = tokenizer(sentences, return_tensors='pt', padding=True,truncation=True)
featuresINeed = model(inputs['input_ids'])['pooler_output']
</code></pre>
<p>In the case above, I run out of memory. I tried breaking it into chunks and using <code>torch.cuda.empty_cache()</code>, but it doesn't seem to clear all the memory. I tried both with and without GPU. In my case, I am using a dataset of size 60,000 (possibly larger in the future) and using a fine-tuned model of BERT large. I will have a 24GB GPU available for me.</p>
<p>Any suggestions?</p>
<p>To keep in mind, my main goal is to have 1 Language Model predict the next token and extract features of the current sentence.</p>
","caching, huggingface-transformers, torch, bert-language-model, transformer-model","<p>The code snippet has several issues.</p>
<ol>
<li>It does not use GPU at all. You need to send both model and the data to a GPU explicitly, e.g., by doing the following:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;CPU&quot;)
model.to(device)
inputs.to(device)
</code></pre>
<ol start=""2"">
<li><p>PyTorch automatically prepares for computing the gradients, which requires storing intermediate results. You can wrap the call in <code>with torch.no_grad()</code> to ensure no gradients are collected.</p>
</li>
<li><p>60k sentences is too much, regardless of your GPU memory. In any case, you need to split it into more batches. You can use the <a href=""https://huggingface.co/docs/datasets/index"" rel=""nofollow noreferrer""><code>Dataset</code></a> interface from Transformers for that.</p>
</li>
</ol>
",0,0,605,2023-02-27 20:04:16,https://stackoverflow.com/questions/75585069/why-is-bert-storing-cache-even-after-caching-is-disabled
How to read a BERT attention weight matrix?,"<p>I have extracted from the <strong>last layer</strong> and the <strong>last attention head</strong> of my <strong>BERT</strong> model the <strong>attention score/weights matrix</strong>. However I am not too sure how to read them. The matrix is the following one. I tried to find some more information in the literature but it was not successful. Any insights? Since the matrix is not symmetric and each rows sums to 1, I am confused. Thanks a lot !</p>
<p><a href=""https://i.sstatic.net/IdWPo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IdWPo.png"" alt=""enter image description here"" /></a></p>
<pre><code>  tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')
  inputs = tokenizer(input_text, return_tensors='pt') 
  attention_mask=inputs['attention_mask']
  outputs = model(inputs['input_ids'],attention_mask) #len 30 as the model layers #outpus.attentions
  attention = outputs[-1]
attention = attention[-1] #last layer attention
layer_attention = layer_attention[-1] #last head attention
#... code to read it as a matrix with token labels
</code></pre>
","huggingface-transformers, bert-language-model, attention-model, self-attention, multihead-attention","<p>The attention matrix is asymmetric because <code>query</code> and <code>key</code> matrices differ.</p>
<p>At its core (leaving normalization constants and the multi-head trick aside) <em>(dot-product) self-attention</em> is computed as follows:</p>
<ol>
<li><p><strong>Compute <em>key-query affinities</em></strong> (<code>e_ij</code>): given <img src=""https://latex.codecogs.com/svg.image?q_i,&space;k_j&space;%5Cin&space;%5Cmathbb%7BR%7D%5E%7Bd_%7Bembedding%7D%7D,&space;%5Cquad&space;i,&space;j&space;=&space;1,&space;%5Cdots,&space;T"" title=""https://latex.codecogs.com/svg.image?q_i, k_j \in \mathbb{R}^{d_{embedding}}, \quad i, j = 1, \dots, T"" /> (<code>T</code> being the sequence length, <code>q_i</code> and <code>k_j</code> being query and key vectors)</p>
<img src=""https://latex.codecogs.com/svg.image?e_%7Bij%7D&space;=&space;q_i%5ETk_j"" title=""https://latex.codecogs.com/svg.image?e_{ij} = q_i^Tk_j"" /> 
</li>
<li><p><strong>Compute <em>attention weights</em> from affinities</strong> (<code>alpha_ij</code>):</p>
<img src=""https://latex.codecogs.com/svg.image?%5Calpha_%7Bij%7D&space;=&space;%5Cdfrac%7Bexp(e_%7Bij%7D)%7D%7B%5Csum_%7Bj%27%7Dexp(e_%7Bij%27%7D)%7D"" title=""https://latex.codecogs.com/svg.image?\alpha_{ij} = \dfrac{exp(e_{ij})}{\sum_{j'}exp(e_{ij'})}"" />
</li>
</ol>
<p>As you can see, you get the normalization of the affinities by summing over all keys given a query; said differently, in the denominator you're summing affinities by row (thus, probabilities sum to 1 over rows).</p>
<p>The way you should read the attention matrix is the following: <em>row tokens</em> (queries) attend to <em>column tokens</em> (keys) and the matrix weights represent a way to probabilistically measure where <em>attention</em> is directed to when querying over keys (i.e. to which key - and so to which token of the sentence - each query (token) mainly focuses to). Such interaction is <em>unidirectional</em> (you might look at each query as looking for information somewhere in the keys, the opposite interaction being irrelevant). I found the interpretation of the attention matrix as a <em>directed graph</em> within this <a href=""https://theaisummer.com/self-attention/#:%7E:text=Self%2Dattention%20is%20not%20symmetric!&amp;text=The%20arrows%20that%20correspond%20to,Q%E2%80%8B%3DWK%E2%80%8B."" rel=""noreferrer"">blogpost</a> very effective.</p>
<p>Eventually, I'd also suggest the first <a href=""https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77"" rel=""noreferrer"">BertViz medium post</a> which distinguishes different attention patterns and according to which your example would fall in the case where attention is mostly directed to the delimiter token [CLS].</p>
",5,3,4195,2023-03-17 21:15:46,https://stackoverflow.com/questions/75772288/how-to-read-a-bert-attention-weight-matrix
use fine-tuned BERT to train a new sentence-transformer,"<p>I have fine-tuned BERT on domain specific data, now I am going to train a sentence transformer based on this fine-tuned BERT and my own labelled data. I created sentence transformer by below code:</p>
<pre><code>model_name = &quot;path/to/model&quot; 
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) # when I fine tuned the BERT, I used this tokenizer
model = AutoModel.from_pretrained(model_name)

word_embedding_model = models.Transformer(model, tokenizer)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
sentence_transformer = SentenceTransformer(modules=[word_embedding_model, pooling_model])
</code></pre>
<p>but I got error:</p>
<pre><code>We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like BertModel(...)  is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
</code></pre>
<p>I saved to google drive by below code:</p>
<pre><code>model.save_pretrained('/content/drive/MyDrive/testForSentTransformer')
</code></pre>
<p>and got below two errors:</p>
<pre><code>HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
</code></pre>
<p>and</p>
<pre><code>During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    647             except Exception:
    648                 # For any other exception, we throw a generic error.
--&gt; 649                 raise EnvironmentError(
    650                     f&quot;Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it&quot;
    651                     &quot; from 'https://huggingface.co/models', make sure you don't have a local directory with the same&quot;

OSError: Can't load the configuration of 'BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
</code></pre>
<p>how to fix these errors? Thanks for helping!</p>
","nlp, bert-language-model","<p>should do:</p>
<pre><code>model_path = &quot;path/to/model&quot; 
# tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) 
# model = AutoModel.from_pretrained(model_name)

word_embedding_model = models.Transformer(model_name_or_path=model_path, tokenizer_name_or_path=&quot;bert-base-uncased&quot;)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
sentence_transformer = SentenceTransformer(modules=[word_embedding_model, pooling_model])
</code></pre>
",0,0,796,2023-03-20 09:37:11,https://stackoverflow.com/questions/75788612/use-fine-tuned-bert-to-train-a-new-sentence-transformer
Is it possible to train a deep learning model using low precision and subsequently fine-tune it with high precision?,"<p>Assuming a BERT model is trained on fp16 and then fine-tuned on fp32 for a specific task, would this result in an increase or decrease in accuracy?</p>
<p>It can take less memory on GPU, training time will be reduced.</p>
","tensorflow, deep-learning, pytorch, huggingface-transformers, bert-language-model","<p>What you're referring to is called mixed-precision training, and it's basically training the model with low-precision floating-point numbers (e.g., fp16) for most of the layers and using high-precision numbers (e.g., fp32) only for certain layers that require more accuracy. Fine-tuning a low-precision model with high precision on accuracy can vary depending on the specific model and task. In some cases, fine-tuning can result in an increase in accuracy, while in other cases it may not. here is a quick and dirty script to try for mixed precision, to see if it is worth trying as a temperature check:</p>
<pre><code>import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_text as text
import numpy as np
import os
import json
import math
import time

# Set mixed precision policy
policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
tf.keras.mixed_precision.experimental.set_policy(policy)

# Load BERT model and tokenizer
bert_model_name = 'bert-base-cased'
bert_dir = f'bert_models/{bert_model_name}'
tokenizer = BertTokenizer.from_pretrained(bert_dir)
bert_model = TFBertForSequenceClassification.from_pretrained(bert_dir)

# Define optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)

# Define loss function
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Define metrics
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

# Define batch size
batch_size = 32

# Load training data
train_data = tfds.load('glue/mrpc', split='train', shuffle_files=True)
train_data = train_data.batch(batch_size)

# Fine-tune BERT model
epochs = 5
for epoch in range(epochs):
    start_time = time.time()
    metric.reset_states()
    for batch_idx, data in enumerate(train_data):
        input_ids = data['input_ids']
        attention_mask = data['attention_mask']
        token_type_ids = data['token_type_ids']
        labels = data['label']
        
        # Cast input data to mixed precision
        input_ids = tf.cast(input_ids, tf.float16)
        attention_mask = tf.cast(attention_mask, tf.float16)
        token_type_ids = tf.cast(token_type_ids, tf.float16)
        labels = tf.cast(labels, tf.float16)
        
        with tf.GradientTape() as tape:
            outputs = bert_model(input_ids, attention_mask, token_type_ids)
            loss_value = loss(labels, outputs.logits)
            
        grads = tape.gradient(loss_value, bert_model.trainable_weights)
        optimizer.apply_gradients(zip(grads, bert_model.trainable_weights))
        metric.update_state(labels, outputs.logits)
        
    epoch_time = time.time() - start_time
    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss_value:.4f}, Accuracy: {metric.result().numpy():.4f}, Time: {epoch_time:.2f}s')
</code></pre>
",0,0,296,2023-04-01 07:15:02,https://stackoverflow.com/questions/75904748/is-it-possible-to-train-a-deep-learning-model-using-low-precision-and-subsequent
Most similar words within a given context,"<p>I want to create a deep learning model that can generate context-aware synonyms. I've been thinking about using BERT since it is bidirectional and creates good representations, and my idea was to use an approach where I provide the model with both the original sentence (e.g. &quot;It is a beautiful house&quot;) and the same sentence but where the word I want to find synonyms for is masked (e.g. &quot;It is a [MASK] house&quot;, if I want to find synonyms for beautiful).</p>
<p>A normal fill-mask obviously wouldn't work since it would not provide the model with the actual word we want to find synonyms for. I was thinking about using a machine translation-model (e.g. T5) instead, where you don't translate the sentence from one language to another but make an e.g. English-to-English translation where you provide the original sentence (&quot;It is a beautiful house&quot;) as input to the encoder and the masked sentence (&quot;It is a [MASK] house&quot;) as another input - this sentence would so to speak be the equivalent of an almost completed translation of the original sentence, and instead of simply translating the missing word, it would give me the top k most probable logits as synonyms.</p>
<p>However, I'm not sure how I can make this work at all... Another approach would be to train BERT on a domain-specific corpus and then get the k-nearest neighbors of the word I want to find synonyms for, but from what I've read it's not possible to get the word representations from a model like BERT the same way you would from Word2Vec and GloVe.</p>
<p>Any suggestions on how I can solve this challenge? Any help would be greatly appreciated...</p>
","deep-learning, nlp, bert-language-model, machine-translation","<p>If I were you, I would first push the sentence through a word sense disambiguation system like <a href=""http://nlp.uniroma1.it/amuse-wsd/about"" rel=""nofollow noreferrer"">AMUSE</a>. This will give you the WordNet synset that your word belongs to (and so you can find its synonyms from the WordNet synset).</p>
<p>Now you have a list of synonyms for the word in this context. As you planned, you can now use the MASK technique to find the probabilities of all the synonyms that you found earlier.</p>
<p>Important note: in BERT the MASK will always be filled by <em>one</em> token which is not necessarily a full word. This means that there will be a bias against your longer synonyms because they will likely never be generated.</p>
",0,0,413,2023-04-04 17:32:41,https://stackoverflow.com/questions/75932270/most-similar-words-within-a-given-context
Getting the input text from transformers pipeline,"<p>I am following the tutorial on <a href=""https://huggingface.co/docs/transformers/pipeline_tutorial"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pipeline_tutorial</a> to use transformers pipeline for inference. For example, the following code snippet works for getting the NER results from ner pipeline.</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(extracted_entities)
</code></pre>
<p>In NER, as well as many applications, we would like to also get the input so that I can store the result as (text, extracted_entities) pair for later processing. Basically I am looking for something like:</p>
<pre><code>    # KeyDataset is a util that will just output the item we're interested in.
    from transformers.pipelines.pt_utils import KeyDataset
    from datasets import load_dataset
    model = ...
    tokenizer = ...
    pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
    dataset = load_dataset(&quot;my_ner_dataset&quot;, split=&quot;test&quot;)
    
    for text, extracted_entities in pipe(KeyDataset(dataset, &quot;text&quot;)):
        print(text, extracted_entities)
</code></pre>
<p>Where <code>text</code> is the raw input text (possibly batched) that get fed into the pipeline.</p>
<p>Is this doable ?</p>
","nlp, pipeline, huggingface-transformers, bert-language-model, named-entity-recognition","<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code># Datasets 2.11.0
from datasets import load_dataset
# Transformers 4.27.4, Torch 2.0.0+cu118, 
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    pipeline
)
from transformers.pipelines.pt_utils import KeyDataset

model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)

pipe = pipeline(task=&quot;ner&quot;, model=model, tokenizer=tokenizer)
dataset = load_dataset(&quot;argilla/gutenberg_spacy-ner&quot;, split=&quot;train&quot;)
results = pipe(KeyDataset(dataset, &quot;text&quot;))

for idx, extracted_entities in enumerate(results):
    print(&quot;Original text:\n{}&quot;.format(dataset[idx][&quot;text&quot;]))
    print(&quot;Extracted entities:&quot;)
    for entity in extracted_entities:
        print(entity)
</code></pre>
<h3>Example output</h3>
<pre class=""lang-none prettyprint-override""><code>Original text:
Would I wish to send up my name now ? Again I declined , to the polite astonishment of the concierge , who evidently considered me a queer sort of a friend . He was called to his desk by a guest , who wished to ask questions , of course , and I waited where I was . At a quarter to eleven Herbert Bayliss emerged from the elevator . His appearance almost shocked me . Out late the night before ! He looked as if he had been out all night for many nights .
Extracted entities:
{'entity': 'B-PER', 'score': 0.9996532, 'index': 68, 'word': 'Herbert', 'start': 289, 'end': 296}
{'entity': 'I-PER', 'score': 0.9996567, 'index': 69, 'word': 'Bay', 'start': 297, 'end': 300}
{'entity': 'I-PER', 'score': 0.9991698, 'index': 70, 'word': '##lis', 'start': 300, 'end': 303}
{'entity': 'I-PER', 'score': 0.96547437, 'index': 71, 'word': '##s', 'start': 303, 'end': 304}

...

Original text:
And you think our run will be better than five hundred and eighty ? '' `` It should be , unless there is a remarkable change . This ship makes over six hundred , day after day , in good weather . She should do at least six hundred by to-morrow noon , unless there is a sudden change , as I said . '' `` But six hundred would be -- it would be the high field , by Jove ! '' `` Anything over five hundred and ninety-four would be that . The numbers are very low to-night .
Extracted entities:
{'entity': 'B-MISC', 'score': 0.40225995, 'index': 90, 'word': 'Jo', 'start': 363, 'end': 365}
</code></pre>
<h2>Brief Explanation</h2>
<p>Each sample in the dataset created by the <code>load_dataset</code> call can be accessed using an index and the associated dictionary key.</p>
<p>Calls to the <code>pipeline</code> object with a <code>KeyDataset</code> as input returns <code>PipelineIterator</code> object that is iterable. Hence, one can <code>enumerate</code> the PipelineIterator object to get both the result and the index for the particular result, and then use that index to retrieve the associated sample in the dataset.</p>
<h2>Detailed Explanation</h2>
<p>The Huggingface <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline"" rel=""nofollow noreferrer"">pipeline</a> abstraction is a wrapper for all available pipelines. When one instantiates a <code>pipeline</code> object it will <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">return the appropriate pipeline</a> based on the <code>task</code> argument:</p>
<pre class=""lang-py prettyprint-override""><code>pipe = pipeline(task=&quot;ner&quot;, model=model, tokenizer=tokenizer)
</code></pre>
<p>Given that the NER task is specified, a <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">TokenClassificationPipeline</a> will be returned (side note: &quot;ner&quot; is an alias for &quot;token-classification&quot;). This pipeline (and all others) inherits the base class <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/__init__.py#L523"" rel=""nofollow noreferrer"">Pipeline</a>. The <code>Pipeline</code> base class defines the <code>__call__</code> function which the <code>TokenClassificationPipeline</code> class <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/token_classification.py#L214"" rel=""nofollow noreferrer"">relies on</a> whenever the instantiated <code>pipeline</code> is called.</p>
<p>Once a pipeline is instantiated (see above), it is called with data passed in as either a single string, a list, or when working with full datasets, a <a href=""https://pypi.org/project/datasets/"" rel=""nofollow noreferrer"">Huggingface dataset</a> via the transformers.pipelines.pt_utils <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/pt_utils.py#L296"" rel=""nofollow noreferrer"">KeyDataset</a> class.</p>
<pre class=""lang-py prettyprint-override""><code>dataset = load_dataset(&quot;argilla/gutenberg_spacy-ner&quot;, split=&quot;train&quot;)
results = pipe(KeyDataset(dataset, &quot;text&quot;))  # pipeline call
</code></pre>
<p>When the pipeline is called, it <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/base.py#L1080"" rel=""nofollow noreferrer"">checks</a> whether the data passed in is iterable, and then calls an appropriate function. For Huggingface <code>Dataset</code> objects, the <code>get_iterator</code> function <a href=""https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/pipelines/base.py#L1087"" rel=""nofollow noreferrer"">is called</a> which returns a <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/pt_utils.py#L23"" rel=""nofollow noreferrer"">PipelineIterator</a> object. Given the <a href=""https://docs.python.org/3/glossary.html#term-iterator"" rel=""nofollow noreferrer"">known behaviour of iterator objects</a>, one can <a href=""https://docs.python.org/3/library/functions.html#enumerate"" rel=""nofollow noreferrer"">enumerate</a> the object to return a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable. The values are the NER extractions for each sample in the dataset. Hence, the following produces the desired results:</p>
<pre class=""lang-py prettyprint-override""><code>for idx, extracted_entities in enumerate(results):
    print(&quot;Original text:\n{}&quot;.format(dataset[idx][&quot;text&quot;]))
    print(&quot;Extracted entities:&quot;)
    for entity in extracted_entities:
        print(entity)
</code></pre>
",3,2,5374,2023-04-04 18:18:04,https://stackoverflow.com/questions/75932605/getting-the-input-text-from-transformers-pipeline
bert_vocab.bert_vocab_from_dataset returning wrong vocabulary,"<p>i'm trying to build a tokenizer following the tf's tutorial <a href=""https://www.tensorflow.org/text/guide/subwords_tokenizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/guide/subwords_tokenizer</a>. I'm basically doing the same thing only with a different dataset. The dataset in question is a txt file in which the first two columns are an english sentence or word and the translation in italian, here a snippet:</p>
<pre><code>Hi. Ciao!   CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #607364 (Cero)
Hi. Ciao.   CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #4522287 (Guybrush88)
Run!    Corri!  CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906347 (Guybrush88)
Run!    Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906348 (Guybrush88)
Run!    Correte!    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906350 (Guybrush88)
Who?    Chi?    CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #2126402 (Guybrush88)
</code></pre>
<p>it can be downloaded at <a href=""http://www.manythings.org/anki/"" rel=""nofollow noreferrer"">http://www.manythings.org/anki/</a></p>
<p>i've preprocessed it and turned the english and italian sentences to tensorflow datasets to be fed to the tokenizer as illustrated in this code:</p>
<pre><code>import tensorflow as tf
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab
import tensorflow_text as tf_text
import os
import numpy as np

eng_dataset, ita_dataset = np.genfromtxt('ita_eng_dataset.txt',
                                         usecols=(0, 1),
                                         encoding='utf-8',
                                         unpack=True,
                                         dtype='str')

eng_dataset_tensor = tf.convert_to_tensor(eng_dataset)
ita_dataset_tensor = tf.convert_to_tensor(ita_dataset)

eng_tf_dataset = tf.data.Dataset.from_tensor_slices(eng_dataset_tensor)
ita_tf_dataset = tf.data.Dataset.from_tensor_slices(ita_dataset_tensor)
</code></pre>
<p>The problems arise when i try to fed it to <code>bert_vocab_from_dataset</code>:</p>
<pre><code>bert_tokenizer_params = dict(lower_case=True)
reserved_tokens = [&quot;[PAD]&quot;, &quot;[UNK]&quot;, &quot;[START]&quot;, &quot;[END]&quot;]

bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size=8000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

eng_vocab = bert_vocab.bert_vocab_from_dataset(eng_tf_dataset, **bert_vocab_args)
ita_vocab = bert_vocab.bert_vocab_from_dataset(ita_tf_dataset, **bert_vocab_args)
</code></pre>
<p>but the results are wrong:</p>
<pre><code>print(eng_vocab[:20])
print(ita_vocab[1980:2000])
print(len(eng_vocab), len(ita_vocab))
</code></pre>
<p>which outputs</p>
<pre><code>['about', 'breakfast', 'coffee', 'correct', 'finally', 'heat', 'japanese', 'large', 'lie', 'old', 'peel', 'science', 'step', 'swimming', 'work', '##ans', '##b', '##der', '##ins', '##ish']
['##omfortable', '##ong', '##ony', '##op', '##ouse', '##ply', '##rch', '##rous', '##rove', '##roved', '##sists', '##tained', '##ten', '##unted', '##val', '##ze', 'advice', 'agitated', 'amazed', 'argued']
665 2413
</code></pre>
<p>as you can see the italian vocabulary contains english text and are both very little (it maybe due to the dataset but seems odd to be so small, less than 1000 vocabs even).</p>
<p>I also tried batching the input dataset as in the tensorflow tutorial but it gave the same results.</p>
<p>I'm using python 3.8 on pycharm with windows 11 and tensorflow 2.10</p>
","python, tensorflow, deep-learning, tokenize, bert-language-model","<p>Solved, it was just the np.genfromtxt non using '\t' as delimiter by default.</p>
",-1,2,193,2023-04-08 10:30:57,https://stackoverflow.com/questions/75964691/bert-vocab-bert-vocab-from-dataset-returning-wrong-vocabulary
Failing to create a transformer from scratch and push it on cuda (use it on Gpu),"<p>In order to learn Pytorch and understand how transformers works i  tried to implement from scratch (inspired from HuggingFace book) a transformer classifier:</p>
<pre><code>from transformers import AutoTokenizer,DataCollatorWithPadding
from bertviz.transformers_neuron_view import BertModel
from transformers import AutoConfig
import torch
from torch import nn
import torch.nn.functional as F
from math import sqrt

model_ckpt = &quot;bert-base-uncased&quot;

# config = AutoConfig.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
# model = BertModel.from_pretrained(model_ckpt)
config = {
    &quot;vocab_size&quot;: 30522,
    &quot;hidden_size&quot;: 768,
    &quot;max_position_embeddings&quot;: 512,
    &quot;num_attention_heads&quot;: 12,
    &quot;num_hidden_layers&quot;: 12,
    &quot;hidden_dropout_prob&quot;: 0.1,
    &quot;num_labels&quot;: 6,
    &quot;intermediate_size&quot;: 3072,
}
config = dotdict(config)


class dotdict(dict):
    &quot;&quot;&quot;dot.notation access to dictionary attributes&quot;&quot;&quot;
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

config = dotdict(config)

class Embeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size, 
                                             config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
                                                config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        # Create position IDs for input sequence
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        # Create token and position embeddings
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        # Combine token and position embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = F.softmax(scores, dim=-1)
    return torch.bmm(weights, value)

class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)

    def forward(self, hidden_state):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))
        return attn_outputs

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x

class FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
    def forward(self, x):
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        x = self.dropout(x)
        return x
        
class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)
        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)
        self.attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)

    def forward(self, x):
        # Apply layer normalization and then copy input into query, key, value
        hidden_state = self.layer_norm_1(x)
        # Apply attention with a skip connection
        x = x + self.attention(hidden_state)
        # Apply feed-forward layer with a skip connection
        x = x + self.feed_forward(self.layer_norm_2(x))
        return x

class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = Embeddings(config)
        self.layers = nn.ModuleList([TransformerEncoderLayer(config) 
                                     for _ in range(config.num_hidden_layers)])

    def forward(self, x):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x)
        return x
        
#Adding a Classification Head
class TransformerForSequenceClassification(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = TransformerEncoder(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
        
    def forward(self, x):
        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token
        x = self.dropout(x)
        x = self.classifier(x)
        return x

config.num_labels = 6
encoder_classifier = TransformerForSequenceClassification(config)
</code></pre>
<p>Then i preprocess data:</p>
<pre><code>from datasets import load_dataset
import pandas as pd
emotions = load_dataset(&quot;emotion&quot;)

def tokenize(batch):
    return tokenizer(batch[&quot;text&quot;], padding=True, truncation=True)

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

tokenized_datasets = emotions_encoded.remove_columns([&quot;text&quot;])
tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)
tokenized_datasets.set_format(&quot;torch&quot;)

from torch.utils.data import DataLoader
train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8)
eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8)

from torch.optim import AdamW
optimizer = AdamW(encoder_classifier.parameters(), lr=5e-5)

loss_fn = nn.CrossEntropyLoss()

from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name=&quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

from tqdm.auto import tqdm
import torch

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
encoder_classifier.to(device)
#next(encoder_classifier.parameters()).is_cuda
progress_bar = tqdm(range(num_training_steps))

encoder_classifier.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        import pdb;pdb.set_trace()
        outputs = encoder_classifier(batch[&quot;input_ids&quot;])
        loss = loss_fn(outputs, batch[&quot;labels&quot;])
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>I finally got the error:
&quot;RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when
checking argument for argument index in method wrapper__index_select)&quot;</p>
<p>I am not sure that pushing my custom model of bert on device (cuda) works.
Do you have an idea why and how to correct the code to make it works on gpu.</p>
<p>Edit:</p>
<p>the error is raised at this line:
---&gt; 25         outputs = encoder_classifier(batch[&quot;input_ids&quot;])</p>
<p>and i check that arguments are on cuda as followed:</p>
<pre><code>ipdb&gt;  !next(encoder_classifier.parameters()).is_cuda
True
ipdb&gt;  batch[&quot;input_ids&quot;].device
device(type='cuda', index=0)
ipdb&gt;   batch[&quot;labels&quot;].device
device(type='cuda', index=0)
</code></pre>
","pytorch, bert-language-model","<p>I had to push position_ids  to cuda .... I feel stupid now ::)</p>
<pre><code>position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0).to(device)

</code></pre>
",0,0,430,2023-04-14 07:33:55,https://stackoverflow.com/questions/76012610/failing-to-create-a-transformer-from-scratch-and-push-it-on-cuda-use-it-on-gpu
How can I find the cosine similarity between two song lyrics represented as strings?,"<p>My friends and I are doing an NLP project on song recommendation.</p>
<p>Context: We originally planned on giving the model a recommended song playlist that has the most similar lyrics based on the random input corpus(from the literature etc), however we didn't really have a concrete idea of its implementation.</p>
<p>Currently our task is to find similar lyrics to a random lyric fed as a string input. We are using sentence BERT model(sbert) and cosine similarity to find the similarity between the songs and it seems like the output numbers are meaningful enough to find the most similar song lyrics.</p>
<p>Is there any other way that we can improve this approach?</p>
<p>We'd like to use BERT model and are open to suggestions that can be used on top of BERT if possible, but if there is any other models that should be used instead of BERT, we'd be happy to learn. Thanks.</p>
","nlp, stanford-nlp, bert-language-model, cosine-similarity, nlp-question-answering","<p><strong>Computing cosine similarity</strong></p>
<p>You can use the <code>util.cos_sim(embeddings1, embeddings2)</code> from the <code>sentence-transformers</code> package to compute the cosine similarity of two embeddings.</p>
<p>Alternatively, you can also use <code>sklearn.metrics.pairwise.cosine_similarity(X, Y, dense_output=True)</code> from the <code>scikit-learn</code> package.</p>
<p><strong>Improvements for representation and models</strong></p>
<p>Since you want recommendations just on top of BERT, you can consider RoBERTa as well with Byte-pair encoding for tokenizer over BERT's Wordpeice tokenizers.  Consider the <a href=""https://huggingface.co/roberta-base"" rel=""nofollow noreferrer"">roberta-base</a> model as a feature extractor from the HuggingFace<code>transformers</code> package.</p>
<pre class=""lang-python prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaModel
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
text = &quot;song lyrics in text.&quot;
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
</code></pre>
<p>Tokenizers work at various text granularity level of syntax &amp; semantics. They help generate quality vectors/embeddings. Each can yield different and better results if fine-tunned for the correct task and model.</p>
<p>Some other tokenizers you can consider are:
Character Level BPE, Byte-Level BPE, WordPiece (BERT uses this), SentencePiece, and Unigram tokenizer with LM Character.</p>
<p>Also consider exploring the HuggingFace official Tokenizer Library guide <a href=""https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt"" rel=""nofollow noreferrer"">here</a>.</p>
",0,3,710,2023-05-06 13:43:56,https://stackoverflow.com/questions/76189213/how-can-i-find-the-cosine-similarity-between-two-song-lyrics-represented-as-stri
"Failed to find data adapter that can handle input: , ( containing values of types {&quot;&quot;}) Content classification","<p>Im doing text classification using bert</p>
<p>please help me to solve these</p>
<h1>my data set</h1>
<p>these is my data set</p>
<pre><code>            content                                     catalogPath_level_1 
    mobility academy rolling stock technical and p...   TM
    trackguard sicas ecc ...                            TM  
    zuverlässigkeit von verteiltem juridical recor...   TMR 
    wbt simis d hardware projektierung innenraum...     TMR 
</code></pre>
<h1>model</h1>
<p>model that convert text and create tokenizer</p>
<pre><code>def tokenize_function(text):
    return tokenizer(text.numpy(), padding=True, truncation=True, return_tensors='tf')

def tf_tokenize(text):
    result = tf.py_function(tokenize_function, [text], Tout=tf.int32)
    result.set_shape([None, None])
    return result
</code></pre>
<h2>layers</h2>
<p>layers</p>
<pre><code>text_input = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='input_ids')
tokenized_input = tf.keras.layers.Lambda(tf_tokenize)(text_input)
outputs = bert_encoder(tokenized_input)
pooled_output = outputs[0][:, 0]

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(pooled_output)
l = tf.keras.layers.Dense(4, activation='sigmoid', name=&quot;output&quot;)(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l]) 
</code></pre>
<h2>optimizers</h2>
<p>optimizers</p>
<pre><code>optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.CategoricalCrossentropy()

model.compile(optimizer=optimizer,
              loss=loss,
              metrics='accuracy')
</code></pre>
<h2>fit</h2>
<pre><code>model.fit(X_train, Y_train, epochs=10)
</code></pre>
<h1>error at fit</h1>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[23], line 1
----&gt; 1 model.fit(X_train, Y_train, epochs=10)

File ~\AppData\Roaming\Python\Python310\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Roaming\Python\Python310\site-packages\keras\engine\data_adapter.py:1082, in select_data_adapter(x, y)
   1079 adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
   1080 if not adapter_cls:
   1081     # TODO(scottzhu): This should be a less implementation-specific error.
-&gt; 1082     raise ValueError(
   1083         &quot;Failed to find data adapter that can handle input: {}, {}&quot;.format(
   1084             _type_name(x), _type_name(y)
   1085         )
   1086     )
   1087 elif len(adapter_cls) &gt; 1:
   1088     raise RuntimeError(
   1089         &quot;Data adapters should be mutually exclusive for &quot;
   1090         &quot;handling inputs. Found multiple adapters {} to handle &quot;
   1091         &quot;input: {}, {}&quot;.format(adapter_cls, _type_name(x), _type_name(y))
   1092     )

ValueError: Failed to find data adapter that can handle input: , ( containing values of types {&quot;&quot;})
</code></pre>
","python, bert-language-model","<p>I think the problem can be with the data type, some advices:</p>
<ul>
<li>for the <code>text_input</code>, it should be <code>dtype=tf.string</code>, since your data are strings</li>
<li><code>tokenize_function</code>: should process the string directly with <code>text = text.numpy().decode('utf-8')</code> before the <code>return</code></li>
<li><code>Y_train</code> should be one-hot encoded, try using <code>to_categorical</code> from <code>tensorflow.keras.utils</code></li>
<li>change <code>X_train</code> too <code>X_train_list = X_train.to_list()</code></li>
<li>remember to use the previous on the <code>model.fit(...)</code></li>
</ul>
",1,0,100,2023-05-08 10:37:38,https://stackoverflow.com/questions/76199747/failed-to-find-data-adapter-that-can-handle-input-containing-values-of-type
Evaluating a BERTopic model based on classification metrics,"<p>I am unable to find a solution to a problem I have with checking coherence scores for my topic models created using BERTopic. I am new to using these methods for NLP and especially new to using Python. I am currently looking at how to derive Topic coherence scores from my model. However, it might be the case that another classification metric would be more suitable.</p>
<p>Here's my code showing my data setup and to show how I am working with a pre-trained and locally saved model from my drive.</p>
<pre><code># load libraries 
%%capture
!pip install bertopic
from bertopic import BERTopic

# mount google drive, permit access
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# import data and define columns needed
import pandas as pd
data = pd.read_csv(&quot;/content/drive/MyDrive/BERTopic_test_data.csv&quot;)
docs = data[&quot;text&quot;]

# load in pre saved model
my_model = BERTopic.load('/content/drive/MyDrive/my_model')

# create the topics using pre-saved model 
topic_model = my_model
topics, _ = topic_model.fit_transform(docs)
</code></pre>
<p>To provide some more context, here are the components of the BERT model, as well as the parameters chosen when training <code>my_model</code></p>
<pre><code>from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
import spacy
from spacy.lang.en.examples import sentences 

# defining model components, as well as parameter tuning
embedding_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
umap_model = UMAP(n_neighbors = 15, n_components = 5, min_dist = 0.05, random_state = 42
hdbscan_model = HDBSCAN(min_cluster_size = 25, min_samples = 10,
                        gen_min_span_tree = True,
                        prediction_data = True)
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words = stopwords)


# building the model 
my_model = BERTopic(
    umap_model = umap_model,
    hdbscan_model = hdbscan_model,
    embedding_model = embedding_model,
    vectorizer_model = vectorizer_model,
    top_n_words = 10,
    language = 'english',
    verbose = True
)
</code></pre>
<p>I have tried this solution found online but I'm met with error message
&quot;AttributeError: 'BERTopic' object has no attribute 'id2word&quot;</p>
<pre><code># import library from gensim  
from gensim.models import CoherenceModel

# instantiate topic coherence model
cm = CoherenceModel(model=topic_model, texts=docs, coherence='c_v')

# get topic coherence score
coherence_bert = cm.get_coherence() 
print(coherence_bert)
</code></pre>
","python, nlp, bert-language-model, topic-modeling","<p>Usually, the performance of an NLP model is assessed through metric of Precision (P), Recall (R) and F1. You basically have 4 types of prediction outcome, but so far you are only interested in two of them : True Positives (TP) and False Positives (FP), basically wether your prediction equals your expected outcome or not.</p>
<ul>
<li><em>P</em> corresponds to the number of TP/TP+FP that means, out of all predicted labels, how much you have correctly classified.</li>
<li><em>R</em> corresponds to the number of TP/(number of class instances in the dataset)</li>
<li><em>F1</em> is the hamornic mean between those two metrics and gives you an overall view of its performance. The higher, the better.</li>
</ul>
<p>You can easily obtain those metrics using the scikit-learn library if you build two lists to compare both of them :</p>
<pre class=""lang-py prettyprint-override""><code># Library required
from sklearn.metrics import precision_recall_fscore_support

# List holding true (expected) outcomes
ny_true = []

# List holding predicted outcomes
ny_pred = []

print(precision_recall_fscore_support(ny_true, ny_pred, average='macro'))
</code></pre>
",-1,1,3341,2023-05-12 10:04:27,https://stackoverflow.com/questions/76235208/evaluating-a-bertopic-model-based-on-classification-metrics
Bert model splits words by its own,"<p>I am tokenizing the input words using bert model.
The code is :</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case = False)
model = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;, add_pooling_layer=False, output_hidden_states=True, output_attentions=True)

marked_text =  text + &quot; [SEP]&quot;
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    print(tokenized_text)
    print(indexed_tokens)
</code></pre>
<p>The model I used is from HuggingFace.</p>
<p>My goal is to print the embedded vectors of all words Bert model has, so I searched and found that this model has 119296 tokens available.</p>
<p>I don't know this number of the tokens is reason, but the model splits the words by its own, which is unwanted for me.</p>
<p>for example,</p>
<pre><code>
only -&gt; [only]
ONLY -&gt; [ON,L,Y]

stradivarius -&gt; ['St', '##radi', '##vari', '##us']
</code></pre>
<p>Is this natural Bert thing or I am doing something wrong ?</p>
","python, nlp, huggingface-transformers, bert-language-model","<p>You are not doing anything wrong. Bert uses a so-called <a href=""https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece"" rel=""nofollow noreferrer"">wordpiece</a> subword tokenizer as a compromise for meaningful embeddings and acceptable memory consumption between a character-level (small vocabulary) and a word-level tokenizer (large vocabulary).</p>
<p>A common approach to retrieve word embeddings from a subword-based model is to take the mean of the respective tokens. The code below shows you have you can retrieve the word embeddings (non-contextualized and contextualized) by taking the mean. It uses a fasttokenizer to utilize the methods of the <a href=""https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding"" rel=""nofollow noreferrer"">BatchEncoding</a> object.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizerFast, BertModel

t = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
# whole model
m = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
# token embedding layer
embedding_layer = m.embeddings.word_embeddings

sample_sentence = 'This is an example with token-embeddings and word-embeddings'
encoded = t([sample_sentence])
# The BatchEncoding object allows us to map the token back to the string indices
print(*[(token_id, encoded.token_to_chars(idx)) for idx, token_id in enumerate(encoded.input_ids[0])], sep=&quot;\n&quot;)
# And we can also check the mapping of word to token indices
print(*[(word, encoded.word_to_tokens(idx)) for idx, word in enumerate(sample_sentence.split())], sep=&quot;\n&quot;)
</code></pre>
<p>Output:</p>
<pre><code>(101, None)
(10747, CharSpan(start=0, end=4))
(10124, CharSpan(start=5, end=7))
(10151, CharSpan(start=8, end=10))
(14351, CharSpan(start=11, end=18))
(10169, CharSpan(start=19, end=23))
(18436, CharSpan(start=24, end=27))
(10136, CharSpan(start=27, end=29))
(118, CharSpan(start=29, end=30))
(10266, CharSpan(start=30, end=32))
(33627, CharSpan(start=32, end=35))
(13971, CharSpan(start=35, end=39))
(10107, CharSpan(start=39, end=40))
(10111, CharSpan(start=41, end=44))
(12307, CharSpan(start=45, end=49))
(118, CharSpan(start=49, end=50))
(10266, CharSpan(start=50, end=52))
(33627, CharSpan(start=52, end=55))
(13971, CharSpan(start=55, end=59))
(10107, CharSpan(start=59, end=60))
(102, None)
('This', TokenSpan(start=1, end=2))
('is', TokenSpan(start=2, end=3))
('an', TokenSpan(start=3, end=4))
('example', TokenSpan(start=4, end=5))
('with', TokenSpan(start=5, end=6))
('token-embeddings', TokenSpan(start=6, end=8))
('and', TokenSpan(start=8, end=9))
('word-embeddings', TokenSpan(start=9, end=13))
</code></pre>
<p>To retrieve the word embeddings:</p>
<pre><code>with torch.inference_mode():
  token_embeddings = embedding_layer(encoded.convert_to_tensors(&quot;pt&quot;).input_ids).squeeze()
  # we need the attention mechanism of the whole model to get the contextualized token representations
  contextualized_token_embeddings = m(**encoded.convert_to_tensors(&quot;pt&quot;)).last_hidden_state.squeeze()

def fetch_word_embeddings(sample_sentence:str, encoded, embeddings:torch.Tensor) -&gt; dict[str,torch.Tensor]:
  word_embeddings = {}
  for idx, word in enumerate(sample_sentence.split()):
    start, end = encoded.word_to_tokens(idx)
    word_embeddings[word] = embeddings[start:end].mean(dim=0)
  return word_embeddings

word_embeddings = fetch_word_embeddings(sample_sentence, encoded, token_embeddings)
contextualized_word_embeddings = fetch_word_embeddings(sample_sentence, encoded, contextualized_token_embeddings)
print(word_embeddings[&quot;token-embeddings&quot;])
print(contextualized_word_embeddings[&quot;token-embeddings&quot;])
</code></pre>
<p>Output:</p>
<pre><code>tensor([ 1.2455e-02, -3.8478e-02,  8.0834e-03, ..., -1.8502e-02,  1.1511e-02, -6.5307e-02])
tensor([-5.1564e-01, -1.6266e-01, -3.9420e-01, ..., -5.9969e-02,  3.0784e-01, -3.4451e-01])
</code></pre>
",2,2,1930,2023-05-12 16:17:03,https://stackoverflow.com/questions/76238212/bert-model-splits-words-by-its-own
Bert NER model start and end position None after fine-tuning,"<p>I have fine-tuned a BERT NER model to my dataset. The base model that I am fine-tuning is “dslim/bert-base-NER”. I have been successfully able to train the model using the following script as refrence:
<a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax</a></p>
<p>The code which does the prediction:</p>
<pre><code>from transformers import pipeline, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER', return_offsets_mapping=True, is_split_into_words=True)
model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')

pipe = pipeline(task=&quot;ner&quot;, model=model.to(&quot;cpu&quot;), tokenizer=tokenizer, grouped_entities=True)
pipe(&quot;this is a Abc Corp. Ltd&quot;)
</code></pre>
<p>The prediction form the base model contained the start and end position of the word in the original text like:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.9992545247077942, ‘word’: ‘A’, ‘start’: 10, ‘end’: 11}
{‘entity_group’: ‘ORG’, ‘score’: 0.998507097363472, ‘word’: ‘##bc Corp Ltd’, ‘start’: 11, ‘end’: 22}
</code></pre>
<p>While the prediction from the re-trained model is:</p>
<pre><code>{‘entity_group’: ‘ORG’, ‘score’: 0.747031033039093, ‘word’: ‘##7’, ‘start’: None, ‘end’: None},
{‘entity_group’: ‘ORG’, ‘score’: 0.9055356582005819, ‘word’: ‘Games , Inc’, ‘start’: None, ‘end’: None}
</code></pre>
<p>I am passing the <em><strong>position ids</strong></em> to the model during the training process. I looked at the model training parameters but, could not find a way to pass <em><strong>start and end position</strong></em> of the words to model training process. I have the start and end position of the tokenized words.</p>
","nlp, huggingface-transformers, bert-language-model, named-entity-recognition","<p>The pipeline can not return positions when you pass a &quot;slow&quot;-tokenizer. Use a &quot;fast&quot;-tokenizer to get the positions as well:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, BertTokenizer, BertTokenizerFast, BertForTokenClassification

fast_t = BertTokenizerFast.from_pretrained('dslim/bert-base-NER')
slow_t = BertTokenizer.from_pretrained('dslim/bert-base-NER')
model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')


text= &quot;this is a Abc Corp. Ltd&quot;

slow_p = pipeline(task=&quot;ner&quot;, model=model, tokenizer=slow_t, device=&quot;cpu&quot;, aggregation_strategy=&quot;simple&quot;)
print(slow_p(text))
fast_p = pipeline(task=&quot;ner&quot;, model=model, tokenizer=fast_t, device=&quot;cpu&quot;, aggregation_strategy=&quot;simple&quot;)
print(fast_p(text))
</code></pre>
<p>Output:</p>
<pre><code>[{'entity_group': 'ORG', 'score': 0.9992956, 'word': 'A', 'start': None, 'end': None}, {'entity_group': 'ORG', 'score': 0.97245616, 'word': '##bc Corp . Ltd', 'start': None, 'end': None}]
[{'entity_group': 'ORG', 'score': 0.9992956, 'word': 'A', 'start': 10, 'end': 11}, {'entity_group': 'ORG', 'score': 0.97245616, 'word': '##bc Corp. Ltd', 'start': 11, 'end': 23}]
</code></pre>
",2,1,551,2023-06-02 23:23:53,https://stackoverflow.com/questions/76393971/bert-ner-model-start-and-end-position-none-after-fine-tuning
How do I fine tune BERT&#39;s self attention mechanism?,"<p>My goal is to fine tune BERT's self attention so that I can see to what extent two random sentences in a document (with positional encoding) rely on each other contextually.</p>
<p>Many explanations and article that I see talk about the implementation of self-attention but do not mention how to further train self attention.</p>
<p>Here is what I'm thinking of doing to train BERT's self attention:</p>
<ol>
<li><p>Use some kind of word to vector algorithm and vectorize all the words in the article.</p>
</li>
<li><p>Add positional encoding to each sentence [where the sentence is an array of vectors (words)] using a sinusoidal function.</p>
</li>
<li><p>Make matrix of each sentence concatenated with every other sentence</p>
</li>
<li><p>For each sentence-sentence pair, iterate through each words masking them.  The model must guess the word based on context; back prop is based on accuracy of the guess.</p>
</li>
<li><p>The finished model should be able to take in an arbitrary sentence-sentence pair and output an attention matrix.</p>
</li>
</ol>
<p>I'm not sure if such a method is the right one for fine tuning (or if this even counts as continuing pre-training) a self attention mechanism, or if BERT is even the best model to train a self attention function on.</p>
<p>I'm obviously very new to fine tuning LLMs, so any guidance would be greatly appreciated!</p>
","pytorch, nlp, bert-language-model, fine-tuning","<p>Hugginface provides a model class, that you can use for your task:
<a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForNextSentencePrediction"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForNextSentencePrediction</a>.
See also the small example provided in the link. You can use the logits in the output to create your attention matrix.</p>
<p>This is just an inference task. Fine-tuning means to do further training on custom data.</p>
",1,1,572,2023-06-19 19:28:01,https://stackoverflow.com/questions/76509563/how-do-i-fine-tune-berts-self-attention-mechanism
How to chain multiple PromptNodes together in a Haystack GenerativeQAPipeline,"<p>I'm trying to chain together a simple question answering prompt to an elaboration prompt using Haystack.</p>
<p>I had the following code working just fine:</p>
<pre><code>import os

from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import BM25Retriever
from haystack.nodes import PromptNode, PromptTemplate, AnswerParser
from haystack.pipelines import Pipeline, TextIndexingPipeline


class Bert:
pipe = None

def __init__(self, data_path):
    print(&quot;Initializing model...&quot;)
    doc_dir = data_path
    document_store = InMemoryDocumentStore(use_bm25=True)

    files_to_index = [os.path.join(doc_dir, f) for f in os.listdir(doc_dir)]
    indexing_pipeline = TextIndexingPipeline(document_store)
    indexing_pipeline.run_batch(file_paths=files_to_index)

    print(&quot;Done indexing&quot;)

    retriever = BM25Retriever(document_store=document_store, top_k=2)

    lfqa_prompt = PromptTemplate(
        prompt=&quot;&quot;&quot;Synthesize a comprehensive answer from the following text for the given 
question.
                                 Provide a clear and concise response that summarizes the key 
points and information presented in the text.
                                 Your answer should be in your own words and be no longer than 
50 words.
                                 \n\n Related text: {join(documents)} \n\n Question: {query} 
\n\n Answer:&quot;&quot;&quot;,
        output_parser=AnswerParser(),
    )

    prompt_node = PromptNode(model_name_or_path=&quot;google/flan-t5-large&quot;, 
default_prompt_template=lfqa_prompt)

    elaboration_prompt = PromptTemplate(
        prompt=&quot;&quot;&quot;Elaborate on the answer to the following question given the related texts.
                                 Provide additional details to the answer in your own words.
                                 The final response should be between 100-200 words.
                                 \n\n Related text: {join(documents)} \n\n Question: {query} 
 \n\n Answer: {prompt_node}&quot;&quot;&quot;,
        output_parser=AnswerParser(),
    )
    elaboration_node = PromptNode(model_name_or_path=&quot;google/flan-t5-large&quot;, 
default_prompt_template=elaboration_prompt)

    self.pipe = Pipeline()
    self.pipe.add_node(component=retriever, name=&quot;retriever&quot;, inputs=[&quot;Query&quot;])
    self.pipe.add_node(component=prompt_node, name=&quot;prompt_node&quot;, inputs=[&quot;retriever&quot;])
    #self.pipe.add_node(component=elaboration_node, name=&quot;elaboration_node&quot;, inputs=[&quot;Query&quot;, 
&quot;retriever&quot;, &quot;prompt_node&quot;])




def generate(self, query):
    prediction = self.pipe.run(query=query)

    return prediction
</code></pre>
<p>But when I tried to chain another PromptNode to the end of the lfqa_prompt, I ran into errors. I did some research online and saw that I may need to use Shapers and I edited my code as follows:</p>
<pre><code>import os

from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import AnswerParser, BM25Retriever, BaseComponent, PromptNode, 
PromptTemplate, Shaper
from haystack.schema import Answer, Document, List
from haystack.pipelines import Pipeline, TextIndexingPipeline


class QAPromptOutputAdapter(BaseComponent):
outgoing_edges = 1

def run(self, **kwargs):
    print(kwargs)
    return {&quot;answers&quot;: [Answer(answer=result, type=&quot;generative&quot;) for result in results]}, 
&quot;output_1&quot;

def run_batch(self):
    pass


class Bert:
pipe = None

def __init__(self, data_path):
    print(&quot;Initializing model...&quot;)
    doc_dir = data_path
    document_store = InMemoryDocumentStore(use_bm25=True)

    files_to_index = [os.path.join(doc_dir, f) for f in os.listdir(doc_dir)]
    indexing_pipeline = TextIndexingPipeline(document_store)
    indexing_pipeline.run_batch(file_paths=files_to_index)

    print(&quot;Done indexing&quot;)

    retriever = BM25Retriever(document_store=document_store, top_k=2)

    lfqa_prompt = PromptTemplate(
        prompt=&quot;&quot;&quot;Synthesize a comprehensive answer from the following text for the given 
question.
                                 Provide a clear and concise response that summarizes the key 
points and information presented in the text.
                                 Your answer should be in your own words and be no longer than 
50 words.
                                 \n\n Related text: {join(documents)} \n\n Question: {query} 
\n\n Answer:&quot;&quot;&quot;,
        #output_parser=AnswerParser(),
    )

    prompt_node = PromptNode(model_name_or_path=&quot;google/flan-t5-large&quot;, 
default_prompt_template=lfqa_prompt)

    question_shaper = Shaper(func=&quot;value_to_list&quot;, inputs={&quot;value&quot;: &quot;query&quot;, &quot;target_list&quot;: 
&quot;documents&quot;},
                             outputs=[&quot;questions&quot;])
    answer_shaper = Shaper(func=&quot;value_to_list&quot;,
                           inputs={&quot;value&quot;: &quot;prompt_node.results&quot;, 
&quot;target_list&quot;: &quot;documents&quot;}, outputs=[&quot;answers&quot;])

    elaboration_prompt = PromptTemplate(
        prompt=&quot;&quot;&quot;Elaborate on the answer to the following question given the related texts.
                                 Provide additional details to the answer in your own words.
                                 The final response should be between 100-200 words.
                                 \n\n Related text: {join(documents)} \n\n Question: 
{questions} \n\n Answer: {outputs}&quot;&quot;&quot;,
        output_parser=AnswerParser(),
    )
    elaboration_node = PromptNode(model_name_or_path=&quot;google/flan-t5-large&quot;,
                                  default_prompt_template=elaboration_prompt)

    self.pipe = Pipeline()
    self.pipe.add_node(component=retriever, name=&quot;retriever&quot;, inputs=[&quot;Query&quot;])
    self.pipe.add_node(component=prompt_node, name=&quot;prompt_node&quot;, inputs=[&quot;retriever&quot;])
    self.pipe.add_node(component=question_shaper, name=&quot;question_shaper&quot;, inputs= 
[&quot;prompt_node&quot;])
    self.pipe.add_node(component=answer_shaper, name=&quot;answer_shaper&quot;, inputs=[&quot;prompt_node&quot;])
    self.pipe.add_node(component=elaboration_node, name=&quot;elaboration_node&quot;,
                       inputs=[&quot;question_shaper&quot;, &quot;retriever&quot;, &quot;answer_shaper&quot;])

def generate(self, query):
    prediction = self.pipe.run(query=query)

    return prediction
</code></pre>
<p>Now I just get:</p>
<blockquote>
<p>Exception: Exception while running node 'answer_shaper': name 'results' is not defined</p>
</blockquote>
<p>Is this the correct solution to chaining two prompt nodes together? Should I be using shapers or am I going about this completely wrong? I'm fairly new to Haystack and generative AI models in general, so help is greatly appreciated.</p>
","python, huggingface-transformers, bert-language-model, haystack","<p>The answer is supposedly to set the &quot;output_variable&quot; parameter of the PromptNode like this:</p>
<pre><code>lfqa_node = PromptNode(
    model_name_or_path=&quot;google/flan-t5-large&quot;, 
    default_prompt_template=lfqa_prompt, 
    output_variable=&quot;my_answer&quot;
)
</code></pre>
<p>And then you can use the output like:</p>
<pre><code>elaboration_prompt = PromptTemplate(
    prompt=&quot;&quot;&quot;
         ...
         Previous answer: {my_answer} \n\n New answer: 
    &quot;&quot;&quot;
)
</code></pre>
<p>However, this solution did not seem to work for me, so I simply wrote two separate pipelines, and manually parsed the response from the first pipeline and inputted the answer variable into the second pipeline like this:</p>
<pre><code>lfqa = self.pipe.run(query=query)
lfqa_answer = lfqa['results'][0]
elaboration = self.elaboration_pipeline.run(query=lfqa_answer)
</code></pre>
",1,2,814,2023-07-10 08:06:00,https://stackoverflow.com/questions/76651826/how-to-chain-multiple-promptnodes-together-in-a-haystack-generativeqapipeline
ber-base-uncase does not use newly added suffix token,"<p>I want to add custom tokens to the BertTokenizer. However, the model does not use the new token.</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer.add_tokens('##oldert')

text = &quot;DocumentOlderThan&quot;
tokens = tokenizer.tokenize(text)
print(tokens)
</code></pre>
<p>Output is:</p>
<p><code>['document', '##old', '##ert', '##han']</code></p>
<p>But I would expect:</p>
<p><code>['document', '##oldert', '##han']</code></p>
<p>How can I make the tokenizer use the new token instead of multiple old ones?</p>
","python, nlp, tokenize, bert-language-model, sentence-transformers","<p>You need to update the tokenizers vocabulary, as such.</p>
<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
added_tokens = ['##oldert']

# Add new tokens to the tokenizer's vocabulary
tokenizer.vocab.update({token: len(tokenizer.vocab) for token in added_tokens})
tokenizer.ids_to_tokens.update({v: k for k, v in tokenizer.vocab.items()})

text = &quot;DocumentOlderThan&quot;
tokens = tokenizer.tokenize(text)
print(tokens)
</code></pre>
<p>Which results in:
<code>['document', '##oldert', '##han']</code></p>
",0,1,34,2023-07-14 13:53:51,https://stackoverflow.com/questions/76688344/ber-base-uncase-does-not-use-newly-added-suffix-token
RuntimeError when trying to extract text features from a BERT model then using KNN for classification,"<p>I'm trying to use camembert model to just to extract text features. After that, I'm trying to use a KNN classifier to classify the feature vectors as inputs.</p>
<p>This is the code I wrote</p>
<pre><code>import torch
from transformers import AutoTokenizer, CamembertModel
from sklearn.neighbors import KNeighborsClassifier

tokenizer = AutoTokenizer.from_pretrained(&quot;camembert-base&quot;)
model = CamembertModel.from_pretrained(&quot;camembert-base&quot;)

data = df.to_dict(orient='split')
data = dict(zip(data['index'], data['data']))

# Collect all the input texts into a list of strings
input_texts = [str(text) for text in data.values()]

# Tokenize all the input texts together
inputs = tokenizer(input_texts, return_tensors=&quot;pt&quot;, padding=True, truncation=True)

# Get the model outputs for all the input texts
with torch.no_grad():
    outputs = model(**inputs)

# Extract the last hidden states and convert them to a numpy array
last_hidden_states = outputs.last_hidden_state
input_features = last_hidden_states[:, 0, :].numpy()

# Extract the labels from the data dictionary
input_labels = list(data.keys())

neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(input_features, input_labels)

</code></pre>
<p>However, I get this error</p>
<pre><code>RuntimeError: [enforce fail at ..\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 19209424896 bytes.
</code></pre>
<p>The data I'm using in the dictionary has this form:</p>
<pre><code>{
    'index': [row_index_1, row_index_2, ...],
    'columns': [column_name_1, column_name_2, ...],
    'data': [
        [cell_value_row_1_col_1, cell_value_row_1_col_2, ...],
        [cell_value_row_2_col_1, cell_value_row_2_col_2, ...],
        ...
    ]
}
</code></pre>
","python, machine-learning, nlp, bert-language-model, knn","<p>It seems that you are feeding ALL your data to the model at once and you don't have enough memory to do that. Instead of doing that, you can invoke the model sentence by sentence or with small sentence batches, so that you keep the needed memory within the available system resources.</p>
",1,2,138,2023-07-31 09:02:21,https://stackoverflow.com/questions/76802096/runtimeerror-when-trying-to-extract-text-features-from-a-bert-model-then-using-k
Low f1 score and also low loss function score,"<p>I am trying to build a multi label text classification model to classify toxic comments.
I followed a medium article from this link: <a href=""https://kyawkhaung.medium.com/multi-label-text-classification-with-bert-using-pytorch-47011a7313b9"" rel=""nofollow noreferrer"">Multi-label Text Classification with BERT using Pytorch</a></p>
<p>I also used this data set from kaggle: <a href=""https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge?select=train.csv"" rel=""nofollow noreferrer"">jigsaw-toxic-comment-classification-challenge</a></p>
<p>Im using google colab to run my model with a V100 gpu runtime settings.</p>
<p>Unfortunately after several hours of training (4 epochs), I suffer from a f1 score of only 0.04214842148421484. my final loss score is 0.00354736</p>
<p>I know that the loss function and the f1 score are 2 different things but for my understanding a low cost function score should affect the f1 score. where did i go wrong?</p>
<p>here is the code:</p>
<pre><code>import torch
import numpy as np
import pandas as pd
import shutil, sys
import transformers
from sklearn import metrics
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel, BertConfig

val_targets=[]
val_outputs=[]

class CustomDataset(Dataset):

    def __init__(self, dataframe, tokenizer, max_len,):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.title = dataframe['comment_text']
        self.targets = self.data.target_list
        self.max_len = max_len

    def __len__(self):
        return len(self.title)

    def __getitem__(self, index):
        title = str(self.title[index])
        title = &quot; &quot;.join(title.split(&quot; &quot;))

        inputs = self.tokenizer.encode_plus(
            title,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs[&quot;token_type_ids&quot;]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }
    

class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)
        self.l2 = torch.nn.Dropout(0.3)
        self.l3 = torch.nn.Linear(768, 6)

    def forward(self, ids, mask, token_type_ids):
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        return output
    
def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)


def save_ckp(state, is_best, checkpoint_path, best_model_path):
    &quot;&quot;&quot;
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    &quot;&quot;&quot;
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    &quot;&quot;&quot;
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into
    optimizer: optimizer we defined in previous training
    &quot;&quot;&quot;
    # load checkpoint
    checkpoint = torch.load(checkpoint_fpath)

    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])

    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])

    # handle valid_loss_min based on its type
    valid_loss_min = checkpoint['valid_loss_min']
    if isinstance(valid_loss_min, torch.Tensor):
        valid_loss_min = valid_loss_min.item()

    # return model, optimizer, epoch value, min validation loss
    return model, optimizer, checkpoint['epoch'], valid_loss_min


def train_model(start_epochs,  n_epochs, valid_loss_min_input,
                training_loader, validation_loader, model,
                optimizer, checkpoint_path, best_model_path):

  # initialize tracker for minimum validation loss
  valid_loss_min = valid_loss_min_input


  for epoch in range(start_epochs, n_epochs+1):
    train_loss = 0
    valid_loss = 0

    model.train()
    print('############# Epoch {}: Training Start   #############'.format(epoch))
    for batch_idx, data in enumerate(training_loader):
        #print('yyy epoch', batch_idx)
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)

        optimizer.zero_grad()
        outputs = model(ids, mask, token_type_ids)
        print(outputs.shape)

        loss = loss_fn(outputs, targets)
        if batch_idx%100==0:
           print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')

        loss.backward()
        optimizer.step()
        #print('before loss data in training', loss.item(), train_loss)
        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))
        #print('after loss data in training', loss.item(), train_loss)

    print('############# Epoch {}: Training End     #############'.format(epoch))

    print('############# Epoch {}: Validation Start   #############'.format(epoch))
    ######################
    # validate the model #
    ######################

    model.eval()


    outputs, targets = do_validation(validation_loader)
    val_preds = (np.array(outputs) &gt; 0.5).astype(int)
    val_targets = (np.array(targets) &gt; 0.5).astype(int)
    accuracy = metrics.accuracy_score(val_targets, val_preds)
    f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')
    f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')
    print(f&quot;Accuracy Score = {accuracy}&quot;)
    print(f&quot;F1 Score (Micro) = {f1_score_micro}&quot;)
    print(f&quot;F1 Score (Macro) = {f1_score_macro}&quot;)

          
    print('############# Epoch {}: Validation End     #############'.format(epoch))
    # calculate average losses
    #print('before cal avg train loss', train_loss)
    train_loss = train_loss/len(training_loader)
    valid_loss = valid_loss/len(validation_loader)
    # print training/validation statistics
    print('Epoch: {} \tAvgerage Training Loss: {:.6f} \tAverage Validation Loss: {:.6f}'.format(
          epoch,
          train_loss,
          valid_loss
          ))

    # create checkpoint variable and add important data
    checkpoint = {
          'epoch': epoch + 1,
          'valid_loss_min': valid_loss,
          'state_dict': model.state_dict(),
          'optimizer': optimizer.state_dict()
    }

      # save checkpoint
    save_ckp(checkpoint, False, checkpoint_path, best_model_path)

    ## TODO: save the model if validation loss has decreased
    if valid_loss &lt;= valid_loss_min:
      print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))
      # save checkpoint as best model
      save_ckp(checkpoint, True, checkpoint_path, best_model_path)
      valid_loss_min = valid_loss

    print('############# Epoch {}  Done   #############\n'.format(epoch))


  return model


def do_validation(dataloader):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        for _, data in enumerate(dataloader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs, fin_targets


if __name__ == '__main__':

    # If there's a GPU available...
    if torch.cuda.is_available():

        # Tell PyTorch to use the GPU.
        device = torch.device(&quot;cuda&quot;)

        print('There are %d GPU(s) available.' % torch.cuda.device_count())

        print('We will use the GPU:', torch.cuda.get_device_name(0))

    # If not...
    else:
        print('No GPU available, using the CPU instead.')
        device = torch.device(&quot;cpu&quot;)

    train_df =  pd.read_csv(train_data_location,on_bad_lines='skip')
    test_df = pd.read_csv(test_data_location,on_bad_lines='skip')
    select_labels = train_df.columns.values.tolist()[2:]
    train_df['target_list'] = train_df[select_labels].values.tolist()
    test_df['target_list'] = test_df[select_labels].values.tolist()
    MAX_LEN = 64
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 8
    EPOCHS = 10
    LEARNING_RATE = 1e-05
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    training_set = CustomDataset(train_df, tokenizer, MAX_LEN)
    validation_set = CustomDataset(test_df, tokenizer, MAX_LEN)
    train_params = {'batch_size': TRAIN_BATCH_SIZE,
                    'shuffle': True,
                    'num_workers': 0
                    }

    test_params = {'batch_size': VALID_BATCH_SIZE,
                    'shuffle': False,
                    'num_workers': 0
                    }

    training_loader = DataLoader(training_set, **train_params)
    validation_loader = DataLoader(validation_set, **test_params)
    model = BERTClass()
    model.to(device)
    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)
    checkpoint_path = '/content/checkpoints/current_checkpoint.pt'
    best_model = '/content/checkpoints/best_model.pt'
    trained_model = train_model(1, EPOCHS, np.Inf, training_loader, validation_loader, model,
                        optimizer,checkpoint_path,best_model)
    

 
</code></pre>
","machine-learning, pytorch, bert-language-model, text-classification","<p>The F1 score is the <a href=""https://en.wikipedia.org/wiki/F-score#Definition"" rel=""nofollow noreferrer"">harmonic mean of precision and recall</a>. It allows the programmer to see precision and recall in a single number. Loss scores are not directly correlated to other performance metrics.</p>
<p>For multi-label text classification, <a href=""https://shiffdag.medium.com/what-is-accuracy-precision-and-recall-and-why-are-they-important-ebfcb5a10df2"" rel=""nofollow noreferrer"">accuracy, precision, and recall</a> are the important metrics. Specifically, examine your overall accuracy score, and then the precision and recall scores per class. Outside of research and commercial purposes, an F1 score is not particularly useful.</p>
<p>As for why you're getting a low F1 score in the first place, did you <a href=""https://www.geeksforgeeks.org/how-to-do-train-test-split-using-sklearn-in-python/"" rel=""nofollow noreferrer"">split your  dataset</a>? I see you imported Sklearn's train_test_split library but you never call it in your code. It seems like you're just passing the entire raw dataset to your training function.</p>
",2,-1,835,2023-08-01 14:10:31,https://stackoverflow.com/questions/76812516/low-f1-score-and-also-low-loss-function-score
why nn.Embedding layer is used for positional encoding in bert?,"<p>In the huggingface implementation of <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L186"" rel=""nofollow noreferrer"">bert model</a>, for positional embedding nn.Embedding is used.
Why it is used instead of traditional sin/cos positional embedding described in the transformer paper? how this two things are same?</p>
<p>Also I am confused about the nn.Embedding layer? there are many word embedding like word2vec, glove. among them which is actually nn.Embedding layer? Can you please explain the inner structure of nn.Embedding in detail?
<a href=""https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear"">This question</a> also comes in my mind.</p>
","pytorch, nlp, huggingface-transformers, bert-language-model, word-embedding","<p><code>nn.Embedding</code> is just a table of vectors. Its input are indices to the table. Its output are the vectors associated to the indices from the input. Conceptually, it is equivalent to having one-hot vectors multiplied by a matrix, because the result is just the vector within the matrix selected by the one-hot input.</p>
<p>BERT is based on the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer architecture</a>. The Transformer architecture needs positional information to be added to the normal tokens for it to distinguish where each token is at. The positional information in the original formulation of the Transformer architecture can be incorporated in 2 different ways (both with equal performance numbers):</p>
<ul>
<li>Static sinusoidal vectors: the positional embeddings are precomputed.</li>
<li>Trained positional embeddings: the positional embeddings are learned.</li>
</ul>
<p>The authors of the BERT article decided to go with trained positional embeddings. Anyway, in both cases the positional encodings are implemented with a normal embedding layer, where each vector of the table is associated with a different position in the input sequence.</p>
<p><strong>Update</strong>:</p>
<p>Positional embeddings are not essentially different from word embeddings. The only difference is how they are trained.</p>
<p>In word embeddings, you obtain the vectors so that they can be used to predict other words that appear close to the vector's word in the training data.</p>
<p>In positional embeddings, each vector of the table is associated with an index representing a token position, and you train the embeddings so that the vector associated with a specific position, when added to the token embedding at that position, is helpful for the task the model is trained on (masked LM for BERT, machine translation for the original Transformer).</p>
<p>Therefore, the positional embeddings end up with information that depends on the position, because the positional embedding vector is selected based on the position of the token it will be used for, and which has been trained to be useful for the task.</p>
<p>Later, the authors of the Transformer article discovered that they could simply devise a &quot;static&quot; (not trained) version of the embeddings (i.e. the sinusoidal embeddings), which reduced the total size of the model to be stored. In this case, the information in the precomputed positional vectors, together with the learned token embeddings is enough for the model to reach the same level of performance (in the machine translation task at least).</p>
",3,4,4725,2023-08-03 05:11:31,https://stackoverflow.com/questions/76825022/why-nn-embedding-layer-is-used-for-positional-encoding-in-bert
HuggingFace transformer evaluation process is too slow,"<p>I used the HuggingFace <code>transformers</code> library to train a BERT model for sequence classification.</p>
<p>The training process is good on GPU, but the evaluation process(which is running GPU) is too slow. For example, when I just have a sanity check for just 20 short text inputs, the evaluation runtime is about 160 seconds per step.</p>
<p>Here's the snippet code:</p>
<pre><code>def compute_metrics(eval_pred):
    accuracy_metric = evaluate.load(&quot;accuracy&quot;)
    f1_metric = evaluate.load(&quot;f1&quot;, average=&quot;macro&quot;)

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    f1_score = f1_metric.compute(predictions=predictions, references=labels, average=&quot;macro&quot;)

    return {**accuracy, **f1_score}
</code></pre>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(
        base_model_path,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id
    )

    training_args = TrainingArguments(
        output_dir=&quot;.&quot;,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=n_epoch,
        weight_decay=weight_decay,
        evaluation_strategy=&quot;steps&quot;,
        eval_steps=eval_steps,
        logging_strategy=&quot;steps&quot;,
        logging_steps=logging_steps,
        save_strategy=&quot;steps&quot;,
        save_steps=saving_steps,
        load_best_model_at_end=True,
        report_to=[&quot;tensorboard&quot;],
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_ds,
        eval_dataset=tokenized_valid_ds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()
</code></pre>
<p>The properties of the environment:</p>
<pre><code>transformers              4.29.2
Python                    3.10.9
</code></pre>
<p>and the configuration of training is like the following:</p>
<pre><code>len(train_data) ~= 36K
len(valid_data) ~= 2K
len(test_data) ~= 2K

model_name = 'bert-base-uncased'

per_device_train_batch_size=16
per_device_eval_batch_size=16
num_train_epochs=30

</code></pre>
<p>P.S.: The length of all data is small(less than ten tokens).</p>
<p>Can anyone suggest a solution to reduce the time overhead of the evaluation process?</p>
","machine-learning, huggingface-transformers, bert-language-model, training-data, huggingface","<p>So I finally got the problem. It's related to <code>evaluate.load()</code> calls inside the <code>compute_metrics</code> function. It seems this method has a significant overhead in time, so it shouldn't be inside some functions e.g. <code>compute_metrics</code> which are called many times. I moved out two <code>load()</code> methods of <code>compute_metrics</code> function and it works quickly now.</p>
",3,1,3116,2023-08-26 09:06:42,https://stackoverflow.com/questions/76982260/huggingface-transformer-evaluation-process-is-too-slow
How to get Attentions Part from the output of a Bert model?,"<h4>I am using <code>Bert-Model</code> for  Query Expansion and I am trying to extract the keywords from the Document I have</h4>
<pre><code>tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
sentence=&quot;This is a sentence&quot;
tokens = tokenizer.tokenize(sentence)
print(tokens,&quot;-tokens&quot;)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(\[input_ids\])
print(input_ids)
with torch.no_grad():
output = model(input_ids)
attention_scores = output.attentions
print(attention_scores)   #this prints None
</code></pre>
<p>this is my code I am using a Simple Sentence and trying to extract keywords from it In order to Do that I need attention of the output part of <code>Bert-Model</code></p>
<p>I tried with Different tokenizer methods(tokenize,encode,encode_plus) to tokenize and tried with
different bert variants (bert-large-uncased)</p>
<p>I want to extract the attention Part form the model output but I am not able to do that</p>
<p>I get None in that place I am not able to get any value in the attention part of  the output</p>
","python, nlp, huggingface-transformers, bert-language-model, word-embedding","<p>you can't really extract keywords from the code you provided. Your <code>output</code> has no attribute called <code>attentions</code>, that's why <code>output.attentions</code> is returning <code>None</code>, hence the error you're facing.</p>
<p>I benchmarked some transformer models for keyword extraction last year for my university project. I will provide you with a solution from there. This script will return you the extracted keywords with their score as a dictionary.</p>
<h4>Solution</h4>
<ul>
<li>Install the dependencies</li>
</ul>
<pre><code>pip install nltk
pip install spacy
pip install torch
pip install transformers
</code></pre>
<ul>
<li>Necessary imports</li>
</ul>
<pre><code>from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import spacy

nlp = spacy.load('en_core_web_sm')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
</code></pre>
<ul>
<li>Utility functions</li>
</ul>
<pre><code>def pre_process(text):
    # lowercase
    text=text.lower()
    #remove tags
    text=re.sub(&quot;&amp;lt;/?.*?&amp;gt;&quot;,&quot; &amp;lt;&amp;gt; &quot;,text)
    # remove special characters and digits
    text=re.sub(&quot;(\\d|\\W)+&quot;,&quot; &quot;,text)
    ##Convert to list from string
    text = text.split()
    # remove stopwords
    text = [word for word in text if word not in stop_words]
    # remove words less than three letters
    text = [word for word in text if len(word) &gt;= 3]
    #lemmatize
    lmtzr = WordNetLemmatizer()
    text = [lmtzr.lemmatize(word) for word in text]
    return ' '.join(text)

def get_candidates(text):
    # group of words
    n_gram_range = (1, 1)

    # Extract candidate words/phrases
    count = CountVectorizer(ngram_range=n_gram_range, stop_words=&quot;english&quot;).fit([text])
    all_candidates = count.get_feature_names_out()
    
    doc = nlp(text)
    noun_phrases = set(chunk.text.strip() for chunk in doc.noun_chunks)

    # Only pass the Noun/Adjective keywords for extraction
    noun_adj = set()
    for token in doc:
        if token.pos_ == &quot;NOUN&quot;:
            noun_adj.add(token.text)
        if token.pos_ == &quot;ADJ&quot;:
            noun_adj.add(token.text)

    all_words = noun_adj.union(noun_phrases)
    candidates = list(filter(lambda candidate: candidate in all_words, all_candidates))
    return candidates
</code></pre>
<ul>
<li>Keyword Extraction Method</li>
</ul>
<pre><code># The keyword extraction method
def keyword_extract(string, model_name):

    # Obtaining candidate keywords and document representations
    model = AutoModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    text=pre_process(string)
    candidates=get_candidates(text)
    candidate_tokens = tokenizer(candidates, padding=True, return_tensors=&quot;pt&quot;)
    candidate_embeddings = model(**candidate_tokens)[&quot;pooler_output&quot;]

    # Determination of keywords:
    text_tokens = tokenizer([text], padding=True, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)
    text_embedding = model(**text_tokens)[&quot;pooler_output&quot;]

    candidate_embeddings = candidate_embeddings.detach().numpy()
    text_embedding = text_embedding.detach().numpy()

    distances = cosine_similarity(text_embedding, candidate_embeddings)
    keywords = {i:j for i,j in zip(candidates, distances[0])}

    # sort based on attention score and return the dictionary
    return dict(sorted(keywords.items(), key=lambda x:x[1], reverse=True))

# put the model name here
model_name = &quot;bert-base-uncased&quot;

my_sentence = &quot;Hello there! This is an example sentence and this is the code that I'm using to extract keywords from a sentence&quot;

keywords = keyword_extract(my_sentence, model_name)
print(keywords)
</code></pre>
<h4>Output</h4>
<p>Output returned the keywords sorted according to their score</p>
<pre><code>{'keywords': 0.97554314, 'example': 0.94142365, 'extract': 0.885766, 'code': 0.83722556, 'sentence': 0.76782566}
</code></pre>
",0,0,553,2023-08-26 11:00:44,https://stackoverflow.com/questions/76982659/how-to-get-attentions-part-from-the-output-of-a-bert-model
Doubts regarding ELECTRA Paper Implementation,"<p>I am a master's student currently studying NLP. I was reading the ELECTRA paper by Clark et al. I had a few doubts regarding the implementation and training.</p>
<p>I was wondering if you could help me with those.</p>
<ol>
<li>What exactly does the &quot;Step&quot; mean in step count? Does it mean 1 epoch or 1 minibatch?</li>
<li>Also, in the paper I saw (specifically in Table 1), ELECTRA-SMALL and BERT-SMALL both have 14M parameters, how is that possible as ELECTRA should have more parameters because its generator and discriminator module are both BERT-based?</li>
<li>Also, what is the architecture of both the generator and discriminator? Are they both BERT to something else?</li>
<li>Also, we have a sampling step between the generator and the discriminator. How are you back-propagating the gradients through this?</li>
</ol>
<p>Thanks in advance</p>
<p>Well, I tried looking online for answers, but they were not cconclusive. Regarding backpropagating the gradients, i think the gradients in discriminator are not backpropagated to the generator , both are trained separately, although the generated input of current step is put as input to the discriminator.</p>
","bert-language-model, transformer-model, large-language-model","<p>Okay, from the paper itself the answers can be given.</p>
<ol>
<li>Yes step means 1 minibatch. Basically everytime optimizer.step() is called, it counts as 1 step.</li>
<li>It is considered for fine-tuning, so only discriminator is used.</li>
<li>Any transformer Encoder can be used, but for their implementation they have used BERT.</li>
<li>They(discriminator and generator) are both trained jointly, i.e. one sample is passed through generator and then through discriminator , the corresponding losses are calculated and backpropogated.</li>
</ol>
",0,-1,39,2023-09-08 07:55:14,https://stackoverflow.com/questions/77065093/doubts-regarding-electra-paper-implementation
Map BERT token indices to Spacy token indices,"<p>I’m trying to make Bert’s (<code>bert-base-uncased</code>) tokenization token indices (not ids, token indices) map to Spacy’s tokenization token indices. In the following example, my approach doesn’t work becos Spacy’s tokenization behaves a bit more complex than I anticipated. Thoughts on solving this?</p>
<pre><code>import spacy
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
nlp = spacy.load(&quot;en_core_web_sm&quot;)

sent = nlp(&quot;BRITAIN'S railways cost £20.7bn during the 2020-21 financial year, with £2.5bn generated through fares and other income, £1.3bn through other sources and £16.9bn from government, figures released by the regulator the Office of Rail and Road (ORR) on November 30 revealed.&quot;)
# Get spacy word index to BERT token indice mapping
wd_to_tok_map = [wd.i for wd in sent for el in tokenizer.encode(wd.text, add_special_tokens=False)]
len(sent) # 55
len(wd_to_tok_map) # 67     &lt;- Should be 65

input_ids = tokenizer.encode(sent.text, add_special_tokens=False)
len(input_ids) # 65
</code></pre>
<p>I can print both tokenizations and look for perfect text matches, but the problem I run into is what if a word repeats twice in the tokenization? Looking for a word match will return two indices at different sections of the sentence.</p>
<pre><code>[el.text for el in sent]
['BRITAIN', ''S', 'railways', 'cost', '£', '20.7bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', 'with', '£','2.5bn','generated','through', 'fares', 'and','other', 'income', ',', '£', '1.3bn', 'through', 'other', 'sources', 'and', '£', '16.9bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'Office', 'of', 'Rail', 'and', 'Road', '(', 'ORR', ')', 'on', 'November', '30', 'revealed', '.']

[tokenizer.ids_to_tokens[el] for el in input_ids]
['britain',''', 's', 'railways', 'cost', '£2', '##0', '.', '7', '##bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', 'with', '£2', '.', '5', '##bn', 'generated', 'through', 'fares', 'and', 'other', 'income', ',', '£1', '.', '3', '##bn', 'through', 'other', 'sources', 'and', '£1', '##6', '.', '9', '##bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'office', 'of', 'rail', 'and', 'road', '(', 'orr', ')', 'on', 'november', '30', 'revealed', '.']
</code></pre>
<p>decode() doesn’t seem to give me what I want, as I’m after the indices.</p>
","python, mapping, spacy, tokenize, bert-language-model","<p>Use a fast tokenizer to get the character offsets directly from the transformer tokenizer with <code>return_offsets_mapping=True</code>, and then map those to the spacy tokens however you'd like:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
text = &quot;BRITAIN'S railways cost £20.7bn&quot;
output = tokenizer([text], return_offsets_mapping=True)

print(output[&quot;input_ids&quot;])
# [[101, 3725, 1005, 1055, 7111, 3465, 21853, 2692, 1012, 1021, 24700, 102]]

print(tokenizer.convert_ids_to_tokens(output[&quot;input_ids&quot;][0]))
# ['[CLS]', 'britain', &quot;'&quot;, 's', 'railways', 'cost', '£2', '##0', '.', '7', '##bn', '[SEP]']

print(output[&quot;offset_mapping&quot;])
# [[(0, 0), (0, 7), (7, 8), (8, 9), (10, 18), (19, 23), (24, 26), (26, 27), (27, 28), (28, 29), (29, 31), (0, 0)]]
</code></pre>
",0,0,196,2023-10-25 13:58:12,https://stackoverflow.com/questions/77360174/map-bert-token-indices-to-spacy-token-indices
Semantic search with pretrained BERT models giving irrelevant results with high similarity,"<p>I'm trying to create a Semantic search system and have experimented with multiple pretrained models from the SentenceTransformers library: LaBSE, MS-MARCO etc. The system is working well in returning relevant documents first with high probability but issue is documents which are not relevant are also coming with relatively high probabilities. Hence it has become difficult to determine a cutoff threshold for what is relevant and what isnt.</p>
<p>For computing vector similarity i have experimented with Elasticsearch approximate KNN and FAISS with similar results in both. Have checked exact cosine similarities with Scikit-learn also.</p>
<p>My corpus generally has sentences of 15-30 words and the input sentence is &lt; 10 words long. Example is given below</p>
<p>Corpus text 1: &lt;brand_name&gt; is a Fashion House. We design, manufacture and retail men's and women's apparel
Input sentence 1: men's fashion
Cosine similarity 1: 0.21</p>
<p>Corpus text 2:  is an app for pizza delivery
Input sentence 2: personal loan
Cosine similarity 2: 0.16</p>
<p>Please suggest pretrained models that might be good for this purpose.</p>
<p>I have experimented with many pretrained models like LaBSE, ms-marco-roberta-base-v3 from the sentence transformers but seeing the same behaviour in all of them. Expecting embeddings of dissimilar sentences to have less cosine similarity</p>
","elasticsearch, bert-language-model, knn, semantic-search","<p>If you haven't already done so have a look at the distinction between symmetric and asymmetric semantic search and respective models trained specifically for this:</p>
<p><a href=""https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"" rel=""nofollow noreferrer"">https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search</a></p>
<p>From what I understand from your use case you might get better results with asymmetric search.</p>
<p>Reranking can help a lot too. See this:</p>
<p><a href=""https://www.sbert.net/examples/applications/retrieve_rerank/README.html"" rel=""nofollow noreferrer"">https://www.sbert.net/examples/applications/retrieve_rerank/README.html</a></p>
<p>Also you might want to have a look at <a href=""https://weaviate.io/"" rel=""nofollow noreferrer"">Weaviate</a>. For their vector search they have implemented an AutoCut function:</p>
<p><a href=""https://weaviate.io/developers/weaviate/search/similarity#autocut"" rel=""nofollow noreferrer"">https://weaviate.io/developers/weaviate/search/similarity#autocut</a></p>
<blockquote>
<p><strong>Autocut</strong> takes a positive integer parameter N, looks at the distance
between each result and the query, and stops returning results after
the Nth &quot;jump&quot; in distance. For example, if the distances for six
objects returned by nearText were [0.1899, 0.1901, 0.191, 0.21, 0.215,
0.23] then autocut: 1 would return the first three objects, autocut: 2 would return all but the last object, and autocut: 3 would return all
objects.</p>
</blockquote>
<p>Weaviate also has a <a href=""https://weaviate.io/developers/weaviate/search/hybrid"" rel=""nofollow noreferrer"">nice hybrid search implementation</a> (combining vector and lexical search) that might help you as well.</p>
",3,3,1175,2023-11-06 08:02:35,https://stackoverflow.com/questions/77429596/semantic-search-with-pretrained-bert-models-giving-irrelevant-results-with-high
Implementing Dynamic Data Sampling for BERT Language Model Training with PyTorch DataLoader,"<p>I'm currently in the process of building a BERT language model from scratch for educational purposes. While constructing the model itself was a smooth journey, I encountered challenges in creating the data processing pipeline, particularly with an issue that has me stuck.</p>
<p><strong>Overview:</strong></p>
<p>I am working with the IMDB dataset, treating each review as a document. Each document can be segmented into several sentences using punctuation marks (<code>.</code> <code>!</code> <code>?</code>). Each data sample consists of a <code>sentence A</code>, a <code>sentence B</code>, and an <code>is_next</code> label indicating whether the two sentences are consecutive. This implies that from each document (review), I can generate multiple training samples.</p>
<p>I am utilizing PyTorch and attempting to leverage the <code>DataLoader</code> for handling multiprocessing and parallelism.</p>
<p><strong>The Problem:</strong></p>
<p>The <code>__getitem__</code> method in the Dataset class is designed to return a single training sample for each index. However, in my scenario, each index references a document (review), and an undefined number of training samples may be generated for each index.</p>
<p><strong>The Question:</strong></p>
<p>Is there a recommended way to handle such a situation? Alternatively, I am considering the following approach:</p>
<p>For each index, an undefined number of samples are returned to the <code>DataLoader</code>. The <code>DataLoader</code> would then assess whether the number of samples is sufficient to form a batch. Here are the three possible cases:</p>
<ol>
<li>The number of samples returned for an index is less than the batch size. In this case, the <code>DataLoader</code> fetches additional samples from the next index (next document), and any excess is retained to form the next batch.</li>
<li>The number of samples returned for an index equals the batch size, and it passes it to the model.</li>
</ol>
<p>I appreciate any guidance or insights into implementing this dynamic data sampling approach with PyTorch <code>DataLoader</code>.</p>
","python, pytorch, bert-language-model, dataloader","<h2>I've got the answer <a href=""https://discuss.pytorch.org/t/implementing-dynamic-data-sampling-for-bert-language-model-training-with-pytorch-dataloader/192124"" rel=""nofollow noreferrer"">here</a>.</h2>
<pre><code>my_dataset = MyDataset(data)
data_loader = DataLoader(my_dataset, batch_size=None, batch_sampler=None)

batch_size = 32
current_batch = []

# The data_loader may return one or more samples...
for samples in data_loader:
    samples = process_samples(samples)

    # Accumulate samples until reaching the desired batch size
    # ...
    current_batch.append(samples)
    # ...

    if len(current_batch) == batch_size:
        processed_batch = process_batch(current_batch)

        # Forward pass, backward pass, and optimization steps...

        current_batch = []
</code></pre>
<p>By disabling automatic batching and manually forming batches in the training loop, I have the flexibility to handle an undefined number of training samples for each index directly within the <code>__getitem__</code> method. This approach aligns with the dynamic nature of my data, where the number of samples generated for each index may vary.</p>
<p>While it might be more optimized to perform everything in <code>__getitem__</code>, I found this trade-off between simplicity and optimization to be reasonable for my specific dataset and processing requirements.</p>
<p>Feel free to adapt and refine this approach based on your own dataset characteristics and processing needs. If you have any further questions or insights, I'd be happy to discuss them.</p>
<p>Happy coding!</p>
",0,0,244,2023-11-18 12:32:43,https://stackoverflow.com/questions/77506897/implementing-dynamic-data-sampling-for-bert-language-model-training-with-pytorch
How to resolve BERT HF Model ValueError: too many values to unpack (expected 2)?,"<p>I have a dummy dataset of two text columns and labels as below.</p>
<pre><code>import tensorflow as tf
from transformers import BertTokenizer, TFAutoModelForSequenceClassification
import numpy as np
from datasets import Dataset, DatasetDict

# Create a synthetic dataset with two text columns and a label column (0 or 1)
data_size = 1000
text_column1 = [&quot;This is sentence {}.&quot;.format(i) for i in range(data_size)]
text_column2 = [&quot;Another sentence {} for tokenization.&quot;.format(i) for i in range(data_size)]
labels = np.random.choice([0, 1], size=data_size)
</code></pre>
<p>I am using the HF bert model for classification.(TFAutoModelForSequenceClassification)</p>
<pre><code># Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model2 = TFAutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<p>When using the below code for preparing the dataset and model training, the execution is successful.</p>
<pre><code>def tokenize_dataset(df):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(df['text_column1'], df['text_column2'])
# Convert to a DataFrame
import pandas as pd
df = pd.DataFrame({'text_column1': text_column1, 'text_column2': text_column2, 'label': labels})
df =  Dataset.from_pandas(df).map(tokenize_dataset) 
tf_train = model2.prepare_tf_dataset(df, batch_size=4, shuffle=True, tokenizer=tokenizer)
model2.compile(optimizer=Adam(3e-5))  # No loss argument!
model2.fit(tf_train)
</code></pre>
<p>The above code works successfully.</p>
<p>However when I use padding, truncation and max_length in the tokenizer, i.e as below</p>
<pre><code>def tokenize_dataset(df):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(df['text_column1'], df['text_column2'], padding=True,truncation=True,max_length=30, return_tensors=&quot;tf&quot;)
# Convert to a DataFrame
import pandas as pd
df = pd.DataFrame({'text_column1': text_column1, 'text_column2': text_column2, 'label': labels})
df =  Dataset.from_pandas(df).map(tokenize_dataset) 
tf_train = model2.prepare_tf_dataset(df, batch_size=4, shuffle=True, tokenizer=tokenizer)
model2.compile(optimizer=Adam(3e-5))  # No loss argument!
model2.fit(tf_train)
</code></pre>
<p>This code gave the following error:</p>
<pre><code>ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).
        
   

 in user code:
    
        File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1557, in run_call_with_unpacked_inputs  *
            return func(self, **unpacked_inputs)
        File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py&quot;, line 766, in call  *
            batch_size, seq_length = input_shape
    
        ValueError: too many values to unpack (expected 2)
</code></pre>
<p>I am not able to understand why that will happen. If it happens then, why is it so and how to resolve the error?</p>
","python, tensorflow, machine-learning, huggingface-transformers, bert-language-model","<p>The <code>prepare_tf_dataset</code> function does not require the dataframe to have <code>tensor</code> value types. Removing <code>return_tensors='tf'</code> should solve the problem.</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize_dataset(df):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(
        df['text_column1'],
        df['text_column2'],
        padding=True,
        truncation=True,
        max_length=30)
</code></pre>
",1,0,226,2023-11-27 07:04:00,https://stackoverflow.com/questions/77555030/how-to-resolve-bert-hf-model-valueerror-too-many-values-to-unpack-expected-2
Bert topic clasiffying over a quarter of documents in outlier topic -1,"<p>I am running Bert topic with default options</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer
import time
import pickle
from bertopic import BERTopic

llm_mod =  &quot;all-MiniLM-L6-v2&quot;
model = SentenceTransformer(llm_mod)

embeddings = model.encode(skills_augmented, show_progress_bar=True)
bertopic_model = BERTopic(verbose=True)
</code></pre>
<p>I have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics).</p>
<pre><code>-1      13573
 0       1593
 1       1043
 2        628
 3        627
</code></pre>
<p>From the documentation: The -1 refers to all outliers and should typically be ignored. Is there a parameter I can use to get less documents in -1? Perhaps get a more even distribution across topics? Would running kmeans be better?</p>
","python, nlp, bert-language-model, topic-modeling","<p>From the documentation :</p>
<blockquote>
<p>The main way to reduce your outliers in BERTopic is by using the .reduce_outliers function. To make it work without too much tweaking, you will only need to pass the docs and their corresponding topics. You can pass outlier and non-outlier documents together since it will only try to reduce outlier documents and label them to a non-outlier topic.</p>
</blockquote>
<p>The following is a minimal example:</p>
<pre><code>from bertopic import BERTopic

# Train your BERTopic model
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)

# Reduce outliers
new_topics = topic_model.reduce_outliers(docs, topics)
</code></pre>
<p>You can find all the Strategies for reducing outliers in this page <a href=""https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html"" rel=""nofollow noreferrer"">Outlier reduction</a></p>
",1,-1,1083,2023-11-28 12:32:26,https://stackoverflow.com/questions/77563897/bert-topic-clasiffying-over-a-quarter-of-documents-in-outlier-topic-1
Truncating a training dataset so that it fits exactly within the context window,"<p>I have a dataset where the total tokens once tokenised is around 5000. I was to feed that into a BERT-style model so I have to shrink it down to 512 tokens but I want to rearrange the text to train it on fill-in-the-middle tasks using the techniques outlined in this paper: <a href=""https://arxiv.org/abs/2207.14255"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2207.14255</a>
My issue is that I want to take the last 512 - 1 tokens and then prepend my <code>&lt;PRE&gt;</code> token to the beginning, but I'm finding it difficult to simply prepend a single token to my tokenised text without going through the process of encoding the text to tokens, truncating off the text on the left, then decoding the text, adding my <code>&lt;PRE&gt;</code> token and then re-encoding the text again. Is there a simpler way?</p>
<p>Here's what I have so far:</p>
<pre class=""lang-py prettyprint-override""><code>additional_special_tokens = [&quot;&lt;PRE&gt;&quot;, &quot;&lt;SUF&gt;&quot;, &quot;&lt;MID&gt;&quot;]

tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side=&quot;left&quot;)
tokenizer.additional_special_tokens = additional_special_tokens

small_eval_dataset = full_dataset[&quot;validation&quot;].shuffle(42).select(range(1))

def build_training_data(examples):
    to_tokenized = examples[&quot;context&quot;] + &quot;&lt;SUF&gt;&lt;MID&gt;&quot; + examples[&quot;gt&quot;]
    tokenized = tokenizer(to_tokenized, truncation=True)
    tokenized[&quot;input_ids&quot;][0] = tokenizer(&quot;&lt;PRE&gt;&quot;)
    return tokenized


small_eval_dataset = small_eval_dataset.map(build_training_data)
</code></pre>
<p>I would like to have the text truncated on the left to 512 tokens, so that then I can feed that into my BERT-style model and have it train on this specific task.</p>
","huggingface-transformers, bert-language-model, huggingface-tokenizers","<p>First of all, to add special tokens to your tokenizer, you should use <code>add_tokens</code> method. Simply setting <code>tokenizer.additional_special_tokens</code> has no effect.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side=&quot;left&quot;)
tokenizer.add_tokens(additional_special_tokens, special_tokens=True)

print(tokenizer.added_tokens_encoder)
&gt;&gt;&gt; {'[PAD]': 0, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102, '[MASK]': 103, '&lt;PRE&gt;': 28996, '&lt;SUF&gt;': 28997, '&lt;MID&gt;': 28998}
</code></pre>
<p>After doing this, you should also resize the model embeddings (i.e. initialize random embeddings for the new tokens we just added) (source: <a href=""https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer"">How to add new special token to the tokenizer?</a> ):</p>
<pre><code>model.resize_token_embeddings(len(tokenizer))
</code></pre>
<p><em>Note that after training the model, it's a good idea to save the tokenizer using <code>tokenizer.save_pretrained</code> method, so that you can easily load it later.</em></p>
<p>Second, to prepend the <code>&lt;PRE&gt;</code> token to the tokenized text, you can directly modify the <code>tokenized</code> object:</p>
<pre class=""lang-py prettyprint-override""><code>def build_training_example(text):
    
    pre_token_id = tokenizer.get_vocab()['&lt;PRE&gt;']
    
    tokenized = tokenizer(text)
    tokenized['input_ids'] = [pre_token_id] + tokenized['input_ids'][-511:]
    tokenized['attention_mask'] = [1] + tokenized['attention_mask'][-511:]
    
    return tokenized
</code></pre>
<p>This function will truncate the last 511 tokens from the tokenized ids, and prepend the id of the <code>&lt;PRE&gt;</code> token.</p>
<p>I also added re-assigning the <code>attention_mask</code>, this might be useful if you have examples shorter than 512 and use padding to extend them to length 512, but you may or may not need this depending on how you plan to use the tokenized data. You may also consider using setting <code>add_special_tokens=False</code> in the <code>tokenizer</code> call, because if you want your data to always start with <code>&lt;PRE&gt;</code> token, it may be a good idea to avoid the <code>[CLS]</code> token, with which the tokenization starts if <code>add_special_tokens</code> is <code>True</code>.</p>
",2,1,601,2023-12-02 00:05:48,https://stackoverflow.com/questions/77588646/truncating-a-training-dataset-so-that-it-fits-exactly-within-the-context-window
Google Colab unable to Hugging Face model,"<p>I like to tag parts of speech using the BERT model. I used the <a href=""https://huggingface.co/AdapterHub/bert-base-uncased-pf-ud_pos"" rel=""nofollow noreferrer"">Hugging face</a> library for this purpose.</p>
<p>When I run the model on Hugging face API I got the output
<a href=""https://i.sstatic.net/RjeUj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RjeUj.png"" alt=""enter image description here"" /></a></p>
<p>However, when I run the code on Google Colab I got errors.</p>
<p>My code</p>
<pre><code>from transformers import AutoModelWithHeads
from transformers import pipeline
from transformers import AutoTokenizer

model = AutoModelWithHeads.from_pretrained(&quot;bert-base-uncased&quot;)
adapter_name = model.load_adapter(&quot;AdapterHub/bert-base-uncased-pf-ud_pos&quot;, source=&quot;hf&quot;)
model.active_adapters = adapter_name

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
token_classification = pipeline(&quot;token-classification&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;NONE&quot;)
res = token_classification(&quot;Take out the trash bag from the bin and replace it.&quot;)
print(res)
</code></pre>
<p>The error is</p>
<pre><code> The model 'BertModelWithHeads' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'YosoForTokenClassification', 'XLMRobertaAdapterModel', 'RobertaAdapterModel', 'AlbertAdapterModel', 'BeitAdapterModel', 'BertAdapterModel', 'BertGenerationAdapterModel', 'DistilBertAdapterModel', 'DebertaV2AdapterModel', 'DebertaAdapterModel', 'BartAdapterModel', 'MBartAdapterModel', 'GPT2AdapterModel', 'GPTJAdapterModel', 'T5AdapterModel', 'ViTAdapterModel'].
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-18-79b43720402e&gt; in &lt;cell line: 12&gt;()
     10 tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
     11 token_classification = pipeline(&quot;token-classification&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;NONE&quot;)
---&gt; 12 res = token_classification(&quot;Take out the trash bag from the bin and replace it.&quot;)
     13 print(res)

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py in aggregate(self, pre_entities, aggregation_strategy)
    346                 score = pre_entity[&quot;scores&quot;][entity_idx]
    347                 entity = {
--&gt; 348                     &quot;entity&quot;: self.model.config.id2label[entity_idx],
    349                     &quot;score&quot;: score,
    350                     &quot;index&quot;: pre_entity[&quot;index&quot;],
</code></pre>
<p>KeyError: 16</p>
<p>I don't understand if the model ran ok in the Hugging Face API then why it was unable to run on Google Colab?</p>
<p>Thank you in advance.</p>
","python, nlp, google-colaboratory, huggingface-transformers, bert-language-model","<p>Here is how you can do it:</p>
<pre><code>from transformers import AutoModelWithHeads, AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelWithHeads.from_pretrained(&quot;bert-base-uncased&quot;)
model.load_adapter(
    &quot;AdapterHub/bert-base-uncased-pf-ud_pos&quot;,
    source=&quot;hf&quot;,
    set_active=True,
)
token_classification = pipeline(
    &quot;token-classification&quot;,
    model=model,
    tokenizer=tokenizer,
)
</code></pre>
<p>This pipeline creation part is going to show a warning, i.e.</p>
<pre><code>The model 'BertModelWithHeads' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'YosoForTokenClassification', 'XLMRobertaAdapterModel', 'RobertaAdapterModel', 'AlbertAdapterModel', 'BeitAdapterModel', 'BertAdapterModel', 'BertGenerationAdapterModel', 'DistilBertAdapterModel', 'DebertaV2AdapterModel', 'DebertaAdapterModel', 'BartAdapterModel', 'MBartAdapterModel', 'GPT2AdapterModel', 'GPTJAdapterModel', 'T5AdapterModel', 'ViTAdapterModel'].
</code></pre>
<p>You can just ignore it, the pipeline will still work. Here is an example:</p>
<pre><code>&gt;&gt;&gt; token_classification(&quot;Take out the trash bag from the bin and replace it&quot;)
[{'entity': 'VERB','score': 0.99986637, 'index': 1, 'word': 'take', 'start': 0, 'end': 4},
 {'entity': 'ADP', 'score': 0.9829973, 'index': 2, 'word': 'out', 'start': 5, 'end': 8},
 {'entity': 'DET', 'score': 0.9998791, 'index': 3, 'word': 'the', 'start': 9, 'end': 12},
 {'entity': 'NOUN', 'score': 0.9958676, 'index': 4, 'word': 'trash', 'start': 13, 'end': 18},
 {'entity': 'NOUN', 'score': 0.99657273, 'index': 5, 'word': 'bag', 'start': 19, 'end': 22},
 {'entity': 'ADP', 'score': 0.99989176, 'index': 6, 'word': 'from', 'start': 23, 'end': 27},
 {'entity': 'DET', 'score': 0.99982834, 'index': 7, 'word': 'the', 'start': 28, 'end': 31},
 {'entity': 'NOUN', 'score': 0.99584526, 'index': 8, 'word': 'bin', 'start': 32, 'end': 35},
 {'entity': 'CCONJ', 'score': 0.99962616, 'index': 9, 'word': 'and', 'start': 36, 'end': 39},
 {'entity': 'VERB', 'score': 0.99976975, 'index': 10, 'word': 'replace', 'start': 40, 'end': 47},
 {'entity': 'PRON', 'score': 0.9989698, 'index': 11, 'word': 'it', 'start': 48, 'end': 50}]
</code></pre>
",1,0,1161,2023-12-18 03:07:35,https://stackoverflow.com/questions/77676747/google-colab-unable-to-hugging-face-model
How to convert model.safetensor to pytorch_model.bin?,"<p>I'm fine tuning a pre-trained bert model and i have a weird problem:
When i'm fine tuning using the CPU, the code saves the model like this:</p>
<p><a href=""https://i.sstatic.net/ve8EQ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ve8EQ.png"" alt=""model fine tuned with cpu"" /></a></p>
<p>With the &quot;pytorch_model.bin&quot;. But when i use CUDA (that i have to), the model is saved like this:</p>
<p><a href=""https://i.sstatic.net/vOfy1.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/vOfy1.png"" alt=""model fine tuned with gpu"" /></a></p>
<p>When i try to load this &quot;model.safetensors&quot; in the future, it raises an error &quot;pytorch_model.bin&quot; not found. I'm using two differents venvs to test using the CPU and CUDA.</p>
<p>How to solve this? is some version problem?</p>
<p>I'm using sentence_transformers framework to fine-tune the model.</p>
<p>Here's my training code:</p>
<pre class=""lang-py prettyprint-override""><code>checkpoint = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'

word_embedding_model = models.Transformer(checkpoint, cache_dir=f'model/{checkpoint}')
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')
model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device='cuda')


train_loss = losses.CosineSimilarityLoss(model)

evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='sbert')

model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=5, evaluator=evaluator, show_progress_bar=True, output_path=f'model_FT/{checkpoint}', save_best_model=True)
</code></pre>
<p>I did try the tests in two differentes venvs, and i'm expecting the code to save a &quot;pytorch_model.bin&quot; not a &quot;model.safetensors&quot;.</p>
<p>EDIT: i really don't know yet, but it seems that is the newer versions of transformers library that causes this problem. I saw that with hugging-face is possible to load the safetensors, but with Sentence-transformers (that i need to use) it's not.</p>
","machine-learning, pytorch, bert-language-model, sentence-transformers","<p>Probably you figured it out already but updating the transformer library now to the newest version resolves the issue.</p>
<pre><code>pip install -U transformers
</code></pre>
<p>U don't need to transform the model anymore you can load the load the model.safetensor with SentenceTransformer(&quot;Modelpath&quot;)</p>
",5,6,13134,2023-12-23 20:43:20,https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin
Shap value for binary classification using Pre-Train Bert: How to extract summary graph?,"<p>I used pre-train bert model for binary classification. After training my model with my small data, I wanted to extract summary graph like this <a href=""https://i.sstatic.net/xspuL.png"" rel=""nofollow noreferrer"">the graph I want</a>. However, I want to replace these important features  with words.</p>
<p>However, I am not sure everything is okay because the shape of shap_value is only two dimensional. Actually, this is sensible. Nevertheless, I did not get the graph because I encountered two problems if I use this code:</p>
<pre><code>shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=comments_text)`
</code></pre>
<p>Problem is too unsensible: If I change <code>shap_values[:,:10]</code> with <code>shap_values</code>  or <code>shap_values[0]</code> or <code>shap_values.values</code> vb. I always come across</p>
<pre><code>516: assert len(shap_values.shape) != 1, &quot;Summary plots need a matrix of 
shap_values, not a vector.&quot; ==&gt; AssertionError: Summary plots need a matrix of 
shap_values, not a vector.
</code></pre>
<p>(fist problem)</p>
<p>By the way, my shap_value consist of 10 input(shape_value.shape). If I choose for max value a range from 1 to 147 everything fine for drawing the graph. However,in this time, the graph is not suitable: My graph consist of only blue dot(-second problem-). Like this <a href=""https://i.sstatic.net/rjAXU.png"" rel=""nofollow noreferrer"">only blue not</a>.</p>
<p>Note: <code>shap_values[:,:10]</code> if the number (10) change different number, the graph show diffent word however the total number of the graph same (max 20). Only some words order can be changing.</p>
<p>Minimal reproducible example:</p>
<pre><code>import nlp
import numpy as np
import pandas as pd
import scipy as sp
import torch
import transformers
import torch
import shap

# load a BERT sentiment analysis model
tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(
    &quot;distilbert-base-uncased&quot;
)
model = transformers.DistilBertForSequenceClassification.from_pretrained(
    &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
).cuda()


if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device(&quot;cpu&quot;)

def f(x):
    # Encode the batch of sentenc
    inputs = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True,return_tensors='pt')

    # Send the tensors to the same device as the model
    input_ids = inputs['input_ids'].to(device)
    attention_masks = inputs['attention_mask'].to(device)
    # Predict
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units
    return val
# Build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer )

imdb_train = nlp.load_dataset(&quot;imdb&quot;)[&quot;train&quot;]
shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=16)
cohorts = {&quot;&quot;: shap_values}
cohort_labels = list(cohorts.keys())
cohort_exps = list(cohorts.values())
for i in range(len(cohort_exps)):
    if len(cohort_exps[i].shape) == 2:
        cohort_exps[i] = cohort_exps[i].abs.mean(0)
features = cohort_exps[0].data
feature_names = cohort_exps[0].feature_names
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])
feature_importance = pd.DataFrame(list(zip(feature_names, sum(values))), columns=['features', 'importance'])
feature_importance.sort_values(by=['importance'], ascending=False, inplace=True)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=imdb_train['text'][10:20],show=False)

</code></pre>
<p>The above code produce the same result. I spent approximately 200 computer units and I did not succeed it :(. How can I do?</p>
","python, machine-learning, bert-language-model, text-classification, shap","<p>Will you try:</p>
<pre><code>sv = np.array([arr[:100] for arr in shap_values.values])
data = np.array([arr[:100] for arr in shap_values.data])
shap.summary_plot(sv, data, feature_names=feature_importance['features'].tolist())
</code></pre>
<p>I've got a grey plot. This is because your data is non-numeric.</p>
",0,0,768,2024-01-09 08:49:10,https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar
Why doesn&#39;t BERT give me back my original sentence?,"<p>I've started playing with <code>BERT</code> encoder through the <code>huggingface</code> module.
<a href=""https://i.sstatic.net/N6J8m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N6J8m.png"" alt=""enter image description here"" /></a></p>
<p>I passed it a normal unmasked sentence and got the following results:
<a href=""https://i.sstatic.net/GPUBZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GPUBZ.png"" alt=""enter image description here"" /></a></p>
<p>However, when I try to manually apply the softmax and decode the output:
<a href=""https://i.sstatic.net/6oZ2i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6oZ2i.png"" alt=""enter image description here"" /></a></p>
<p>I get back a bunch of unexpected <code>tensor(1012)</code> instead of my original sentence. BERT is an autoencoder, no?</p>
<p>Shouldn't it be giving me back the original sentence with fairly high probability since none of the input words was <code>[MASK]</code>? Can anyone explain to me what is going on?</p>
","python, huggingface-transformers, bert-language-model, huggingface","<p>BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning model trained on a large corpus in self-learning fashion. It is devleoped to understand the context of a word based on its surrounding words in a sentence.</p>
<p>However, BERT is not an autoencoder as the prediction of non-masked words is ignored during training and does not reconstruct the input sentence. Rather,it generates a high-dimensional vector representation for each word in the sentence. These representations are context-dependent, means the same word can have different representations based on its context.</p>
<p>When user inputs a sentence to the BERT, it does not return the original sentence because it is not developed to generate text, but to understand the context and semantic meaning of the given text. BERT returns a set of vectors representing the contextualized meaning of each word in the sentence.</p>
",1,-1,249,2024-01-11 10:19:52,https://stackoverflow.com/questions/77799262/why-doesnt-bert-give-me-back-my-original-sentence
Truncate texts in the middle for Bert,"<p>I am learning about Bert, which only deals with texts with fewer than 512 tokens, and came across this <a href=""https://stackoverflow.com/a/59778726"">answer</a> which says that truncating text in the middle (as opposed to at the start or at the end) may work well for Bert. I wonder whether there is any library to do that type of truncation because as far as I understand, one word can consist of multiple Bert token so I cannot simply get the middle 512 words. Thanks in advance</p>
","nlp, token, tokenize, bert-language-model","<p>The post references <a href=""https://arxiv.org/pdf/1905.05583.pdf"" rel=""nofollow noreferrer"">a paper</a> which says that the first 128 tokens and the last 382 tokens (not including the CLS and SEP tokens) should be kept.</p>
<p>For tokenization, you can use the Bert Tokenizer from HuggingFace's Transformers library to tokenize the full String and then trim out everything besides the first 129 and last 383 tokens. 129 because we include the initial CLS token and 383 because we include the ending SEP token.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code># Generate sample text
from string import ascii_lowercase
sample_text = &quot;&quot;
for c1 in ascii_lowercase:
    for c2 in ascii_lowercase:
        sample_text += f&quot;{c1}{c2} &quot;

# Get tokenizer
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

# Perform tokenization
tokenized = tokenizer(sample_text)

# Trim tokens
if len(tokenized[&quot;input_ids&quot;]) &gt; 512:
    for k,v in tokenized.items():
        tokenized[k] = v[:129] + v[-383:]

# Verify result
print(tokenizer.decode(tokenized[&quot;input_ids&quot;]))
print(len(tokenized[&quot;input_ids&quot;]))
</code></pre>
",1,0,240,2024-01-17 21:05:10,https://stackoverflow.com/questions/77835506/truncate-texts-in-the-middle-for-bert
What and how LLM is used for ranking organization job title?,"<p>Suppose there's a context like this</p>
<p>context = <code>Andy is a vice manager of finance department. \n Rio is a general manager finance deparment. \n Jason is a general manager finance deparment.</code></p>
<p>question = <code>who is the leader of finance department ?</code></p>
<p>what task is this called ?
what model is used ?
how does the model know which title is higher ?
how does the model handle two same data ? (e.g., Rio and Jason)</p>
<p>thanks</p>
","nlp, bert-language-model, large-language-model, nlp-question-answering","<p>This task is a question-answering task. Likely, your model already knows which title is higher based on the training data.</p>
<p>However, since 'Rio' and 'Jason' both hold the same position, it would be difficult to determine who the leader of the finance department is without additional information.</p>
<p>The model might respond by suggesting all possible answers or asking for more information.</p>
",0,0,63,2024-02-21 02:03:50,https://stackoverflow.com/questions/78031203/what-and-how-llm-is-used-for-ranking-organization-job-title
How to apply a linear layer atop a sentence transformer,"<p>Hey there I am trying to create a basic Sentence Transformer model for few shot learning, however while fitting I observed that the changes made to the model are miniscule because the model has been trained on 1B+ pairs whereas I train it on around 40 pairs per epochs, to deal with this problem I decided to apply a linear layer on top of the sentence transformer in order to learn the embeddings corresponding to a specific data set.
However there seems to be no forward function for the sentence transformers. Their is an alternative with the model.encode() method but it does not change the model parameters.
So summarizing I want to create a network that does a forward pass on the sentence transformer, then on the linear layer and then finally get a loss which can be used across the model.
Any help would be useful.
Thank you.</p>
","python, pytorch, nlp, bert-language-model","<p>Here is a simple code snippet that adds one simple linear layer on top of a sentence transformer:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from sentence_transformers import SentenceTransformer

class SentenceTransformerWithLinearLayer(torch.nn.Module):
    def __init__(self, transformer_model_name):
        super(SentenceTransformerWithLinearLayer, self).__init__()
        
        # Load the sentence transformer model
        self.sentence_transformer = SentenceTransformer(transformer_model_name)
        last_layer_dimension = self.sentence_transformer.get_sentence_embedding_dimension()
        
        # New linear layer with 16 output dimensions
        self.linear = torch.nn.Linear(last_layer_dimension, 16)

    def forward(self, x):
        # Pass the input through the sentence transformer
        x = self.sentence_transformer.encode(x, convert_to_numpy=False).unsqueeze(0)
        
        # Pass through the linear layer
        x = self.linear(x)
        return x
</code></pre>
<p>This can than be used similarly to a simple sentence transformer. In this example I loaded the <code>all-mpnet-base-v2</code> model as the base sentence transformer. The input of <code>&quot;Hello world&quot;</code> is passed through the sentence transformer and then the linear layer, resulting in a 16 dimensional vector.</p>
<pre class=""lang-py prettyprint-override""><code>model = SentenceTransformerWithLinearLayer(&quot;all-mpnet-base-v2&quot;)
output = model.forward(&quot;Hello world&quot;)
</code></pre>
<p>This vector can then be used in a loss function e.g. a MSELoss</p>
<pre class=""lang-py prettyprint-override""><code>loss_function = torch.nn.MSELoss()

...
expected = ...
loss = loss_function(output, expected)
loss.backward()
...
</code></pre>
",1,0,520,2024-03-02 06:20:46,https://stackoverflow.com/questions/78091661/how-to-apply-a-linear-layer-atop-a-sentence-transformer
TypeError: Exception encountered when calling layer &#39;embeddings&#39; (type TFBertEmbeddings),"<p>My model was wholly workable two weeks back, but now it's showing the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-a3e5a45f06c9&gt; in &lt;cell line: 14&gt;()
     12 
     13 # Encode input using BERT model
---&gt; 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
     15 
     16 # Get pooled output and pass through dropout layer

8 frames
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).

Could not build a TypeSpec for name: &quot;tf.debugging.assert_less/assert_less/Assert/Assert&quot;
op: &quot;Assert&quot;
input: &quot;tf.debugging.assert_less/assert_less/All&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_0&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_1&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_2&quot;
input: &quot;Placeholder&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_4&quot;
input: &quot;tf.debugging.assert_less/assert_less/y&quot;
attr {
  key: &quot;T&quot;
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_INT32
    }
  }
}
attr {
  key: &quot;summarize&quot;
  value {
    i: 3
  }
}
 of unsupported type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;.

Call arguments received by layer 'embeddings' (type TFBertEmbeddings):
  • input_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'input_ids')&gt;
  • position_ids=None
  • token_type_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'token_type_ids')&gt;
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>I think this error occurs when the input layers are sent to the corresponding BERT layer. If I use old versions of TensorFlow instead of 2.15.0, the error is resolved. However, with those old versions, I did not get GPU and faced a Graph Execution Error.</p>
<p>Can anyone help me with this</p>
","tensorflow, deep-learning, nlp, bert-language-model, transformer-model","<p>it's a problem with the transformers library, I had the same problem and solved it using version 4.31.0</p>
",0,0,1373,2024-03-08 16:48:31,https://stackoverflow.com/questions/78129126/typeerror-exception-encountered-when-calling-layer-embeddings-type-tfbertemb
topic modeling from quotes,"<p>Based on the folloiwng link : <a href=""https://quotes.toscrape.com/"" rel=""nofollow noreferrer"">quotes</a></p>
<p>with help of  following code(this site was based on  javascript, so first i have disabled it)</p>
<pre><code>import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pandas as pd
from selenium.webdriver.common.keys import Keys
browser =webdriver.Chrome()
browser.get(&quot;https://quotes.toscrape.com/&quot;)
elem = browser.find_elements(By.CLASS_NAME, 'author')  # Find the search box
quot_choosing =browser.find_elements(By.CLASS_NAME,'text')
autors=[]
quotes =[]
for  author in elem:
    autors.append(author.text)
for quote in quot_choosing:
    quotes.append(quote.text)
print(autors)
print(quotes)

autor_saying =pd.DataFrame({&quot;Author&quot;:autors,&quot;Quotes&quot;:quotes})
autor_saying.to_csv(&quot;quotes.csv&quot;,index=False)
print(autor_saying.head())
browser.quit()
</code></pre>
<p>i haved  author's and quote's information in csv file and  then read it  as it is given it bellow :</p>
<pre><code>import pandas as pd
from bertopic import BERTopic
model =BERTopic()

summarization =[]
data =pd.read_csv(&quot;quotes.csv&quot;)
print(data.head())
for  index, row in data.iterrows():
    topics, probs =model.fit_transform([row['Quotes']])
    print(topics)
</code></pre>
<p>here is  result :</p>
<pre><code>   Author                                             Quotes
0  Albert Einstein  “The world as we have created it is a process ...
1     J.K. Rowling  “It is our choices, Harry, that show what we t...
2  Albert Einstein  “There are only two ways to live your life. On...
3      Jane Austen  “The person, be it gentleman or lady, who has ...
4   Marilyn Monroe  “Imperfection is beauty, madness is genius and...
</code></pre>
<p>additionally i want to  use bertopic model to detect  topic from given  site :
<a href=""https://deepgram.com/learn/python-topic-modeling-with-a-bert-model"" rel=""nofollow noreferrer"">topic modeling</a></p>
<p>but my code gives me following error :</p>
<pre><code>ValueError: Transform unavailable when model was fit with only a single data sample.
</code></pre>
<p>could you help me please  how to fix it? how to detect  topic presented in sentences?</p>
","python, bert-language-model, topic-modeling","<p>You should train using all quotes at once and not one-by-one. So instead of</p>
<pre class=""lang-py prettyprint-override""><code>for  index, row in data.iterrows():
    topics, probs =model.fit_transform([row['Quotes']])
    print(topics)
</code></pre>
<p>try</p>
<pre class=""lang-py prettyprint-override""><code>topics, probs = model.fit_transform(data['Quotes'].tolist())
data['Topic'] = topics
data['Probability'] = probs
print(data.head())
</code></pre>
",0,0,153,2024-03-08 21:51:21,https://stackoverflow.com/questions/78130426/topic-modeling-from-quotes
Google Colab Bert instantiation Error using Tensorflow,"<p>I'm trying to construct a Bert Model using Tensorflow on Colab. This code was perfectly working weeks ago. Now if I try to instantiate the model I obtain the following error:</p>
<pre><code>Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFBertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-b0e769ef7890&gt; in &lt;cell line: 7&gt;()
      5 SC_mask_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;attention_mask&quot;)
      6 SC_bert_model = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, attention_mask=SC_mask_layer)[1]  # Estrai il secondo output, che è il pooler_output
      8 
      9 # Aggiungi un layer di Dropout

36 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py in type_spec_from_value(value)
   1002         3, &quot;Failed to convert %r to tensor: %s&quot; % (type(value).__name__, e))
   1003 
-&gt; 1004   raise TypeError(f&quot;Could not build a TypeSpec for {value} of &quot;
   1005                   f&quot;unsupported type {type(value)}.&quot;)
   1006 

TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).

Could not build a TypeSpec for name: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert&quot;
op: &quot;Assert&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/All&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2&quot;
input: &quot;Placeholder&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4&quot;
input: &quot;tf.debugging.assert_less_5/assert_less/y&quot;
attr {
  key: &quot;summarize&quot;
  value {
    i: 3
  }
}
attr {
  key: &quot;T&quot;
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_INT32
    }
  }
}
 of unsupported type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;.

Call arguments received by layer 'embeddings' (type TFBertEmbeddings):
  • input_ids=&lt;KerasTensor: shape=(None, 128) dtype=int32 (created by layer 'input_ids')&gt;
  • position_ids=None
  • token_type_ids=&lt;KerasTensor: shape=(None, 128) dtype=int32 (created by layer 'tf.fill_5')&gt;
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>The code for the model is:</p>
<pre><code>SC_input_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_ids&quot;)
SC_mask_layer = Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;attention_mask&quot;)
SC_bert_model = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)
SC_pooler_output = SC_bert_model(SC_input_layer, attention_mask=SC_mask_layer)[1]  

# Aggiungi un layer di Dropout
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = Dense(6, activation='sigmoid')(SC_dropout_layer)
SC_model = Model(inputs=[SC_input_layer, SC_mask_layer], outputs=SC_output_layer)
</code></pre>
<p>I found that installing tensorflow 2.10.0 it works but, using Google Colab i have problems with the CUDA version and using tensorflow 2.10 it doesn't recognize the GPU.
This code was working weeks ago, someone has a solution?</p>
<p>EDIT: the same error appears on Kaggle.</p>
","tensorflow, machine-learning, google-colaboratory, bert-language-model","<p>UPDATE: The problem is related to the version of Transformers. Using version 4.31.0 should solve.</p>
",0,0,157,2024-03-17 17:03:42,https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow
Is BertForSequenceClassification using the CLS vector?,"<p>In the hugging face <a href=""https://github.com/huggingface/transformers/blob/536ea2aca234fb48c5c69769431d643b0d93b233/src/transformers/models/bert/modeling_bert.py#L1552"" rel=""nofollow noreferrer"">source code</a>, <code>pooled_output = outputs[1]</code> is used.</p>
<pre class=""lang-py prettyprint-override""><code>        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]
</code></pre>
<p>Shouldn't it be <code>pooled_output = outputs[0]</code>? (This <a href=""https://stackoverflow.com/a/60883003/20898396"">answer</a> mentioning BertPooler seems to be outdated)</p>
<p>Based on <a href=""https://stackoverflow.com/a/62981360/20898396"">this</a> answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that <code>BertForSequenceClassification</code> freezes the Bert model and only trains the classification head, but maybe that's not the case)</p>
<p>Would a sentence embedding be equivalent or even better than the [CLS] token embedding?</p>
","nlp, huggingface-transformers, bert-language-model","<blockquote>
<p>Would a sentence embedding be equivalent or even better than the [CLS] token embedding?</p>
</blockquote>
<p>A sentence embedding is everything that represents the input sequence as a numerical vector. The question is whether this embedding is semantical meaningful (e.g. can we use it with similarity metrics). This is for example not the case for the pretrained Bert weights released by google (refer to this <a href=""https://stackoverflow.com/a/64237402/6664872"">answer</a> for more information).</p>
<p>Is the CLS token a sentence embedding? Yes.
Is some kind of pooling a sentence embedding? Yes.
Are they semantically meaningful with the Bert weights release by google? No.</p>
<blockquote>
<p>Shouldn't it be pooled_output = outputs[0]?</p>
</blockquote>
<p>No, because when you check the <a href=""https://github.com/huggingface/transformers/blob/536ea2aca234fb48c5c69769431d643b0d93b233/src/transformers/models/bert/modeling_bert.py#L1005"" rel=""nofollow noreferrer"">code</a>, you will see that the first element of the tuple is the last_hidden_state</p>
<pre class=""lang-py prettyprint-override""><code>sequence_output = encoder_outputs[0]
pooled_output = self.pooler(sequence_output) if self.pooler is not None else None

if not return_dict:
    return (sequence_output, pooled_output) + encoder_outputs[1:]
</code></pre>
<blockquote>
<p>I am confused as to why/how masked language modeling would lead to the start token learning a sentence level representation.</p>
</blockquote>
<p>Because it is included in every training sequence and the <code>[CLS]</code> &quot;absorbs&quot; the other tokens. You can also see this in the attention mechanism (compare Revealing the Dark Secrets of BERT <a href=""https://arxiv.org/abs/1908.08593"" rel=""nofollow noreferrer"">paper</a>). As mentioned above, the questions is if they are semantically meaningful without any further finetuning. No (compare this StackOverflow <a href=""https://stackoverflow.com/a/64237402/6664872"">answer</a>).</p>
",2,1,209,2024-03-28 20:54:17,https://stackoverflow.com/questions/78240828/is-bertforsequenceclassification-using-the-cls-vector
Fine-tuning BERT with deterministic masking instead of random masking,"<p>I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons).</p>
<p>To do so, I created a dataset that has two columns: <code>text</code> in which some tokens have been replaced with <code>[MASK]</code> (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and <code>label</code> where I have the whole text.</p>
<p>Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's <code>transformers</code> library, but I do not want to use <code> DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2)</code> where the masking is done randomly and I only can control the probability. What can I do?</p>
","nlp, huggingface-transformers, bert-language-model","<p>This is what I did to solve my problem. I created a custom class and changed the tokenization in a way that I needed (mask one of the numerical spans in the input).</p>
<pre class=""lang-py prettyprint-override""><code>class CustomDataCollator(DataCollatorForLanguageModeling):

    mlm: bool = True
    return_tensors: str = &quot;pt&quot;

    def __post_init__(self):
        if self.mlm and self.tokenizer.mask_token is None:
            raise ValueError(
                &quot;This tokenizer does not have a mask token which is necessary &quot;
                &quot;for masked language modeling. You should pass `mlm=False` to &quot;
                &quot;train on causal language modeling instead.&quot;
            )

    def torch_mask_tokens(self, inputs, special_tokens_mask):
        &quot;&quot;&quot;
        Prepare masked tokens inputs/labels for masked language modeling.
        NOTE: keep `special_tokens_mask` as an argument for avoiding error
        &quot;&quot;&quot;

        # labels is batch_size x length of the sequence tensor
        # with the original token id
        # the length of the sequence includes the special tokens (2)
        labels = inputs.clone()

        batch_size = inputs.size(0)
        # seq_len = inputs.size(1)
        # in each seq, find the indices of the tokens that represent digits
        dig_ids = [1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023]
        dig_idx = torch.zeros_like(labels)
        for dig_id in dig_ids:
            dig_idx += (labels == dig_id)
        dig_idx = dig_idx.bool()
        # in each seq, find the spans of Trues using `find_spans` function
        spans = []
        for i in range(batch_size):
            spans.append(find_spans(dig_idx[i].tolist()))
        masked_indices = torch.zeros_like(labels)
        # spans is a list of lists of tuples
        # in each tuple, the first element is the start index
        # and the second element is the length
        # in each child list, choose a random tuple
        for i in range(batch_size):
            if len(spans[i]) &gt; 0:
                idx = torch.randint(0, len(spans[i]), (1,))
                start, length = spans[i][idx[0]]
                masked_indices[i, start:start + length] = 1
            else:
                print(&quot;No digit found in the sequence!&quot;)
        masked_indices = masked_indices.bool()

        # We only compute loss on masked tokens
        labels[~masked_indices] = -100

        # change the input's masked_indices to self.tokenizer.mask_token
        inputs[masked_indices] = self.tokenizer.mask_token_id

        return inputs, labels

def find_spans(lst):
    spans = []
    for k, g in groupby(enumerate(lst), key=itemgetter(1)):
        if k:
            glist = list(g)
            spans.append((glist[0][0], len(glist)))

    return spans
</code></pre>
",0,0,342,2024-04-22 10:39:05,https://stackoverflow.com/questions/78365608/fine-tuning-bert-with-deterministic-masking-instead-of-random-masking
No Attention returned even when output_attentions= True,"<p>I'm using a pretrained model based BERT (github link:<a href=""https://github.com/MAGICS-LAB/DNABERT_2"" rel=""nofollow noreferrer"">DNABERT-2</a>)</p>
<p>It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base.</p>
<p>I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem.
<code>output</code> is of length 2 and each element is of shape: <code>torch.Size([1, 7, 768])</code> and
<code>torch.Size([1, 768])</code>. When trying to get <code>output.attentions</code> I get <code>None</code>.</p>
<p>I'm not sure where to search and what a solution would be.</p>
<p>I'm providing my whole code:</p>
<p>Defining model, trainer, data, tokenizer:</p>
<pre><code>from copy import deepcopy

from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback
# END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List

import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset

@dataclass class ModelArguments:
    model_name_or_path: Optional[str] = field(default=&quot;facebook/opt-125m&quot;)
    use_lora: bool = field(default=False, metadata={&quot;help&quot;: &quot;whether to use LoRA&quot;})
    lora_r: int = field(default=8, metadata={&quot;help&quot;: &quot;hidden dimension for LoRA&quot;})
    lora_alpha: int = field(default=32, metadata={&quot;help&quot;: &quot;alpha for LoRA&quot;})
    lora_dropout: float = field(default=0.05, metadata={&quot;help&quot;: &quot;dropout rate for LoRA&quot;})
    lora_target_modules: str = field(default=&quot;query,value&quot;, metadata={&quot;help&quot;: &quot;where to perform LoRA&quot;})


@dataclass class DataArguments:
    data_path: str = field(default=None, metadata={&quot;help&quot;: &quot;Path to the training data.&quot;})
    kmer: int = field(default=-1, metadata={&quot;help&quot;: &quot;k-mer for input sequence. -1 means not using k-mer.&quot;})


@dataclass class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    run_name: str = field(default=&quot;run&quot;)
    optim: str = field(default=&quot;adamw_torch&quot;)
    model_max_length: int = field(default=512, metadata={&quot;help&quot;: &quot;Maximum sequence length.&quot;})
    gradient_accumulation_steps: int = field(default=1)
    per_device_train_batch_size: int = field(default=1)
    per_device_eval_batch_size: int = field(default=1)
    num_train_epochs: int = field(default=1)
    logging_steps: int = field(default=100)
    save_steps: int = field(default=100)
    fp16: bool = field(default=False)
    # START NEW
    # eval_steps: int = field(default=100)
    eval_steps: int = field(default=0.1)
    # END NEW
    evaluation_strategy: str = field(default=&quot;steps&quot;)
    warmup_steps: int = field(default=50)
    weight_decay: float = field(default=0.01)
    learning_rate: float = field(default=1e-4)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    output_dir: str = field(default=&quot;output&quot;)
    find_unused_parameters: bool = field(default=False)
    checkpointing: bool = field(default=False)
    dataloader_pin_memory: bool = field(default=False)
    eval_and_save_results: bool = field(default=True)
    save_model: bool = field(default=False)
    seed: int = field(default=42)


def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    &quot;&quot;&quot;Collects the state dict and dump to disk.&quot;&quot;&quot;
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


&quot;&quot;&quot; Get the reversed complement of the original DNA sequence. &quot;&quot;&quot;


def get_alter_of_dna_sequence(sequence: str):
    MAP = {&quot;A&quot;: &quot;T&quot;, &quot;T&quot;: &quot;A&quot;, &quot;C&quot;: &quot;G&quot;, &quot;G&quot;: &quot;C&quot;}
    # return &quot;&quot;.join([MAP[c] for c in reversed(sequence)])
    return &quot;&quot;.join([MAP[c] for c in sequence])


&quot;&quot;&quot; Transform a dna sequence to k-mer string &quot;&quot;&quot;


def generate_kmer_str(sequence: str, k: int) -&gt; str:
    &quot;&quot;&quot;Generate k-mer string from DNA sequence.&quot;&quot;&quot;
    return &quot; &quot;.join([sequence[i:i + k] for i in range(len(sequence) - k + 1)])


&quot;&quot;&quot; Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved  to the same directory as the original data with the same name but with a suffix of &quot;_{k}mer&quot;. &quot;&quot;&quot;


def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -&gt; List[str]:
    &quot;&quot;&quot;Load or generate k-mer string for each DNA sequence.&quot;&quot;&quot;
    kmer_path = data_path.tokenizerreplace(&quot;.csv&quot;, f&quot;_{k}mer.json&quot;)
    if os.path.exists(kmer_path):
        logging.warning(f&quot;Loading k-mer from {kmer_path}...&quot;)
        with open(kmer_path, &quot;r&quot;) as f:
            kmer = json.load(f)
    else:
        logging.warning(f&quot;Generating k-mer...&quot;)
        kmer = [generate_kmer_str(text, k) for text in texts]
        with open(kmer_path, &quot;w&quot;) as f:
            logging.warning(f&quot;Saving k-mer to {kmer_path}...&quot;)
            json.dump(kmer, f)

    return kmer


class SupervisedDataset(Dataset):
    &quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;

    def __init__(self,
                 data_path: str,
                 tokenizer: transformers.PreTrainedTokenizer,
                 kmer: int = -1):

        super(SupervisedDataset, self).__init__()

        # load data from the disk
        with open(data_path, &quot;r&quot;) as f:
            data = list(csv.reader(f))[1:]
        if len(data[0]) == 2:
            # data is in the format of [text, label]
            logging.warning(&quot;Perform single sequence classification...&quot;)
            texts = [d[0] for d in data]
            labels = [int(d[1]) for d in data]
        # All genes sequences are concat: we don't work with the sequence-pair,
        # But we are tricking the model to think it is single sequence.
        elif len(data[0]) == 3:
            # data is in the format of [text1, text2, label]
            logging.warning(&quot;Perform sequence-pair classification...&quot;)
            texts = [[d[0], d[1]] for d in data]
            labels = [int(d[2]) for d in data]
        else:
            raise ValueError(&quot;Data format not supported.&quot;)

        if kmer != -1:
            # only write file on the first process
            if torch.distributed.get_rank() not in [0, -1]:
                torch.distributed.barrier()

            logging.warning(f&quot;Using {kmer}-mer as input...&quot;)
            texts = load_or_generate_kmer(data_path, texts, kmer)

            if torch.distributed.get_rank() == 0:
                torch.distributed.barrier()

        output = tokenizer(
            texts,
            return_tensors=&quot;pt&quot;,
            padding=&quot;longest&quot;,
            max_length=tokenizer.model_max_length,
            truncation=True,
        )

        self.input_ids = output[&quot;input_ids&quot;]
        # CHANGE
        self.input_ids[0][self.input_ids[0] == 0] = 2
        # Change to which tokens we want to attend and to which we don't
        self.attention_mask = output[&quot;attention_mask&quot;]
        self.labels = labels
        self.num_labels = len(set(labels))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:
        return dict(input_ids=self.input_ids[i], labels=self.labels[i])


@dataclass class DataCollatorForSupervisedDataset(object):
    &quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in (&quot;input_ids&quot;, &quot;labels&quot;))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.Tensor(labels).long()
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )


&quot;&quot;&quot; Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. &quot;&quot;&quot;

def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):
    if logits.ndim == 3:
        # Reshape logits to 2D if needed
        logits = logits.reshape(-1, logits.shape[-1])
    predictions = np.argmax(logits, axis=-1)
    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)
    valid_predictions = predictions[valid_mask]
    valid_labels = labels[valid_mask]
    return {
        # START NEW
        &quot;sum prediction&quot;: f'{sum(valid_predictions)}/{len(valid_predictions)}',
        # END NEW
        &quot;accuracy&quot;: sklearn.metrics.accuracy_score(valid_labels, valid_predictions),
        &quot;f1&quot;: sklearn.metrics.f1_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;matthews_correlation&quot;: sklearn.metrics.matthews_corrcoef(
            valid_labels, valid_predictions
        ),
        &quot;precision&quot;: sklearn.metrics.precision_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;recall&quot;: sklearn.metrics.recall_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
    }

&quot;&quot;&quot; Compute metrics used for huggingface trainer. &quot;&quot;&quot; def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):  # Unpack logits if it's a tuple
        logits = logits[0]
    return calculate_metric_with_sklearn(logits, labels)


class CustomTrainer(transformers.Trainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_predictions = []
        self.epoch_labels = []
        self.epoch_loss = []

    def compute_loss(self, model, inputs, return_outputs=False):
        &quot;&quot;&quot;
        MAX: Subclassed to compute training accuracy.

        How the loss is computed by Trainer. By default, all models return the loss in
        the first element.

        Subclass and override for custom behavior.
        &quot;&quot;&quot;
        if self.label_smoother is not None and &quot;labels&quot; in inputs:
            labels = inputs.pop(&quot;labels&quot;)
        else:
            labels = None
        outputs = model(**inputs, output_attentions=True)
        # TEST
        try:
            print(f&quot;Attention: {outputs.attentions}&quot;)
        except Exception:
            print(&quot;No Attention returned&quot;)

        if &quot;labels&quot; in inputs:
            preds = outputs.logits.detach()

            # Log accuracy
            acc = (
                (preds.argmax(axis=1) == inputs[&quot;labels&quot;])
                .type(torch.float)
                .mean()
                .item()
            )
            # Uncomment it if you want to plot the batch accuracy
            # wandb.log({&quot;batch_accuracy&quot;: acc})  # Log accuracy

            # Store predictions and labels for epoch-level metrics
            self.epoch_predictions.append(preds.cpu().numpy())
            self.epoch_labels.append(inputs[&quot;labels&quot;].cpu().numpy())

        # Save past state if it exists
        if self.args.past_index &gt;= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]
            # Uncomment it if you want to plot the batch loss
            # wandb.log({&quot;batch_loss&quot;: loss})
            self.epoch_loss.append(loss.item())  # Store loss for epoch-level metrics

        return (loss, outputs) if return_outputs else loss

# Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -&gt; None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        # Aggregate predictions and labels for the entire epoch
        epoch_predictions = np.concatenate(self._trainer.epoch_predictions)
        epoch_labels = np.concatenate(self._trainer.epoch_labels)

        # Compute accuracy
        accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels)

        # Compute mean loss
        mean_loss = np.mean(self._trainer.epoch_loss)

        # Compute precision, recall, and F1-score
        precision, recall, f1, _ = precision_recall_fscore_support(
            epoch_labels, epoch_predictions.argmax(axis=1), average=&quot;weighted&quot;
        )

        # Log epoch-level metrics
        wandb.log({&quot;epoch_accuracy&quot;: accuracy, &quot;epoch_loss&quot;: mean_loss})
        wandb.log({&quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1})

        # Clear stored predictions, labels, and loss for the next epoch
        self._trainer.epoch_predictions = []
        self._trainer.epoch_labels = []
        self._trainer.epoch_loss = []
        return None

        # TODO: use this function to gather the prediction and labels and get the metrics
#%%
</code></pre>
<p>Instantiating and training:</p>
<pre><code>from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \
    TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \
    compute_metrics


from copy import deepcopy
from transformers import TrainerCallback
# END NEW
import os
import json
import torch
import transformers

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
)

import wandb
run = wandb.init()
assert run is wandb.run

def train(device):
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # load tokenizer
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=&quot;right&quot;,
        use_fast=True,
        trust_remote_code=True,
    )

    if &quot;InstaDeepAI&quot; in model_args.model_name_or_path:
        tokenizer.eos_token = tokenizer.pad_token

    # define datasets and data collator
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
                                      data_path=os.path.join(data_args.data_path, &quot;train.csv&quot;),
                                      kmer=data_args.kmer)
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
                                    data_path=os.path.join(data_args.data_path, &quot;dev.csv&quot;),
                                    kmer=data_args.kmer)
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
                                     data_path=os.path.join(data_args.data_path, &quot;test.csv&quot;),
                                     kmer=data_args.kmer)
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)

    # load model
    model = transformers.AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        num_labels=train_dataset.num_labels,
        trust_remote_code=True,
        output_attentions = True
    ).to(device)

    # configure LoRA
    if model_args.use_lora:
        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=list(model_args.lora_target_modules.split(&quot;,&quot;)),
            lora_dropout=model_args.lora_dropout,
            bias=&quot;none&quot;,
            task_type=&quot;SEQ_CLS&quot;,
            inference_mode=False,
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    trainer = CustomTrainer(model=model,
                            tokenizer=tokenizer,
                            args=training_args,
                            compute_metrics=compute_metrics,
                            train_dataset=train_dataset,
                            eval_dataset=val_dataset,
                            data_collator=data_collator
                            )

    trainer.add_callback(CustomCallback(trainer))
    trainer.train()

    # train_result = trainer.train()
    # loss = train_result[&quot;loss&quot;]
    # print(f&quot;loss issss: {loss}&quot;)
    # print(f&quot;Train reusults: {train_result}&quot;) # NEW: result: only returns metrics at the end of training

    if training_args.save_model:
        trainer.save_state()
        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)

    # get the evaluation results from trainer
    if training_args.eval_and_save_results:
        results_path = os.path.join(training_args.output_dir, &quot;results&quot;, training_args.run_name)
        results = trainer.evaluate(eval_dataset=test_dataset)
        os.makedirs(results_path, exist_ok=True)
        with open(os.path.join(results_path, &quot;eval_results.json&quot;), &quot;w&quot;) as f:
            json.dump(results, f)


if __name__ == &quot;__main__&quot;:
    # Define device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Using device:', device)
    # Call the train function with the device
    train(device)
</code></pre>
<p>After training, I try to run it on an example:</p>
<pre><code>model_path = './finetune/output/dnabert2'
tokenizer = AutoTokenizer.from_pretrained(model_path)
# Load the model with output_attention=True
model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True)
model_input = tokenizer(&quot;ACTGACGGGTAGTGACTG&quot;, return_tensors=&quot;pt&quot;)

with torch.inference_mode():
  output = model(**model_input, output_attentions=True)
</code></pre>
<p>My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.</p>
","nlp, huggingface-transformers, bert-language-model, transformer-model, attention-model","<p>The problem was deep in the structure: attention was discarded early in the model, I had therefore to go through the code to understand what is happening, and change it.</p>
<p><a href=""https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted"" rel=""nofollow noreferrer"">https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted</a></p>
<p>I had to extract attention_probs.
Here are the changes I have done.</p>
",0,0,412,2024-04-23 10:48:50,https://stackoverflow.com/questions/78371741/no-attention-returned-even-when-output-attentions-true
"ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 512), output.shape=(None, 3)","<p>I was trying to train a BERT model to solve a multi-classification problem.</p>
<p>I got this error while run the code below:</p>
<pre><code>Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 512), output.shape=(None, 3)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

epochs = 4

train_dataloader = train_dataset.shuffle(buffer_size=10000).batch(batch_size)
validation_dataloader = val_dataset.batch(batch_size)

# start training 
history = model.fit(
    train_dataloader,  # train_data
    validation_data=validation_dataloader,  # validation_data
    epochs=epochs,  
    verbose=1 
)
# save the model
model.save(&quot;bert_model.h5&quot;)
</code></pre>
<p>This is a test:</p>
<pre class=""lang-py prettyprint-override""><code>for batch in train_dataloader.take(1):
    input_ids, attention_masks, labels = batch
    print(&quot;Batch input_ids shape:&quot;, input_ids.shape) 
    print(&quot;Batch attention_masks shape:&quot;, attention_masks.shape) 
    print(&quot;Batch labels shape:&quot;, labels.shape)  

# I got this output
Batch input_ids shape: (16, 512)
Batch attention_masks shape: (16, 512)
Batch labels shape: (16,)
</code></pre>
<p>I already checked the tensor shape.</p>
","python, tensorflow, artificial-intelligence, classification, bert-language-model","<p>Your labels have a shape of (16,), while your model's output has a shape of (None,3).</p>
<p>Probably the issue is that your labels are not <strong>one-hot encoded</strong>. They should have the same second dimension as your output layer:</p>
<pre><code>from tensorflow.keras.utils import to_categorical
num_classes = 3
labels = to_categorical(labels, num_classes=num_classes)
print(labels.shape)
</code></pre>
",2,1,7893,2024-05-13 07:32:02,https://stackoverflow.com/questions/78470683/valueerror-arguments-target-and-output-must-have-the-same-shape-received
Different embeddings for same sentences with torch transformer,"<p>Hey all and apologies in advance for what is probably a fairly basic question - I have a theory about what's causing the issue here, but would be great to confirm with people who know more about this than I do.</p>
<p>I've been trying to implement this python code snippet in Google colab. The snippet is meant to work out similarity for sentences. The code runs fine, but what I'm finding is that the embeddings and distances change every time I run it, which isn't ideal for my intended use case.</p>
<pre><code>import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

# Tokenize input texts
texts = [
    &quot;There's a kid on a skateboard.&quot;,
    &quot;A kid is skateboarding.&quot;,
    &quot;A kid is inside the house.&quot;
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)

# Get the embeddings
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[1], cosine_sim_0_1))
print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[2], cosine_sim_0_2))
</code></pre>
<p>I think the issue must be model specific since I receive the warning about newly initialized pooler weights, and pooler_output is ultimately what the code reads to inform similarity:</p>
<pre><code>Some weights of RobertaModel were not initialized from the model checkpoint at qiyuw/pcl-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>Switching to an alternative model which does not give this warning (for example, sentence-transformers/all-mpnet-base-v2) makes the outputs reproducible, so I think it is because of the above warning about initialization of weights.  So here are my questions:</p>
<ol>
<li>Can I make the output reproducible by initialising/seeding the model differently?</li>
<li>If I can't make the outputs reproducible, is there a way in which I can improve the accuracy to reduce the amount of variation between runs?</li>
<li>Is there a way to search huggingface models for those which will initialise the pooler weights so I can find a model which does suit my purposes?</li>
</ol>
<p>Thanks in advance</p>
","python, pytorch, huggingface-transformers, bert-language-model","<p>You are correct the model layer weights for <code>bert.pooler.dense.bias</code> and <code>bert.pooler.dense.weight</code> are initialized randomly. You can initialize these layers always the same way for a reproducible output, but I doubt the inference code that you have copied from there <a href=""https://github.com/qiyuw/PeerCL/blob/main/README.md"" rel=""nofollow noreferrer"">readme</a> is correct. As already mentioned by you the pooling layers are not initialized and their <a href=""https://github.com/qiyuw/PeerCL/blob/5e40ea5a6e94202c597c8ef51ed464c18e08d191/pcl/models.py#L407"" rel=""nofollow noreferrer"">model class</a> also makes sure that the pooling_layer is not added:</p>
<pre class=""lang-py prettyprint-override""><code>...
self.bert = BertModel(config, add_pooling_layer=False)
...
</code></pre>
<p>The evaluation script of the repo should be called, according to the readme with the following command:</p>
<pre class=""lang-bash prettyprint-override""><code>python evaluation.py --model_name_or_path qiyuw/pcl-bert-base-uncased --mode test --pooler cls_before_pooler
</code></pre>
<p>When you look into it, your inference code for <a href=""https://huggingface.co/qiyuw/pcl-bert-base-uncased"" rel=""nofollow noreferrer"">qiyuw/pcl-bert-base-uncased</a> should be the following way:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

# Tokenize input texts
texts = [
    &quot;There's a kid on a skateboard.&quot;,
    &quot;A kid is skateboarding.&quot;,
    &quot;A kid is inside the house.&quot;
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)

# Get the embeddings
with torch.inference_mode():
    embeddings = model(**inputs)
    embeddings = embeddings.last_hidden_state[:, 0]

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[1], cosine_sim_0_1))
print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[2], cosine_sim_0_2))
</code></pre>
<p>Output:</p>
<pre><code>Cosine similarity between &quot;There's a kid on a skateboard.&quot; and &quot;A kid is skateboarding.&quot; is: 0.941
Cosine similarity between &quot;There's a kid on a skateboard.&quot; and &quot;A kid is inside the house.&quot; is: 0.779
</code></pre>
<blockquote>
<p>Can I make the output reproducible by initialising/seeding the model differently?</p>
</blockquote>
<p>Yes, you can. Use <a href=""https://pytorch.org/docs/stable/generated/torch.manual_seed.html"" rel=""nofollow noreferrer"">torch.maunal_seed</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModel, AutoTokenizer

model_random = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
torch.manual_seed(42)
model_repoducible1 = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

torch.manual_seed(42)
model_repoducible2 = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

print(torch.allclose(model_random.pooler.dense.weight, model_repoducible1.pooler.dense.weight))
print(torch.allclose(model_random.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
print(torch.allclose(model_repoducible1.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
</code></pre>
<p>Output:</p>
<pre><code>False
False
True
</code></pre>
",1,3,524,2024-06-30 20:21:45,https://stackoverflow.com/questions/78689702/different-embeddings-for-same-sentences-with-torch-transformer
Capitalized words in sentiment analysis,"<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .
A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>
<p>this is my current code :</p>
<pre><code>from itertools import chain

def is_upper_case(text):
  return [word for word in text.split() if word.isupper() and word != 'I']

unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))
print(unique_upper_words)
</code></pre>
","nlp, sentiment-analysis, bert-language-model, data-preprocessing","<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>
<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>
<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=""https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression"" rel=""nofollow noreferrer"">this repository</a> as an example.</p>
",0,-1,130,2024-08-30 13:49:56,https://stackoverflow.com/questions/78932356/capitalized-words-in-sentiment-analysis
"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing","<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>
<p><a href=""https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>
<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>
<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import pipeline
import torch

model_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;
model = RobertaForSequenceClassification.from_pretrained(model_name)
tokenizer = RobertaTokenizer.from_pretrained(model_name)

classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)

result1 = classifier('Fhdf')
result2 = classifier('Als gisteren inclusief blauw')
print(result1)
print(result2)
</code></pre>
<p>Output:</p>
<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]
[{'label': 'Negative', 'score': 0.7538396120071411}]
</code></pre>
","python, nlp, bert-language-model, roberta-language-model","<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>
<p>what you can do is to:
1- Find other models that was trained to include <code>neutral</code> label.
2- Fine-tune this model on a dataset that includes <code>neutral</code> label.
3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>
<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>
<p>Here's a sample:</p>
<pre><code>def classify_with_neutral(text, threshold=0.5):
    result = classifier(text)[0]  # Get the classification result
    if result['score'] &lt; threshold:
        result['label'] = 'Neutral'  # Override label to 'Neutral'
    return result
</code></pre>
",3,2,54,2024-11-04 11:36:35,https://stackoverflow.com/questions/79155290/dutch-sentiment-analysis-robbertje-outputs-just-positive-negative-labels-netura
How to convert character indices to BERT token indices,"<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)
</code></pre>
<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>
<pre><code>d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'Brain',
 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'What sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}
</code></pre>
<p>After tokenization, the answer indices are 56  and 16:</p>
<pre><code>from transformers import BertTokenizerFast
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'
</code></pre>
<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>
<p>This is from a <a href=""https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false"" rel=""nofollow noreferrer"">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=""https://i.sstatic.net/GsZ6mfcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GsZ6mfcQ.png"" alt=""QA dataset with token answer indices"" /></a></p>
","python, nlp, dataset, large-language-model, bert-language-model","<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>
<p>The following function does the above for you:</p>
<pre><code>def get_token_indices(example):
    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token
    encoded = tokenizer(
        example['question'], 
        example['context'], 
        return_offsets_mapping=True
    )

    # Find character start and end from the original answer
    char_start = example['answers']['answer_start'][0]
    char_end = char_start + len(example['answers']['text'][0])

    # Identify token indices for the answer
    start_token_idx = None
    end_token_idx = None
    
    for i, (start, end) in enumerate(encoded['offset_mapping']):
        if start &lt;= char_start &lt; end: 
            start_token_idx = i
        if start &lt; char_end &lt;= end:
            end_token_idx = i
            break

    example['answer_start_token_idx'] = start_token_idx
    example['answer_end_token_idx'] = end_token_idx
    return example
</code></pre>
<p>Here's how you can use and test this function:</p>
<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)
tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

tokenized_ds = ds['train'].map(get_token_indices)


# Example
d0_tokenized = tokenized_ds[0]
print(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])
print(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])

answer_tokens = tokenizer.decode(
    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]
)
print(&quot;Tokenized answer:&quot;, answer_tokens)
</code></pre>
<p>Output:</p>
<pre><code>Tokenized start index: 56
Tokenized end index: 60
Tokenized answer: isolated from the bloodstream
</code></pre>
",2,2,33,2024-11-09 15:15:33,https://stackoverflow.com/questions/79173053/how-to-convert-character-indices-to-bert-token-indices
Normalization of token embeddings in BERT encoder blocks,"<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>
","nlp, normalization, bert-language-model, attention-model","<p>I tracked down full details of layer normalization (LN) in BERT <a href=""https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para"">here</a>.</p>
<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>
",0,2,163,2024-11-11 14:30:31,https://stackoverflow.com/questions/79178041/normalization-of-token-embeddings-in-bert-encoder-blocks
AdapterConfig.__init__() got an unexpected keyword argument &#39;mh_adapter&#39;,"<p>I am trying <a href=""https://docs.adapterhub.ml/classes/adapter_config.html"" rel=""nofollow noreferrer"">adapters</a> on <a href=""https://github.com/dapowan/LIMU-BERT-Public/tree/master"" rel=""nofollow noreferrer"">LIMU-BERT</a>, which is a lightweight BERT for IMU data. I pretrained LIMU-BERT on Dataset A and planned to add adapters and tune them on Dataset B. Here is my adapter-adding code:</p>
<pre><code>import adapters

class AdapterBERTClassifier(nn.Module):
    def __init__(self, bert_cfg, classifier=None):
        super().__init__()
        self.limu_bert = LIMUBertModel4Pretrain(bert_cfg, output_embed=True)
        self.classifier = classifier

        # Add adapter
        adapter_config = adapters.AdapterConfig(
            mh_adapter=True, 
            output_adapter=True,
            reduction_factor=16,  
            non_linearity=&quot;relu&quot;  
        )
        self.limu_bert.add_adapter(&quot;classification_adapter&quot;, config=adapter_config)
        self.limu_bert.train_adapter(&quot;classification_adapter&quot;)
</code></pre>
<p>However, I encountered an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\Documents\Code\LIMU-BERT\classifier_adapter.py&quot;, line 71, in &lt;module&gt;
    label_test, label_estimate_test = bert_classify(args, args.label_index, train_rate, label_rate, balance=balance)
  File &quot;D:\Documents\Code\LIMU-BERT\classifier_adapter.py&quot;, line 37, in bert_classify
    model = AdapterBERTClassifier(model_bert_cfg, classifier=classifier)
  File &quot;D:\Documents\Code\LIMU-BERT\models.py&quot;, line 332, in __init__
    adapter_config = adapters.AdapterConfig(
TypeError: AdapterConfig.__init__() got an unexpected keyword argument 'mh_adapter'
</code></pre>
<p>Since the <a href=""https://docs.adapterhub.ml/classes/adapter_config.html"" rel=""nofollow noreferrer"">document</a> for Adapter Configuration mentions there is a parameter named <code>mh_adapter</code> for <code>adapters.AdapterConfig</code>, can anyone tell me what is the problem and how to solve it? Thank you for your help!</p>
<p>By the way, here is my adapters package info:</p>
<pre><code># Name    Version   Build    Channel
adapters   1.0.1    pypi_0    pypi
</code></pre>
","adapter, bert-language-model","<p>The <code>adapters.AdapterConfig</code> class that you are using is actually a base class for all adaptation methods. And according to the documentation: &quot;This class does not define specific configuration keys, but only provides some common helper methods.&quot; I think this explains why the whole thing.</p>
<p>You don't want to use this base class; instead, you should use an adapter corresponding to your exact use case. The document explains a few adapters that include the <code>mh_adapter</code>, such as <code>adapters.BnConfig</code>. You have been looking at the input parameters defined for this class and had mistaken it for the base class.</p>
<p>This is how your code might look like after the modification:</p>
<pre><code>adapter_config = adapters.BnConfig(
    mh_adapter=True, 
    output_adapter=True,
    reduction_factor=16,  
    non_linearity=&quot;relu&quot;  
)
</code></pre>
",0,1,55,2024-11-23 07:27:05,https://stackoverflow.com/questions/79217353/adapterconfig-init-got-an-unexpected-keyword-argument-mh-adapter
Why do LayerNorm layers in BERT base have 768 (and not 512) weight and bias parameters?,"<p>The following will print 768 weight and bias parameters for each LayerNorm layer.</p>
<pre><code>from transformers import BertModel
model = BertModel.from_pretrained('bert-base-uncased')
for name, param in model.named_parameters():
    if 'LayerNorm' in name:
        print(f&quot;Layer: {name}, Parameters: {param.numel()}&quot;)
</code></pre>
<p>As per <a href=""https://youtu.be/G45TuC6zRf4?t=453"" rel=""nofollow noreferrer"">this video</a>, mean and std values are computed for each token in the input. And each mean, std pair has its own learned weight and bias in layer normalization. Since BERT takes in a max of 512 tokens, I'd expect a total of 512 weight and bias parameters in LayerNorm layers.</p>
<p>So why is it 768? Is the video incorrect? Is normalization performed for each of the 768 dimensions across all tokens, meaning mean and std values are computed across a max of 512 values?</p>
","pytorch, bert-language-model","<p>After quite a bit of research and code review, I was able to isolate details of layer normalization (LN), an aspect of transformers that's confusing a lot of people.</p>
<p>TL;DR: The assumption I made in my original question that each mean, std pair has its own weight, bias pair is incorrect. In LN, mean and std stats are computed across embedding dimensions of each token, i.e., there are as many mean, std pairs as there are tokens. But the weight and bias values are learned per embedding dimension, i.e., tokens share the weight and bias values during LN. This means, in the case of BERT base, there are a max of 512 mean and std values, and there are 768 weight and bias values.</p>
<p>For complete details, see my answer to <a href=""https://datascience.stackexchange.com/questions/88552/layer-normalization-details-in-gpt-2"">this question</a>.</p>
",0,0,243,2024-11-27 21:48:49,https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para
How can one obtain the &quot;correct&quot; embedding layer in BERT?,"<p>I want to utilize BERT to assess the similarity between two pieces of text:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
model = AutoModel.from_pretrained(&quot;bert-classifier&quot;)

def calc_similarity(s1, s2):
    inputs = tokenizer(s1, s2, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    cosine_similarity = F.cosine_similarity(embeddings[0], embeddings[1])
    return cosine_similarity
</code></pre>
<p>The similarity presented here is derived from a BERT sentiment classifier, which is a model fine-tuned based on the BERT architecture.</p>
<p>My inquiry primarily revolves around this line of code：</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
</code></pre>
<p>I have observed at least three different implementations regarding this line; in addition to the aforementioned version that retrieves the first row, there are two other variations:</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state.mean(axis=1).cpu().numpy()
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = model.bert.pooler(outputs.last_hidden_state.cpu().numpy())
</code></pre>
<p>In fact, vector <code>outputs.last_hidden_state</code> is a 9*768 tensor, and the three aforementioned methods can transform it into a 1*768 vector, thereby providing a basis for subsequent similarity calculations. From my perspective, the first approach is not appropriate within the semantic space defined by the classification task, as our objective is not to predict the next word. What perplexes me is the choice between the second and third methods, specifically whether to employ a simple average or to utilize the pooling layer of the model itself.</p>
<p>Any assistance would be greatly appreciated!</p>
","pytorch, sentiment-analysis, similarity, bert-language-model","<p>1st approach is not a good choice because leveraging the [CLS] token embedding directly might not be the best approach, in case if the BERT  was fine tuned for a task other than similarity matching.</p>
<ul>
<li><strong>Task-Specific Embeddings</strong>: The [CLS] token embedding is affected by the task the bert model was trained on.</li>
<li><strong>Averaging</strong> : Taking the mean of all token embeddings, we can get a more general representation of the input. This method balances out the representation by considering the contextual embeddings of all tokens.</li>
</ul>
<p>Consider taking average or pooling (passing through another dense layer) will work.</p>
",1,3,37,2024-12-04 03:15:26,https://stackoverflow.com/questions/79249787/how-can-one-obtain-the-correct-embedding-layer-in-bert
ValueError: Exception encountered when calling layer &#39;tf_bert_model&#39; (type TFBertModel),"<p>I have been trying to run <strong>TFBertModel</strong> from Transformers, but it kept on throwing me this error</p>
<pre><code>ValueError                                Traceback (most recent call last)
Cell In[9], line 1
----&gt; 1 bert_output = bert_model([input_ids, attention_mask])

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\tf_keras\src\utils\traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_tf_utils.py:436, in unpack_inputs.&lt;locals&gt;.run_call_with_unpacked_inputs(self, *args, **kwargs)
    433 else:
    434     config = self.config
--&gt; 436 unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)
    437 return func(self, **unpacked_inputs)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_tf_utils.py:530, in input_processing(func, config, **kwargs)
    528             output[parameter_names[i]] = input
    529         else:
--&gt; 530             raise ValueError(
    531                 f&quot;Data of type {type(input)} is not allowed only {allowed_types} is accepted for&quot;
    532                 f&quot; {parameter_names[i]}.&quot;
    533             )
    534 elif isinstance(main_input, Mapping):
    535     if &quot;inputs&quot; in main_input:

ValueError: Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).

Data of type &lt;class 'keras.src.backend.common.keras_tensor.KerasTensor'&gt; is not allowed only (&lt;class 'tensorflow.python.framework.tensor.Tensor'&gt;, &lt;class 'bool'&gt;, &lt;class 'int'&gt;, &lt;class 'transformers.utils.generic.ModelOutput'&gt;, &lt;class 'tuple'&gt;, &lt;class 'list'&gt;, &lt;class 'dict'&gt;, &lt;class 'numpy.ndarray'&gt;) is accepted for input_ids.

Call arguments received by layer 'tf_bert_model' (type TFBertModel):
  • input_ids=['&lt;KerasTensor shape=(None, 128), dtype=int32, sparse=False, name=input_ids&gt;', '&lt;KerasTensor shape=(None, 128), dtype=int32, sparse=False, name=attention_mask&gt;']
  • attention_mask=None
  • token_type_ids=None
  • position_ids=None
  • head_mask=None
  • inputs_embeds=None
  • encoder_hidden_states=None
  • encoder_attention_mask=None
  • past_key_values=None
  • use_cache=None
  • output_attentions=None
  • output_hidden_states=None
  • return_dict=None
  • training=False
</code></pre>
<p>This error is coming from this particular line</p>
<p><code>bert_output = bert_model([input_ids, attention_mask])</code></p>
<p>Here is the entire code, im new to using BERT and followed ChatGPT</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TFBertModel
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers

bert_model = TFBertModel.from_pretrained('bert-base-uncased')

input_ids = tf.keras.layers.Input(shape=(128,), dtype='int32', name='input_ids')
attention_mask = tf.keras.layers.Input(shape=(128,), dtype='int32', name='attention_mask')

bert_output = bert_model([input_ids, attention_mask]) # &lt;= this line is what giving the error

pooled_output = bert_output.pooler_output

# Custom classifier layers
x = layers.Dense(128, activation='relu')(pooled_output)
x = layers.Dropout(0.3)(x)
output = layers.Dense(2, activation='softmax')(x)  # Adjust num_classes as needed

# Build model
model = Model(inputs=[input_ids, attention_mask], outputs=output)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(train_dataset, epochs=3, validation_data=test_dataset)
</code></pre>
<p>This error persists even after researching and trying to solve it from ages now, i can't seem to solve nor understand the issue, i have tried downgrading both Tensorflow and Transformers version (4.17), but the error still persists, Tried asking Gemini and ChatGPT and they aren't able to solve it either and kept saying that i should use Tensorflow's tensor and not KerasTensor, then proceed to provide me the same code.</p>
<p>I'm using the latest version of both Tensorflow (2.18.0) and Transformers (4.47.1) and my Python version is 3.12</p>
<p>I also tried downgrading the transformers version to 4.17 however, it still persists, i tried  passing <code>input_ids</code> and <code>attention_mask</code> differently such as this:</p>
<p><code>bert_output = bert_model([{'input_ids': input_ids, 'attention_mask': attention_mask}])</code></p>
<p>but the error remains</p>
","tensorflow, tensorflow2.0, huggingface-transformers, bert-language-model, transformer-model","<p>TLDR; use this</p>
<pre><code>import os

os.environ['TF_USE_LEGACY_KERAS'] = '1'
</code></pre>
<p><strong>Explanation:</strong>
Transformers package uses Keras 2 objects, current version is Keras 3, packed in Tensorflow since version 2.16. Fastest fix without downgrading tensorflow is to set legacy keras usage flag as above. More info can be found <a href=""https://github.com/huggingface/transformers/issues/29470"" rel=""nofollow noreferrer"">here</a>.</p>
",0,1,183,2024-12-26 15:53:31,https://stackoverflow.com/questions/79309854/valueerror-exception-encountered-when-calling-layer-tf-bert-model-type-tfber
