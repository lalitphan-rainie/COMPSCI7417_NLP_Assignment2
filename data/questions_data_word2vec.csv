How to use word2vec to calculate the similarity distance by giving 2 words?,"<p>Word2vec is a open source tool to calculate the words distance provided by Google. It can be used by inputting a word and output the ranked word lists according to the similarity. E.g.</p>

<p>Input:</p>

<pre><code>france
</code></pre>

<p>Output:</p>

<pre><code>            Word       Cosine distance

            spain              0.678515
          belgium              0.665923
      netherlands              0.652428
            italy              0.633130
      switzerland              0.622323
       luxembourg              0.610033
         portugal              0.577154
           russia              0.571507
          germany              0.563291
        catalonia              0.534176
</code></pre>

<p>However, what I need to do is to calculate the similarity distance by giving 2 words. If I give the 'france' and 'spain', how can I get the score 0.678515 without reading the whole words list by giving just 'france'.</p>
",word2vec,"<p>gensim has a Python implementation of Word2Vec which provides an in-built utility for finding similarity between two words given as input by the user. You can refer to the following:</p>
<ol>
<li>Intro: <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">http://radimrehurek.com/gensim/models/word2vec.html</a></li>
<li>Tutorial: <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial/"" rel=""noreferrer"">http://radimrehurek.com/2014/02/word2vec-tutorial/</a></li>
</ol>
<p>UPDATED: Gensim 4.0.0 and above</p>
<p>The syntax in Python for finding similarity between two words goes like this:</p>
<pre><code>&gt;&gt; from gensim.models import Word2Vec
&gt;&gt; model = Word2Vec.load(path/to/your/model)
&gt;&gt; model.wv.similarity('france', 'spain')
</code></pre>
",64,52,126813,2014-02-24 05:58:34,https://stackoverflow.com/questions/21979970/how-to-use-word2vec-to-calculate-the-similarity-distance-by-giving-2-words
How to compute a language model with word2vec tool?,"<p>I'm trying to build a neural network language model and it seems that word2vec tool by Mikolov et al is a good tool for this purpose. I tried that but it just produces word representations.
Does anybody know how i can produce a language model by that tool or any other reasonable deep learning framework?</p>
","theano, word2vec, deep-learning","<p>Doc2Vec implemented in Gensim does the job. The trick is that they use the document ID as a context word, which is present in all window sizes of all the words in the document.</p>

<p>Code is <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">here in Python/Gensim</a></p>
",3,3,3832,2014-02-26 09:15:35,https://stackoverflow.com/questions/22036710/how-to-compute-a-language-model-with-word2vec-tool
How to calculate the sentence similarity using word2vec model of gensim with python,"<p>According to the <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Gensim Word2Vec</a>, I can use the word2vec model in gensim package to calculate the similarity between 2 words.</p>

<p>e.g.</p>

<pre><code>trained_model.similarity('woman', 'man') 
0.73723527
</code></pre>

<p>However, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?</p>
","python, gensim, word2vec","<p>This is actually a pretty challenging problem that you are asking. Computing sentence similarity requires building a grammatical model of the sentence, understanding equivalent structures (e.g. ""he walked to the store yesterday"" and ""yesterday, he walked to the store""), finding similarity not just in the pronouns and verbs but also in the proper nouns, finding statistical co-occurences / relationships in lots of real textual examples, etc.</p>

<p>The simplest thing you could try -- though I don't know how well this would perform and it would certainly not give you the optimal results -- would be to first remove all ""stop"" words (words like ""the"", ""an"", etc. that don't add much meaning to the sentence) and then run word2vec on the words in both sentences, sum up the vectors in the one sentence, sum up the vectors in the other sentence, and then find the difference between the sums. By summing them up instead of doing a word-wise difference, you'll at least not be subject to word order. That being said, this will fail in lots of ways and isn't a good solution by any means (though good solutions to this problem almost always involve some amount of NLP, machine learning, and other cleverness).</p>

<p>So, short answer is, no, there's no easy way to do this (at least not to do it well).</p>
",99,145,135950,2014-03-02 16:04:53,https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt
Word2Vec: Effect of window size used,"<p>I am trying to train a word2vec model on very short phrases (5 grams). Since each sentence or example is very short, I believe the window size I can use can atmost be 2. I am trying to understand what the implications of such a small window size are on the quality of the learned model, so that I can understand whether my model has learnt something meaningful or not. I tried training a word2vec model on 5-grams but it appears the learnt model does not capture semantics etc very well.</p>

<p>I am using the following test to evaluate the accuracy of model:
<a href=""https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt"" rel=""noreferrer"">https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a></p>

<p>I used gensim.Word2Vec to train a model and here is a snippet of my accuracy scores (using a window size of 2)</p>

<pre><code>[{'correct': 2, 'incorrect': 304, 'section': 'capital-common-countries'},
 {'correct': 2, 'incorrect': 453, 'section': 'capital-world'},
 {'correct': 0, 'incorrect': 86, 'section': 'currency'},
 {'correct': 2, 'incorrect': 703, 'section': 'city-in-state'},
 {'correct': 123, 'incorrect': 183, 'section': 'family'},
 {'correct': 21, 'incorrect': 791, 'section': 'gram1-adjective-to-adverb'},
 {'correct': 8, 'incorrect': 544, 'section': 'gram2-opposite'},
 {'correct': 284, 'incorrect': 976, 'section': 'gram3-comparative'},
 {'correct': 67, 'incorrect': 863, 'section': 'gram4-superlative'},
 {'correct': 41, 'incorrect': 951, 'section': 'gram5-present-participle'},
 {'correct': 6, 'incorrect': 1089, 'section': 'gram6-nationality-adjective'},
 {'correct': 171, 'incorrect': 1389, 'section': 'gram7-past-tense'},
 {'correct': 56, 'incorrect': 936, 'section': 'gram8-plural'},
 {'correct': 52, 'incorrect': 705, 'section': 'gram9-plural-verbs'},
 {'correct': 835, 'incorrect': 9973, 'section': 'total'}]
</code></pre>

<p>I also tried running the demo-word-accuracy.sh script outlined here with a window size of 2 and get poor accuracy as well:</p>

<pre><code>Sample output:
    capital-common-countries:
    ACCURACY TOP1: 19.37 %  (98 / 506)
    Total accuracy: 19.37 %   Semantic accuracy: 19.37 %   Syntactic accuracy: -nan % 
    capital-world:
    ACCURACY TOP1: 10.26 %  (149 / 1452)
    Total accuracy: 12.61 %   Semantic accuracy: 12.61 %   Syntactic accuracy: -nan % 
    currency:
    ACCURACY TOP1: 6.34 %  (17 / 268)
    Total accuracy: 11.86 %   Semantic accuracy: 11.86 %   Syntactic accuracy: -nan % 
    city-in-state:
    ACCURACY TOP1: 11.78 %  (185 / 1571)
    Total accuracy: 11.83 %   Semantic accuracy: 11.83 %   Syntactic accuracy: -nan % 
    family:
    ACCURACY TOP1: 57.19 %  (175 / 306)
    Total accuracy: 15.21 %   Semantic accuracy: 15.21 %   Syntactic accuracy: -nan % 
    gram1-adjective-to-adverb:
    ACCURACY TOP1: 6.48 %  (49 / 756)
    Total accuracy: 13.85 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 6.48 % 
    gram2-opposite:
    ACCURACY TOP1: 17.97 %  (55 / 306)
    Total accuracy: 14.09 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 9.79 % 
    gram3-comparative:
    ACCURACY TOP1: 34.68 %  (437 / 1260)
    Total accuracy: 18.13 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 23.30 % 
    gram4-superlative:
    ACCURACY TOP1: 14.82 %  (75 / 506)
    Total accuracy: 17.89 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.78 % 
    gram5-present-participle:
    ACCURACY TOP1: 19.96 %  (198 / 992)
    Total accuracy: 18.15 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.31 % 
    gram6-nationality-adjective:
    ACCURACY TOP1: 35.81 %  (491 / 1371)
    Total accuracy: 20.76 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.14 % 
    gram7-past-tense:
    ACCURACY TOP1: 19.67 %  (262 / 1332)
    Total accuracy: 20.62 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 24.02 % 
    gram8-plural:
    ACCURACY TOP1: 35.38 %  (351 / 992)
    Total accuracy: 21.88 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.52 % 
    gram9-plural-verbs:
    ACCURACY TOP1: 20.00 %  (130 / 650)
    Total accuracy: 21.78 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.08 % 
    Questions seen / total: 12268 19544   62.77 % 
</code></pre>

<p>However the word2vec site claims its possible to obtain an accuracy of ~60% on these tasks.
Hence I would like to gain some insights into the effect of these hyperparameters like window size and how they affect quality of learnt models.</p>
","gensim, word2vec","<p>To your question: ""I am trying to understand what the implications of such a small window size are on the quality of the learned model"".</p>

<p>For example ""stackoverflow great website for programmers"" with 5 words (suppose we save the stop words great and for here)
if the window size is 2 then the vector of word ""stackoverflow"" is directly affected by the word ""great"" and ""website"", if the window size is 5 ""stackoverflow"" can be directly affected by two more words ""for"" and ""programmers"". The 'affected' here means it will pull the vector of two words closer.</p>

<p>So it depends on the material you are using for training, if the window size of 2 can capture the context of a word, but 5 is chosen, it will decrease the quality of the learnt model, and vise versa.</p>
",16,25,35531,2014-03-08 17:07:51,https://stackoverflow.com/questions/22272370/word2vec-effect-of-window-size-used
Gensim train word2vec on wikipedia - preprocessing and parameters,"<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia
""<a href=""http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>""</p>

<p>However, I am not sure what is the best preprocessing for this corpus.</p>

<p><code>gensim</code> model accepts a list of tokenized sentences.
My first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>

<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>

<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>

<p>This the code of my first trial:</p>

<pre><code>from gensim.corpora import WikiCorpus
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
corpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)
max_sentence = -1

def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence==-1:
            yield text
        else:
            break

from gensim.models.word2vec import BrownCorpus, Word2Vec
model = Word2Vec() 
model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
model.train(generate_lines(),chunksize=500)
</code></pre>
","nlp, gensim, word2vec","<p>Your approach is fine.</p>

<pre><code>model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
</code></pre>

<p>This could be because of pruning infrequent words (the default is <code>min_count=5</code>).</p>

<p>To speed up computation, you can consider ""caching"" the preprocessed articles as a plain <code>.txt.gz</code> file, one sentence (document) per line, and then simply using <a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"">word2vec.LineSentence</a> corpus. This saves parsing the bzipped wiki XML on every iteration.</p>

<p>Why word2vec doesn't produce ""meaningful similarities"" for Italian wiki, I don't know. English wiki seems to work fine. See also <a href=""https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw"">here</a>.</p>
",9,17,9432,2014-05-19 10:37:21,https://stackoverflow.com/questions/23735576/gensim-train-word2vec-on-wikipedia-preprocessing-and-parameters
&#39;file&#39; object has no attribute &#39;rfind&#39;,"<p>I am trying to save a word2vec to a file.</p>

<pre><code>model = Word2Vec(sentences, size=vector_size, window=5, min_count=5, workers=4)
fo = open(""foo.txt"", ""wb"")
model.save(fo)
</code></pre>

<p>I am getting the following error in genericpath.py</p>

<pre><code>File ""word2Vec_impl.py"", line 39, in &lt;module&gt;
model.save(fo, separately=None)
File ""C:\Python27\lib\site-packages\gensim\models\word2vec.py"", line 669, in s
ave
super(Word2Vec, self).save(*args, **kwargs)
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 269, in save
pickle(self, fname)
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 608, in pickle
with smart_open(fname, 'wb') as fout: # 'b' for binary, needed on Windows
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 596, in smart_open
_, ext = path.splitext(fname)
File ""C:\Python27\lib\ntpath.py"", line 190, in splitext
return genericpath._splitext(p, sep, altsep, extsep)
File ""C:\Python27\lib\genericpath.py"", line 91, in _splitext
sepIndex = p.rfind(sep)
AttributeError: 'file' object has no attribute 'rfind'
</code></pre>

<p>Where I am going wrong?</p>
","python, word2vec","<p><code>Word2Vec.save</code> accepts a filename, not a file object.</p>

<pre><code>model = Word2Vec(sentences, size=vector_size, window=5, min_count=5, workers=4)
model.save('foo.txt')
</code></pre>
",9,4,18102,2014-05-24 11:29:06,https://stackoverflow.com/questions/23844457/file-object-has-no-attribute-rfind
word2vec lemmatization of corpus before training,"<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
","nlp, word2vec, gensim, lemmatization","<p>I think it really matters about what you want to solve with this. It depends on the task. </p>

<p>Essentially by lemmatization, you make the input space sparser, which can help if you don't have enough training data. </p>

<p>But since Word2Vec is fairly big, if you have big enough training data, lemmatization shouldn't gain you much. </p>

<p>Something more interesting is, how to do tokenization with respect to the existing diction of words-vectors inside the W2V (or anything else).  Like ""Good muffins cost $3.88\nin New York."" needs to be tokenized to ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New York.'] Then you can replace it with its vectors from W2V. The challenge is that some tokenizers my tokenize ""New York"" as ['New' 'York'], which doesn't make much sense. (For example, NLTK is making this mistake <a href=""https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html"" rel=""noreferrer"">https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html</a>) This is a problem when you have many multi-word phrases.</p>
",9,32,17221,2014-05-26 20:35:36,https://stackoverflow.com/questions/23877375/word2vec-lemmatization-of-corpus-before-training
python word2vec not installing,"<p>I've been trying to install word2vec on my Windows 7 machine using my Python2.7 interpreter: <a href=""https://github.com/danielfrg/word2vec"" rel=""noreferrer"">https://github.com/danielfrg/word2vec</a> </p>

<p>I've tried downloading the zip &amp; running python <code>setup.py</code> install from the unzipped directory and running <code>pip install</code>. however in both instances it returns the below errors:</p>

<pre><code>Downloading/unpacking word2vec
  Downloading word2vec-0.5.1.tar.gz
  Running setup.py egg_info for package word2vec
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 16, in &lt;module&gt;
      File ""c:\users\georgioa\appdata\local\temp\pip_build_georgioa\word2vec\setup.py"", line 17, in &lt;module&gt;
        subprocess.call(['make', '-C', 'word2vec-c'])
      File ""C:\Python27\lib\subprocess.py"", line 524, in call
        return Popen(*popenargs, **kwargs).wait()
      File ""C:\Python27\lib\subprocess.py"", line 711, in __init__
        errread, errwrite)
      File ""C:\Python27\lib\subprocess.py"", line 948, in _execute_child
        startupinfo)
    WindowsError: [Error 2] The system cannot find the file specified
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 16, in &lt;module&gt;
  File ""c:\users\georgioa\appdata\local\temp\pip_build_georgioa\word2vec\setup.py"", line 17, in &lt;module&gt;
    subprocess.call(['make', '-C', 'word2vec-c'])
  File ""C:\Python27\lib\subprocess.py"", line 524, in call
    return Popen(*popenargs, **kwargs).wait()
  File ""C:\Python27\lib\subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""C:\Python27\lib\subprocess.py"", line 948, in _execute_child
    startupinfo)
WindowsError: [Error 2] The system cannot find the file specified
</code></pre>

<p>There seemed to be a problem accessing <code>subprocess.call()</code>, so after a bit of googling I managed to add <code>shell=True</code> to the line the the word2vec <code>setup.py</code> and it then throws this error:</p>

<pre><code>'make' is not recognized as an internal or external command,
operable program or batch file.
C:\Python27\lib\distutils\dist.py:267: UserWarning: Unknown distribution option: 'install_requires'
  warnings.warn(msg)
running install
running build
running build_py
running install_lib
running install_data
error: can't copy 'bin\word2vec': doesn't exist or not a regular file 
</code></pre>

<p>To be honest I'm not even sure where I should go from here. I've also tried installing make and setting the path variable to the .exe file in the install, any advice would be greatly appreciated, thanks.</p>

<p><strong>UPDATE:</strong></p>

<p>While the word2vec module wouldn't work a package called <code>genism</code> seems to work pretty well, it's got some great other NLP functionality too <a href=""http://radimrehurek.com/gensim/"" rel=""noreferrer"">http://radimrehurek.com/gensim/</a> </p>
","python, pip, gnuwin32, word2vec","<p>The word2vec is meant for Linux. See: <a href=""https://github.com/danielfrg/word2vec"" rel=""noreferrer"">https://github.com/danielfrg/word2vec</a></p>

<p>At the bottom it says there is a very experimental Windows build at: support:<a href=""https://github.com/zhangyafeikimi/word2vec-win32"" rel=""noreferrer"">https://github.com/zhangyafeikimi/word2vec-win32</a></p>

<p>EDIT:</p>

<p>Looks like you can also install gensim: <a href=""https://pypi.python.org/pypi/gensim"" rel=""noreferrer"">https://pypi.python.org/pypi/gensim</a></p>

<p>Then do: </p>

<pre><code>from gensim.models import word2vec
</code></pre>
",16,10,21278,2014-09-03 11:20:13,https://stackoverflow.com/questions/25643004/python-word2vec-not-installing
Keep alive the object for using them in another program in python,"<p>I am using word2vec to calculate similarity between two words. So for the model I am using GoogleNews.
This model is quite huge and hence takes lot of time to load.</p>

<pre><code>model = Word2Vec.load_word2vec_format('D:/Userfiles/vsachidananda/Downloads/GoogleNews-vectors negative300.bin.gz', binary=True)
</code></pre>

<p>I would like to load this and keep in a variable/object so that whenever I run a python program I should be able to call just </p>

<pre><code>model.similarity('word1','word2')
</code></pre>

<p>How can this be achieved? Any idea?</p>
","python, object, python-3.x, word2vec","<p>The only way I know to share complex objects between Python processes is to use <code>multiprocessing.Manager</code>.
But <code>model</code> would be <strong>pickled</strong> and <strong>unpickled</strong> each time it needs to be shared with a subprocess. I guess it would be as slow as <code>load_word2vec_format</code>.</p>

<p>You could instead run a launcher that loads <code>model</code> once, then waits and executes another python script on demand. A very simple launcher would look like this:</p>

<pre><code>import Word2Vec
model = Word2Vec.load_word2vec_format(...)

try:
    import traceback
    import script

    while True:
        raw_input()

        try:
            reload(script)
            script.main(model)
        except:
            print traceback.print_exc()

except KeyboardInterrupt:
    print 'exit launcher'
</code></pre>

<p>With this basic launcher, <code>script.py</code> should be in the same folder and need to define a <code>main()</code>:</p>

<pre><code>def main(model):
    model.similarity('word1','word2')
    ...
</code></pre>
",1,2,970,2014-09-11 08:30:50,https://stackoverflow.com/questions/25782727/keep-alive-the-object-for-using-them-in-another-program-in-python
Why the similarity beteween two bag-of-words in gensim.word2vec calculated this way?,"<pre><code>def n_similarity(self, ws1, ws2):
    v1 = [self[word] for word in ws1]
    v2 = [self[word] for word in ws2]
    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))
</code></pre>

<p>This is the code I excerpt from gensim.word2Vec, I know that two single words' similarity can be calculated by cosine distances, but what about two word sets? The code seems to use the mean of each wordvec and then calculated on the two mean vectors' cosine distance. I know few in word2vec, is there some foundations of such process?</p>
","nlp, gensim, word2vec","<p>Taking the mean of all word vectors is the simplest way of reducing them to a single vector so cosine similarity can be used. The intuition is that by adding up all word vectors you get a bit of all of them (the meaning) in the result. You then divide by the number of vectors so that larger bag of words don't end up with longer vectors (not that it matters for cosine similarity anyway).</p>

<p>There are other ways to reduce an entire sentence to a single vector is a complex one. I wrote a bit about it in a <a href=""https://stackoverflow.com/a/15173821/419338"">related question</a> on SO. Since then a bunch of new algorithms have been proposed. One of the more accessible ones is <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Paragraph Vector</a>, which you shouldn't have problems understanding if you are familiar with <code>word2vec</code>.</p>
",3,3,3355,2014-09-24 07:08:51,https://stackoverflow.com/questions/26010645/why-the-similarity-beteween-two-bag-of-words-in-gensim-word2vec-calculated-this
"How to use vector representation of words (as obtained from Word2Vec,etc) as features for a classifier?","<p>I am familiar with using BOW features for text classification, wherein we first find the size of the vocabulary for the corpus which becomes the size of our feature vector. For each sentence/document, and for all its constituent words, we then put 0/1 depending on the absence/presence of that word in that sentence/document. </p>

<p>However, now that I am trying to use vector representation of each word, is creating a global vocabulary essential? </p>
","text, vector, nlp, text-classification, word2vec","<p>Suppose the size of the vectors is N (usually between 50 or 500). The naive way of generalizing the traditional of generalizing BOW is just replacing 0 bit (in BOW) with N zeros, and replacing 1 bit (in BOW) with the the real vector (say from Word2Vec). Then the size of the features would be N * |V| (Compared to |V| feature vectors in the BOW, where |V| is the size of the vocabs). This simple generalization should work fine for decent number of training instances.</p>

<p>To make the feature vectors smaller, people use various techniques like using recursive combination of vectors with various operations. (See Recursive/Recurrent Neural Network and similar tricks, for example: <a href=""http://web.engr.illinois.edu/~khashab2/files/2013_RNN.pdf"">http://web.engr.illinois.edu/~khashab2/files/2013_RNN.pdf</a> or <a href=""http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf"">http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf</a> ) </p>
",7,15,3622,2014-10-26 03:45:28,https://stackoverflow.com/questions/26569592/how-to-use-vector-representation-of-words-as-obtained-from-word2vec-etc-as-fea
Load PreComputed Vectors Gensim,"<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>

<p>Thanks! </p>
","python, nlp, gensim, word2vec","<p>You can download pre-trained word vectors from here (get the file 'GoogleNews-vectors-negative300.bin'):
<a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a></p>

<p>Extract the file and then you can load it in python like:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)

model.most_similar('dog')
</code></pre>

<p>EDIT (May 2017):
As the above code is now deprecated, this is how you'd load the vectors now:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)
</code></pre>
",24,25,20415,2014-11-26 01:35:41,https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim
Convert word2vec bin file to text,"<p>From the <a href=""https://code.google.com/p/word2vec/"">word2vec</a> site I can download GoogleNews-vectors-negative300.bin.gz.  The .bin file (about 3.4GB) is a binary format not useful to me.  Tomas Mikolov <a href=""https://groups.google.com/d/msg/word2vec-toolkit/lxbl_MB29Ic/g4uEz5rNV08J"">assures us</a> that ""It should be fairly straightforward to convert the binary format to text format (though that will take more disk space). Check the code in the distance tool, it's rather trivial to read the binary file.""  Unfortunately, I don't know enough C to understand <a href=""http://word2vec.googlecode.com/svn/trunk/distance.c"">http://word2vec.googlecode.com/svn/trunk/distance.c</a>.</p>

<p>Supposedly <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial/"">gensim</a> can do this also, but all the tutorials I've found seem to be about converting <em>from</em> text, not the other way.</p>

<p>Can someone suggest modifications to the C code or instructions for gensim to emit text?</p>
","python, c, gensim, word2vec","<p>On the word2vec-toolkit mailing list Thomas Mensink has provided an <a href=""https://groups.google.com/forum/#!topic/word2vec-toolkit/5Qh-x2O1lV4"" rel=""noreferrer"">answer</a> in the form of a small C program that will convert a .bin file to text.  This is a modification of the distance.c file.  I replaced the original distance.c with Thomas's code below and rebuilt word2vec (make clean; make), and renamed the compiled distance to readbin.  Then <code>./readbin vector.bin</code> will create a text version of vector.bin.</p>

<pre><code>//  Copyright 2013 Google Inc. All Rights Reserved.
//
//  Licensed under the Apache License, Version 2.0 (the ""License"");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an ""AS IS"" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;math.h&gt;
#include &lt;malloc.h&gt;

const long long max_size = 2000;         // max length of strings
const long long N = 40;                  // number of closest words that will be shown
const long long max_w = 50;              // max length of vocabulary entries

int main(int argc, char **argv) {
  FILE *f;
  char file_name[max_size];
  float len;
  long long words, size, a, b;
  char ch;
  float *M;
  char *vocab;
  if (argc &lt; 2) {
    printf(""Usage: ./distance &lt;FILE&gt;\nwhere FILE contains word projections in the BINARY FORMAT\n"");
    return 0;
  }
  strcpy(file_name, argv[1]);
  f = fopen(file_name, ""rb"");
  if (f == NULL) {
    printf(""Input file not found\n"");
    return -1;
  }
  fscanf(f, ""%lld"", &amp;words);
  fscanf(f, ""%lld"", &amp;size);
  vocab = (char *)malloc((long long)words * max_w * sizeof(char));
  M = (float *)malloc((long long)words * (long long)size * sizeof(float));
  if (M == NULL) {
    printf(""Cannot allocate memory: %lld MB    %lld  %lld\n"", (long long)words * size * sizeof(float) / 1048576, words, size);
    return -1;
  }
  for (b = 0; b &lt; words; b++) {
    fscanf(f, ""%s%c"", &amp;vocab[b * max_w], &amp;ch);
    for (a = 0; a &lt; size; a++) fread(&amp;M[a + b * size], sizeof(float), 1, f);
    len = 0;
    for (a = 0; a &lt; size; a++) len += M[a + b * size] * M[a + b * size];
    len = sqrt(len);
    for (a = 0; a &lt; size; a++) M[a + b * size] /= len;
  }
  fclose(f);
  //Code added by Thomas Mensink
  //output the vectors of the binary format in text
  printf(""%lld %lld #File: %s\n"",words,size,file_name);
  for (a = 0; a &lt; words; a++){
    printf(""%s "",&amp;vocab[a * max_w]);
    for (b = 0; b&lt; size; b++){ printf(""%f "",M[a*size + b]); }
    printf(""\b\b\n"");
  }  

  return 0;
}
</code></pre>

<p>I removed the ""\b\b"" from the <code>printf</code>.  </p>

<p>By the way, the resulting text file still contained the text word and some unnecessary whitespace which I did not want for some numerical calculations.  I removed the initial text column and the trailing blank from each line with bash commands.</p>

<pre><code>cut --complement -d ' ' -f 1 GoogleNews-vectors-negative300.txt &gt; GoogleNews-vectors-negative300_tuples-only.txt
sed 's/ $//' GoogleNews-vectors-negative300_tuples-only.txt
</code></pre>
",20,68,56245,2014-12-05 20:39:00,https://stackoverflow.com/questions/27324292/convert-word2vec-bin-file-to-text
how to create word vector,"<p>How to create word vector? I used one hot key to create word vector, but it is very huge and not generalized for similar semantic word. So I have heard about word vector using neural network that finds word similarity and word vector. So I wanted to know how to generate this vector (algorithm) or good material to start creating word vector ?.</p>
","nlp, neural-network, word2vec","<p>Word-vectors or so-called distributed representations have a long history by now, starting perhaps from work of S. Bengio (<em>Bengio, Y., Ducharme, R., &amp; Vincent, P. (2001).A neural probabilistic language model. NIPS.</em>) where he obtained word-vectors as by-product of training neural-net lanuage model.</p>

<p>A lot of researches demonstrated that these vectors do capture semantic relationship between words (see for example <a href=""http://research.microsoft.com/pubs/206777/338_Paper.pdf"" rel=""noreferrer"">http://research.microsoft.com/pubs/206777/338_Paper.pdf</a>). Also this important paper (<a href=""http://arxiv.org/abs/1103.0398"" rel=""noreferrer"">http://arxiv.org/abs/1103.0398</a>) by Collobert et al, is a good starting point with understanding word vectors, the way they are obtained and used. </p>

<p>Besides word2vec there is a lot of methods to obtain them. Expamples include SENNA embeddings by Collobert et al (<a href=""http://ronan.collobert.com/senna/"" rel=""noreferrer"">http://ronan.collobert.com/senna/</a>), RNN embeddings by T. Mikolov that can be computed using RNNToolkit (<a href=""http://www.fit.vutbr.cz/~imikolov/rnnlm/"" rel=""noreferrer"">http://www.fit.vutbr.cz/~imikolov/rnnlm/</a>) and much more. For English, ready-made embeddings can be downloaded from these web-sites. word2vec really uses skip-gram model (not neural network model).  Another fast code for computing  word representations is GloVe (<a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). It is an open question whatever deep neural networks are essential for obtaining good embeddings or not.</p>

<p>Depending of your application, you may prefer using different types of word-vectors, so its a good idea to try several popular algorithms and see what works better for you.  </p>
",10,7,11812,2014-12-19 08:07:54,https://stackoverflow.com/questions/27561971/how-to-create-word-vector
What is the concept of negative-sampling in word2vec?,"<p>I'm reading the 2014 paper <em><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">word2vec Explained: Deriving Mikolov et al.â€™s
Negative-Sampling Word-Embedding Method</a></em> (note: direct download link) and it references the concept of &quot;negative-sampling&quot;:</p>
<blockquote>
<p>Mikolov et al. present the negative-sampling approach as a more efficient
way of deriving word embeddings. While negative-sampling is based on the
skip-gram model, it is in fact optimizing a different objective.</p>
</blockquote>
<p>I have some issue understanding the concept of negative-sampling.</p>
<p><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722v1.pdf</a></p>
<p>Can anyone explain in layman's terms what negative-sampling is?</p>
","machine-learning, nlp, word2vec","<p>The idea of <code>word2vec</code> is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have</p>
<pre><code>      v_c . v_w
 -------------------
   sum_i(v_ci . v_w)
</code></pre>
<p>The numerator is basically the similarity between words <code>c</code> (the context) and <code>w</code> (the target) word. The denominator computes the similarity of all other contexts <code>ci</code> and the target word <code>w</code>. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts <code>ci</code>. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts <code>ci</code> at random. The end result is that if <code>cat</code> appears in the context of <code>food</code>, then the vector of <code>food</code> is more similar to the vector of <code>cat</code> (as measures by their dot product) than the vectors of <strong>several other randomly chosen words</strong> (e.g. <code>democracy</code>, <code>greed</code>, <code>Freddy</code>), instead of <strong>all other words in language</strong>. This makes <code>word2vec</code> much much faster to train.</p>
",204,124,63649,2015-01-09 12:31:25,https://stackoverflow.com/questions/27860652/what-is-the-concept-of-negative-sampling-in-word2vec
Extrapolate Sentence Similarity Given Word Similarities,"<p>Assuming that I have a word similarity score for each pair of words in two sentences, what is a decent approach to determining the overall sentence similarity from those scores?</p>

<p>The word scores are calculated using cosine similarity from vectors representing each word.  </p>

<p>Now that I have individual word scores, is it too naive to sum the individual word scores and divide by the total word count of both sentences to get a score for the two sentences?</p>

<p>I've read about further constructing vectors to represent the sentences, using the word scores, and then again using cosine similarity to compare the sentences.  But I'm not familiar with how to construct sentence vectors from the existing word scores.  Nor am I aware of what the tradeoffs are compared with the naive approach described above, which at the very least, I can easily comprehend.  :).</p>

<p>Any insights are greatly appreciated.</p>

<p>Thanks.  </p>
","wordnet, cosine-similarity, word2vec, sentence-similarity","<p>What I ended up doing, was taking the mean of each set of vectors, and then applying cosine-similarity to the two means, resulting in a score for the sentences.</p>

<p>I'm not sure how mathematically sound this approach is, but I've seen it done in other places (like python's gensim).</p>
",0,3,671,2015-01-27 04:31:53,https://stackoverflow.com/questions/28163289/extrapolate-sentence-similarity-given-word-similarities
Gensim Word2vec storing attribute syn0norm,"<p>I am trying to use <em>word2vec</em> for a project and after training I get:</p>

<pre><code>INFO : not storing attribute syn0norm
</code></pre>

<p>Is there any way I could save the <code>syn0norm</code>.</p>

<p>How can I do so?</p>
","python, gensim, word2vec","<p>This is fine -- you shouldn't need to store the array syn0norm.</p>

<p>It's computed in the init_sims procedure, and only on an as needed basis.  After training, it's actually not defined so there's nothing to train.</p>

<p>When you query the model (such as most_similar), it will call init_sims which checks to see if syn0norm is defined.  If not it assigns it with the following line:</p>

<pre><code>self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
</code></pre>

<p>EDIT:</p>

<p>After looking through code (for other things) I see that you can specify if you want to save syn0norm -- there's an ignore setting which defaults to ['syn0norm'], so the following will save all:</p>

<pre><code>In [239]: model.save('test',ignore=[])
2015-03-17 09:07:54,733 : INFO : saving Word2Vec object under test, separately None
2015-03-17 09:07:54,734 : INFO : storing numpy array 'syn0' to test.syn0.npy
2015-03-17 09:08:15,908 : INFO : storing numpy array 'table' to test.table.npy
2015-03-17 09:08:17,908 : INFO : storing numpy array 'syn1neg' to test.syn1neg.npy
2015-03-17 09:08:35,037 : INFO : storing numpy array 'syn1' to test.syn1.npy
2015-03-17 09:09:03,766 : INFO : storing numpy array 'syn0norm' to test.syn0norm.npy
</code></pre>

<p>The problem is, it usually will take less time to calculate than save and reload.</p>
",3,3,4251,2015-02-13 20:53:48,https://stackoverflow.com/questions/28508548/gensim-word2vec-storing-attribute-syn0norm
cosine similarity between two words in a list,"<p>I am defining a function which takes a list of words and returns information about the words in the list that have non-zero, cosine similarity between each other (along with the similarity value).</p>

<p>Can anyone help me out with this. I was thinking if I can get a precomputed word2vec vector file then it would be very helpful,but there is none on the internet. </p>
","python, word2vec","<p>You could define these two functions</p>

<pre><code>def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the ""length"" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
</code></pre>

<p>and use them as in this example</p>

<pre><code>&gt;&gt;&gt; a = 'safasfeqefscwaeeafweeaeawaw'
&gt;&gt;&gt; b = 'tsafdstrdfadsdfdswdfafdwaed'
&gt;&gt;&gt; c = 'optykop;lvhopijresokpghwji7'
&gt;&gt;&gt; 
&gt;&gt;&gt; va = word2vec(a)
&gt;&gt;&gt; vb = word2vec(b)
&gt;&gt;&gt; vc = word2vec(c)
&gt;&gt;&gt; 
&gt;&gt;&gt; print cosdis(va,vb)
0.551843662321
&gt;&gt;&gt; print cosdis(vb,vc)
0.113746579656
&gt;&gt;&gt; print cosdis(vc,va)
0.153494378078
</code></pre>

<p>BTW, the <a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer""><code>word2vec</code></a> that you mention in a tag is quite a different business, that requires that one of us take a great deal of time and commitment for studying it and guess what, I'm not that one...</p>
",11,7,18937,2015-04-07 05:39:18,https://stackoverflow.com/questions/29484529/cosine-similarity-between-two-words-in-a-list
Gensim word2vec augment or merge pre-trained vectors,"<p>I am loading pre-trained vectors from a binary file generated from the word2vec C code with something like:</p>

<pre><code>model_1 = Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am using those vectors to generate vector representations of sentences that contain words that may not have already existing vectors in <code>vectors.bin</code>. For example, if <code>vectors.bin</code> has no associated vector for the word ""yogurt"", and I try</p>

<pre><code>yogurt_vector = model_1['yogurt']
</code></pre>

<p>I get <code>KeyError: 'yogurt'</code>, which makes good sense. What I want is to be able to take the sentence words that do not have corresponding vectors and add representations for them to <code>model_1</code>. I am aware from <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">this post</a> that you cannot continue to train the C vectors. Is there then a way to train a new model, say <code>model_2</code>, for the words with no vectors and merge <code>model_2</code> with <code>model_1</code>?</p>

<p>Alternatively, is there a way to test if the model contains a word before I actually try to retrieve it, so that I can at least avoid the KeyError?</p>
","python, gensim, keyerror, word2vec","<p>Avoiding the key error is easy:</p>

<pre><code>[x for x in 'this model hus everything'.split() if x in model_1.vocab]
</code></pre>

<p>The more difficult problem is merging a new word to an existing model.  The problem is that word2vec calculates the likelihood of 2 words being next to each other, and if the word 'yogurt' wasn't in the first body that the model was trained on it's not next to any of those words, so the second model would not correlate to the first.</p>

<p>You can look at the internals when a model is saved (uses numpy.save)  and I would be interested in working with you to come up with code to allow adding vocabulary.</p>
",5,4,3736,2015-04-12 16:14:54,https://stackoverflow.com/questions/29591581/gensim-word2vec-augment-or-merge-pre-trained-vectors
Missing sentences from the Doc2vec representation,"<p>I am using the Doc2vec class from the gensim framework to compute the vectorial representation of each document in a corpus.</p>

<p>The corpus contains very short sentences, they can have even one word. I observed that for many sentences, especially the short ones, Doc2vec does not provide any representations. Could someone explain the reasons for this?</p>
","gensim, word2vec","<p>I had this same problem. I solved it by setting the parameter min_count=1.</p>

<pre><code>model = doc2vec.Doc2Vec(size=100)
</code></pre>

<p>became</p>

<pre><code>model = doc2vec.Doc2Vec(size=100, min_count=1)
</code></pre>

<p>Made my problem go away!</p>

<p>I found my answer in the comments of the doc2vec tutorial <a href=""http://radimrehurek.com/2014/12/doc2vec-tutorial/"" rel=""nofollow"">http://radimrehurek.com/2014/12/doc2vec-tutorial/</a></p>
",2,1,323,2015-04-16 13:35:30,https://stackoverflow.com/questions/29676413/missing-sentences-from-the-doc2vec-representation
Word2Vec and Gensim parameters equivalence,"<p>Gensim is an optimized python port of Word2Vec (see <a href=""http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/</a>)</p>

<p>I am currently using these vectors: <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow"">http://clic.cimec.unitn.it/composes/semantic-vectors.html</a></p>

<p>I am going to rerun the model training with gensim because there was some noisy tokens in their models. So i would like to find out what are some equivalent parameters for <code>word2vec</code> in <code>gensim</code></p>

<p>And the parameters they used from <code>word2vec</code> are:</p>

<ul>
<li>2-word context window, PMI weighting, no compression, 300K dimensions</li>
</ul>

<p>What is the gensim equivalence when i train a Word2Vec model?</p>

<p>Is it:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)
</code></pre>

<p><strong>Is there a PMI weight option in gensim?</strong></p>

<p><strong>What is the default min_count used in word2vec?</strong></p>

<p>There's another set of parameters from word2vec as such:</p>

<ul>
<li>5-word context window, 10 negative samples, subsampling, 400 dimensions.</li>
</ul>

<p><strong>Is there a negative samples parameter in gensim?</strong></p>

<p><strong>What is the parameter equivalence of subsampling in gensim?</strong></p>
","python, nlp, neural-network, gensim, word2vec","<ol>
<li><p>The paper you link to compares word embeddings from a number of schemes, including Continuous Bag of Words (CBOW). CBOW is one of the models implemented in Gensim's ""word2vec"" model. The paper also discusses word embeddings obtained from Singular Value Decomposition with various weighting schemes, some involving PMI. There is no equivalence between SVD and word2vec, but if you want to do an SVD in gensim, it's called ""LSA"" or ""Latent Semantic Analysis"" when done in natural language processing.</p></li>
<li><p>The <code>min_count</code> parameter is set to 5 by default, as can be seen <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow"">here</a>. </p></li>
<li><p>Negative Sampling and Hierarchical Softmax are two approximate inference methods for estimating a probability distribution over a discrete space (used when a normal softmax is too computationally expensive). Gensim's <code>word2vec</code> implements both. It uses hierarchical softmax by default, but you can use negative sampling by setting the hyperparameter <code>negative</code> to be greater than zero. This is documented in comments in gensim's code <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow"">here</a> as well. </p></li>
</ol>
",3,3,4841,2015-04-29 09:44:22,https://stackoverflow.com/questions/29939984/word2vec-and-gensim-parameters-equivalence
Problems in training text on AdaGram.jl,"<p>I'm a newbie to Julia programming language. I am trying to install Adaptive Skip-gram (AdaGram) model on my machine. I'm facing the following problems. Before training a model we need the tokenized file and a dictionary file. Now my question is, what is the input that should be given for tokenize.sh and dictionary.sh. Please let me know the actual way in which the generation of output files happen and also the extension of the same.</p>

<p>This is the website link I'm referring to : <a href=""https://github.com/sbos/AdaGram.jl"" rel=""nofollow"">https://github.com/sbos/AdaGram.jl</a> .
This is exactly similar to <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a></p>
","machine-learning, julia, word2vec","<p>The package provides a few shell scripts to pre-process the data and fit the model:
you have to call them from the shell, i.e., outside Julia.</p>

<pre><code># Install the package
julia -e 'Pkg.clone(""https://github.com/sbos/AdaGram.jl.git"")'
julia -e 'Pkg.build(""AdaGram"")'

# Download some text
wget http://www.gutenberg.org/ebooks/100.txt.utf-8

# Tokenize the text, and count the words
~/.julia/v0.3/AdaGram/utils/tokenize.sh 100.txt.utf-8 text.txt
~/.julia/v0.3/AdaGram/utils/dictionary.sh text.txt dictionary.txt

# Train the model
~/.julia/v0.3/AdaGram/train.sh text.txt dictionary.txt model
</code></pre>

<p>You can then use the model, from Julia:</p>

<pre><code>using AdaGram
vm, dict = load_model(""model"");
expected_pi(vm, dict.word2id[""hamlet""])
nearest_neighbors(vm, dict, ""hamlet"", 1, 10)
</code></pre>
",5,2,669,2015-05-02 12:31:32,https://stackoverflow.com/questions/30002329/problems-in-training-text-on-adagram-jl
Using word2vec to calculate similarity between users,"<p>I recently came to know about this tool called word2vec. For my current work, I need to find out users that are similar to a given user. A single user has entities associated with it like age, qualifications, insitute/organisaions, languages known and list of certains tags. If we consider a each of these entities/columns together as random chunk of words for a user, can we correspondingly calculate the vector value for that user and use these values to deduce similarities between users? Would a wiki training vector help us get meaningful results?Any other way to do it?</p>
","nlp, recommendation-engine, mahout-recommender, word2vec","<p>What you need is a simple unsupervised (or semi-supervised) clustering algorithm. word2vec with its pre-trained vectors may not be very helpful because institutions, etc. are unlikely to be in it.</p>

<p>Also, it seems that the number of ""aspects"" a user has it small, so you can simply have a clustering algorithm on vector representations where each dimension of your vector space is one of these aspects (age, qualification, organization, etc.).</p>

<p>A continuous space model like word2vec can be helpful if you want the similarity of users to reflect the similarity of these aspects (as opposed to exact equality).</p>

<p>If, for example, you want the qualification ""Python expert"" to be measured as something close to ""Scripting expert"", then go for word2vec. But if you are looking for exact matches among a finite predefined number of aspects, go for a simple clustering algorithm.</p>

<p>P.S. More detailed Q&amp;A on this topic should be on <a href=""https://stats.stackexchange.com/"">Cross Validated</a>.</p>
",4,2,1147,2015-05-07 08:01:11,https://stackoverflow.com/questions/30095167/using-word2vec-to-calculate-similarity-between-users
"word2vec, sum or average word embeddings?","<p>I'm using word2vec to represent a small phrase (3 to 4 words) as a unique vector, either by adding each individual word embedding or by calculating the average of word embeddings.</p>
<p>From the experiments I've done I always get the same cosine similarity. I suspect it has to do with the word vectors generated by word2vec being normed to unit length (Euclidean norm) after training? or either I have a BUG in the code, or I'm missing something.</p>
<p>Here is the code:</p>
<pre><code>import numpy as np
from nltk import PunktWordTokenizer
from gensim.models import Word2Vec
from numpy.linalg import norm
from scipy.spatial.distance import cosine

def pattern2vector(tokens, word2vec, AVG=False):
    pattern_vector = np.zeros(word2vec.layer1_size)
    n_words = 0
    if len(tokens) &gt; 1:
        for t in tokens:
            try:
                vector = word2vec[t.strip()]
                pattern_vector = np.add(pattern_vector,vector)
                n_words += 1
            except KeyError, e:
                continue
        if AVG is True:
            pattern_vector = np.divide(pattern_vector,n_words)
    elif len(tokens) == 1:
        try:
            pattern_vector = word2vec[tokens[0].strip()]
        except KeyError:
            pass
    return pattern_vector


def main():
    print &quot;Loading word2vec model ...\n&quot;
    word2vecmodelpath = &quot;/data/word2vec/vectors_200.bin&quot;
    word2vec = Word2Vec.load_word2vec_format(word2vecmodelpath, binary=True)
    pattern_1 = 'founder and ceo'
    pattern_2 = 'co-founder and former chairman'

    tokens_1 = PunktWordTokenizer().tokenize(pattern_1)
    tokens_2 = PunktWordTokenizer().tokenize(pattern_2)
    print &quot;vec1&quot;, tokens_1
    print &quot;vec2&quot;, tokens_2

    p1 = pattern2vector(tokens_1, word2vec, False)
    p2 = pattern2vector(tokens_2, word2vec, False)
    print &quot;\nSUM&quot;
    print &quot;dot(vec1,vec2)&quot;, np.dot(p1,p2)
    print &quot;norm(p1)&quot;, norm(p1)
    print &quot;norm(p2)&quot;, norm(p2)
    print &quot;dot((norm)vec1,norm(vec2))&quot;, np.dot(norm(p1),norm(p2))
    print &quot;cosine(vec1,vec2)&quot;,     np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2)))
    print &quot;\n&quot;
    print &quot;AVG&quot;
    p1 = pattern2vector(tokens_1, word2vec, True)
    p2 = pattern2vector(tokens_2, word2vec, True)
    print &quot;dot(vec1,vec2)&quot;, np.dot(p1,p2)
    print &quot;norm(p1)&quot;, norm(p1)
    print &quot;norm(p2)&quot;, norm(p2)
    print &quot;dot(norm(vec1),norm(vec2))&quot;, np.dot(norm(p1),norm(p2))
    print &quot;cosine(vec1,vec2)&quot;,     np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2)))


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>and here is the output:</p>
<pre><code>Loading word2vec model ...

Dimensions 200
vec1 ['founder', 'and', 'ceo']
vec2 ['co-founder', 'and', 'former', 'chairman']

SUM
dot(vec1,vec2) 5.4008677771
norm(p1) 2.19382594282
norm(p2) 2.87226958166
dot((norm)vec1,norm(vec2)) 6.30125952303
cosine(vec1,vec2) 0.857109242583


AVG
dot(vec1,vec2) 0.450072314758
norm(p1) 0.731275314273
norm(p2) 0.718067395416
dot(norm(vec1),norm(vec2)) 0.525104960252
cosine(vec1,vec2) 0.857109242583
</code></pre>
<p>I'm using the cosine similarity as defined here <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">Cosine Similarity (Wikipedia)</a>. The values for the norms and dot products are indeed different.</p>
<p>Why the cosine is the same?</p>
","cosine-similarity, word2vec, sentence-similarity","<p>Cosine measures the angle between two vectors and does not take the length of either vector into account. When you divide by the length of the phrase, you are just shortening the vector, not changing its angular position. So your results look correct to me. </p>
",8,11,7674,2015-05-09 16:23:20,https://stackoverflow.com/questions/30142345/word2vec-sum-or-average-word-embeddings
How to check if a key exists in a word2vec trained model or not,"<p>I have trained a word2vec model using a corpus of documents with Gensim. Once the model is training, I am writing the following piece of code to get the raw feature vector of a word say ""view"".</p>

<pre><code>myModel[""view""]
</code></pre>

<p>However, I get a KeyError for the word which is probably because this doesn't exist as a key in the list of keys indexed by word2vec. How can I check if a key exits in the index before trying to get the raw feature vector?</p>
","python, gensim, word2vec","<p>convert the model into vectors with </p>

<pre><code>word_vectors = model.wv
</code></pre>

<p>then we can use </p>

<pre><code>if 'word' in word_vectors.vocab
</code></pre>
",36,45,57000,2015-05-18 11:24:45,https://stackoverflow.com/questions/30301922/how-to-check-if-a-key-exists-in-a-word2vec-trained-model-or-not
Merging pretrained models in Word2Vec?,"<p>I have download 100 billion word Google news pretrained vector file. On top of that i am also training my own 3gb data producing another pretrained vector file. Both has 300 feature dimensions and more than 1gb size.</p>

<p>How do i merge these two huge pre-trained vectors?  or how do i train a new model and update vectors on top of another? I see that C based word2vec does not support batch training.</p>

<p>I am looking to compute word analogy from these two models. I believe that vectors learned from these two sources will produce pretty good results.</p>
","machine-learning, word2vec","<p>There's no straightforward way to merge the end-results of separate training sessions.</p>

<p>Even for the exact same data, slight randomization from initial seeding or thread scheduling jitter will result in diverse end states, making vectors only fully comparable within the same session.</p>

<p>This is because every session finds <em>a</em> useful configuration of vectors... but there are many equally useful configurations, rather than a single best. </p>

<p>For example, whatever final state you reach has many rotations/reflections that can be exactly as good on the training prediction task, or perform exactly as well on some other task (like analogies-solving). But most of these possible alternatives will not have coordinates that can be mixed-and-matched for useful comparisons against each other. </p>

<p>Preloading your model with data from prior training runs <em>might</em> improve the results after more training with new data, but I'm not aware of any rigorous testing of this possibility. The effect likely depends on your specific goals, your parameter choices, and how much the new and old data are similar, or representative of the eventual data against which the vectors will be used. </p>

<p>For example, if the Google News corpus is unlike your own training data, or the text you'll be using the word-vectors to understand, using it as a starting point might just slow or bias your training. On the other hand, if you train on your new data long enough, eventually any influence of the preloaded values could be diluted to nothingness. (If you really wanted a 'blended' result, you might have to simultaneously train on the new data with an interleaved goal for nudging the vectors back towards the prior-dataset values.)</p>

<p>Ways to combine the results from independent sessions might make a good research project. Maybe the method used in the word2vec language-translation projects â€“ learning a projection between vocabulary spaces â€“ could also 'translate' between the different coordinates of different runs. Maybe locking some vectors in place, or training on the dual goals of 'predict the new text' and 'stay close to the old vectors' would give meaningfully improved combined results. </p>
",13,8,3859,2015-05-27 12:37:02,https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec
Doc2vec MemoryError,"<p>I am using the doc2vec model from teh gensim framework to represent a corpus of 15 500 000  short documents (up to 300 words): </p>

<pre><code>gensim.models.Doc2Vec(sentences, size=400, window=10, min_count=1, workers=8 )
</code></pre>

<p>After creating the vectors there are  more than  18 000 000 vectors representing words and documents. </p>

<p>I want to find the most similar items (words or documents) for a given item: </p>

<pre><code> similarities = model.most_similar(â€˜uid_10693076â€™)
</code></pre>

<p>but I get a MemoryError when the similarities are computed:</p>

<pre><code>Traceback (most recent call last):

   File ""article/test_vectors.py"", line 31, in &lt;module&gt; 
    similarities = model.most_similar(item) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 639, in most_similar 
    self.init_sims() 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 827, in init_sims 
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL) 
</code></pre>

<p>I have a Ubuntu machine  with 60GB Ram and 70GB swap . I checked the memory allocation (in htop) and I observed that never the memory was completely used. I also set to unlimited the the maximum address space which may be locked in memory in python:</p>

<pre><code>resource.getrlimit(resource.RLIMIT_MEMLOCK)
</code></pre>

<p>Could someone explain the reason for this MemoryError? In my opinion the available memory should be enough for doing this computations. Could be some memory limits in python or OS?</p>

<p>Thanks in advance!</p>
","python, memory, gensim, word2vec","<p>18M vectors * 400 dimensions * 4 bytes/float = 28.8GB for the model's syn0 array (trained vectors)</p>

<p>The syn1 array (hidden weights) will also be 28.8GB â€“ even though syn1 doesn't really need entries for doc-vectors, which are never target-predictions during training.</p>

<p>The vocabulary structures (vocab dict and index2word table) will likely add another GB or more. So that's all your 60GB RAM. </p>

<p>The syn0norm array, used for similarity calculations, will need another 28.8GB, for a total usage of around 90GB. It's the syn0norm creation where you're getting the error. But even if syn0norm creation succeeded, being that deep into virtual memory would likely ruin performance. </p>

<p>Some steps that might help:</p>

<ul>
<li><p>Use a min_count of at least 2: words appearing once are unlikely to contribute much, but likely use a lot of memory. (But since words are a tiny portion of your syn0, this will only save a little.)</p></li>
<li><p>After training but before triggering init_sims(), discard the the syn1 array. You won't be able to train more, but your existing word/doc vectors remain accessible. </p></li>
<li><p>After training but before calling most_similar(), call init_sims() yourself with a replace=True parameter, to discard the non-normalized syn0 and replace it with the syn0norm. Again you won't be able to train more, but you'll save the syn0 memory. </p></li>
</ul>

<p>In-progress work separating out the doc and word vectors, which will appear in gensim past verstion 0.11.1, should also eventually offer some relief. (It'll shrink the syn1 to only include word entries, and allow doc-vectors to come from a file-backed (memmap'd) array.)</p>
",15,6,4215,2015-05-27 16:53:24,https://stackoverflow.com/questions/30488695/doc2vec-memoryerror
How to train Word2vec on very large datasets?,"<p>I am thinking of training word2vec on huge large scale data of more than 10 TB+ in size on web crawl dump. </p>

<p>I personally trained c implementation GoogleNews-2012 dump (1.5gb) on my iMac took about 3 hours to train and generate vectors (impressed with speed). I did not try python implementation though :( I read somewhere that generating vectors on wiki dump (11gb) of 300 vector length takes about 9 days to generate. </p>

<ol>
<li><p>How to speed up word2vec? Do i need to use distributed models or what type of hardware i need to do it within 2-3 days? i have iMac with 8gb ram.</p></li>
<li><p>Which one is faster? Gensim python or C implemention?</p></li>
</ol>

<p>I see that word2vec implementation does not support GPU training.</p>
","python, c, machine-learning, word2vec","<p>There are a number of opportunities to create Word2Vec models at scale. As you pointed out, candidate solutions are distributed (and/or multi-threaded) or GPU. This is not an exhaustive list but hopefully you get some ideas as to how to proceed.</p>

<p>Distributed / Multi-threading options:</p>

<ul>
<li><a href=""https://radimrehurek.com/gensim/index.html"">Gensim</a> uses Cython where it matters, and is equal to, or not
much slower than C implementations. Gensim's multi-threading works
well, and using a machine with ample memory and a large number of
cores significantly decreases vector generation time. You may want to
investigate using Amazon EC2 16 or 32-core instances.</li>
<li><a href=""http://deepdist.com/"">Deepdist</a> can utilize gensim and Spark to distribute gensim workloads across a cluster. Deepdist also has some clever SGD
optimizations which synchronize gradient across nodes. If you use
multi-core machines as nodes, you can take advantage of both
clustering and multi-threading.</li>
</ul>

<p>A number of Word2Vec GPU implementations exist. Given the large dataset size, and limited GPU memory you may have to consider a clustering strategy.</p>

<ul>
<li><a href=""https://github.com/BIDData/BIDMach"">Bidmach</a> is apparently very fast (documentation is however lacking, and admittedly I've struggled to get it working).</li>
<li><a href=""http://deeplearning4j.org/"">DL4J</a> has a Word2Vec implementation but the team has yet to implement cuBLAS gemm and it's relatively slow vs CPUs.</li>
<li><a href=""http://keras.io/"">Keras</a> is a Python deep learning framework that utilizes Theano. While it does not implement word2vec per se, it does implement an embedding layer and can be used to create and query word vectors.</li>
</ul>

<p>There are a number of other CUDA implementations of Word2Vec, at varying degrees of maturity and support:</p>

<ul>
<li><a href=""https://github.com/whatupbiatch/cuda-word2vec"">https://github.com/whatupbiatch/cuda-word2vec</a> [memory mgmt looks great, though non-existant documentation on how to create datasets]</li>
<li><a href=""https://github.com/fengChenHPC/word2vec_cbow"">https://github.com/fengChenHPC/word2vec_cbow</a> [super-fast, but GPU memory issues on large datasets]</li>
</ul>

<p>I believe the SparkML team has recently got going a prototype cuBLAS-based Word2Vec implementation. You may want to investigate this.</p>
",35,20,14349,2015-06-01 12:46:10,https://stackoverflow.com/questions/30573873/how-to-train-word2vec-on-very-large-datasets
Are word-vector orientations universal?,"<p>I have recently been experimenting with Word2Vec and I noticed whilst trawling through forums that a lot of other people are also creating their own vectors from their own databases.</p>

<p>This has made me curious as to how vectors look across databases and whether vectors take a universal orientation?</p>

<p>I understand that the vectors are created as a result of the context they are found in the corpus. So in that sense perhaps you wouldn't expect words to have the same orientation across databases. However, if the language of the documents are constant, then the contexts should be at least somewhat similar across different databases (excluding ambiguous words like bank (for money) and (river) bank). And if they are somewhat similar, it seems plausible that as we look at more commonly occurring words their direction may converge?</p>
","nlp, word2vec","<p>As outlined in the comments, ""orientation"" is not a well-defined concept in this context.  A traditional word vector space has one dimension for each term.</p>

<p>In order for word vectors to be compatible, they will need to have the same term order.  This is typically not the case between different vector collections, unless you build them from exactly the same documents in exactly the same order with exactly the same algorithms.</p>

<p>You could construe ""orientation"" as ""vectors with the same terms in the same order"" but the parallel to three-dimensional geometry is already strained as it is.  It's probably better to avoid this term.</p>

<p>Given two collections of vectors from reasonably representative input in a known language, the most frequent terms will probably have similar distributions, so you could perhaps derive a mapping from one representation to another with some accuracy (see <a href=""http://en.wikipedia.org/wiki/Zipf&#39;s_law"" rel=""nofollow"">Zipf's Law</a>).  Back in the <a href=""http://en.wikipedia.org/wiki/Long_tail"" rel=""nofollow"">long tail</a> of rare terms, you will certainly not be able to identify any useful mappings.</p>
",1,0,94,2015-06-12 04:09:00,https://stackoverflow.com/questions/30795152/are-word-vector-orientations-universal
semantic matching strings - using word2vec or s-match?,"<p>I have this problem of matching two strings for 'more general', 'less general', 'same meaning', 'opposite meaning' etc.</p>

<p>The strings can be from any domain. Assume that the strings can be from people's emails. </p>

<p>To give an example, </p>

<pre><code>String 1 = ""movies""
String 2 = ""Inception""
</code></pre>

<p>Here I should know that Inception is less general than movies (sort of is-a relationship)</p>

<pre><code>String 1 = ""Inception""
String 2 = ""Christopher Nolan""
</code></pre>

<p>Here I should know that Inception is less general than Christopher Nolan </p>

<pre><code>String 1 = ""service tax""
String 2 = ""service tax 2015""
</code></pre>

<p>At a glance it appears to me that S-match will do the job. But I am not sure if S-match can be made to work on knowledge bases other than WordNet or GeoWordNet (as mentioned in their page). </p>

<p>If I use <code>word2vec</code> or <code>dl4j</code>, I guess it can give me the similarity scores. But does it also support telling a string is <code>more general</code> or <code>less general</code> than the other?</p>

<p>But I do see word2vec can be based on a training set or large corpus like wikipedia etc.</p>

<p>Can some one throw light on the way to go forward?</p>
","semantic-analysis, word2vec","<p>The current usage of machine learning methods such as <code>word2vec</code> and <code>dl4j</code> for modelling words are based on <a href=""http://www.aclweb.org/aclwiki/index.php?title=Distributional_Hypothesis"" rel=""nofollow noreferrer"">distributional hypothesis</a>. They train models of words and phrases based on their context. There is no ontological aspects in these word models. At its best trained case a model based on these tools can say if two words can appear in similar contexts. That is how their similarity measure works.</p>
<p>The Mikolov papers (<a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">a</a>, <a href=""http://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">b</a> and <a href=""http://research.microsoft.com/pubs/189726/rvecs.pdf"" rel=""nofollow noreferrer"">c</a>) which suggests that these models can learn &quot;Linguistic Regularity&quot; doesn't have any ontological test analysis, it only suggests that these models are capable of predicting &quot;similarity between members of the word pairs&quot;. This kind of prediction doesn't help your task. These models are even incapable of recognising <em>similarity</em> in contrast with <em>relatedness</em> (e.g. read this page <a href=""http://www.cl.cam.ac.uk/%7Efh295/simlex.html"" rel=""nofollow noreferrer"">SimLex test set</a>).</p>
<p>I would say that you need an ontological database to solve your problem. More specifically about your examples, it seems for <code>String 1</code> and <code>String 2</code> in your examples:</p>
<pre><code>String 1 = &quot;a&quot;
String 2 = &quot;b&quot;
</code></pre>
<p>You are trying to check <a href=""https://en.wikipedia.org/wiki/Entailment_(pragmatics)"" rel=""nofollow noreferrer"">entailment</a> relations in sentences:</p>
<blockquote>
<p>(1) &quot;<em>c</em> is <em>b</em>&quot;</p>
<p>(2) &quot;<em>c</em> is <em>a</em>&quot;</p>
<p>(3) &quot;<em>c</em> is related to <em>a</em>&quot;.</p>
</blockquote>
<p>Where:</p>
<blockquote>
<p>(1) entails (2)</p>
</blockquote>
<p>or</p>
<blockquote>
<p>(1) entails (3)</p>
</blockquote>
<p>In your two first examples, you can probably use semantic knowledge bases to solve the problem. But your third example will probably need a syntactical parsing before understanding the difference between two phrases. For example, these phrases:</p>
<blockquote>
<p>&quot;men&quot;</p>
<p>&quot;all men&quot;</p>
<p>&quot;tall men&quot;</p>
<p>&quot;men in black&quot;</p>
<p>&quot;men in general&quot;</p>
</blockquote>
<p>It needs a logical understanding to solve your problem. However, you can analyse that based on <em>economy of language</em>, adding more words to a phrase usually makes it <em>less general</em>. Longer phrases are less general comparing to shorter phrases. It doesn't give you a precise tool to solve the problem, but it can help to analyse some phrases without special words such as <code>all</code>, <code>general</code> or <code>every</code>.</p>
",1,1,1296,2015-06-12 06:13:56,https://stackoverflow.com/questions/30796385/semantic-matching-strings-using-word2vec-or-s-match
How to apply word2vec on images?,"<p>I have been studying word2vec model by Google. I was able to generate vectors for text word corpus for maximum 300 dimensions. It is a very impressive tool and accuracy goes much further, on big data.</p>

<p>I am curious, is there any way to use word2vec to generate vectors on grayscale images. I am sure the approach is same, you generate vectors based on pixel intensity and then compute a cosine similarity. </p>

<p>I am trying to do build a model to compute similarity distance on grayscale images. Any library is capable of doing this besides word2vec or glove that works on text?</p>
","python, image-processing, machine-learning, word2vec","<p>Word2vec is not a good model for images, however I think what you really need is a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision"" rel=""nofollow noreferrer"">bag of word model</a>. In a basic method of image comparison, you convert images to a list of key point features (e.g. SIFT, SURF or etc.), then you match clusters of points with each other (e.g. <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html"" rel=""nofollow noreferrer"">FLANN</a>).</p>

<p>The high amount of features in an image and uncertainty of each point representation makes it difficult to use a basic one layer network learning such as word2vec for image recognition. You may find better examples in this <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_table_of_contents_ml/py_table_of_contents_ml.html"" rel=""nofollow noreferrer"">tutorials</a>.</p>

<p><strong>UPDATE after 3 years</strong>: I should also mention ConvNets and several pre-trained models available now which you can extract visual features from pixels. </p>
",3,4,2718,2015-06-18 12:14:00,https://stackoverflow.com/questions/30915056/how-to-apply-word2vec-on-images
Python34 word2vec.Word2Vec OverFlowError,"<p>I'm studying word2vec, but when I use word2vec to train text data, occur OverFlowError with Numpy.</p>

<p>the message is,</p>

<pre><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)]
Warning (from warnings module):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 636
    warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""
UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Python34\lib\threading.py"", line 920, in _bootstrap_inner
    self.run()
  File ""C:\Python34\lib\threading.py"", line 868, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 675, in worker_loop
    if not worker_one_job(job, init):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 666, in worker_one_job
    job_words = self._do_train_job(items, alpha, inits)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 623, in _do_train_job
    tally += train_sentence_sg(self, sentence, alpha, work)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 112, in train_sentence_sg
    word_vocabs = [model.vocab[w] for w in sentence if w in model.vocab and
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 113, in &lt;listcomp&gt;
    model.vocab[w].sample_int &gt; model.random.randint(2**32)]
  File ""mtrand.pyx"", line 935, in mtrand.RandomState.randint (numpy\random\mtrand\mtrand.c:9520)
OverflowError: Python int too large to convert to C long
</code></pre>

<p>Can you tell me the cases?</p>

<p>My machine is x64 and OS is windows 7, but python34 is 32bit. numpy and scipy are also 32bit.</p>
","python-3.x, windows-7-x64, integer-overflow, gensim, word2vec","<p>I get this as well. It looks like gensim has a potential workaround in the dev branch.</p>

<p><a href=""https://github.com/piskvorky/gensim/commit/726102df66000f2afcea82d95634b055e6521dc8"" rel=""nofollow"">https://github.com/piskvorky/gensim/commit/726102df66000f2afcea82d95634b055e6521dc8</a></p>

<p>This doesn't solve the core issue of navigating between different hardware and install int sizes, but I think it should alleviate issues with this particular line.</p>

<p>The necessary change involves switching out</p>

<p><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)</code></p>

<p>for</p>

<p><code>model.vocab[w].sample_int &gt; model.random.rand() * 2**32</code></p>

<p>This avoids the 64 bit / 32 bit int issue created in randint.</p>

<p>UPDATE: I manually incorporated that change into my gensim install and it prevents the error.</p>
",1,2,1537,2015-07-08 07:48:57,https://stackoverflow.com/questions/31286574/python34-word2vec-word2vec-overflowerror
Using semantic word representation (e.g. word2vec) to build a classifier,"<p>I want to build a classifier for forum posts that will automatically categorize these
posts into some defined categories(so multiclass classification not only binary
classification) by using semantic word representations. For this task I want to make use
of word2vec and doc2vec and check the feasability of using these models to support a fast
selection of training data for the classifier. At this moment I have tried both models and
they work like charm. However, as I do not want to manually label each sentence to predict
what is it describing, I want to leave this task for the word2vec or doc2vec models. So,
my question is : what algorithm can I use in Python for the classifier? ( I was thinking
to apply some clustering over word2vec or doc2vec - manually label each cluster (this
would require some time and is not the best solution). Previously, I made use of
""LinearSVC""(from SVM) and OneVsRestClassifier, however, I labeled each sentence (by
manually training a vector ""y_train"" ) in order to predict to which class a new test
sentence would belong to. What would be a good alghorithm and method in python to use for
this type of classifier(making use of semantic word representations to train data)?</p>
","python, classification, word2vec","<p>The issue with things like word2vec/doc2vec and so on - actually any usupervised classifier - is that it just uses context. So, for example if I have a sentence like ""Today is a hot day"" and another like ""Today is a cold day"" it thinks hot and cold are very very similar and should be in the same cluster.</p>

<p>This makes it pretty bad for tagging. Either way, there is a good implementation of Doc2Vec and Word2Vec in gensim module for python - you can quickly use the google-news dataset's prebuilt binary and test whether you get meaningful clusters.</p>

<p>The other way you could try is implement a simple lucene/solr system on your computer and begin tagging a few sentences randomly. Over time lucene/solr will suggest tags clearfor your document, and they do come out to be pretty decent tags if your data is not really bad.</p>

<p>The issue here is the problem youre trying to solve isnt particularly easy nor is completely solvable - If you have very good/clear data, then you may be able to auto classify about 80-90% of your data ... but if it is bad, you wont be able to auto classify it much.</p>
",2,1,1206,2015-07-13 18:32:23,https://stackoverflow.com/questions/31390838/using-semantic-word-representation-e-g-word2vec-to-build-a-classifier
"CUDA code runs when compiled with sm_35, but fails with sm_30","<p>The GPU device that I have is GeForce GT 750M, which I found is compute capability 3.0. I downloaded the CUDA code found here: (<a href=""https://github.com/fengChenHPC/word2vec_cbow"" rel=""nofollow"">https://github.com/fengChenHPC/word2vec_cbow</a>. Its makefile had the flag -arch=sm_35.</p>

<p>Since my device is compute capability 3.0, I changed the flag to -arch=sm_30. It compiled fine, but when I run the code, I get the following error:</p>

<pre><code>word2vec.cu 449 : unspecified launch failure

word2vec.cu 449 : unspecified launch failure
</code></pre>

<p>It shows it multiple times, because there are multiple CPU threads launching the CUDA kernel. Please note that the threads do not use different streams to launch the kernel, so the kernel launches are all in order.</p>

<p>Now, when I let the flag be, i.e. -arch=sm_35, then the code runs fine. Can someone please explain why the code won't run when I set the flag to match my device?</p>
","debugging, cuda, gpgpu, word2vec","<p>Unfortunately your conclusion that the code works when compiled for sm_35 and run on an sm_30 GPU is incorrect. The culprit is this:</p>

<pre><code>void cbow_cuda(long window, long negative, float alpha, long sentence_length, 
               int *sen, long layer1_size, float *syn0, long hs, float *syn1, 
               float *expTable, int *vocab_codelen, char *vocab_code,
               int *vocab_point, int *table, long table_size, 
               long vocab_size, float *syn1neg){
    int blockSize = 256;
    int gridSize = (sentence_length)/(blockSize/32);
    size_t smsize = (blockSize/32)*(2*layer1_size+3)*sizeof(float);
//printf(""sm size is %d\n"", smsize);
//fflush(stdout);
    cbow_kernel&lt;1&gt;&lt;&lt;&lt;gridSize, blockSize, smsize&gt;&gt;&gt;
                   (window, negative, alpha, sentence_length, sen,
                    layer1_size, syn0, syn1, expTable, vocab_codelen,
                    vocab_code, vocab_point, table, table_size,
                    vocab_size, syn1neg);
}
</code></pre>

<p>This code will silently fail if the kernel launch fails because of incomplete API error checking. And the kernel launch does fail if you build for sm_35 and run on sm_30. If you change the code of that function to this (adding kernel launch error checking):</p>

<pre><code>void cbow_cuda(long window, long negative, float alpha, long sentence_length, 
               int *sen, long layer1_size, float *syn0, long hs, float *syn1, 
               float *expTable, int *vocab_codelen, char *vocab_code,
               int *vocab_point, int *table, long table_size, 
               long vocab_size, float *syn1neg){
    int blockSize = 256;
    int gridSize = (sentence_length)/(blockSize/32);
    size_t smsize = (blockSize/32)*(2*layer1_size+3)*sizeof(float);
//printf(""sm size is %d\n"", smsize);
//fflush(stdout);
    cbow_kernel&lt;1&gt;&lt;&lt;&lt;gridSize, blockSize, smsize&gt;&gt;&gt;
                   (window, negative, alpha, sentence_length, sen,
                    layer1_size, syn0, syn1, expTable, vocab_codelen,
                    vocab_code, vocab_point, table, table_size,
                    vocab_size, syn1neg);
    checkCUDAError( cudaPeekAtLastError() );
}
</code></pre>

<p>and compile and run it for sm_35, you should get this on an sm_30 device:</p>

<pre><code>~/cbow/word2vec_cbow$ make
nvcc word2vec.cu -o word2vec -O3 -Xcompiler -march=native -w  -Xptxas=""-v"" -arch=sm_35 -lineinfo
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z11cbow_kernelILx1EEvllflPKilPVfS3_PKfS1_PKcS1_S1_llS3_' for 'sm_35'
ptxas info    : Function properties for _Z11cbow_kernelILx1EEvllflPKilPVfS3_PKfS1_PKcS1_S1_llS3_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 34 registers, 448 bytes cmem[0], 8 bytes cmem[2]

~/cbow/word2vec_cbow$ ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 7 -negative 1 -hs 1 -sample 1e-3 -threads 1 -binary 1 -save-vocab voc #&gt; out 2&gt;&amp;1
Starting training using file text8
Vocab size: 71290
Words in train file: 16718843
vocab size = 71290
cbow.cu 114 : invalid device function
</code></pre>

<p>ie. the kernel launch failed because no appropriate device code was found in the CUDA cubin payload in your application. This also answers your <a href=""https://stackoverflow.com/q/31391154/681865"">earlier question</a> about why the output of this code is incorrect. The analysis kernel simply never runs on your hardware when built with the default options.</p>

<p>If I build this code for sm_30 and run it on a GTX 670 with 2gb of memory (compute capability 3.0), I get this:</p>

<pre><code>~/cbow/word2vec_cbow$ make
nvcc word2vec.cu -o word2vec -O3 -Xcompiler -march=native -w  -Xptxas=""-v"" -arch=sm_30 -lineinfo
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z11cbow_kernelILx1EEvllflPKilPVfS3_PKfS1_PKcS1_S1_llS3_' for 'sm_30'
ptxas info    : Function properties for _Z11cbow_kernelILx1EEvllflPKilPVfS3_PKfS1_PKcS1_S1_llS3_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 34 registers, 448 bytes cmem[0], 12 bytes cmem[2]

~/cbow/word2vec_cbow$ ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 7 -negative 1 -hs 1 -sample 1e-3 -threads 1 -binary 1 -save-vocab voc #&gt; out 2&gt;&amp;1
Starting training using file text8
Vocab size: 71290
Words in train file: 16718843
vocab size = 71290
Alpha: 0.000009  Progress: 100.00%  Words/thread/sec: 1217.45k
</code></pre>

<p>ie. the code runs correctly to completion without any errors. I can't tell you why you are not able to get the code to run on your hardware because I cannot reproduce your error on my hardware. You will need to do some debugging on your own to find the root cause of that.</p>
",2,1,1894,2015-07-15 19:33:25,https://stackoverflow.com/questions/31439471/cuda-code-runs-when-compiled-with-sm-35-but-fails-with-sm-30
Combining Doc2Vec sentences into paragraph vectors,"<p>In Gensim's Doc2Vec, how do you combine sentence vectors to make a single vector for a paragraph?  I realise you can train on the entire paragraph, but it would obviously be better to train on individual sentences, for context, etc. (I think...?)</p>

<p>Any advice or normal use case?</p>

<p>Also, how would I retrieve sentence/paragraph vectors from the model?</p>
","gensim, word2vec","<p>Doc2Vec's architecture itself doesn't involve any parsing and it makes sense to train/test on the entire paragraph. </p>

<p>In original <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow"">paper</a>, author shows results with just treating entire paragraph as one sentence, outperforming existing techniques.</p>
",2,1,678,2015-08-05 08:47:26,https://stackoverflow.com/questions/31827623/combining-doc2vec-sentences-into-paragraph-vectors
How can I get vector from the saved vector text file?,"<p>I have already train the model by <code>Word2vec</code> in Python, and save the vector(which is size = 300) corresponding for all those words as in <code>vec.txt</code> file, now if I got one word, which I need get the corresponding vectors and do some aclatue for those vectors.
But I do not know how to get those vectors from the txt file.<br>
Following are part of <code>vec.txt</code>:</p>

<blockquote>
  <p>new -0.000113 0.000211 -0.000170 0.000346 -0.000251 -0.001012 0.001647 -0.001331 0.001267 0.000876 0.001243 -0.000600 -0.000667 -0.001241 0.001204 -0.000726 -0.001023 0.001476 -0.001380 0.000065 0.000145 0.001451 0.001275 0.001482 -0.001011 0.001131 0.001095 -0.001637 0.000289 -0.000846 0.001599 -0.001027 -0.000768 -0.000595 0.000825 0.000639 -0.001097 -0.001654 -0.000977 -0.000351 0.001410 0.001182 0.000318 -0.000454 -0.000622 0.000343 0.000508 -0.000258 0.001347 0.000362 0.000372 -0.000208 0.000896 0.001408 0.001412 -0.001566 0.001642 -0.000865 -0.000656 0.001095 -0.001503 -0.000483 0.000465 0.001352 0.000602 -0.000017 0.000011 0.001219 0.001363 0.001296 -0.000474 0.000718 -0.000544 0.000779 -0.001225 -0.001141 -0.001061 -0.000550 0.001446 0.000735 0.001267 0.001269 0.001115 0.001023 0.001564 -0.000947 0.000320 -0.001648 0.001605 -0.000900 -0.000734 -0.000344 0.000376 -0.001550 0.001241 0.000294 0.000207 -0.001420 0.000297 0.001122 0.000834 -0.001423 -0.001499 0.001060 0.000898 0.001609 -0.000512 -0.001185 -0.001648 0.001328 0.001620 0.001344 0.000160 0.000567 -0.001665 -0.000246 -0.000274 0.001234 0.000659 0.000144 -0.001370 0.001457 -0.000025 0.001117 0.000249 0.000137 -0.000048 -0.000527 -0.000428 0.000305 -0.001058 0.001374 0.000369 0.001588 0.000085 0.000749 -0.001584 0.000918 -0.001196 0.000424 0.000651 -0.001387 0.000815 -0.000959 0.001261 -0.001246 0.000258 -0.000887 0.001583 0.000102 -0.001337 0.000428 -0.000004 0.000131 0.000487 -0.001659 0.000093 0.001464 0.000356 -0.001479 -0.001217 -0.000626 0.001019 0.001179 -0.000599 0.000825 0.000858 -0.000841 0.000399 -0.001587 -0.000923 -0.000496 -0.000668 0.000567 0.001308 0.001042 -0.000676 0.001292 -0.001345 0.000113 0.000021 -0.000577 0.000292 0.001052 -0.001646 -0.001186 0.000184 0.000747 -0.001190 -0.001472 0.000535 0.000199 0.000522 -0.000229 -0.000277 -0.000136 0.001568 -0.000509 -0.000065 0.000305 0.001245 -0.001371 -0.001378 -0.000742 0.000411 -0.000461 0.001547 0.001272 0.001339 0.000181 -0.001335 0.000257 -0.000001 0.001494 -0.001379 -0.000635 -0.001195 -0.001483 0.000744 -0.000203 0.000407 -0.000061 -0.001561 0.000239 0.000370 0.000227 -0.000043 -0.001377 -0.000961 -0.001038 0.001575 0.000618 0.000218 0.001260 0.000971 0.000572 0.001307 0.000362 -0.000844 -0.000281 0.000440 -0.001122 0.000097 0.001392 0.000427 0.000913 -0.000537 -0.000889 0.000799 -0.001422 0.001501 0.001130 -0.000633 -0.000747 0.001198 0.000235 0.001335 0.000273 -0.000906 -0.000551 0.000527 0.000900 -0.001294 0.000451 -0.001180 -0.001376 0.000287 0.001508 0.000068 0.000225 0.000504 0.000137 -0.001071 -0.001383 0.001414 -0.000946 0.001358 -0.001146 -0.000623 0.000656 0.001605 0.000519 0.000106 0.001341 -0.000560 -0.001359 0.000721 0.001653 -0.000643 0.000625 0.000133 -0.000321 0.001230 0.000046 -0.001030 0.000752 0.000108 0.001263 0.000562 0.001224</p>
</blockquote>

<p>if I got 'new', i need get 300 corresponding vectors for new from <code>vec.txt</code> file.</p>
","python, vector, word2vec","<p>You can read the file, split it at spaces, remove the first word ('new') and convert the resulting 300 strings to floats. </p>

<pre><code>with open('vec.txt') as f:
   file_string = f.read().strip()
   numbers = [float(s) for s in file_string.split()[1:]]
   print numbers
</code></pre>
",0,1,143,2015-08-06 08:06:21,https://stackoverflow.com/questions/31850162/how-can-i-get-vector-from-the-saved-vector-text-file
Error while loading Word2Vec model in gensim,"<p>I'm getting an <code>AttributeError</code> while loading the gensim model available at word2vec repository:</p>

<pre><code>from gensim import models
w = models.Word2Vec()
w.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-8219e36ba1f6&gt; in &lt;module&gt;()
----&gt; 1 w[""queen""]

C:\Anaconda64\lib\site-packages\gensim\models\word2vec.pyc in __getitem__(self, word)
    761 
    762         """"""
--&gt; 763         return self.syn0[self.vocab[word].index]
    764 
    765 

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Is this a known issue ?</p>
","python, gensim, word2vec","<p>Fixed the problem with:</p>

<pre><code>from gensim import models
w = models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]
</code></pre>
",11,10,33011,2015-08-19 17:17:41,https://stackoverflow.com/questions/32101795/error-while-loading-word2vec-model-in-gensim
spark word2vec window size,"<p>Is there anyway to change the window size used in <code>pyspark.mllib.feature.Word2Vec</code> or is it permanently fixed at 5?  This seems like a fairly important feature.</p>

<p>I don't see the option here:
<a href=""https://spark.apache.org/docs/1.4.1/api/scala/index.html#org.apache.spark.mllib.feature.Word2Vec"" rel=""nofollow"">https://spark.apache.org/docs/1.4.1/api/scala/index.html#org.apache.spark.mllib.feature.Word2Vec</a></p>
","machine-learning, apache-spark, word2vec","<p>Thats correct, looking at the Word2Vec code we can see that it is a private val. If you wanted to you could maybe override Word2Vec to change the window size. I've created a JIRA ( <a href=""https://issues.apache.org/jira/browse/SPARK-10299"" rel=""noreferrer"">https://issues.apache.org/jira/browse/SPARK-10299</a> ) to allow the window size to be set, since 1.5 is already in the RC phase this probably won't make it in until 1.6 time.</p>
",6,3,960,2015-08-26 16:30:05,https://stackoverflow.com/questions/32231975/spark-word2vec-window-size
CNN: initializing unknown words from word2vec,"<p>I came across these slides, presentation from Kim about CNN's using word2vec:
<a href=""http://www.people.fas.harvard.edu/~yoonkim/data/Kim_EMNLP_2014_slides.pdf"" rel=""nofollow"">http://www.people.fas.harvard.edu/~yoonkim/data/Kim_EMNLP_2014_slides.pdf</a></p>

<p>On slide 20, the fourth bullet point reads:</p>

<pre><code>Words not in word2vec are initialized randomly from U[âˆ’a, a] 
where a is chosen such that the unknown words have the
same variance as words already in word2vec.
</code></pre>

<p>Now I am wondering how ""a"" is being computed and also how the entire vector for the entirely unknown word is computed.</p>
","convolution, deep-learning, word2vec","<p>According to an <a href=""https://groups.google.com/forum/#!topic/word2vec-toolkit/TgMeiJJGDc0"" rel=""nofollow"">answer</a> by Mikolov himself, you can initialize the vector based on the space described by the infrequent words. In his answer he mentions that you should average the infrequent words and in that way build the unknown token.</p>

<p>Following up this idea, I think that <em>a</em> refers to the radius of the infrequent words space. What you could do is get the centroid <strong>C</strong> of the infrequent words (through a mean), calculate the diameter <strong>2*a</strong> of the infrequent vector space <strong>Q</strong>, and generate a random vector <strong>u</strong> through uniformly distributed samples located within <strong>Q</strong>.</p>
",6,3,1066,2015-08-28 18:14:57,https://stackoverflow.com/questions/32277377/cnn-initializing-unknown-words-from-word2vec
How to find the closest word to a vector using word2vec,"<p>I have just started using Word2vec and I was wondering how can we find the closest word to a vector suppose.
 I have this vector which is the average vector for a set of vectors:</p>

<pre><code>array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
</code></pre>

<p>Is there a straight forward way to find the most similar word in my training data to this vector?</p>

<p>Or the only solution is to calculate the cosine similarity between this vector and the vectors of each word in my training data, then select the closest one?</p>

<p>Thanks.</p>
","python, text-mining, data-analysis, word2vec","<p>For <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a> implementation of word2vec there is <code>most_similar()</code> function that lets you find words semantically close to a given word:</p>

<pre><code>&gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'])
[('queen', 0.50882536), ...]
</code></pre>

<p>or to it's vector representation:</p>

<pre><code>&gt;&gt;&gt; your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
&gt;&gt;&gt; model.most_similar(positive=[your_word_vector], topn=1))
</code></pre>

<p>where <code>topn</code> defines the desired number of returned results.</p>

<p>However, my gut feeling is that function does exactly the same that you proposed, i.e. calculates cosine similarity for the given vector and each other vector in the dictionary (which is quite inefficient...)</p>
",52,33,48363,2015-09-24 11:03:04,https://stackoverflow.com/questions/32759712/how-to-find-the-closest-word-to-a-vector-using-word2vec
DeepLearning4J NoSuchMethodError,"<p>I'm new to neural networks and NLP. I've found this library: DeepLearning4J. I'm trying to get it to work but whenever I execute this instruction:</p>

<pre><code>Collection&lt;String&gt; similar = vec.wordsNearest(""word_to_search"", 10);
</code></pre>

<p>If the word I'm searching is mapped into the network I get the following exception:</p>

<pre><code>java.lang.IllegalArgumentException: XERBLA: Error on argument 6 (LDA) in SGEMV
at org.jblas.NativeBlas.sgemv(Native Method)
at org.nd4j.linalg.jblas.blas.JblasLevel2.sgemv(JblasLevel2.java:25)
at org.nd4j.linalg.api.blas.impl.BaseLevel2.gemv(BaseLevel2.java:53)
at org.nd4j.linalg.api.ndarray.BaseNDArray.mmuli(BaseNDArray.java:2569)
at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2377)
at org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.wordsNearest(WordVectorsImpl.java:290)
at org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.wordsNearest(WordVectorsImpl.java:337)
at word2vec.Word2VecTest.main(Word2VecTest.java:74)
Exception in thread ""main"" java.lang.NoSuchMethodError: org.nd4j.linalg.api.ndarray.INDArray.mean(I)Lorg/nd4j/linalg/api/ndarray/INDArray;
at org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.wordsNearest(WordVectorsImpl.java:283)
at word2vec.Word2VecTest.main(Word2VecTest.java:89)
</code></pre>

<p>I know that the NoSuchMethodError may be due to libraries different versions. In this specific case, this is probably caused by nd4j. I've checked the versions lots of time and this is what I'm importing at the moment:</p>

<ul>
<li>akka-actor_2.11-2.4-M3.jar</li>
<li>akka-cluster_2.11-2.4-M3.jar</li>
<li>akka-remote_2.11-2.4-M3.jar</li>
<li>akka-slf4j_2.11-2.4-M3.jar</li>
<li>byte-buddy-0.6.15.jar</li>
<li>config-1.3.0.jar</li>
<li>deeplearning4j-core-0.0.3.3.4.alpha2.jar</li>
<li>deeplearning4j-nlp-0.0.3.3.4.alpha2.jar</li>
<li>deeplearning4j-scaleout-akka-0.0.3.3.4.alpha2.jar</li>
<li>deeplearning4j-ui-0.0.3.3.4.alpha2.jar</li>
<li>javassist-3.12.1.GA.jar</li>
<li>jblas-1.2.4.jar</li>
<li>jcublas-6.5.jar</li>
<li>lucene-analyzers-common-4.10.3.jar</li>
<li>lucene-core-4.10.3.jar</li>
<li>nd4j-api-0.4-rc3.4.jar</li>
<li>nd4j-bytebuddy-0.4-rc3.4.jar</li>
<li>nd4j-jblas-0.4-rc3.4.jar</li>
<li>nd4j-jcublas-common-0.4-rc3.4.jar</li>
<li>netty-3.10.4.Final.jar</li>
<li>protobuf-java-2.6.1.jar</li>
<li>reflections-0.9.10.jar</li>
<li>scala-library-2.12.0-M2.jar</li>
<li>selenium-server-standalone-2.47.1.jar</li>
</ul>

<p>Can someone explain to me the problem?</p>
","java, lucene, neural-network, deep-learning, word2vec","<p>The error is telling you that DeepLearning4J tried to call the method <code>INDArray INDArray.mean(int value)</code> but this method was not found.</p>

<p>Looking at <a href=""https://github.com/deeplearning4j/nd4j/blob/nd4j-0.4-rc3.4/nd4j-api/src/main/java/org/nd4j/linalg/api/ndarray/INDArray.java#L1410"" rel=""nofollow"">nd4j 0.4-rc3.4 source code</a>, you can see that the <code>mean</code> method actually takes a vararg <code>int...</code> as input. Since this is not <code>int</code>, the error is thrown.</p>

<p>This change was made by <a href=""https://github.com/deeplearning4j/nd4j/commit/f139952229efad88a1e0e3f5ba6f8d2fc0099ff6#diff-97e0fa927feddd349589443d487cd5a0L1280"" rel=""nofollow"">this commit</a> when <code>nd4j</code> bumped version from <code>0.0.3.5.5.5</code> to <code>0.4-rc0</code>.</p>

<p>As a result, you need to downgrade <code>nd4j</code> to version <code>0.0.3.5.5.5</code>. With this downgrade, you will not have any more incompatibility since this is the actual version that DeepLearning4J is depending on. You can see that in the Maven dependencies of <a href=""http://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-core/0.0.3.3.4.alpha2"" rel=""nofollow""><code>deeplearning4j-core-0.0.3.3.4.alpha2</code></a>.</p>
",4,3,839,2015-09-25 12:56:34,https://stackoverflow.com/questions/32782728/deeplearning4j-nosuchmethoderror
Python int too large to convert to C long in python 3.4,"<p>I am getting this error when I am trying to run word2vec from gensim library of python. I am using python 3.4 and OS is windows 7. I have also attached complete stacktrace as well.
I read online and it says that this is an issue with python 2.x, but I am getting in python 3.4</p>

<pre><code>model = word2vec.Word2Vec(sentences, workers=num_workers, \
        size=num_features, min_count = min_word_count, \
        window = context, sample = downsampling)

Traceback (most recent call last):
  File ""&lt;pyshell#137&gt;"", line 3, in &lt;module&gt;
    window = context, sample = downsampling)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 417, in __init__
    self.build_vocab(sentences)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 483, in build_vocab
    self.finalize_vocab()  # build tables &amp; arrays
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 611, in finalize_vocab
    self.reset_weights()
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 888, in reset_weights
    self.syn0[i] = self.seeded_vector(self.index2word[i] + str(self.seed))
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 900, in seeded_vector
    once = random.RandomState(uint32(self.hashfxn(seed_string)))
OverflowError: Python int too large to convert to C long
</code></pre>
","python-2.7, python-3.x, machine-learning, word2vec","<p>Take note that python (2 and 3) support integers of <a href=""https://docs.python.org/2/library/stdtypes.html#numeric-types-int-float-long-complex"" rel=""nofollow"">arbitrary size</a> - python will just keep on adding additional ""digits"" (actually groups of them in <code>long</code>s) when you reach the current maximum size. The only differences between py2 and py3 is that the former will start with an actual C <code>int</code> or <code>long</code> before going to the arbitrary size python <code>long</code>. In py3, you always get the python <code>long</code> type.</p>

<p>Long story short: check the size of your integers.</p>
",1,0,1637,2015-10-11 22:03:37,https://stackoverflow.com/questions/33070598/python-int-too-large-to-convert-to-c-long-in-python-3-4
libsvm: read vectors from word2vec,"<p>Is there an easy way to use w2v's output vectors in libsvm?
There are two output formats for w2v: binary and text. In the text format each line begins with a word followed by a space-separated vector. e.g.:</p>

<p><code>something -0.197045 -0.292196 -0.107292 -0.168469 0.114897 -0.006383 -0.000056 0.068514 -0.079548 0.251488 0.185607 0.248675 -0.058647 0.062771 0.129014 -0.024715 -0.168974 -0.035367 -0.009597 0.090379 0.030133 0.017338 0.062264 -0.219165 -0.214198 0.226869 -0.058710 0.034563 -0.046304 0.2</code></p>
","ruby, libsvm, word2vec","<p>Found a way with ruby:</p>

<p>First require the <a href=""https://github.com/febeling/rb-libsvm"" rel=""nofollow"">libsvm wrapper</a>:</p>

<p><code>require 'libsvm'</code></p>

<p>read the vectors file (assuming textual form):</p>

<p><code>lines = File.readlines('vectors.txt')</code></p>

<p>insert to a hash</p>

<p><code>words = {}
lines[1..-1].each{ |l| sp = l.strip.split; words[sp[0]] = sp[1..-1].map(&amp;:to_f) }</code></p>

<p>and finally use libsvm:</p>

<p><code>examples = words.values.map { |ary| Libsvm::Node.features(ary) }</code> </p>
",1,1,529,2015-10-22 15:33:11,https://stackoverflow.com/questions/33284895/libsvm-read-vectors-from-word2vec
Does Word2Vec has a hidden layer?,"<p>When I am reading one of papers of Tomas Mikolov: <a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">http://arxiv.org/pdf/1301.3781.pdf</a></p>

<p>I have one concern on the Continuous Bag-of-Words Model sectionï¼š</p>

<blockquote>
  <p>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).</p>
</blockquote>

<p>I find some people mention that there is a hidden layer in Word2Vec model, but from my understanding, there is only one projection layer in that model. Does this projection layer do the same work as hidden layer?</p>

<p>The another question is that how to project input data into the projection layer? </p>

<p>""the projection layer is shared for all words (not just the projection matrix)"", what does that mean?</p>
","machine-learning, nlp, neural-network, word2vec","<p>From the <a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""noreferrer"">original paper</a>, section 3.1, it is clear that there is no hidden layer:</p>

<blockquote>
  <p>""the first proposed architecture is similar to the feedforward NNLM
  where the non-linear hidden layer is removed and the projection layer is shared for all words"".</p>
</blockquote>

<p>With respect to your second question (what does sharing the projection layer means), it means that you consider only one single vector, which is the centroid of the vectors of all the words in context. Thus, instead of having <code>n-1</code> word vectors as input, you consider only one vector. This is why it is called Continuous <strong>Bag of Words</strong> (because word order is lost within the context of size <code>n-1</code>).</p>
",7,11,3105,2015-10-27 16:57:34,https://stackoverflow.com/questions/33374010/does-word2vec-has-a-hidden-layer
Is there any tested &quot;Word2Vector&quot; code example in java or python?,"<p>I want tried couple of examples to learn <code>word2Vec</code> working by doing implementation but none of them worked out for me. I have tried <code>dl4j</code> and other <code>word2vector</code> examples. They all have some compilation issues and results are not same as the ones posted. Please help me and provide some tested and working example code.</p>
",word2vec,"<p>I stronly advice to use Python and gensim.
This two articles should be helpful:</p>

<p><a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">http://rare-technologies.com/doc2vec-tutorial/</a> Here rememeber to read <strong>IMPORTANT NOTE</strong>, some parts of the API of gensim has been updated recently, but it is really informative tutorial.</p>

<p><a href=""https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow"">https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a> - new API with descriptions. </p>
",4,2,2273,2015-10-28 14:17:12,https://stackoverflow.com/questions/33393493/is-there-any-tested-word2vector-code-example-in-java-or-python
Error in PySpark trying to run Word2Vec example,"<p>I'm trying to run the very simple example for Word2Vec given in the documentation here:</p>

<p><a href=""https://spark.apache.org/docs/1.4.1/api/python/_modules/pyspark/ml/feature.html#Word2Vec"" rel=""nofollow"">https://spark.apache.org/docs/1.4.1/api/python/_modules/pyspark/ml/feature.html#Word2Vec</a></p>

<pre><code>from pyspark import SparkContext, SQLContext
from pyspark.mllib.feature import Word2Vec
sqlContext = SQLContext(sc)

sent = (""a b "" * 100 + ""a c "" * 10).split("" "")
doc = sqlContext.createDataFrame([(sent,), (sent,)], [""sentence""])
model = Word2Vec(vectorSize=5, seed=42, inputCol=""sentence"", outputCol=""model"").fit(doc)
model.getVectors().show()
model.findSynonyms(""a"", 2).show()
</code></pre>

<hr>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-e57e9f694961&gt; in &lt;module&gt;()
      5 sent = (""a b "" * 100 + ""a c "" * 10).split("" "")
      6 doc = sqlContext.createDataFrame([(sent,), (sent,)], [""sentence""])
----&gt; 7 model = Word2Vec(vectorSize=5, seed=42, inputCol=""sentence"", outputCol=""model"").fit(doc)
      8 model.getVectors().show()
      9 model.findSynonyms(""a"", 2).show()

TypeError: __init__() got an unexpected keyword argument 'vectorSize'
</code></pre>

<p>Any idea why this is failing?</p>
","python, machine-learning, apache-spark, pyspark, word2vec","<p>You are referring to documentation from <code>ml</code> but importing from the <code>mllib</code> package. In <code>mllib</code> <code>Word2Vec</code> doesn't take any parameters in <code>__init__</code>.<br>
Did you intend:</p>

<pre><code>from pyspark.ml.feature import Word2Vec
</code></pre>

<p>Output:</p>

<pre><code>+----+--------------------+
|word|              vector|
+----+--------------------+
|   a|[-0.3511952459812...|
|   b|[0.29077222943305...|
|   c|[0.02315592765808...|
+----+--------------------+

+----+-------------------+
|word|         similarity|
+----+-------------------+
|   b|0.29255685145799626|
|   c|-0.5414068302988307|
+----+-------------------+
</code></pre>
",3,2,2219,2015-11-03 00:13:31,https://stackoverflow.com/questions/33489351/error-in-pyspark-trying-to-run-word2vec-example
Tensorflow - no module named &#39;embeddings&#39; in tensorflow.models.embeddings,"<p>I am trying to run the vector representations of words tutorial using Tensorflow found here:</p>

<p><a href=""http://www.tensorflow.org/tutorials/word2vec/index.md"" rel=""nofollow"">http://www.tensorflow.org/tutorials/word2vec/index.md</a></p>

<p>The first script, word2vec_basic.py, runs fine, but the second one (found here)</p>

<p><a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py"" rel=""nofollow"">https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py</a></p>

<p>gives me the error ""ImportError: No module named embedding"" at line 28. I installed tensorflow as per the instructions on the page, any reason why I wouldn't have gotten the embeddings module, containing word2vec?</p>
","python, word2vec, tensorflow","<p>For now, the more full-featured versions of word2vec such as word2vec.py require building from source.</p>
",2,4,2996,2015-11-10 15:54:50,https://stackoverflow.com/questions/33634126/tensorflow-no-module-named-embeddings-in-tensorflow-models-embeddings
Python: gensim: RuntimeError: you must first build vocabulary before training the model,"<p>I know that this question has been asked already, but I was still not able to find a solution for it. </p>

<p>I would like to use gensim's <code>word2vec</code> on a custom data set, but now I'm still figuring out in what format the dataset has to be. I had a look at <a href=""http://streamhacker.com/2014/12/29/word2vec-nltk/"">this post</a> where the input is basically a list of lists (one big list containing other lists that are tokenized sentences from the NLTK Brown corpus). So I thought that this is the input format I have to use for the command <code>word2vec.Word2Vec()</code>. However, it won't work with my little test set and I don't understand why.</p>

<p>What I have tried:</p>

<p><strong>This worked</strong>:</p>

<pre><code>from gensim.models import word2vec
from nltk.corpus import brown
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

brown_vecs = word2vec.Word2Vec(brown.sents())
</code></pre>

<p><strong>This didn't work</strong>:</p>

<pre><code>sentences = [ ""the quick brown fox jumps over the lazy dogs"",""yoyoyo you go home now to sleep""]
vocab = [s.encode('utf-8').split() for s in sentences]
voc_vec = word2vec.Word2Vec(vocab)
</code></pre>

<p>I don't understand why it doesn't work with the ""mock"" data, even though it has the same data structure as the sentences from the Brown corpus:</p>

<p><strong>vocab</strong>:</p>

<pre><code>[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]
</code></pre>

<p><strong>brown.sents()</strong>: (the beginning of it)</p>

<pre><code>[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', ""Atlanta's"", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', ""''"", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', ""''"", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]
</code></pre>

<p>Can anyone please tell me what I'm doing wrong? </p>
","python, gensim, word2vec","<p>Default <code>min_count</code> in gensim's Word2Vec is set to 5. If there is no word in your vocab with frequency greater than 4, your vocab will be empty and hence the error. Try</p>

<pre><code>voc_vec = word2vec.Word2Vec(vocab, min_count=1)
</code></pre>
",80,33,40313,2015-11-30 00:30:37,https://stackoverflow.com/questions/33989826/python-gensim-runtimeerror-you-must-first-build-vocabulary-before-training-th
"All of the words, those I use to train the word2vec model, must be in model.vocab, aren&#39;t they?","<p>I use the next code to train the model:</p>

<pre><code>norms_train = [ [''], [ u'word', u'to', u'learn', ... ], ...]
model = word2vec.Word2Vec(norms_train, size=100, window=10)
</code></pre>

<p>With procedure to check the results:</p>

<pre><code>i, j = 0, 0
for text in norms_train:
    j += len(text)
    for word in text:
        if word not in model.vocab:
            i += 1
print i, '/', j
</code></pre>

<p>13129 / 185379</p>
","python, gensim, training-data, word2vec","<p>All words that you have used to train the Word2Vec model should be in model.vocab. There may be a threshold on the minimum number of occurrences of a word, that have to be present for it to be included in the model vocabulary.</p>

<p>I suppose the argument <code>min_count</code> is set to 5 by default i.e. if a word has occurred less than 5 times in the training data, that word would not be present in the model.vocab.</p>
",1,0,308,2015-12-08 10:39:55,https://stackoverflow.com/questions/34153708/all-of-the-words-those-i-use-to-train-the-word2vec-model-must-be-in-model-voca
Generator is not an iterator?,"<p>I have an generator (a function that yields stuff), but when trying to pass it to <code>gensim.Word2Vec</code> I get the following error:</p>

<blockquote>
  <p>TypeError: You can't pass a generator as the sentences argument. Try an iterator.</p>
</blockquote>

<p>Isn't a generator a kind of iterator? If not, how do I make an iterator from it?</p>

<p>Looking at the library code, it seems to simply iterate over sentences like <code>for x in enumerate(sentences)</code>, which works just fine with my generator. What is causing the error then?</p>
","python, gensim, word2vec","<p>Generator is <strong>exhausted</strong> after one loop over it. Word2vec simply needs to traverse sentences multiple times (and probably get item for a given index, which is not possible for generators which are just a kind of stacks where you can only pop), thus requiring something more solid, like a list.</p>

<p>In particular in their code they call two different functions, both iterate over sentences (thus if you use generator, the second one would run on an empty set)</p>

<pre><code>self.build_vocab(sentences, trim_rule=trim_rule)
self.train(sentences)
</code></pre>

<p>It should work with anything implementing <code>__iter__</code>  which is not <code>GeneratorType</code>. So wrap your function in an iterable interface and make sure that you can traverse it multiple times, meaning that</p>

<pre><code>sentences = your_code
for s in sentences:
  print s
for s in sentences:
  print s
</code></pre>

<p>prints your collection twice</p>
",15,26,8018,2015-12-08 21:28:53,https://stackoverflow.com/questions/34166369/generator-is-not-an-iterator
Understanding Word2Vec&#39;s Skip-Gram Structure and Output,"<p>I have a two-fold question about the Skip-Gram model in Word2Vec:</p>
<ul>
<li><p>The first part is about structure: as far as I understand it, the Skip-Gram model is based on one neural network with one input weight matrix <strong>W</strong>, one hidden layer of size N, and C output weight matrices <strong>W'</strong> each used to produce one of the C output vectors. Is this correct?</p>
</li>
<li><p>The second part is about the output vectors: as far as I understand it, each output vector is of size V and is a result of a Softmax function. Each output vector <em>node</em> corresponds to the index of a word in the vocabulary, and the value of each node is the probability that the corresponding word occurs at that context location (for a given input word). The target output vectors are not, however, one-hot encoded, even if the training instances are. Is this correct?</p>
</li>
</ul>
<p>The way I imagine it is something along the following lines (made-up example):</p>
<p>Assuming the vocabulary ['quick', 'fox', 'jumped', 'lazy', 'dog'] and a context of C=1, and assuming that for the input word 'jumped' I see the two output vectors looking like this:</p>
<p>[0.2 <strong>0.6</strong> 0.01 0.1 0.09]</p>
<p>[0.2 0.2 0.01 0.16 <strong>0.43</strong>]</p>
<p>I would interpret this as 'fox' being the most likely word to show up before 'jumped' (p=0.6), and 'dog' being the most likely to show up after it (p=0.43).</p>
<p>Do I have this right? Or am I completely off?</p>
","vector, machine-learning, nlp, word2vec","<p>Your understanding in both parts seem to be correct, according to this paper :</p>
<p><a href=""http://arxiv.org/abs/1411.2738"" rel=""nofollow noreferrer"">http://arxiv.org/abs/1411.2738</a></p>
<p>The paper explains word2vec in detail and at the same time, keeps it very simple - it's worth a read for a thorough understanding of the neural net architecture used in word2vec.</p>
<ul>
<li>The structure of Skip Gram does use a single neural net, with input as one-hot encoded target-word and <strong>expected-output</strong> as one-hot encoded context words. After the neural-net is trained on the text-corpus, the input weight matrix <strong>W</strong>   is used as the input-vector representations of words in the corpus and the output weight matrix <strong>W'</strong> which is shared across all the <strong>C</strong> outputs (output-vectors in the terminology of the question, but avoiding that to prevent confusion with output-vector representations used next..), becomes the output-vector representations of words. Usually the output-vector representations are ignored, and the input-vector representations, <strong>W</strong> are used as the word embeddings. To get into the dimensionality of the matrices, if we assume a vocabulary size of <strong>V</strong>, size of hidden layer as <strong>N</strong>, we will have <strong>W</strong> as <strong>(V,N)</strong> matrix, with each row representing the input vector of the indexed word in the vocabulary. <strong>W'</strong> will be a <strong>(N,V)</strong> matrix, with each column representing the output vector of the indexed word. In this way we get N-dimensional vectors for words.</li>
<li>As you mentioned, each of the outputs(avoiding using the term output vector) is of size <strong>V</strong> and are the result of a softmax function, with each node in the output giving the probability of the word occurring as a context word for the given target word, resulting in the outputs not being one-hot encoded.But the expected outputs are indeed one-hot encoded, i.e in training phase, the error is computed by subtracting the one-hot encoded vector of the actual word occurring at that context position, from the neural-net output and then the weights are updated using gradient descent.</li>
</ul>
<p>Referring to the example you mentioned, with <code>C=1</code> and with a vocabulary of <code>['quick', 'fox', 'jumped', 'lazy', 'dog']</code>.</p>
<p>If the output from the skip-gram is <code>[0.2 0.6 0.01 0.1 0.09]</code>, where the correct target word is <code>'fox'</code> then error is calculated as:</p>
<pre><code>[0 1 0 0 0] - [0.2 0.6 0.01 0.1 0.09] = [-0.2 0.4 -0.01 -0.1 -0.09]
</code></pre>
<p>and the weight matrices are updated to minimize this error.</p>
",5,6,4487,2015-12-18 20:12:08,https://stackoverflow.com/questions/34363250/understanding-word2vecs-skip-gram-structure-and-output
&#39;utf-8&#39; decode error when loading a word2vec module,"<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>

<p>I installed <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">gensim</a> and tries to load the module, but following error occurred: </p>

<pre><code>In [1]: import gensim  

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
","python, nlp, gensim, word2vec","<p>The module was tons of Chinese characters trained by Java. I cannot figure out the encoding format of the original corpus. The error can be solved as the description in gensim <a href=""https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q10-loading-a-word2vec-model-fails-with-unicodedecodeerror-utf-8-codec-cant-decode-bytes-in-position-"" rel=""noreferrer"">FAQ</a>, </p>

<p>Using load_word2vec_format with a flag for ignoring the character decoding errors:</p>

<pre><code>In [1]: import gensim

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True, unicode_errors='ignore')
</code></pre>

<p>But I've no idea whether it matters when ignoring the encoding errors.</p>
",6,5,8572,2015-12-23 02:24:53,https://stackoverflow.com/questions/34427678/utf-8-decode-error-when-loading-a-word2vec-module
using Word2VecModel.transform() does not work in map function,"<p>I have built a Word2Vec model using Spark and save it as a model. Now, I want to use it in another code as offline model. I have loaded the model and used it to present vector of a word (e.g. Hello) and it works well. But, I need to call it for many words in an RDD using map.</p>

<p>When I call model.transform() in a map function, it throws this error:</p>

<blockquote>
  <p>""It appears that you are attempting to reference SparkContext from a broadcast ""
  Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.</p>
</blockquote>

<p>the code:</p>

<pre><code>from pyspark import SparkContext
from pyspark.mllib.feature import Word2Vec
from pyspark.mllib.feature import Word2VecModel

sc = SparkContext('local[4]',appName='Word2Vec')

model=Word2VecModel.load(sc, ""word2vecModel"")

x= model.transform(""Hello"")
print(x[0]) # it works fine and returns [0.234, 0.800,....]

y=sc.parallelize([['Hello'],['test']])
y.map(lambda w: model.transform(w[0])).collect() #it throws the error
</code></pre>

<p>I will really appreciate your help.</p>
","python, apache-spark, pyspark, apache-spark-mllib, word2vec","<p>It is an expected behavior. Like other <code>MLlib</code> models Python object is just a wrapper around Scala model and actual processing is delegated to its JVM counterpart. Since Py4J gateway is not accessible on workers (see <a href=""https://stackoverflow.com/q/31684842/1560062"">How to use Java/Scala function from an action or a transformation?</a>) you cannot call Java / Scala method from an action or transformation.</p>

<p>Typically MLlib models provide a helper method which can work directly on RDDs but it is not the case here. <code>Word2VecModel</code> provides <code>getVectors</code> method which returns a map from words to vector but unfortunately it is a <code>JavaMap</code> so it won't work inside transformation. You could try something like this:</p>

<pre><code>from pyspark.mllib.linalg import DenseVector

vectors_ = model.getVectors() # py4j.java_collections.JavaMap
vectors = {k: DenseVector([x for x in vectors_.get(k)])
    for k in vectors_.keys()}
</code></pre>

<p>to get Python dictionary but it will be extremely slow. Another option is to dump this object to disk in a form that can be consumed by Python but it requires some tinkering with Py4J and it is better to avoid this. Instead lets read model as a DataFrame:</p>

<pre><code>lookup = sqlContext.read.parquet(""path_to_word2vec_model/data"").alias(""lookup"")
</code></pre>

<p>and we'll get a following structure:</p>

<pre><code>lookup.printSchema()
## root
## |-- word: string (nullable = true)
## |-- vector: array (nullable = true)
## |    |-- element: float (containsNull = true)
</code></pre>

<p>which can be used to map words to vectors for example through <code>join</code>:</p>

<pre><code>from pyspark.sql.functions import col

words = sc.parallelize([('hello', ), ('test', )]).toDF([""word""]).alias(""words"")

words.join(lookup, col(""words.word"") == col(""lookup.word""))

## +-----+-----+--------------------+
## | word| word|              vector|
## +-----+-----+--------------------+
## |hello|hello|[-0.030862354, -0...|
## | test| test|[-0.13154022, 0.2...|
## +-----+-----+--------------------+
</code></pre>

<p>If data fits into driver / worker memory you can try to collect and map with broadcast:</p>

<pre><code>lookup_bd = sc.broadcast(lookup.rdd.collectAsMap())
rdd = sc.parallelize([['Hello'],['test']])
rdd.map(lambda ws: [lookup_bd.value.get(w) for w in ws])
</code></pre>
",10,5,2965,2015-12-24 06:41:17,https://stackoverflow.com/questions/34448456/using-word2vecmodel-transform-does-not-work-in-map-function
stopword removing when using the word2vec,"<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>

<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>

<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>
","nlp, gensim, word2vec","<p>Personaly I think, removal of stop word will give better results, check <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""noreferrer"">link</a></p>

<p>Also for topic modeling, you shlould perform preprocessing on the text, following things you must do,</p>

<ol>
<li>Remove of stop words.</li>
<li><a href=""http://www.nltk.org/book/ch03.html"" rel=""noreferrer"">Tokenization.</a></li>
<li><a href=""http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"" rel=""noreferrer"">Stemming and Lemmatization</a>.</li>
</ol>
",12,27,21645,2016-01-11 12:49:10,https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec
Ensure the gensim generate the same Word2Vec model for different runs on the same data,"<p>In <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus"">LDA model generates different topics everytime i train on the same corpus</a> , by setting the <code>np.random.seed(0)</code>, the LDA model will always be initialized and trained in exactly the same way. </p>

<p><strong>Is it the same for the Word2Vec models from <code>gensim</code>? By setting the random seed to a constant, would the different run on the same dataset produce the same model?</strong></p>

<p>But strangely, it's already giving me the same vector at different instances. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; exit()
alvas@ubi:~$ python
Python 2.7.11 (default, Dec 15 2015, 16:46:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; word0 = sentences[0][0]
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
</code></pre>

<p><strong>Is it true then that the default random seed is fixed?</strong> If so, what is the default random seed number? Or is it because I'm testing on a small dataset? </p>

<p>If it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.  </p>
","python, random, gensim, word2vec, word-embedding","<p>Yes, default random seed is fixed to <code>1</code>, as described by the author in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a>. Vectors for each word are initialised using a hash of the concatenation of word + str(seed).</p>
<p>Hashing function used, however, is Pythonâ€™s rudimentary built in hash function and can produce different results if two machines differ in</p>
<ul>
<li>32 vs 64 bit, <a href=""https://stackoverflow.com/questions/16452252/does-pythons-hash-function-remain-identical-across-different-versions"">reference</a></li>
<li>python versions, <a href=""https://stackoverflow.com/questions/34058947/hashing-tuple-in-python-causing-different-results-in-different-systems"">reference</a></li>
<li>different Operating Systems/ Interpreters, <a href=""https://stackoverflow.com/questions/17192418/hash-function-in-python"">reference1</a>, <a href=""https://stackoverflow.com/questions/793761/built-in-python-hash-function"">reference2</a></li>
</ul>
<p>Above list is not exhaustive. Does it cover your question though?</p>
<p><strong>EDIT</strong></p>
<p>If you want to ensure consistency, you can provide your own hashing function as an argument in word2vec</p>
<p>A very simple (and bad) example would be:</p>
<pre><code>def hash(astring):
   return ord(astring[0])

model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4, hashfxn=hash)

print model[sentences[0][0]]
</code></pre>
",11,15,9923,2016-01-16 20:05:51,https://stackoverflow.com/questions/34831551/ensure-the-gensim-generate-the-same-word2vec-model-for-different-runs-on-the-sam
word2vec gives vectors of very few words in a text.Why?,"<p>When I provide a text document as input to word2vec. It assigns vectors to a very few words from the vocabulary of the text. Why does this happen? And how to overcome this problem?</p>
",word2vec,"<p>I think the reason you are seeing very few vectors being created is that your corpus is too small. Word2vec will remove infrequently occurring words from the vocabulary. This is controlled by the <em>t-min-count</em> command line switch. The default for the original source code is set to 5. Any words that occur less than this many times in your corpus will be removed. </p>
",2,0,216,2016-01-21 12:04:48,https://stackoverflow.com/questions/34923360/word2vec-gives-vectors-of-very-few-words-in-a-text-why
Word2vec + Tensorflow and the shape of everything,"<p>i'm looking for a solution to a simple Text classification issue with tensorflow. I made a model with the IMDB dataset to know wether a comment is positive or negative. The data was processed through word2vec so now i have a bunch of vector to classify. I think my problem here are due to the bad shape of the y_labels since they are one dimensionnal and i want to classify them through tensorflow with a two classes output, or maybe i am wrong. Final info, the model is working well, with an accuracy of 1.0, maybe too well! Thanks for the help !</p>

<pre><code>X_train called train_vecs = (25000, 300) dtype: float64
X_test called test_vecs = (25000, 300) dtype: float64
y_test = shape (25000, 1) dtype: int64
y_train = shape: (25000, 1) dtype: int64

x = tf.placeholder(tf.float32, shape = [None, 300])
y = tf.placeholder(tf.float32, shape = [None, 2])
# Input -&gt; Layer 1
W1 = tf.Variable(tf.zeros([300, 2]))
b1 = tf.Variable(tf.zeros([2]))
#h1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)
# Calculating difference between label and output
pred = tf.nn.softmax(tf.matmul(x, W1) + b1)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,y))
train_step = tf.train.GradientDescentOptimizer(0.3).minimize(cost)

with tf.Session() as sess:
        for i in xrange(200):
                init_op = tf.initialize_all_variables()
                sess.run(init_op)
                train_step.run(feed_dict = {x: train_vecs, y: y_train})
        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
        # Calculate accuracy
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
        print ""Accuracy:"", accuracy.eval({x: test_vecs, y: y_test})
</code></pre>
","python, machine-learning, tensorflow, word2vec","<p>You are using softmax in your example.  Softmax assigns a probability to N different classes, where the probability adds up to 1. Basically, the model is choosing exactly one of the N choices. For this to make sense you need N to be at least 2.  With N == 1, the probability of that class will always be 1.  You have two possible fixes:</p>

<ol>
<li>Create two classes, one for ""positive sentiment"" and one for ""negative sentiment"", setting N to 2.</li>
<li>Use logistic regression instead of Softmax. In logistic regression each class is independent. That means you have N questions, each of which gets its own ""yes"" or ""no"" answer, which makes sense with N == 1.</li>
</ol>
",8,1,624,2016-02-03 18:36:19,https://stackoverflow.com/questions/35184999/word2vec-tensorflow-and-the-shape-of-everything
Q: Can a Machine Learning model solve rule-based problems?,"<p>Can Machine Learning be used to validate statements or catch errors in text documents?</p>

<p>For example, if you teach a classifier that ""You should eat apples twice per day"", but in a document that you're testing on, the statement is ""You should eat apples three times per day"", can the statement be flagged? </p>

<p>Obviously you can build some rules-based software that catches these, but my question centers around training an ML model to catch these, as rules change.</p>

<p>I have looked at word2vec and NLTK and performed some tests with them, but can't connect the dots for teaching the classifier.</p>

<p>If it's possible, how would one go about it or provide some direction?</p>

<p>Thanks, Doug</p>
","machine-learning, nltk, word2vec","<p>(Got too long for a comment. )</p>

<p>Yes it can. However, it is freakingly complicated. This kind of reasoning and analysis is done by Watson for example. IBM is calling these cognitive computing. As you wrote rule based (or logical reasoning) systems can solve such tasks. So the question you should ask yourself is how you can extract the required facts from text. => NLP , Part Of Speech, Named Entity,... However the task is extremely hard because "" not more then 100times"" a day is not contradicting the sentence. So reasoning would require rich background knowledge.</p>

<p>As said it is an extremely broad topic. You would have to sketch the solution and then pick a tiny piece, which would be called a PhD thesis ;). 
Which is illustrated in this nice image <a href=""http://matt.might.net/articles/phd-school-in-pictures/"" rel=""nofollow"">http://matt.might.net/articles/phd-school-in-pictures/</a></p>

<p>So looking with the right keywords for PhD thesis's turned up <a href=""http://nakashole.com/papers/2012-phd-thesis.pdf"" rel=""nofollow"">http://nakashole.com/papers/2012-phd-thesis.pdf</a> . This one might provide you a few nights of reading.</p>

<p>If you want to try something hands on with NLTK I would generate parse trees for the sentences you want to analyse. Afterwards you could try to align these and check for overlaps and deviations. However I'm not sure how to draw conclusions. A slightly simpler version would be to match word by word. Something along Levenstein Distance calculations.</p>
",1,-2,111,2016-02-10 12:17:16,https://stackoverflow.com/questions/35315068/q-can-a-machine-learning-model-solve-rule-based-problems
How to call to Doc2Vec from the cmd,"<p>Does anyone knows of an implementation to paragraph vector algorithm or Doc2Vec that can be used from the cmd, without changing the source code?</p>
","deep-learning, word2vec","<p>Gensim have ported Tomas Mikolov's word2vec and doc2vec original concepts to Python, which implements distributed memory and distributed bag of words models, using either hierarchical softmax or negative sampling. This will work with sentences, paragraphs and documents.</p>

<p>You can install from this location: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">https://radimrehurek.com/gensim/install.html</a> </p>
",1,1,65,2016-02-14 09:29:08,https://stackoverflow.com/questions/35390260/how-to-call-to-doc2vec-from-the-cmd
gensim word2vec: Find number of words in vocabulary,"<p>After training a word2vec model using python <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>, how do you find the number of words in the model's vocabulary?</p>
","python, neural-network, nlp, gensim, word2vec","<p>In recent versions, the <code>model.wv</code> property holds the words-and-vectors, and can itself can report a length â€“ the number of words it contains. So if <code>w2v_model</code> is your <code>Word2Vec</code> (or <code>Doc2Vec</code> or <code>FastText</code>) model, it's enough to just do:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_len = len(w2v_model.wv)
</code></pre>
<p>If your model is just a raw set of word-vectors, like a <code>KeyedVectors</code> instance rather than a full <code>Word2Vec</code>/etc model, it's just:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_len = len(kv_model)
</code></pre>
<p>Other useful internals in Gensim 4.0+ include <code>model.wv.index_to_key</code>, a plain list of the key (word) in each index position, and <code>model.wv.key_to_index</code>, a plain dict mapping keys (words) to their index positions.</p>
<p>In pre-4.0 versions, the vocabulary was in the <code>vocab</code> field of the Word2Vec model's <code>wv</code> property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length:</p>
<pre><code>len(w2v_model.wv.vocab)
</code></pre>
<p>In very-old gensim versions before 0.13 <code>vocab</code> appeared directly on the model. So way back then you would use <code>w2v_model.vocab</code> instead of <code>w2v_model.wv.vocab</code>.</p>
<p>But if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory &amp; performance improvements, and the changes required in calling code are relatively small â€“ some renamings &amp; moves, covered in the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"" rel=""noreferrer"">4.0 Migration Notes</a>.</p>
",110,55,97308,2016-02-24 07:39:48,https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary
Doc2Vec and PySpark: Gensim Doc2vec over DeepDist,"<p>I am looking at the <code>DeepDist</code> (<a href=""http://deepdist.com"" rel=""nofollow noreferrer"">link</a>) module and thinking to combine it with <code>Gensim</code>'s <code>Doc2Vec</code> API to train paragraph vectors on <code>PySpark</code>. The link actually provides with the following clean example for how to do it for <code>Gensim</code>'s <code>Word2Vec</code> model:</p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext

sc = SparkContext()
corpus = sc.textFile('enwiki').map(lambda s: s.split())

def gradient(model, sentences):  # executes on workers
    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn0, 'syn1': model.syn1 - syn1}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.syn1 += update['syn1']

with DeepDist(Word2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>To my understanding, <code>DeepDist</code> is distributing the work of gradient descent into workers in batches, and the recombining them and updating at master. If I replace <code>Word2Vec</code> with <code>Doc2Vec</code>, there should be the document vectors that are being trained with the word vectors.</p>

<p>So I looked into the source code of <code>gensim.models.doc2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py"" rel=""nofollow noreferrer"">link</a>). There are the following fields in the <code>Doc2Vec</code> model instance:</p>

<ol>
<li><code>model.syn0</code></li>
<li><code>model.syn0_lockf</code></li>
<li><code>model.docvecs.doctag_syn0</code></li>
<li><code>model.docvecs.doctag_syn0_lockf</code></li>
</ol>

<p>Comparing with the source code of <code>gensim.models.word2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">link</a>), the following fields went missing in <code>Doc2Vec</code> model:</p>

<ol start=""3"">
<li><code>model.syn1</code></li>
<li><code>model.syn1neg</code></li>
</ol>

<p>I think I do not touch the <code>lockf</code> vectors because they seem to be used after the training is done when new data points come in. Therefore my code should be something like </p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.doc2vec import Doc2Vec, LabeledSentence
from pyspark import SparkContext

sc = SparkContext()

# assume my dataset is in format 10-char-id followed by doc content
# 1 line per doc
corpus = sc.textFile('data_set').map(
    lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10])
)

def gradient(model, sentence):  # executes on workers
    syn0, doctag_syn0 = model.syn0.copy(), model.docvecs.doctag_syn0.copy()   # previous weights
    model.train(sentence)
    return {'syn0': model.syn0 - syn0, 'doctag_syn0': model.docvecs.doctag_syn0 - doctag_syn0}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.docvecs.doctag_syn0 += update['doctag_syn0']

with DeepDist(Doc2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>Am I missing anything important here? For example:</p>

<ol>
<li>Should I care about <code>model.syn1</code> at all? What do they mean after all?</li>
<li>Am I right that <code>model.*_lockf</code> is the locked matrices after training?</li>
<li>Is it ok that I use <code>lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10]</code> to parse my dataset, assuming I have each document in one line, prefixed by a 0-padded 10-digit id?</li>
</ol>

<p>Any suggestion/contribution are very appreciated. I will write up a blog post to summarize the result, mentioning contributors here, potentially to help others train Doc2Vec models on scaled distributed systems without spending much dev time trying to solve what I am solving now.</p>

<p>Thanks</p>

<hr>

<p>Update 06/13/2018</p>

<p>My apologies as I did not get to implement this. But there are better options nowaday, and <code>DeepDist</code> haven't been maintained for awhile now. Please read comment below.</p>

<p>If you insist on trying out my idea at the moment, be reminded you are proceeding with your own risk. Also, if someone knows that <code>DeepDist</code> still works, please report back in comments. It would help other readers. </p>
","apache-spark, pyspark, gensim, word2vec","<p>To avoid this question from remaining shown as open, here is how the asker resolved the situation:</p>

<blockquote>
  <p>I did not get to implement this, until it's too late that I didn't think it would work. DeepDist uses Flask app in backend to interact with Spark web interface. Since it wasn't maintained anymore, Spark's update very likely broke it already. If you are looking for Doc2Vec training in Spark, just go for Deeplearning4J(deeplearning4j.org/doc2vec#)</p>
</blockquote>
",1,11,3921,2016-02-25 00:40:27,https://stackoverflow.com/questions/35616088/doc2vec-and-pyspark-gensim-doc2vec-over-deepdist
Using different word2vec training data in spaCy,"<p>So I'd like to use some of <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">this training data</a> in spaCy when I use the <code>similarity()</code> method.</p>

<p>I'd also like to maybe use the pre-trained vectors also on this page.</p>

<p>But the spaCy docs seem lacking here, does anyone know how to do this?</p>
","python, nlp, word2vec, spacy","<p>Unfortunately the docs for this still aren't linked on the site! We're reworking the docs. But, does this answer your question: <a href=""https://spacy.io/tutorials/load-new-word-vectors"" rel=""nofollow"">https://spacy.io/tutorials/load-new-word-vectors</a></p>
",3,6,2114,2016-02-26 13:58:46,https://stackoverflow.com/questions/35653631/using-different-word2vec-training-data-in-spacy
Gensim word2vec on predefined dictionary and word-indices data,"<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an ""unknown"" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>

<p><strong>dict.txt</strong></p>

<pre><code>you
love
this
code
</code></pre>

<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>

<pre><code>[[0, 1, 2, 3, 6],
 [3, 5, 5, 1, 6],
 [0, 1, 3, 6, 6]]
</code></pre>

<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>

<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>
","python, nlp, gensim, word2vec","<p>The normal way to initialize a <code>Word2Vec</code> model in <code>gensim</code> is [1]</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>The question is, what is <code>sentences</code>? <code>sentences</code> is supposed to be an iterator of iterables of words/tokens. It is just like the numpy matrix you have, but each row can be of different lengths.</p>

<p>If you look at the documentation for <code>gensim.models.word2vec.LineSentence</code>, it gives you a way of loading a text files as sentences directly. As a hint, according to the documentation, it takes</p>

<blockquote>
  <p>one sentence = one line; words already preprocessed and separated by whitespace.</p>
</blockquote>

<p>When it says <code>words already preprocessed</code>, it is referring to lower-casing, stemming, stopword filtering and all other text cleansing processes. In your case you wouldn't want <code>5</code> and <code>6</code> to be in your list of sentences, so you do need to filter them out.</p>

<p>Given that you already have the numpy matrix, assuming each row is a sentence, it is better to then cast it into a 2d array and filter out all <code>5</code> and <code>6</code>. The resultant 2d array can be used directly as the <code>sentences</code> argument to initialize the model. The only catch is that when you want to query the model after training, you need to input the indices instead of the tokens.</p>

<p>Now one question you have is if the model takes integer directly. In the <code>Python</code> version it doesn't check for type, and just passes the unique tokens around. Your unique indices in that case will work fine. But most of the time you would want to use the C-Extended routine to train your model, which is a big deal because it can give 70x performance. [2] I imagine in that case the C code may check for string type, which means there is a string-to-index mapping stored.</p>

<p>Is this inefficient? I think not, because the strings you have are numbers, which are in generally much shorter than the real token they represent (assuming they are compact indices from <code>0</code>). Therefore models will be smaller in size, which will save some effort in serialization and deserialization of the model at the end. You essentially have encoded the input tokens in a shorter string format and separated it from the <code>word2vec</code> training, and <code>word2vec</code> model do not and need not know this encoding happened before training.</p>

<p>My philosophy is <code>try the simplest way first</code>. I would just throw a sample test input of integers to the model and see what can go wrong. Hope it helps.</p>

<p>[1] <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>[2] <a href=""http://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""noreferrer"">http://rare-technologies.com/word2vec-in-python-part-two-optimizing/</a></p>
",9,11,3972,2016-03-01 11:20:22,https://stackoverflow.com/questions/35721503/gensim-word2vec-on-predefined-dictionary-and-word-indices-data
Bigram vector representations using word2vec,"<p>I want to construct word embeddings for documents using the word2vec tool. I know how to find a vector embedding corresponding to a single word (unigram). Now, I want to find a vector for a bigram. Is it possible to construct a bigram word embedding using word2vec? If yes, how?</p>
","nlp, word2vec, word-embedding","<p>The following snippet will get you the vector representation of a bigram. Note that the bigram you want to convert to a vector needs to have an underscore instead of a space between the words, e.g. <code>bigram2vec(unigrams, ""this report"")</code> is wrong, it should be <code>bigram2vec(unigrams, ""this_report"")</code>. For more details on generating the unigrams, please see the <code>gensim.models.word2vec.Word2Vec</code> class <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">here</a>.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(bigrams[unigrams])
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>
",8,10,6739,2016-03-02 12:27:01,https://stackoverflow.com/questions/35747245/bigram-vector-representations-using-word2vec
"PySpark - Word2Vec load model, can&#39;t use findSynonyms to get words","<p>I have trained a Word2Vec model with PySpark and saved it. When loading the model .findSynonyms method does not work. </p>

<pre><code>model = word2vec.fit(text)
model.save(sc, 'w2v_model')
new_model = Word2VecModel.load(sc, 'w2v_model')
new_model.findSynonyms('word', 4)
</code></pre>

<p>Getting the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/lib/spark/python/pyspark/mllib/feature.py"", line 487, in findSynonyms
words, similarity = self.call(""findSynonyms"", word, num)
ValueError: too many values to unpack
</code></pre>

<p>I found the following, but not sure how the issue was fixed: <a href=""https://issues.apache.org/jira/browse/SPARK-12016"" rel=""nofollow"">https://issues.apache.org/jira/browse/SPARK-12016</a></p>

<p>Please let me know if there are any work arounds!</p>

<p>Many thanks.</p>
","apache-spark, pyspark, word2vec","<p>Looks like it's fixed on 1.6.1 but not on 1.5.2.<p>
The error is not about findSynonyms but about Word2VecModel.load.
I checked it works on 1.6.1.; no error while loading the model and calling findSynonyms method.</p>

<p>I guess v. 1.5.2 is not fixed yet.</p>
",1,2,2293,2016-03-06 01:58:38,https://stackoverflow.com/questions/35822199/pyspark-word2vec-load-model-cant-use-findsynonyms-to-get-words
word2vec how to get words from vectors?,"<p>I use ANN to predict words from words. The input and output are all words vectors. I do not know how to get words from the output of ANN. By the way,  it's gensim I am using</p>
","machine-learning, gensim, word2vec","<p>You can find cosine similarity of the vector with all other word-vectors to find the nearest neighbors of your vector.</p>

<p>The nearest neighbor search on an n-dimensional space, can be brute force, or you can use libraries like FLANN, Annoy, scikit-kdtree to do it more efficiently.</p>

<p><strong>update</strong></p>

<p>Sharing a gist demonstrating the same:
<a href=""https://gist.github.com/kampta/139f710ca91ed5fabaf9e6616d2c762b"" rel=""nofollow"">https://gist.github.com/kampta/139f710ca91ed5fabaf9e6616d2c762b</a></p>
",1,3,1609,2016-03-10 10:48:09,https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors
GenSim Word2Vec unexpectedly pruning,"<p>My objective is to find a vector representation of phrases. Below is the code I have, that works partially for bigrams using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">Word2Vec</a> model provided by the <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow"">GenSim</a> library.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, window=4, sg=1, hs=1, negative=0, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>

<p>The problem is that the Word2Vec model is seemingly doing automatic pruning of some of the bigrams, i.e. <code>len(model.vocab.keys()) != len(bigrams.vocab.keys())</code>. I've tried adjusting various parameters such as <code>trim_rule</code>, <code>min_count</code>, but they don't seem to affect the pruning.</p>

<p>PS - I am aware that bigrams to look up need to be represented using underscore instead of space, i.e. proper way to call my function would be <code>bigram2vec(unigrams, 'this_report')</code></p>
","gensim, word2vec","<p>Thanks to further clarification at the <a href=""https://groups.google.com/d/msg/gensim/lsDU42VksJA/ZasFdb21HAAJ"" rel=""nofollow"">GenSim support forum</a>, the solution is to set the appropriate <code>min_count</code> and <code>threshold</code> values for the <code>Phrases</code> being generated (see <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow"">documentation</a> for details about these parameters in the <code>Phrases</code> class). The corrected solution code is below.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec, Phrases

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams, min_count=1, threshold=0.1)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return []
</code></pre>
",0,0,417,2016-03-15 13:47:16,https://stackoverflow.com/questions/36013137/gensim-word2vec-unexpectedly-pruning
What meaning does the length of a Word2vec vector have?,"<p>I am using Word2vec through <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
&gt;&gt;&gt; king_vector = w2v['king']
&gt;&gt;&gt; numpy.linalg.norm(king_vector)
2.9022589
</code></pre>

<p>However, in the <a href=""https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213"" rel=""noreferrer""><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>

<pre><code>&gt;&gt;&gt; w2v.init_sims()
&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]
&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)
0.99999994
</code></pre>

<p>The larger vector is just a scaled up version of the unit vector:</p>

<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector
array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        ... (some lines omitted) ...
        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)
</code></pre>

<p>Given that word similarity comparisons in Word2Vec are done by <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>

<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>
","python, nlp, gensim, word2vec","<p>I think the answer you are looking for is described in the 2015 paper <a href=""https://arxiv.org/pdf/1508.02297.pdf"" rel=""noreferrer"">Measuring Word Significance
using
Distributed Representations of Words</a> by Adriaan Schakel and Benjamin Wilson. The key points:</p>

<blockquote>
  <p>When a word appears
  in different contexts, its vector gets moved in
  different directions during updates. The final vector
  then represents some sort of weighted average
  over the various contexts. Averaging over vectors
  that point in different directions typically results in
  a vector that gets shorter with increasing number
  of different contexts in which the word appears.
  For words to be used in many different contexts,
  they must carry little meaning. Prime examples of
  such insignificant words are high-frequency stop
  words, which are indeed represented by short vectors
  despite their high term frequencies ...</p>
</blockquote>

<hr>

<blockquote>
  <p>For given term frequency,
  the vector length is seen to take values only in a
  narrow interval. That interval initially shifts upwards
  with increasing frequency. Around a frequency
  of about 30, that trend reverses and the interval
  shifts downwards.</p>
  
  <p>...</p>
  
  <p>Both forces determining the length of a word
  vector are seen at work here. Small-frequency
  words tend to be used consistently, so that the
  more frequently such words appear, the longer
  their vectors. This tendency is reflected by the upwards
  trend in Fig. 3 at low frequencies. High-frequency
  words, on the other hand, tend to be
  used in many different contexts, the more so, the
  more frequently they occur. The averaging over
  an increasing number of different contexts shortens
  the vectors representing such words. This tendency
  is clearly reflected by the downwards trend
  in Fig. 3 at high frequencies, culminating in punctuation
  marks and stop words with short vectors at
  the very end.</p>
  
  <p>...</p>
  
  <p><a href=""https://i.sstatic.net/NI9je.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/NI9je.png"" alt=""Graph showing the trend described in the previous excerpt""></a></p>
  
  <p>Figure 3: Word vector length <em>v</em> versus term frequency
  <em>tf</em> of all words in the hep-th vocabulary.
  Note the logarithmic scale used on the frequency
  axis. The dark symbols denote bin means with the
  <i>k</i>th bin containing the frequencies in the interval
  [2<sup><i>kâˆ’1</i></sup>, 2<sup><i>k</i></sup> âˆ’ 1] with <em>k</em> = 1, 2, 3, . . .. These means
  are included as a guide to the eye. The horizontal
  line indicates the length <em>v</em> = 1.37 of the mean
  vector</p>
</blockquote>

<hr>

<blockquote>
  <h3>4 Discussion</h3>
  
  <p>Most applications of distributed representations of
  words obtained through word2vec so far centered
  around semantics. A host of experiments have
  demonstrated the extent to which the direction of
  word vectors captures semantics. In this brief report,
  it was pointed out that not only the direction,
  but also the length of word vectors carries important
  information. Specifically, it was shown that
  word vector length furnishes, in combination with
  term frequency, a useful measure of word significance. </p>
</blockquote>
",35,32,11703,2016-03-16 11:31:27,https://stackoverflow.com/questions/36034454/what-meaning-does-the-length-of-a-word2vec-vector-have
Using NearestNeighbors and word2vec to detect sentence similarity,"<p>I have calculated a word2vec model using python and <code>gensim</code> in my corpus.</p>

<p>Then I calculated the mean word2vec vector for each sentence (averaging all the vectors for all the words in the sentence) and stored it in a pandas data frame.
The columns of the pandas data frame <code>df</code> are:</p>

<ul>
<li>sentence</li>
<li>Book title (the book where the sentence comes from)</li>
<li>mean-vector (the mean of the word2vec vectors in the sentence - size 100)</li>
</ul>

<p>I am trying to use <code>scikit-learn</code> <code>NearestNeighbors</code> to detect sentence similarity (I could probably use doc2vec instead, but one of the objectives is to compare this method against doc2vec).</p>

<p>This is my code:</p>

<pre><code>X = df['mean_vector'].values
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p>I think somehow I should iterate the vectors, to be able to calculate on a <code>row == sentence</code> basis the nearest neighbours of each row, but it seems this exceeds my current (limited) python skills.</p>

<p>This is the data  of the first cell in <code>df['mean_vector'][0]</code>.  It is a full vector size 100 averaged over the vectors of the sentence.</p>

<pre><code>array([ -2.14208905e-02,   2.42093615e-02,  -5.78106642e-02,
     1.32915592e-02,  -2.43393257e-02,  -1.41872400e-02,
     2.83471867e-02,  -2.02910602e-02,  -5.49359620e-02,
    -6.70913085e-02,  -5.56188896e-02,  -2.95186806e-02,
     4.97652516e-02,   7.16793686e-02,   1.81338750e-02,
    -1.50108105e-02,   1.79438610e-02,  -2.41483524e-02,
     4.97504435e-02,   2.91026086e-02,  -6.87966943e-02,
     3.27585079e-02,   5.10644279e-02,   1.97029337e-02,
     7.73109496e-02,   3.23865712e-02,  -2.81659551e-02,
    -9.69715789e-03,   5.23059331e-02,   3.81100960e-02,
    -3.62489261e-02,  -3.40068117e-02,  -4.90736961e-02,
     8.72346922e-04,   2.27111522e-02,   1.06063476e-02,
    -3.93234752e-02,  -1.10617064e-01,   8.05142429e-03,
     4.56497036e-02,  -1.73281748e-02,   2.35153548e-02,
     5.13465842e-03,   1.88336968e-02,   2.40451116e-02,
     3.79024050e-03,  -4.83284928e-02,   2.10295208e-02,
    -4.92134318e-03,   1.01532964e-02,   8.02216958e-03,
    -6.74675079e-03,  -1.39653292e-02,  -2.07276996e-02,
     9.73508134e-03,  -7.37899616e-02,  -2.58320477e-02,
    -1.10700730e-05,  -4.53227758e-02,   2.31859135e-03,
     1.40053956e-02,   1.61973312e-02,   3.01702786e-02,
    -6.96818605e-02,  -3.47468331e-02,   4.79541793e-02,
    -1.78820305e-02,   5.99209731e-03,  -5.92620336e-02,
     7.34678581e-02,  -5.23381204e-05,  -5.07357903e-02,
    -2.55154949e-02,   5.06089740e-02,  -3.70467864e-02,
    -2.04878468e-02,  -7.62404222e-03,  -5.38200373e-03,
     7.68705690e-03,  -3.27000804e-02,  -2.18365286e-02,
     2.34392099e-03,  -3.02998684e-02,   9.42565035e-03,
     3.24523374e-02,  -1.10793915e-02,   3.06244520e-03,
    -1.82240941e-02,  -5.70741761e-03,   3.13486941e-02,
    -1.15621388e-02,   1.10221673e-02,  -3.55655849e-02,
    -4.56304513e-02,   5.54837054e-03,   4.38252240e-02,
     1.57828294e-02,   2.65670624e-02,   8.08797963e-03,
     4.55569401e-02], dtype=float32)
</code></pre>

<p>I have also tried to do:</p>

<pre><code>for vec in df['mean_vector']:
X = vec
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
</code></pre>

<p>But I only get the following warning:</p>

<pre><code>DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
</code></pre>

<p>If there is an example on github using word2vec and <code>NearestNeighbors</code> in a similar scenario I would love to see it.</p>
","python, scikit-learn, nearest-neighbor, word2vec","<p>The reason your edit is throwing an error is because <code>sklearn</code> expects a 2D input, with each example being in a new row. You can either use <code>X.reshape(1, -1)</code> or <code>[X]</code>, the first is better practice. Without the raw data or a proper MWE it's hard to say exactly is going wrong, but my guess is that something is going wrong with either putting the data in or out of the dataframe. Check that <code>X.shape</code> makes sense to you.</p>

<p>Below is the example I used to check everything worked for me:</p>

<pre><code>from sklearn.neighbors import NearestNeighbors
from gensim.models import Word2Vec
import numpy as np

a = """"""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore
magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea 
commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla 
pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est 
laborum.""""""
a = [x.split(' ') for x in a.split('\n') if len(x)]
model = Word2Vec(a, min_count=1)

# Get the average of all of the words to get data for a sentence
b = np.array([np.mean([model[xx] for xx in x], axis=0) for x in a])
# Check it's the correct shape
print b.shape

nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(b)
</code></pre>
",2,4,5133,2016-03-23 02:29:09,https://stackoverflow.com/questions/36168756/using-nearestneighbors-and-word2vec-to-detect-sentence-similarity
KeyError in Doc2Vec model even when min_count set to 1 during training,"<p>I'm using doc2vec with a corpus of about 1 million titles. To train the corpus, I'm using the following code: </p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=10, size=300, workers=4)
model.build_vocab(corpus)
for epoch in range(10):
    model.train(corpus)
</code></pre>

<p>Everything seems to train properly and I am able to infer a vector using titles.most_similar. </p>

<p>I encounter a problem, however, when I try to use the vectors. It seems as though some documents are missing from the final model! I.e.: </p>

<pre><code>model.docvecs['SENT_157000']
</code></pre>

<blockquote>
  <p>KeyError: 'SENT_157000'</p>
</blockquote>

<p>I checked the gensim forum and stackoverflow and the only suggestion I could find was to ensure that the min_count = 1. I did that but I'm still having this issue.</p>
","python, gensim, word2vec","<p>From <code>gensim</code>'s <code>Doc2Vec</code> <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow noreferrer"">documentation</a>, input to <code>Doc2Vec</code>  should be an iterator of <code>LabeledSentence</code> objects.</p>

<p>Your <code>corpus</code> variable needs to be constructed as follows:</p>

<pre><code>class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for uid, line in enumerate(open(self.filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])


corpus = LabeledLineSentence(filename)
</code></pre>

<p>followed by</p>

<pre><code>model.train(corpus)
</code></pre>
",0,0,726,2016-03-25 16:25:17,https://stackoverflow.com/questions/36223864/keyerror-in-doc2vec-model-even-when-min-count-set-to-1-during-training
Can doc2vec be used if my text data is incrementally increasing?,"<p>I am new to Doc2vec use. In case I could get some advice before I start on it, it will save a LOT of time.
My data is an stream of text data (such as tweets) continuously coming in time. For clustering these tweets, I was thinking of using doc2vec to reduce the text content into a fixed size vector and use that to compare between documents. 
So in this case, the text data is getting accumulated over time, can this be still used with Doc2Vec, I may have to learn the model again and again (may be!) or could I use some large corpus such as Wikipedia or a large newscorpus to train the Doc2Vec model.</p>

<p>Any suggestions will help!</p>

<p>Thanks in Advance.</p>
","twitter, text, gensim, word2vec","<p>The gensim Doc2Vec class does <em>not</em> support adjusting the model with new documents, but it <em>can</em> 'infer' and report a vector for new documents, based on the model learned from an earlier bulk training. </p>

<p>So, you can use that new inferred vector to compare the new document to older ones, or feed it to a trained classifier, etc. </p>

<p>If new documents continue to arrive, and especially if the balance of topics/meaning in your documents drifts over time, you would likely at some point want to discard a model based on older data, and create a new model based on your larger (or more recent) data.</p>

<p>(Note that vectors from the old model and new model <em>won't</em> be directly comparable. Training sessions involve a lot of randomness, and the meanings of dimensions/directions in any one model are somewhat arbitrary. It's the relative positions of vectors, from within the same model, that has some interpretive power.)</p>
",1,0,790,2016-04-01 15:39:02,https://stackoverflow.com/questions/36360367/can-doc2vec-be-used-if-my-text-data-is-incrementally-increasing
Is it possible to use gensim doc2vec for classification,"<p>I have some training sentences generally of warning nature.  Now my goal is to predict weather incoming sentence is a warning message or not. I have gone through <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">Sentiment Analysis Using Doc2Vec</a> but according to my understanding it have not considered newly arriving sentence to predict if its positive or negative. </p>

<p>According to my experience I found that the output vector in <code>gensim.doc2vec</code> for each sentence is dependent on other sentences as well, which means we can not directly use the model to generate vector for newly arriving sentence. Please anyone help me with this. Thanks.</p>
","nlp, gensim, word2vec","<p>One way to generate new vectors is using the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec.infer_vector"" rel=""nofollow"">infer_vector()</a> function, which will generate a new vector based on a trained model. Since the model is frozen when you use this function, the new vector will be based on the existing sentence vectors, but not change them.</p>
",4,1,1477,2016-04-04 08:29:35,https://stackoverflow.com/questions/36397798/is-it-possible-to-use-gensim-doc2vec-for-classification
Why Gensim doc2vec give AttributeError: &#39;list&#39; object has no attribute &#39;words&#39;?,"<p>I am trying to experiment gensim doc2vec, by using following code. As far as  I understand from tutorials, it should work. However it gives <strong>AttributeError: 'list' object has no attribute 'words'.</strong></p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, Doc2Vec
document = LabeledSentence(words=['some', 'words', 'here'], tags=['SENT_1']) 
model = Doc2Vec(document, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>So what did I do wrong? Any help please. Thank you. I am using python 3.5 and gensim 0.12.4</p>
","python-3.x, gensim, word2vec","<p>Input to <code>gensim.models.doc2vec</code> should be an <strong>iterator</strong> over the <code>LabeledSentence</code> (say a list object). Try:</p>

<pre><code>model = Doc2Vec([document], size = 100, window = 1, min_count = 1, workers=1)
</code></pre>

<p>I have reduced the <code>window</code> size, and <code>min_count</code> so that they make sense for the given input. Also go through this nice tutorial on <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">Doc2Vec</a>, if you haven't already.</p>
",4,10,7497,2016-04-08 21:55:54,https://stackoverflow.com/questions/36509957/why-gensim-doc2vec-give-attributeerror-list-object-has-no-attribute-words
Kmeans fit_predict with word2vec,"<p>I am trying to cluster my word vectors using kmeans as described <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors"" rel=""nofollow"">here</a>. 
The code snippet I am using</p>

<pre><code># Set ""k"" (num_clusters) to be 1/5th of the vocabulary size, or an
# average of 5 words per cluster
word_vectors = model.syn0
num_clusters = word_vectors.shape[0] / 5

# Initalize a k-means object and use it to extract centroids
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )
</code></pre>

<p>I am getting the following error
TypeError: 'float' object cannot be interpreted as an integer</p>

<p>Could someone please help</p>
","python-3.x, k-means, word2vec","<p>Found out the error. Number of clusters must be an integer so I did the following </p>

<p>num_clusters = int(word_vectors.shape[0] / 5)</p>
",5,3,1917,2016-04-10 03:09:07,https://stackoverflow.com/questions/36525317/kmeans-fit-predict-with-word2vec
How to concatenate word vectors to form sentence vector,"<p>I have learned in some essays (Tomas Mikolov...) that a better way of forming the vector for a sentence is to concatenate the word-vector.</p>
<p>but due to my clumsy in mathematics, I am still not sure about the details.</p>
<p>for example,</p>
<p>supposing that the dimension of word vector is m; and that a sentence has n words.</p>
<p>what will be the correct result of concatenating operation?</p>
<p>is it a row vector of 1 x m*n ?     or a matrix of m x n ?</p>
","machine-learning, deep-learning, nlp, word2vec","<p>There are at least three common ways to combine embedding vectors; (a) summing, (b) summing &amp; averaging or (c) concatenating. So in your case, with concatenating, that would give you a <code>1 x m*a</code> vector, where <code>a</code> is the number of sentences. In the other cases, the vector length stays the same. See <code>gensim.models.doc2vec.Doc2Vec</code>, <code>dm_concat</code> and <code>dm_mean</code> - it allows you to use any of those three options [1,2].</p>

<p>[1] <a href=""http://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.LabeledLineSentence"" rel=""noreferrer"">http://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.LabeledLineSentence</a></p>

<p>[2] <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py"" rel=""noreferrer"">https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py</a></p>
",5,4,5070,2016-04-20 00:32:34,https://stackoverflow.com/questions/36731784/how-to-concatenate-word-vectors-to-form-sentence-vector
"Word2Vec, most_similar(word1) returns same output on different runs","<p>I'm doing a small experiment where I have 2000 tweets as my input document. I train word2vec on this input tweets and then find the top 10 most similar words to a particular word - <code>w1</code>.</p>

<p>My concern is if I run word2vec 10 times (with same parameters) and inspect the top 10 most similar words to <code>w1</code>, gives me the same set of words (weights are also the same). </p>

<p>Now AFAIK word2vec initializes random weights at the beginning so why it's giving me the same output at different runs?</p>
","python, gensim, word2vec","<p>The default <code>seed</code> of gensim's word2vec is set to 1 as described in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow"">documentation</a>. For different outputs, try varying the seed variable</p>

<pre><code>from gensim.models.word2vec import Word2Vec

sentences = ['a quick brown fox jumps over a dog'.split()]
model = Word2Vec(sentences, min_count=1, size=5, seed=1)
print model['fox']

Out: array([ 0.0629408 ,  0.07386301, -0.00788937, -0.09287976,  0.05859811], dtype=float32)

model = Word2Vec(sentences, min_count=1, size=5, seed=2)
print model['fox']

Out: array([-0.05617044,  0.00294411, -0.09557492,  0.02923537, -0.08322554], dtype=float32)
</code></pre>
",0,0,1128,2016-04-21 00:13:12,https://stackoverflow.com/questions/36757409/word2vec-most-similarword1-returns-same-output-on-different-runs
How to load pre-trained model with in gensim and train doc2vec with it?,"<p>I am having a ready to go word2vec model that I already trained. I have serialized it as a CSV file:</p>

<pre><code>word,  v0,     v1,     ..., vN
house, 0.1234, 0.4567, ..., 0.3461
car,   0.456,  0.677,  ..., 0.3461
</code></pre>

<p>What I'd like to know is how I can load that word vector model in <code>gensim</code> and use that to train a paragraph or doc2vec model.</p>

<p>This <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec tutorial</a> says I can load a model in form of a ""<code># C text format</code>"" but I have no idea what that actually means. What is ""C text format"" in the first place but more important: </p>

<ul>
<li>How can I load my word2vec model and use it for doc2vec training?</li>
</ul>

<p>How do I build the vocabulary from my word2vec model?</p>
","python, gensim, word2vec, doc2vec","<p>Doc2Vec does not need word-vectors as an input: it will create any word-vectors that are needed during its own training. (And some modes, like pure DBOW â€“ <code>dm=0, dbow_words=0</code> â€“ don't use or train word-vectors at all.)</p>

<p>Seeding a Doc2Vec model with word-vectors might help or hurt; there's not much theory or published results to offer guidance. There's an experimental method on Word2Vec, <code>intersect_word2vec_format()</code>, that can merge word2vec-c-format vectors into a model with an existing vocabulary, but you'd need to review the source to really understand its assumptions:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/51753b95415bbc344ea6af671818277464905ea2/gensim/models/word2vec.py#L1140"" rel=""nofollow"">https://github.com/RaRe-Technologies/gensim/blob/51753b95415bbc344ea6af671818277464905ea2/gensim/models/word2vec.py#L1140</a></p>
",1,1,2766,2016-04-23 18:52:33,https://stackoverflow.com/questions/36815038/how-to-load-pre-trained-model-with-in-gensim-and-train-doc2vec-with-it
How to fix &quot;MetadataFetchFailedException: Missing an output location for shuffle&quot;?,"<p>If I increase the model size of my word2vec model I start to get this kind of exception in my <a href=""http://pastebin.com/Nxu3rXk8"" rel=""noreferrer"">log</a>:</p>

<pre><code>org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 6
    at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:542)
    at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:538)
    at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
    at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
    at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:538)
    at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
    at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
    at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
    at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:96)
    at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:95)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
    at scala.collection.AbstractIterator.to(Iterator.scala:1157)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>I tried to write my own ""save model"" version which looks like this:</p>

<pre class=""lang-scala prettyprint-override""><code>  def save(model: Word2VecModel, sc: SparkContext, path: String): Unit = {

    println(""Saving model as CSV .."")

    val vectorSize = model.getVectors.values.head.size

    println(""vectorSize=""+vectorSize)

    val SEPARATOR_TOKEN = "" ""    
    val dataArray = model.getVectors.toSeq.map { case (w, v) =&gt; Data(w, v) }

    println(""Got dataArray .."")
    println(""parallelize(dataArray, 10)"")
    val par = sc.parallelize(dataArray, 10)
          .map(d =&gt; {

            val sb = new mutable.StringBuilder()
            sb.append(d.word)
            sb.append(SEPARATOR_TOKEN)

            for(v &lt;- d.vector) {
              sb.append(v)
              sb.append(SEPARATOR_TOKEN)
            }
            sb.setLength(sb.length - 1)
            sb.append(""\n"")
            sb.toString()
          })
    println(""repartition(1)"")
    val rep = par.repartition(1)
    println(""collect()"")
    val vectorsAsString = rep.collect()

    println(""Collected serialized vectors .."")    

    val cfile = new mutable.StringBuilder()

    cfile.append(vectorsAsString.length)
    cfile.append("" "")
    cfile.append(vectorSize)
    cfile.append(""\n"")

    val sb = new StringBuilder
    sb.append(""word,"")
    for(i &lt;- 0 until vectorSize) {
      sb.append(""v"")
      sb.append(i.toString)
      sb.append("","")
    }
    sb.setLength(sb.length - 1)
    sb.append(""\n"")

    for(vectorString &lt;- vectorsAsString) {
      sb.append(vectorString)
      cfile.append(vectorString)
    }

    println(""Saving file to "" + new Path(path, ""data"").toUri.toString)
    sc.parallelize(sb.toString().split(""\n""), 1).saveAsTextFile(new Path(path+"".csv"", ""data"").toUri.toString)
    sc.parallelize(cfile.toString().split(""\n""), 1).saveAsTextFile(new Path(path+"".cs"", ""data"").toUri.toString)
  }
</code></pre>

<p>Apparently it's working similar to their <a href=""https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/feature/Word2Vec.scala#L625"" rel=""noreferrer"">current implementation</a> - it doesn't.</p>

<p>I'd like to get a word2vec model. It works with small files but not if the model gets larger.</p>
","scala, apache-spark, apache-spark-mllib, word2vec","<p><code>MetadataFetchFailedException</code> is thrown when a <code>MapOutputTracker</code> on an executor could not find requested shuffle map outputs for partitions in local cache and tried to fetch them remotely from the driver's <code>MapOutputTracker</code>.</p>

<p>That could lead to few conclusions:</p>

<ol>
<li>The driver's memory issues</li>
<li>The executors' memory issues</li>
<li>Executors being lost</li>
</ol>

<p>Please review the logs looking for issues reported as ""Executor lost"" INFO messages and/or review web UI's Executors page and see how the executors work.</p>

<p>The root cause of executors being lost may also be that the cluster manager has decided to kill ill-behaved executors (that may have used up more memory than requested).</p>

<p>See the other question <a href=""https://stackoverflow.com/q/34941410/1305344"">FetchFailedException or MetadataFetchFailedException when processing big data set</a> for more insights.</p>
",19,10,30122,2016-04-23 19:38:11,https://stackoverflow.com/questions/36815506/how-to-fix-metadatafetchfailedexception-missing-an-output-location-for-shuffle
Getting Data from wtforms,"<p>i've been experimenting with word2vec and gensim as its python implementation. Now i have to make my model accesible on a Website - so i need Flask. I defined a Form in forms.py like that:</p>

<pre><code>from wtforms Import Form, StringField, SubmitField, validators
class msForm(Form):
    ms_1 = StringField(label='Eingabe_1', default = 'king', validators=[validators.input_required()])
    ms_2 = StringField(label='Eingabe_2', default = 'man', validators=[validators.input_required()])
    ms_3 = StringField(label='Eingabe_3', default = 'queen', validators=[validators.input_required()])
    submit=SubmitField()
</code></pre>

<p>now my views.py looks like:</p>

<pre><code>from app import app
from .forms import msForm
from flask import render_template, flash, request
from gensim.models import word2vec

global model
model = word2vec.Word2Vec.load_word2vec_format('./app/static/GoT.model.vector', binary=True)
global form
form = msForm()

@app.route('/')
def index():
return render_template('my-form.html', form=form)

@app.route('/', methods=['POST'])
def msForm_post():
    text1 = form.ms_1.data
    text2 = form.ms_2.data
    text3 = form.ms_3.data      
    processed_text = model.most_similar(positive=[text3, text2], negative = [text1])        
    return processed_text[0][0]
</code></pre>

<p>When i execute my run.py, go to <a href=""http://localhost:5000/"" rel=""nofollow"">http://localhost:5000/</a>, change my Input and click the 'Submit' button, i only get the answer on my default-input. Why doesnt he send my Input?</p>

<p>Thanks for your help and sorry for my english,
FFoDWindow</p>
","python, input, flask, wtforms, word2vec","<p>Instantiate the form in the view, not as a single global.  You're using the same form instance over and over, and that instance was created without any form data.  You can group both GET and POST in one view.</p>

<pre><code>@app.route('/')
def index():
    form = msForm(request.form)
    # request.form not needed when using Flask-WTF

    if request.method == 'POST' and form.validate():
        processed_text = model.most_similar(positive=[form.ms_3.data, form.ms_2.data], negative=[form.ms_1.data])
        return processed_text[0][0]

    return render_template('my-form.html', form=form)
</code></pre>
",3,0,1495,2016-05-02 18:25:11,https://stackoverflow.com/questions/36989108/getting-data-from-wtforms
How to get vocabulary word count from gensim word2vec?,"<p>I am using gensim word2vec package in python. I know how to get the vocabulary from the trained model. But how to get the word count for each word in vocabulary?</p>
","gensim, word2vec","<p>Each word in the vocabulary has an associated vocabulary object, which contains an index and a count.</p>

<pre><code>vocab_obj = w2v.vocab[""word""]
vocab_obj.count
</code></pre>

<p>Output for google news w2v model: 2998437</p>

<p>So to get the count for each word, you would iterate over all words and vocab objects in the vocabulary.</p>

<pre><code>for word, vocab_obj in w2v.vocab.items():
  #Do something with vocab_obj.count
</code></pre>
",32,16,34091,2016-05-12 15:12:23,https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec
How to open a binary file stored in Google App Engine?,"<p>I have generated a binary file with the <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow"">word2vec</a>, stored the resulting <code>.bin</code> file to my GCS bucket, and ran the following code in my App Engine app handler:</p>

<pre><code>    gcs_file = gcs.open(filename, 'r')
    content = gcs_file.read().encode(""utf-8"")
    """""" call word2vec with content so it doesn't need to read a file itself, as we don't have a filesystem in GAE """"""
</code></pre>

<p>Fails with this error:
<code>content = gcs_file.read().encode(""utf-8"")
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf6 in position 15: ordinal not in range(128)</code></p>

<p>A similar decode error happens if I try <code>gcs_file.read()</code>, or <code>gcs_file.read().decode(""utf-8"").encode(""utf-8"")</code>.</p>

<p>Any ideas on how to read a binary file from GCS?</p>

<p>Thanks</p>
","python, google-app-engine, binary, google-cloud-storage, word2vec","<p>If it is binary then it will not take a character encoding, which is what <code>UTF-8</code> is. <code>UTF-8</code> is just one possible binary encoding of the <code>Unicode</code> specification for character sets ( <code>String</code> data ). You need to go back and read up on what <code>UTF-8</code> and <code>ASCII</code> represent and how they are used.</p>

<p>If it was not text data that was encoded with a specific encoding then it is not going to magically just <code>decode</code>, which is why you are getting that error. <code>can't decode byte 0xf6 in position 15</code> is not a valid <code>ASCII</code> value.</p>
",0,-1,768,2016-05-17 21:28:48,https://stackoverflow.com/questions/37286539/how-to-open-a-binary-file-stored-in-google-app-engine
How to get word vectors from a gensim Doc2Vec?,"<p>I trained a gensim.models.doc2vec.Doc2Vec model<br>
d2v_model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)
and I can get document vectors by
docvec = d2v_model.docvecs[0]</p>

<p>How can I get word vectors from trained model ?</p>
","gensim, word2vec, doc2vec","<p>Doc2Vec inherits from Word2Vec, and thus you can access word vectors the same as in Word2Vec, directly by indexing the model:</p>

<pre><code>wv = d2v_model['apple']
</code></pre>

<p>Note, however, that a Doc2Vec training mode like pure DBOW (<code>dm=0</code>) doesn't need or create word vectors. (Pure DBOW still works pretty well and fast for many purposes!) If you do access word vectors from such a model, they'll just be the automatic randomly-initialized vectors, with no meaning. </p>

<p>Only when the Doc2Vec mode itself co-trains word-vectors, as in the DM mode (default <code>dm=1</code>) or when adding optional word-training to DBOW (<code>dm=0, dbow_words=1</code>), are word-vectors and doc-vectors both learned simultaneously. </p>
",14,4,9324,2016-05-19 23:49:10,https://stackoverflow.com/questions/37335842/how-to-get-word-vectors-from-a-gensim-doc2vec
Should I use word2vec to do word embedding including testing data?,"<p>I am a new people in NLP and I am try do the text classification job. Before doing the job, I know that we should do word embedding.
My question is should I do word embedding job only on training data <strong>(so that testing data get vector just from pre-trained vec-model of training data)</strong>, or both on training data &amp; testing data?</p>
","machine-learning, nlp, text-classification, word2vec, word-embedding","<p>This is a very important question. In NN community what typically people do is to use a threshold (i.e. frequency &lt; = 2) in the training set and replace all words which occur less than that threshold by UNK token. Then in the test time, if there is a word that doesn't match an actual training set word, UNK's representation will replace it.</p>
",-1,0,810,2016-05-22 03:43:42,https://stackoverflow.com/questions/37370299/should-i-use-word2vec-to-do-word-embedding-including-testing-data
install_github(&quot;bmschmidt/wordVectors&quot;),"<p>I am trying to install this package into my R enivornment. <a href=""https://github.com/bmschmidt/wordVectors"" rel=""nofollow"">https://github.com/bmschmidt/wordVectors</a></p>

<p>I am getting compiler errors. It's the same errors identified on the github page. <a href=""https://github.com/bmschmidt/wordVectors/issues/2"" rel=""nofollow"">https://github.com/bmschmidt/wordVectors/issues/2</a></p>

<p>I'll post a shortened version here because there are about 130 lines that are almost identical.</p>

<pre><code>&gt; require(devtools)
&gt; install_github(""bmschmidt/wordVectors"")
Downloading GitHub repo bmschmidt/wordVectors@master
from URL https://api.github.com/repos/bmschmidt/wordVectors/zipball/master
Installing wordVectors
""C:/PROGRA~1/R/R-32~1.2/bin/x64/R"" --no-site-file --no-environ --no-save --no-restore --quiet CMD INSTALL  \
  ""C:/Users/usr/AppData/Local/Temp/Rtmpqw8mSh/devtools249437ed8a0/bmschmidt-wordVectors-6cfb717"" --library=""C:/Program  \
  Files/R/R-3.2.2/library"" --install-tests 

* installing *source* package 'wordVectors' ...
** libs

*** arch - i386
gcc -m32 -I""C:/PROGRA~1/R/R-32~1.2/include"" -DNDEBUG     -I""d:/RCompile/r-compiling/local/local320/include""  -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result -w   -O3 -Wall  -std=gnu99 -mtune=core2 -c tmcn_distance.c -o tmcn_distance.o
gcc -m32 -I""C:/PROGRA~1/R/R-32~1.2/include"" -DNDEBUG     -I""d:/RCompile/r-compiling/local/local320/include""  -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result -w   -O3 -Wall  -std=gnu99 -mtune=core2 -c tmcn_word2vec.c -o tmcn_word2vec.o
gcc -m32 -shared -s -static-libgcc -o wordVectors.dll tmp.def tmcn_distance.o tmcn_word2vec.o -pthread -Ld:/RCompile/r-compiling/local/local320/lib/i386 -Ld:/RCompile/r-compiling/local/local320/lib -LC:/PROGRA~1/R/R-32~1.2/bin/i386 -lR
installing to C:/Program Files/R/R-3.2.2/library/wordVectors/libs/i386

*** arch - x64
gcc -m64 -I""C:/PROGRA~1/R/R-32~1.2/include"" -DNDEBUG     -I""d:/RCompile/r-compiling/local/local320/include""  -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result -w   -O2 -Wall  -std=gnu99 -mtune=core2 -c tmcn_distance.c -o tmcn_distance.o
gcc -m64 -I""C:/PROGRA~1/R/R-32~1.2/include"" -DNDEBUG     -I""d:/RCompile/r-compiling/local/local320/include""  -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result -w   -O2 -Wall  -std=gnu99 -mtune=core2 -c tmcn_word2vec.c -o tmcn_word2vec.o
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s: Assembler messages:
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1138: Error: no such instruction: `vfmadd312ss (%rbx),%xmm0,%xmm1'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1157: Error: no such instruction: `vfmadd312ss 4(%rbx),%xmm0,%xmm9'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1161: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm10'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1166: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm1'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1171: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm8'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1176: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm9'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1181: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm10'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1186: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm1'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:1196: Error: no such instruction: `vfmadd312ss (%rbx,%rdx,4),%xmm0,%xmm1'
... Removed lines here as they all look like above and below ...
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:2677: Error: no such instruction: `vfmadd312ss (%rax,%r8,4),%xmm0,%xmm8'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:2682: Error: no such instruction: `vfmadd312ss (%rax,%r11,4),%xmm0,%xmm5'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:2685: Error: no such instruction: `vfmadd312ss (%rax,%r8,4),%xmm0,%xmm4'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:2691: Error: no such instruction: `vfmadd312ss (%rax,%r11,4),%xmm0,%xmm10'
C:\Users\usr\AppData\Local\Temp\ccr7eSJc.s:2694: Error: no such instruction: `vfmadd312ss (%rax,%r8,4),%xmm0,%xmm9'
make: *** [tmcn_word2vec.o] Error 1
Warning: running command 'make -f ""Makevars.win"" -f ""C:/PROGRA~1/R/R-32~1.2/etc/x64/Makeconf"" -f ""C:/PROGRA~1/R/R-32~1.2/share/make/winshlib.mk"" SHLIB=""wordVectors.dll"" WIN=64 TCLBIN=64 OBJECTS=""tmcn_distance.o tmcn_word2vec.o""' had status 2
ERROR: compilation failed for package 'wordVectors'
* removing 'C:/Program Files/R/R-3.2.2/library/wordVectors'
Error: Command failed (1)
</code></pre>

<p>Here is my version of R running on Windows 7.</p>

<pre><code>&gt; R.version
               _                           
platform       x86_64-w64-mingw32          
arch           x86_64                      
os             mingw32                     
system         x86_64, mingw32             
status                                     
major          3                           
minor          2.2                         
year           2015                        
month          08                          
day            14                          
svn rev        69053                       
language       R                           
version.string R version 3.2.2 (2015-08-14)
nickname       Fire Safety                 
</code></pre>

<p>Is this a compiler issue as the github dev suspected? Is there a specific compiler I should try? What do I need to do with R to tell it to use a different compiler?</p>

<p>I appreciate your help! Thank you!</p>
","r, windows, compiler-errors, word2vec","<p>My solution will be to use this implementation of wordVectors instead where I am not getting the issue.
<a href=""https://github.com/cpeeples/wordVectors"" rel=""nofollow"">https://github.com/cpeeples/wordVectors</a></p>

<p>I didn't realize there were so many copies and extensions of the this same code available before.</p>
",1,1,1867,2016-05-25 18:58:37,https://stackoverflow.com/questions/37445507/install-githubbmschmidt-wordvectors
Word embeddings over user/customer reviews corpus,"<p>Most of the embeddings, publicly available, that I know are done over news articles, which use a different language/words as the one used in user/customer reviews. </p>

<p>Although such embeddings can be used in NLP tasks concerning reviews
and user generated content, I think the difference in language has an important role, and as such I would rather use embeddings trained over user generated content, such as product reviews.</p>

<p>I'm looking for a corpus of reviews or comments in English -- although in German and Dutch would also be useful -- to generate embeddings, or alternatively embeddings already trained over such a corpus.</p>
","nlp, corpus, word2vec","<p>Found two datasets/corpus in English:</p>

<p><a href=""https://www.yelp.com/dataset_challenge"" rel=""nofollow"">https://www.yelp.com/dataset_challenge</a></p>

<p><a href=""https://snap.stanford.edu/data/web-Amazon.html"" rel=""nofollow"">https://snap.stanford.edu/data/web-Amazon.html</a></p>

<p>in German:</p>

<p><a href=""http://www.uni-weimar.de/en/media/chairs/webis/corpora/corpus-webis-cls-10/"" rel=""nofollow"">http://www.uni-weimar.de/en/media/chairs/webis/corpora/corpus-webis-cls-10/</a></p>
",1,0,548,2016-05-27 08:39:42,https://stackoverflow.com/questions/37478789/word-embeddings-over-user-customer-reviews-corpus
Group by and aggregate problems for numpy arrays over word vectors,"<p>My pandas data frame looks something like this:</p>

<pre><code>Movieid review  movieRating     wordEmbeddingVector
 1       ""text""    4          [100 dimensional vector]
</code></pre>

<p>I am trying to run a doc2vec implementation and I want to be able to group by movie ids and the take the sum of the vectors in wordEmbeddingVector and calculate a cosine similarity between the summed vector and the input vector
I tried doing </p>

<pre><code>movie_groupby = movie_data.groupby('movie_id').agg(lambda v : cosineSimilarity(np.sum(movie_data['textvec'])), inputvector)
</code></pre>

<p>But it seemed to run for ages and I thought I might be doing something wrong. So I tried to remove the similarity function and just group by and sum. But this seems to not finish as well (well 1 hour and up now) Am I doing something wrong or is it actually just that slow? I have 135392 rows in my data frame so its not massive.</p>

<pre><code>movie_groupby = movie_data.groupby('movie_id').agg(lambda v : np.sum(movie_data['textvec']))
</code></pre>

<p>Much Appreciated! </p>
","python, pandas, machine-learning, word2vec, doc2vec","<p>There is a bug in your code. Inside your lambda function you sum across the entire dataframe instead of just the group. This should fix things:</p>

<pre><code>movie_groupby = movie_data.groupby('movie_id').agg(lambda v: np.sum(v['textvec']))
</code></pre>

<p>Note: I replaced <code>hotel_data</code> with <code>movie_data</code>, but that must have been just a typo.</p>
",0,0,534,2016-06-02 18:33:28,https://stackoverflow.com/questions/37599647/group-by-and-aggregate-problems-for-numpy-arrays-over-word-vectors
Different models with gensim Word2Vec on python,"<p>I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words).</p>

<p>For instance let us have:</p>

<pre><code>sentences=[['first','second','third','fourth']]*n
</code></pre>

<p>and I implement two identical models:</p>

<pre><code>model = gensim.models.Word2Vec(sententes, min_count=1,size=2)
model2=gensim.models.Word2Vec(sentences, min_count=1,size=2)
</code></pre>

<p>I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. </p>

<p>For instance, if n=100 I obtain</p>

<pre><code>print(model['first']==model2['first'])
True
</code></pre>

<p>while, for n=1000:</p>

<pre><code>print(model['first']==model2['first'])
False
</code></pre>

<p>How is it possible?</p>

<p>Thank you very much!</p>
","python, nlp, gensim, word2vec","<p>Looking at the <code>gensim</code> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">documentation</a>, there is some randomization when you run <code>Word2Vec</code>:</p>

<blockquote>
  <p><code>seed</code> = for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread, to eliminate ordering jitter from OS thread scheduling.</p>
</blockquote>

<p>Thus if you want to have reproducible results, you will need to set the seed:</p>

<pre><code>In [1]: import gensim

In [2]: sentences=[['first','second','third','fourth']]*1000

In [3]: model1 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2)

In [4]: model2 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2)

In [5]: print(all(model1['first']==model2['first']))
False

In [6]: model3 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)

In [7]: model4 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)

In [11]: print(all(model3['first']==model4['first']))
True
</code></pre>
",4,1,1393,2016-06-10 09:55:13,https://stackoverflow.com/questions/37745250/different-models-with-gensim-word2vec-on-python
How to get the Document Vector from Doc2Vec in gensim 0.11.1?,"<p>Is there a way to get the document vectors of unseen and seen documents from Doc2Vec in the gensim 0.11.1 version? </p>

<ul>
<li><p>For example, suppose I trained the model on 1000 thousand - Can I get
the doc vector for those 1000 docs?     </p></li>
<li><p>Is there a way to get document vectors of unseen documents composed<br>
from the same vocabulary?</p></li>
</ul>
","python, gensim, word2vec, doc2vec","<p>For the first bullet point, you can do it in gensim 0.11.1</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence

documents = []
documents.append( LabeledSentence(words=[u'some', u'words', u'here'], labels=[u'SENT_1']) )
documents.append( LabeledSentence(words=[u'some', u'people', u'words', u'like'], labels=[u'SENT_2']) )
documents.append( LabeledSentence(words=[u'people', u'like', u'words'], labels=[u'SENT_3']) )


model = Doc2Vec(size=10, window=8, min_count=0, workers=4)
model.build_vocab(documents)
model.train(documents)

print(model[u'SENT_3'])
</code></pre>

<p>Here SENT_3 is a known sentence.</p>

<p>For the second bullet point, you can NOT do it in gensim 0.11.1, you have to update it to 0.12.4. This latest version has infer_vector function which can generate a vector for an unseen document.</p>

<pre><code>documents = []
documents.append( LabeledSentence([u'some', u'words', u'here'], [u'SENT_1']) )
documents.append( LabeledSentence([u'some', u'people', u'words', u'like'], [u'SENT_2']) )
documents.append( LabeledSentence([u'people', u'like', u'words'], [u'SENT_3']) )


model = Doc2Vec(size=10, window=8, min_count=0, workers=4)
model.build_vocab(documents)
model.train(documents)

print(model.docvecs[u'SENT_3']) # generate a vector for a known sentence
print(model.infer_vector([u'people', u'like', u'words'])) # generate a vector for an unseen sentence
</code></pre>
",9,6,4975,2016-06-11 12:45:03,https://stackoverflow.com/questions/37763883/how-to-get-the-document-vector-from-doc2vec-in-gensim-0-11-1
What is a projection layer in the context of neural networks?,"<p>I am currently trying to understand the architecture behind the <em>word2vec</em> neural net learning algorithm, for representing words as vectors based on their context.</p>

<p>After reading <a href=""http://arxiv.org/pdf/1301.3781v3.pdf"" rel=""noreferrer"">Tomas Mikolov paper</a> I came across what he defines as a <strong>projection layer</strong>. Even though this term is widely used when referred to <em>word2vec</em>, I couldn't find a precise definition of what it actually is in the neural net context.</p>

<p><a href=""https://i.sstatic.net/W46yb.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/W46yb.png"" alt=""Word2Vec Neural Net architecture""></a></p>

<p>My question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind?</p>

<p>ï¿¼Another resource that also refers more broadly to the problem can be found in <a href=""http://www.coling-2014.org/COLING%202014%20Tutorial-fix%20-%20Tomas%20Mikolov.pdf"" rel=""noreferrer"">this tutorial</a>, which also refers to a <em>projection layer</em> around page 67.</p>
","machine-learning, nlp, neural-network, word2vec","<p>I find the previous answers here a bit overcomplicated - a projection layer is just a simple matrix multiplication, or in the context of NN, a regular/dense/linear layer, without the non-linear activation in the end (sigmoid/tanh/relu/etc.) The idea is to <strong>project</strong> the (e.g.) 100K-dimensions discrete vector into a 600-dimensions continuous vector (I chose the numbers here randomly, &quot;your mileage may vary&quot;). The exact matrix parameters are learned through the training process.</p>
<p>What happens before/after already depends on the model and context, and is not what OP asks.</p>
<p>(In <a href=""https://youtu.be/VkjSaOZSZVs"" rel=""noreferrer"">practice</a> you wouldn't even bother with the matrix multiplication (as you are multiplying a 1-hot vector which has 1 for the word index and 0's everywhere else), and would treat the trained matrix as a lookout table (i.e. the 6257th word in the corpus = the 6257th row/column (depends how you define it) in the projection matrix).)</p>
",67,70,30506,2016-06-17 20:30:07,https://stackoverflow.com/questions/37889914/what-is-a-projection-layer-in-the-context-of-neural-networks
TensorFlow Embedding Lookup,"<p>I am trying to learn how to build RNN for Speech Recognition using TensorFlow. As a start, I wanted to try out some example models put up on TensorFlow page <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html"" rel=""noreferrer"">TF-RNN</a></p>

<p>As per what was advised, I had taken some time to understand how word IDs are embedded into a dense representation (Vector Representation) by working through the basic version of word2vec model code. I had an understanding of what <code>tf.nn.embedding_lookup</code> actually does, until I actually encountered the same function being used with two dimensional array in <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html"" rel=""noreferrer"">TF-RNN</a> <code>ptb_word_lm.py</code>, when it did not make sense any more.</p>

<h3>what I though <code>tf.nn.embedding_lookup</code> does:</h3>

<p>Given a 2-d array <code>params</code>, and a 1-d array <code>ids</code>, function <code>tf.nn.embedding_lookup</code> fetches rows from params, corresponding to the indices given in <code>ids</code>, which holds with the dimension of output it is returning.</p>

<h3>What I am confused about:</h3>

<p>When tried with same params, and 2-d array <code>ids</code>, <code>tf.nn.embedding_lookup</code> returns 3-d array, instead of 2-d which I do not understand why.</p>

<p>I looked up the manual for <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#embedding_lookup"" rel=""noreferrer"">Embedding Lookup</a>, but I still find it difficult to understand how the partitioning works, and the result that is returned. I recently tried some simple example with <code>tf.nn.embedding_lookup</code> and it appears that it returns different values each time. Is this behaviour due to the randomness involved in partitioning ?</p>

<p>Please help me understand how <code>tf.nn.embedding_lookup</code> works, and why is used in both <code>word2vec_basic.py</code>, and <code>ptb_word_lm.py</code> i.e., what is the purpose of even using them ?</p>
","tensorflow, word2vec, recurrent-neural-network, language-model","<p>There is already an answer on what does <code>tf.nn.embedding_lookup</code> <a href=""https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do"">here</a>.</p>

<hr>

<blockquote>
  <p>When tried with same params, and 2-d array ids, tf.nn.embedding_lookup returns 3-d array, instead of 2-d which I do not understand why.</p>
</blockquote>

<p>When you had a 1-D list of ids <code>[0, 1]</code>, the function would return a list of embeddings <code>[embedding_0, embedding_1]</code> where <code>embedding_0</code> is an array of shape <code>embedding_size</code>. For instance the list of ids could be a batch of words.</p>

<p>Now, you have a <strong>matrix</strong> of ids, or a list of list of ids. For instance, you now have a batch of <strong>sentences</strong>, i.e. a batch of list of words, i.e. a list of list of words.</p>

<p>If your list of sentences is: <code>[[0, 1], [0, 3]]</code> (sentence 1 is <code>[0, 1]</code>, sentence 2 is <code>[0, 3]</code>), the function will compute a matrix of embeddings, which will be of shape <code>[2, 2, embedding_size]</code>and will look like:</p>

<pre><code>[[embedding_0, embedding_1],
 [embedding_0, embedding_3]]
</code></pre>

<hr>

<p>Concerning the <code>partition_strategy</code> argument, you don't have to bother about it. Basically, it allows you to give a list of embedding matrices as <code>params</code> instead of 1 matrix, if you have limitations in computation.</p>

<p>So, you could split your embedding matrix of shape <code>[1000, embedding_size]</code> in ten matrices of shape <code>[100, embedding_size]</code> and pass this list of Variables as <code>params</code>. The argument <code>partition_strategy</code> handles the distribution of the vocabulary (the 1000 words) among the 10 matrices.</p>
",17,12,11393,2016-06-18 14:15:13,https://stackoverflow.com/questions/37897934/tensorflow-embedding-lookup
word2vec_basic not working (Tensorflow),"<p>I am new to word-embedding and Tensorflow. I am working on a project where I need to apply <strong>word2vec</strong> to health data.<br>
I used the code for Tensorflow website (<a href=""https://i.sstatic.net/1w5GS.png"" rel=""nofollow""><code>word2vec_basic.py</code></a>). I modified a little this code to make it read my data instead of ""text8.zip"" and it runs normally until the last step:</p>

<pre class=""lang-py prettyprint-override""><code>num_steps = 100001

with tf.Session(graph=graph) as session:
# We must initialize all variables before we use them.
  tf.initialize_all_variables().run()
  print('Initialized')
  average_loss = 0
  for step in range(num_steps):
    batch_data, batch_labels = generate_batch(
      batch_size, num_skips, skip_window)
    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
    _, l = session.run([optimizer, loss], feed_dict=feed_dict)
    average_loss += l
    if step % 2000 == 0:
      if step &gt; 0:
        average_loss = average_loss / 2000
      # The average loss is an estimate of the loss over the last 2000 batches.
      print('Average loss at step %d: %f' % (step, average_loss))
      average_loss = 0
    # note that this is expensive (~20% slowdown if computed every 500 steps)
    if step % 10000 == 0:
     sim = similarity.eval()
     for i in range(valid_size):
       valid_word = reverse_dictionary[valid_examples[i]]
       top_k = 8 # number of nearest neighbors
       nearest = (-sim[i, :]).argsort()[1:top_k+1]
       log = 'Nearest to %s:' % valid_word
       for k in range(top_k):
         close_word = reverse_dictionary[nearest[k]]
         log = '%s %s,' % (log, close_word)
       print(log)
  final_embeddings = normalized_embeddings.eval()&lt;code&gt;
</code></pre>

<p>This code is exactly the same as the example so I don't think it is wrong. the error It gave is:</p>

<hr>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-20-fc4c5c915fc6&gt; in &lt;module&gt;()
     34         for k in xrange(top_k):
     35           print(nearest[k])
---&gt; 36           close_word = reverse_dictionary[nearest[k]]
     37           log_str = ""%s %s,"" % (log_str, close_word)
     38         print(log_str)

KeyError: 2868
</code></pre>

<p>I changed the size of the input data but it still gives the same error.<br>
I would really appreciate if someone could give me some advice on how to fix this problem.</p>
","tensorflow, word2vec","<p>If the vocabulary size is less than default maximum (50000), you should modify the number.</p>

<p>At the last of step 2, let's modify vocabulary_size to actual dictionary size.</p>

<pre><code>data, count, dictionary, reverse_dictionary = build_dataset(words)
del words  # Hint to reduce memory.
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])

#add this line to modify
vocabulary_size = len(dictionary)
print('Dictionary size', len(dictionary))
</code></pre>
",1,1,1018,2016-06-25 05:06:33,https://stackoverflow.com/questions/38025162/word2vec-basic-not-working-tensorflow
How to giving a specific word to word2vec model in tensorflow,"<p>How to giving a specific word to test a trained word2vec model?</p>

<p>For example </p>

<p>Input ""dog"", and return nearst word like ""cat"", ""bird"" etc..</p>

<p>Thanks!</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow"">word2vec_basic_py</a></p>
","python, tensorflow, word2vec","<p>That tensor flow example already has a function to compute similarities and show nearest words.
The easiest way is to use that function. </p>

<p>In step 4 after defining valid_examples, you can give your own words.</p>

<pre><code>valid_examples = np.random.choice(valid_window, valid_size, replace=False)
num_sampled = 64    # Number of negative examples to sample.

sample_word = ""dog"";
if sample_word in dictionary:
  sample_index = dictionary[sample_word]
else:
  sample_index = 0  # dictionary['UNK']
valid_examples[0] = sample_index
</code></pre>

<p>Then you can see the result in first line. For ex, my result was</p>

<blockquote>
  <p>Nearest to dog: empower, nephew, stationary, marmoset, wow, kvac,
  dasyprocta, centaur,</p>
</blockquote>
",1,2,516,2016-06-25 10:07:24,https://stackoverflow.com/questions/38027289/how-to-giving-a-specific-word-to-word2vec-model-in-tensorflow
How to get a dataframe from word2vec model using spark,"<p>I am currently working on a sparkling water application and I am a total beginner in spark and h2o.</p>

<p>What I want to do:</p>

<ol>
<li>loading a input textfile</li>
<li>create a word2vec model</li>
<li>create a dataframe with a column <em>word</em> and a column <em>Vector</em></li>
<li>using the dataframe as input for h2o</li>
</ol>

<p>By creating the model i get a map, but i don't know how to create a dataframe of it. The output should look like that:</p>

<p><strong>word</strong>   | <strong>Vector</strong></p>

<p>assert | [0.3, 0.4.....]</p>

<p>sense | [0.6, 0.2.....]
and so on.</p>

<p>This is my code so far: </p>

<pre><code>from pyspark import SparkContext
from pyspark.mllib.feature import Word2Vec
from pysparkling import *
import h2o

from pyspark.sql import SQLContext
from pyspark.mllib.linalg import Vectors
from pyspark.sql import Row


# Starting h2o application on spark cluster
hc = H2OContext(sc).start()

# Loading input file
inp = sc.textFile(""examples/custom/text8.txt"").map(lambda row: row.split("" ""))

# building the word2vec model with a vector size of 10
word2vec = Word2Vec()
model = word2vec.setVectorSize(10).fit(inp)

# Sanity check
model.findSynonyms(""property"",5)

# assign vector representation (map to variable
wordVectorsDF = model.getVectors()

# Transform wordVectorsDF word into dataframe
</code></pre>

<p>Is there any approach to that or functions provided by spark?</p>

<p>Thanks in advance</p>
","apache-spark, machine-learning, pyspark, word2vec, h2o","<p>I found out that there are two libraries for a Word2Vec transformation - I don't know why. </p>

<pre><code>from pyspark.mllib.feature import Word2Vec
from pyspark.ml.feature import Word2Vec
</code></pre>

<p>The second line returns a data frame with the function <code>getVectors()</code>and has diffenrent parameters for building a model from the first line.</p>

<p>Maybe somebody can comment on that concerning the 2 different libraries.</p>

<p>Thanks in advance.</p>
",3,2,2952,2016-06-28 16:38:17,https://stackoverflow.com/questions/38081774/how-to-get-a-dataframe-from-word2vec-model-using-spark
RNTN implementation in java,"<p>I want to implement a Recursive neural tensor network(<strong>RNTN</strong>) in java.</p>

<p>I've used Deeplearning4j for word2vec pipeline to vectorize a corpus of words.</p>

<p>for NLP pipeline I've used Opennlp.( for tokenizing, POStaging and parsing)</p>

<p>Now, I figured out that I need an RNTN for my purpose and I didn't find much support, any references would be helpful. Many libraries are written in R or python or even in Scala and the NLP pipeline most of the people used is stanfordnlp. But I want to do this with Opennlp and java.</p>

<p>After that, I would like to combine the word vectors with neural net and then do the task I want to do something like sentiment analysis.</p>

<p>How can I proceed? Any input will be helpful.</p>

<p>Thanks.</p>
","java, neural-network, opennlp, word2vec, deeplearning4j","<p>Hi you can try having a looking at <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow"">Weka</a> ?</p>

<p>It's a great library for Machine learning in Java.</p>

<p>This link <a href=""http://deeplearning4j.org/recursiveneuraltensornetwork.html"" rel=""nofollow"">here</a> is deeplearning4j's explanation on implementing RNTNs</p>

<p><a href=""https://github.com/fanfannothing/RNTN"" rel=""nofollow"">This</a> is an implementation though I'm not sure how good it is.</p>
",4,12,898,2016-07-04 12:35:29,https://stackoverflow.com/questions/38184920/rntn-implementation-in-java
"how to print Map[String, Array[Float]] in scala?","<p>I am using word2vec function which is inside mllib library of Spark. I want to print word vectors which I am getting as output to ""getVectors"" function
My code looks like this:</p>

<pre><code>import org.apache.spark._

import org.apache.spark.rdd._
import org.apache.spark.SparkContext._
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

object word2vec {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(""word2vec"")
    val sc = new SparkContext(conf)
    val input = sc.textFile(""file:///home/snap-01/balance.csv"").map(line =&gt; line.split("","").toSeq)
    val word2vec = new Word2Vec()
    val model = word2vec.fit(input)
    model.save(sc, ""myModelPath"")
    val sameModel = Word2VecModel.load(sc, ""myModelPath"")
    val vec = sameModel.getVectors
    print(vec)
  }
}
</code></pre>

<p>I am getting ""<strong>Map(Balance -> [F@2932e15f)</strong>""</p>
","scala, dictionary, apache-spark, apache-spark-mllib, word2vec","<p>Try this :</p>
<pre><code>vec.foreach { case (key, values) =&gt; println(&quot;key &quot; + key + &quot; - &quot; + values.mkString(&quot;-&quot;))
}
</code></pre>
",4,2,5101,2016-07-06 11:58:23,https://stackoverflow.com/questions/38223442/how-to-print-mapstring-arrayfloat-in-scala
Word Embedding for Convolution Neural Network,"<p>I am trying to apply word2vec for convolution neural network. I am new with Tensorflow. Here is my code for pre-train layer.</p>

<pre><code>W = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),
                trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size])
embedding_init = W.assign(embedding_placeholder)
sess = tf.Session()
sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})
</code></pre>

<p>I think I should use <code>embedding_lookup</code> but not sure how to use it. I really appreace it someone could give some advice.</p>

<p>Thanks</p>
","tensorflow, conv-neural-network, word2vec","<p>You are on the right track. As <code>embedding_lookup</code> works under the assumption that words are represented as integer ids you need to transform your inputs vectors to comply with that. Furthermore, you need to make sure that your transformed words are correctly indexed into the embedding matrix. What I did was I used the information about the index-to-word-mapping generated from the embedding model (I used gensim for training my embeddings) to create a word-to-index lookup table that I subsequently used to transform my input vectors.</p>
",1,1,1206,2016-07-07 10:28:18,https://stackoverflow.com/questions/38243164/word-embedding-for-convolution-neural-network
How to build Neural Network in Spark?,"<p>The flow should be:</p>

<p>Input -> Word2Vectors -> Output -> NeuralNetwork</p>

<p>I have tried word2vec function of spark but I am confused with the format ""MultilayerPerceptronClassifier"" need as a input?</p>
","scala, apache-spark, neural-network, word2vec","<p>When you define your <code>MultilayerPerceptronClassifier</code> you have to give as parameter an <code>Array[Int]</code> called <code>layers</code>. These describe the number of neurons per layer in that sequence. The first layer's input dimension must match the length of the <code>Word2Vec</code> output dimension. So you should set the parameter to</p>

<pre><code>val layers = Array[Int](featureDim, 5, 4, 5, ...)
</code></pre>

<p>And replace the numbers with the parameters you want your model to have. You should set <code>featureDim</code> to the length of the vectors your <code>Word2VecModel</code> produces. Unfortunately, the attribute with that value is hidden via a <code>private</code> accessor and there is no getter method implemented as of now.</p>
",1,-2,507,2016-07-07 11:07:14,https://stackoverflow.com/questions/38243926/how-to-build-neural-network-in-spark
CBOW v.s. skip-gram: why invert context and target words?,"<p>In <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words"" rel=""noreferrer"">this</a> page, it is said that: </p>

<blockquote>
  <p>[...] skip-gram inverts contexts and targets, and tries to predict each context word from its target word [...]</p>
</blockquote>

<p>However, looking at the training dataset it produces, the content of the X and Y pair seems to be interexchangeable, as those two pairs of (X, Y): </p>

<blockquote>
  <p><code>(quick, brown), (brown, quick)</code></p>
</blockquote>

<p>So, why distinguish that much between context and targets if it is the same thing in the end? </p>

<p>Also, doing <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""noreferrer"">Udacity's Deep Learning course exercise on word2vec</a>, I wonder why they seem to do the difference between those two approaches that much in this problem: </p>

<blockquote>
  <p>An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.</p>
</blockquote>

<p>Would not this yields the same results?</p>
","nlp, tensorflow, deep-learning, word2vec, word-embedding","<p>Here is my oversimplified and rather naive understanding of the difference:</p>
<p>As we know, <strong>CBOW</strong> is learning to predict the word by the context. Or maximize the probability of the target word by looking at the context. And this happens to be a problem for rare words. For example, given the context <code>yesterday was a really [...] day</code> CBOW model will tell you that most probably the word is <code>beautiful</code> or <code>nice</code>. Words like <code>delightful</code> will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.</p>
<p>On the other hand, the <strong>skip-gram</strong> model is designed to predict the context. Given the word <code>delightful</code> it must understand it and tell us that there is a huge probability that the context is <code>yesterday was really [...] day</code>, or some other relevant context. With <strong>skip-gram</strong> the word <code>delightful</code> will not try to compete with the word <code>beautiful</code> but instead, <code>delightful+context</code> pairs will be treated as new observations.</p>
<p><strong>UPDATE</strong></p>
<p>Thanks to @0xF for sharing <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures"" rel=""noreferrer"">this article</a></p>
<blockquote>
<p>According to Mikolov</p>
<p><strong>Skip-gram:</strong> works well with small amount of the training data, represents well even rare words or phrases.</p>
<p><strong>CBOW:</strong> several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
</blockquote>
<p>One more addition to the subject is found <a href=""https://groups.google.com/d/msg/word2vec-toolkit/LNPeC5gyhmQ/p8683JkD6LoJ"" rel=""noreferrer"">here</a>:</p>
<blockquote>
<p>In the &quot;skip-gram&quot; mode alternative to &quot;CBOW&quot;, rather than averaging
the context words, each is used as a pairwise training example. That
is, in place of one CBOW example such as [predict 'ate' from
average('The', 'cat', 'the', 'mouse')], the network is presented with
four skip-gram examples [predict 'ate' from 'The'], [predict 'ate'
from 'cat'], [predict 'ate' from 'the'], [predict 'ate' from 'mouse'].
(The same random window-reduction occurs, so half the time that would
just be two examples, of the nearest words.)</p>
</blockquote>
",108,55,41090,2016-07-10 01:21:34,https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words
Creating sequence vector from text in Python,"<p>I am now trying to prepare the input data for LSTM-based NN.
I have some big number of text documents and what i want is to make sequence vectors for each document so i am able to feed them as train data to LSTM RNN.</p>

<p>My poor approach:</p>

<pre><code>import re
import numpy as np
#raw data
train_docs = ['this is text number one', 'another text that i have']

#put all docs together
train_data = ''
for val in train_docs:
    train_data += ' ' + val

tokens = np.unique(re.findall('[a-zÐ°-Ñ0-9]+', train_data.lower()))
voc = {v: k for k, v in dict(enumerate(tokens)).items()}
</code></pre>

<p>and then brutforce replace each doc with a ""voc"" dict.</p>

<p>Is there any libs which can help with this task?</p>
","python, word2vec, lstm","<p>Solved with Keras text preprocessing classes:
<a href=""http://keras.io/preprocessing/text/"" rel=""noreferrer"">http://keras.io/preprocessing/text/</a></p>

<p>done like this:</p>

<pre><code>from keras.preprocessing.text import Tokenizer, text_to_word_sequence

train_docs = ['this is text number one', 'another text that i have']
tknzr = Tokenizer(lower=True, split="" "")
tknzr.fit_on_texts(train_docs)
#vocabulary:
print(tknzr.word_index)

Out[1]:
{'this': 2, 'is': 3, 'one': 4, 'another': 9, 'i': 5, 'that': 6, 'text': 1, 'number': 8, 'have': 7}

#making sequences:
X_train = tknzr.texts_to_sequences(train_docs)
print(X_train)

Out[2]:
[[2, 3, 1, 8, 4], [9, 1, 6, 5, 7]]
</code></pre>
",5,1,4207,2016-07-11 08:23:16,https://stackoverflow.com/questions/38302280/creating-sequence-vector-from-text-in-python
What means stddev of nce_weights on word2vec?,"<p>I'm trying to understand how <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html"" rel=""nofollow"">word2vec</a> example works and don't really understand why set stddev of nce_weights.</p>

<pre><code>nce_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
                      stddev=1.0 / math.sqrt(embedding_size)))
</code></pre>

<p>I tried to remove the stddev parameter and then run it. But couldn't find any differences.</p>

<p>stddev means standard deviation. Why use stddev parameter and what means the stddev value on the example?</p>

<p>Thank you.</p>
","tensorflow, word2vec","<p><em>nce_weights</em> are the classifier weights used for implementing the noise-contrastive estimation training technique, an efficient method for training classifiers with a large number of labels (see the explanation in the section ""Scaling up with Noise-Contrastive Training"" in the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html"" rel=""nofollow"">word2vec tutorial</a> you originally linked). </p>

<p>These weights will be learned during training, however they first need to be initialized at the beginning of training. We do this by drawing random weights from a truncated normal distribution (a normal distribution with all values within 2 standard deviations of its mean). This is achieved by passing in the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/constant_op.html#truncated_normal"" rel=""nofollow"">tf.truncated_normal</a> op as initializer in the call to <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#Variable"" rel=""nofollow"">tf.Variable</a>. The truncated normal initializer takes as parameters a shape, mean, and standard deviation (see the page linked above for the parameters and their definitions). The last two are the sufficient statistics of the truncated Gaussian distribution that we will be drawing from. </p>

<p>When you don't pass these in (as you mentioned), they simply get initialized to their default values (1.0 in the case of stddev).</p>
",0,0,568,2016-07-12 04:38:48,https://stackoverflow.com/questions/38320138/what-means-stddev-of-nce-weights-on-word2vec
Is Doc2Vec suited for Sentiment Analysis?,"<p>I have been reading more modern posts about sentiment classification (analysis) such as <a href=""http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis"" rel=""nofollow"">this</a>.</p>

<p>Taking the IMDB dataset as an example I find that I get a similar accuracy percentage using Doc2Vec (88%), <strong>however a far better result using a simple tfidf vectoriser with tri-grams for feature extraction (91%)</strong>. I think this is similar to Table 2 in <a href=""http://arxiv.org/pdf/1412.5335v7.pdf"" rel=""nofollow"">Mikolov's 2015 paper</a>.</p>

<p>I thought that by using a bigger data-set this would change. So I re-ran my experiment using a breakdown of 1mill training and 1 mill test from <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow"">here</a>. Unfortunately, in that case my tfidf vectoriser feature extraction method increased to 93% but doc2vec fell to 85%.</p>

<p><strong>I was wondering if this is to be expected and that others find tfidf to be superior to doc2vec even for a large corpus?</strong></p>

<p>My data-cleaning is simple:</p>

<pre><code>def clean_review(review):
    temp = BeautifulSoup(review, ""lxml"").get_text()
    punctuation = """""".,?!:;(){}[]""""""
    for char in punctuation
        temp = temp.replace(char, ' ' + char + ' ')
    words = "" "".join(temp.lower().split()) + ""\n""
    return words
</code></pre>

<p>And I have tried using 400 and 1200 features for the Doc2Vec model:</p>

<pre><code>model = Doc2Vec(min_count=2, window=10, size=model_feat_size, sample=1e-4, negative=5, workers=cores)
</code></pre>

<p>Whereas my tfidf vectoriser has 40,000 max features:</p>

<pre><code>vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)
</code></pre>

<p>For classification I experimented with a few linear methods, however found simple logistic regression to do OK...</p>
","machine-learning, sentiment-analysis, gensim, word2vec, doc2vec","<p>The example code Mikolov once posted (<a href=""https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ"" rel=""nofollow"">https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ</a>) used options <code>-cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1</code> â€“ which in gensim would be similar to <code>dm=0, dbow_words=1, size=100, window=10, hs=0, negative=5, sample=1e-4, iter=20, min_count=1, workers=cores</code>.</p>

<p>My hunch is that optimal values might involve a smaller <code>window</code> and higher <code>min_count</code>, and maybe a <code>size</code> somewhere between 100 and 400, but it's been a while since I've run those experiments. </p>

<p>It can also sometimes help a little to re-infer vectors on the final model, using a larger-than-the-default <code>passes</code> parameter, rather than re-using the bulk-trained vectors. Still, these may just converge on similar performance to Tfidf â€“ they're all dependent on the same word-features, and not very much data. </p>

<p>Going to a semi-supervised approach, where some of the document-tags represent sentiments where known, sometimes also helps. </p>
",3,1,2710,2016-07-12 09:01:53,https://stackoverflow.com/questions/38324328/is-doc2vec-suited-for-sentiment-analysis
Why does word2Vec use cosine similarity?,"<p>I have been reading the papers on Word2Vec (e.g. <a href=""https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"" rel=""noreferrer"">this one</a>), and I think I understand training the vectors to maximize the probability of other words found in the same contexts.</p>

<p>However, I do not understand why cosine is the correct measure of word similarity.  Cosine similarity says that two vectors point in the same direction, but they could have different magnitudes.</p>

<p>For example, cosine similarity makes sense comparing bag-of-words for documents.  Two documents might be of different length, but have similar distributions of words.</p>

<p>Why not, say, Euclidean distance?</p>

<p>Can anyone one explain why cosine similarity works for word2Vec?</p>
","nlp, deep-learning, word2vec","<p>Cosine similarity of two n-dimensional vectors A and B is defined as:</p>

<p><a href=""https://i.sstatic.net/Zc0jx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zc0jx.png"" alt=""enter image description here""></a></p>

<p>which simply is the cosine of the angle between A and B.</p>

<p>while the Euclidean distance is defined as</p>

<p><a href=""https://i.sstatic.net/eh06A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eh06A.png"" alt=""enter image description here""></a></p>

<p>Now think about the distance of two random elements of the vector space. For the cosine distance, the maximum distance is 1 as the range of cos is [-1, 1].</p>

<p>However, for the euclidean distance this can be any non-negative value.</p>

<p>When the dimension n gets bigger, two randomly chosen points have a cosine distance which gets closer and closer to 90Â°, whereas points in the unit-cube of R^n have an euclidean distance of roughly 0.41 (n)^0.5 (<a href=""https://martin-thoma.com/curse-of-dimensionality/#empirical-results"" rel=""nofollow noreferrer"">source</a>)</p>

<h2>TL;DR</h2>

<p>cosine distance is better for vectors in a high-dimensional space because of the <strong>curse of dimensionality</strong>. (I'm not absolutely sure about it, though)</p>
",-1,18,10155,2016-07-17 16:25:09,https://stackoverflow.com/questions/38423387/why-does-word2vec-use-cosine-similarity
Cost function for word2vec,"<p>I am currently doing text classification with pretrain by <code>word2vec</code>. But before feeding to <code>Convolution neural network</code>, I have to write cost function. </p>

<p>Here is my code:</p>

<pre><code>W = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),
            trainable=False, name=""W"")

embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size])
embedding_init = W.assign(embedding_placeholder)

sess = tf.Session()

sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})

embedded_chars = tf.nn.embedding_lookup(W, data)
embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)
</code></pre>

<p>the code for <code>word2vec</code> is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow"">word2vec_basic.py</a>. </p>

<p>When I feed to the convex function:</p>

<pre><code>filter_shape = [filter_size, embedding_size, 1, num_filters]
W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
conv = tf.nn.conv2d(
        embedding_init,
        W,
        strides=[1, 1, 1, 1],
        padding=""VALID"",
        name=""conv"")
</code></pre>

<p>It gave me an following error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-29-9c12d490e7ab&gt; in &lt;module&gt;()
     11             strides=[1, 1, 1, 1],
     12             padding=""VALID"",
---&gt; 13             name=""conv"")
ValueError: Shape (50000, 128) must have rank 4
</code></pre>

<p>I suspect it is my tensor size is wrong but I am not really sure I to set it right.</p>
","python, tensorflow, word2vec","<p>The error you got is because the input vector to <code>tf.nn.conv2d</code> expects a tensor of shape:</p>

<pre><code>[batch, in_height, in_width, in_channels]
</code></pre>

<p>and what you have here is with shape (50000, 128). You might want to use <code>embedded_chars_expanded</code> as the input.</p>
",1,0,576,2016-07-20 10:28:20,https://stackoverflow.com/questions/38478776/cost-function-for-word2vec
How can I use a trained GloVe/word2vec model to extract keywords from articles?,"<p>I have trained a GloVe with ~5M <strong>spanish</strong> articles. I know how to load this GloVe in gensim and use it as if it was a word2vec model.
Now I am facing  the problem of topic modelling and keywords extraction from news articles (also in spanish) so I was wondering how could I use the trained model to do so.</p>

<p>How could I do it?</p>
","nlp, gensim, word2vec","<p>Your question on how to use a word2vec model is very general so my answer is likewise.</p>

<p>What word2vec allows you to do is to provide a generally ""better"" representation of words. So perhaps if you are using ""bag of words"" as a feature in topic modelling you can replace that with a ""bag of word vectors"" from word2vec which hopefully will give you better semantic similarity. Perhaps better keywords too.</p>
",1,0,1445,2016-07-21 15:09:39,https://stackoverflow.com/questions/38507935/how-can-i-use-a-trained-glove-word2vec-model-to-extract-keywords-from-articles
Saving Word2Vec for CNN Text Classification,"<p>I want to train my own Word2Vec model for my text corpus. I can get the code from TensorFlow's tutorial. What I don't know is how to save this model to use for CNN text classification later? Should I use pickle to save it and then read it later?</p>
","tensorflow, deep-learning, text-classification, word2vec","<p>No pickling is not the way of saving the model in case of tensorflow.</p>

<p>Tensorflow provides with tensorflow serving for saving the models as proto bufs(for exporting the model). The way to save model would be to save the  tensorflow session as:
             <strong>saver.save(sess, 'my_test_model',global_step=1000)</strong></p>

<p>Heres the link for complete answer:
            <strong><a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model"">Tensorflow: how to save/restore a model?</a></strong></p>
",0,1,858,2016-07-24 18:06:16,https://stackoverflow.com/questions/38555148/saving-word2vec-for-cnn-text-classification
Matching words and vectors in gensim Word2Vec model,"<p>I have had the <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer"">gensim</a> <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">Word2Vec</a> implementation compute some word embeddings for me. Everything went quite fantastically as far as I can tell; now I am clustering the word vectors created, hoping to get some semantic groupings.</p>
<p>As a next step, I would like to look at the words (rather than the vectors) contained in each cluster. I.e. if I have the vector of embeddings <code>[x, y, z]</code>, I would like to find out which actual word this vector represents. I can get the words/Vocab items by calling <code>model.vocab</code> and the word vectors through <code>model.syn0</code>. But I could not find a location where these are explicitly matched.</p>
<p>This was more complicated than I expected and I feel I might be missing the obvious way of doing it. Any help is appreciated!</p>
<h3>Problem:</h3>
<p>Match words to embedding vectors created by <code>Word2Vec ()</code> -- how do I do it?</p>
<h3>My approach:</h3>
<p>After creating the model (code below*), I would now like to match the indexes assigned to each word (during the <code>build_vocab()</code> phase) to the vector matrix outputted as <code>model.syn0</code>.
Thus</p>
<pre><code>for i in range (0, newmod.syn0.shape[0]): #iterate over all words in model
    print i
    word= [k for k in newmod.vocab if newmod.vocab[k].__dict__['index']==i] #get the word out of the internal dicationary by its index
    wordvector= newmod.syn0[i] #get the vector with the corresponding index
    print wordvector == newmod[word] #testing: compare result of looking up the word in the model -- this prints True
</code></pre>
<ul>
<li><p>Is there a better way of doing this, e.g. by feeding the vector into the model to match the word?</p>
</li>
<li><p>Does this even get me correct results?</p>
</li>
</ul>
<p>*My code to create the word vectors:</p>
<pre><code>model = Word2Vec(size=1000, min_count=5, workers=4, sg=1)
        
model.build_vocab(sentencefeeder(folderlist)) #sentencefeeder puts out sentences as lists of strings

model.save(&quot;newmodel&quot;)
</code></pre>
<p>I found <a href=""https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors"">this question</a> which is similar but has not really been answered.</p>
","python, vector, machine-learning, gensim, word2vec","<p>So I found an easy way to do this, where <code>nmodel</code> is the name of your model. </p>

<pre><code>#zip the two lists containing vectors and words
zipped = zip(nmodel.wv.index2word, nmodel.wv.syn0)

#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word/vector you're looking for) using a list comprehension:
wordresult = [i for i in zipped if i[0] == word]
vecresult = [i for i in zipped if i[1] == vector]
</code></pre>

<p>This is based on the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">gensim code</a>. For older versions of gensim, you might need to drop the <code>wv</code> after the model.  </p>
",5,21,18260,2016-07-29 18:40:54,https://stackoverflow.com/questions/38665556/matching-words-and-vectors-in-gensim-word2vec-model
Unknown word/character in txt file from word2vec,"<p>I recently encountered the <code>&lt;/s&gt;</code> word/character in a vocabulary created by word2vec as a separate word. </p>

<p>Although I did tried to search the web for that character, I cannot actually specify that character at the search engines. </p>

<p>So, does anyone knows what this character is? </p>
","python, word2vec","<p>If you look at the line 82 of <a href=""https://code.google.com/archive/p/word2vec/source/default/source"" rel=""nofollow"">source code</a> of <code>word2vec</code>,</p>

<pre><code>if (ch == '\n') {
  strcpy(word, (char *)""&lt;/s&gt;"");
  return;
}
</code></pre>

<p><code>&lt;/s&gt;</code> is simply a character used by Mikolov et al. to denote the end of line (or more precisely <code>\n</code>). 
I don't think it has any special html/latex reference. Nor does it appears on ASCII <a href=""https://en.wikipedia.org/wiki/ASCII#Control_characters"" rel=""nofollow"">chart</a>.</p>
",1,0,76,2016-08-01 14:52:06,https://stackoverflow.com/questions/38701947/unknown-word-character-in-txt-file-from-word2vec
Cost function for Convolution neural network,"<p>I am doing Text Classification by Convolution Neural Network. In the example <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html"" rel=""nofollow noreferrer"">MNIST</a> they have 60.000 images examples of hand-written digits, each image has size 28 x 28 and there are 10 labels (from 0 to 9). So the size of Weight will be 784 * 10 (28 * 28 = 784)</p>
<p><a href=""https://i.sstatic.net/qKVBF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qKVBF.png"" alt=""enter image description here"" /></a></p>
<p>Here is their code:</p>
<pre><code>x = tf.placeholder(&quot;float&quot;, [None, 784])
W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
</code></pre>
<p>In my case, I applied <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html"" rel=""nofollow noreferrer"">word2vec</a> to encode my documents. The result &quot;dictionary size&quot; of word embedding is 2000 and the embedding size is 128. There are 45 labels. I tried to do the same as the example but It did not work. Here what I did: I treated each document same as image. For instance, The document could be represent as the matrix of 2000 x 128 (for words appear in document I appended the word Vector value for that column and left other equal zero. I have a trouble with creating W and x since my input data is a numpy array of 2000 x 128 while <code>x = tf.placeholder(&quot;float&quot;, [None, 256000])</code>. The size did not match.</p>
<p>Could nay one suggest any advises ?</p>
<p>Thanks</p>
","python, tensorflow, word2vec","<p>Placeholder <code>x</code> is an array of flattened images, where first dimension <code>None</code> corresponds to batch size, i.e. number of images, and <code>256000 = 2000 * 128</code>. So, in order to feed <code>x</code> properly you need to flatten your input. Since you mention that your input is numpy arrays, take a look at <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html"" rel=""nofollow"" title=""numpy.reshape"">numpy.reshape</a> and <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html"" rel=""nofollow"">flatten</a>.</p>
",1,0,293,2016-08-21 10:42:43,https://stackoverflow.com/questions/39063302/cost-function-for-convolution-neural-network
word2vec: CBOW &amp; skip-gram performance wrt training dataset size,"<p>The question is simple. Which of the CBOW &amp; skip-gram works better for a big dataset? (And the answer for small dataset follows.)</p>

<p>I am confused since, by Mikolov himself, <a href=""https://groups.google.com/forum/#!searchin/word2vec-toolkit/c-bow/word2vec-toolkit/NLvYXU99cAM/E5ld8LcDxlAJ"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>Skip-gram: works well with <strong>small amount of the training data</strong>, represents well even rare words or phrases. <br/> <br/>
  CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
</blockquote>

<p>but, according to Google TensorFlow, <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html"" rel=""noreferrer"">[Link]</a></p>

<blockquote>
  <p>CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.<br/><br/>However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have <strong>larger datasets</strong>. We will focus on the skip-gram model in the rest of this tutorial.</p>
</blockquote>

<p>Here is a Quora post which supports the first thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>, and then there is the other Quora post which suggests the second thought <a href=""https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"" rel=""noreferrer"">[Link]</a>--both seem derivable from the aforementioned credible sources.</p>

<p>Or is it like what Mikolov said:</p>

<blockquote>
  <p>Overall, the best practice is to try few experiments and see what works the best for you, as different applications have different requirements.</p>
</blockquote>

<p>But surely there is an empirical or analytical verdict or final saying on this matter?</p>
","nlp, word2vec, word-embedding","<p>When Mikolov meant CBOW works good for bigger dataset and SG for smaller dataset, I suppose the quantity of data is considered. Since CBOW considers one target word and many context words, it needs a bigger dataset to train for target vectors compared to datasets used in SG. As in vice versa, in SG due to many target words for single context word, it needs smaller datasets.</p>

<p>Google Tensor Flow speaks about the distribution of words in the dataset for generating quality vectors rather than the quantity of dataset used. As CBOW model considers more over the same context words for all the target words in a sentence, a bigger (distributed) dataset is needed and vice versa for SG.</p>

<p>In common, they both mean the same:</p>

<ul>
<li>CBOW model = dataset with short sentences but high number of samples (bigger dataset)</li>
<li>SG model = dataset with long sentences and low number of samples (smaller dataset)</li>
</ul>
",8,15,5470,2016-08-30 09:50:59,https://stackoverflow.com/questions/39224236/word2vec-cbow-skip-gram-performance-wrt-training-dataset-size
score_cbow_pair in word2vec (gensim),"<p>I wanted to output the log-probability during learning of the word and doc vectors in gensim. I have taken a look at the implementation of the score function in the ""slow plain numpy"" version.</p>

<pre class=""lang-py prettyprint-override""><code>def score_cbow_pair(model, word, word2_indices, l1):
    l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size
    sgn = (-1.0)**word.code  # ch function, 0-&gt; 1, 1 -&gt; -1
    lprob = -log(1.0 + exp(-sgn*dot(l1, l2a.T)))
    return sum(lprob)
</code></pre>

<p>The score function should make use of the parameters learned during hierarchical softmax training. But in the calculation of the log-probability there is supposed to be a sigmoid function( <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained equation (45)</a>).
So does gensim really calculate the log-probability in <code>lprob</code> or is it just a score for comparison purposes.</p>

<p>I would have calculated the log-probability as follows:
<code>-log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T))))</code></p>

<p>Is this equation not used because it explodes for values close to zero or is it in general wrong?</p>
","python, numpy, probability, gensim, word2vec","<p>I've overlooked that the logarithm of the sigmoid function can be rewritten: <code>log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T)))) = log(1)-log(1.0+exp(-sgn*dot(l1, l2a.T))) = -log(1.0+exp(-sgn*dot(l1, l2a.T)))</code></p>

<p>So the code does compute the log-likelihood.</p>
",0,0,390,2016-08-31 10:45:53,https://stackoverflow.com/questions/39247496/score-cbow-pair-in-word2vec-gensim
updates of the document vectors in doc2vec (PV-DM) in gensim,"<p>I'm trying to understand the PV-DM implementation with averaging in gensim.
In the function <code>train_document_dm</code> in <code>doc2vec.py</code> the return value (""errors"") of <code>train_cbow_pair</code> is in the case of averaging (<code>cbow_mean=1</code>) not divided by the number of input vectors (<code>count</code>).
According to this explanation there should be a division by the number of documents in the case of averaging the input vectors: <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained, equation (23)</a>.
Here is the code from <code>train_document_dm</code>:</p>

<pre class=""lang-py prettyprint-override""><code>l1 = np_sum(word_vectors[word2_indexes], axis=0)+np_sum(doctag_vectors[doctag_indexes], axis=0)  
count = len(word2_indexes) + len(doctag_indexes)  
if model.cbow_mean and count &gt; 1:  
    l1 /= count  
neu1e = train_cbow_pair(model, word, word2_indexes, l1, alpha,
                                learn_vectors=False,  learn_hidden=learn_hidden)  
if not model.cbow_mean and count &gt; 1:  
    neu1e /= count  
if learn_doctags:  
    for i in doctag_indexes:  
        doctag_vectors[i] += neu1e * doctag_locks[i]  
if learn_words:  
    for i in word2_indexes:  
        word_vectors[i] += neu1e * word_locks[i]  
</code></pre>
","python, numpy, gensim, word2vec, doc2vec","<p>Let's say V is defined as the average of <code>A</code>, <code>B</code>, and <code>C</code>: </p>

<p>V = (A + B + C) / 3</p>

<p>Let's set <code>A = 5</code>, <code>B = 6</code>, and <code>C = 10</code>. And let's say we want <code>V</code> equal 10.</p>

<p>We run the calculation (forward propagation), and the value of <code>V</code>, the average of the three numbers, is 7. Thus the correction needed for V is +3. </p>

<p>To apply this correction to A, B, and C, do we also <em>divide</em> that correction by 3, to get +1 against each? In that case <code>A = 6</code>, <code>B = 7</code>, and <code>C = 11</code> â€“ and now <code>V</code> is just 8. It'd still need another +2 to match the target. </p>

<p>So, no. The proper correction to all of the components of <code>V</code>, in the case where <code>V</code> is an average, is the same as the correction to <code>V</code> â€“ in this case, +3. If we were apply that, we'd reach our proper target value of 10:</p>

<pre><code>A = 8, B = 9, C = 13
V = (8 + 9 + 13) / 3 = 10
</code></pre>

<p>The same thing is happening the gensim backpropagation. In the case of averaging, the full corrective value (times the learning-rate <code>alpha</code>) is applied to each of the constituent vectors. </p>

<p>(If using a sum-of-vectors to create V instead, <em>then</em> the error would need to be divided by the count of constituent vectors - to split the error over them all, and not apply it redundantly.)</p>
",0,0,244,2016-08-31 14:51:26,https://stackoverflow.com/questions/39252860/updates-of-the-document-vectors-in-doc2vec-pv-dm-in-gensim
Tokenizing a corpus composed of articles into sentences Python,"<p>I will like to analyze my first deep learning model using Python and in order to do so I have to first split my corpus (8807 articles) into sentences. My corpus is built as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body
</code></pre>

<p>I am trying to use the function <code>Word2Vec.load_word2vec_format</code> from the library <code>gensim.models</code> but I have to first split my corpus (<code>doc_set</code>) into sentences.</p>

<pre><code>from gensim.models import word2vec
model = Word2Vec.load_word2vec_format(doc_set, binary=False)
</code></pre>

<p>Any recommendations? </p>

<p>cheers</p>
","python, deep-learning, gensim, word2vec","<p>So, Gensim's <code>Word2Vec</code> requires this format for its training input: <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code>.</p>

<p>I assume your documents contain more than one sentence. You should first split by sentences, you can do that with <a href=""http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt"" rel=""nofollow"">nltk</a> (you might need to download the model first). Then tokenize each sentence and put everything together in a list.</p>

<pre><code>sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.body.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]
gensim.models.Word2Vec(result)
</code></pre>

<p>Unfortunately I am not good enough with Pandas to perform all the operations in a ""pandastic"" way.</p>

<p>Pay a lot of attention to the parameters of <code>Word2Vec</code> picking them right can make a huge difference.</p>
",2,4,4512,2016-09-01 15:27:12,https://stackoverflow.com/questions/39275547/tokenizing-a-corpus-composed-of-articles-into-sentences-python
Graphical plot of words similarity given by Word2Vec,"<p>I will like to plot in a simple vector space graph the similarity between different words. I have calculated them using the model <code>word2vec</code> given by gensim but I cannot find any graphical examples in the literature. My code is as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

## Building the deep learning model
import itertools

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

from gensim.models import word2vec


result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]

model = gensim.models.Word2Vec(result)
</code></pre>

<p>In a simple vector space graph, I will like to place the following words: bank, finance, market, property, oil, energy, business and economy. I can easily calculate the similarity of these pairs of words with the function:</p>

<pre><code>model.similarity('bank', 'property')
0.25089364531360675
</code></pre>

<p>Thanks a lot</p>
","python, graph, deep-learning, gensim, word2vec","<p>For plotting all the word-vectors in your Word2Vec model, you need to perform Dimensionality reduction. You can use TSNE tool from python's sklearn to visualise multi-dimensional vectors in 2-D space.    </p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"" rel=""noreferrer"">t-distributed Stochastic Neighbor Embedding</a>.</p>

<pre><code>import sklearn.manifold.TSNE

tsne = sklearn.manifold.TSNE(n_components = 0 , random_state = 0)
all_vector_matrix = model.syn0
all_vector_matrix_2d = tsne.fit_transform(all_vector_matrix)
</code></pre>

<p>This will give you a 2-D similarity matrix which you can further parse through pandas and then plot using seaborn and matplotlib's pyplot function.</p>
",5,2,4415,2016-09-02 16:02:43,https://stackoverflow.com/questions/39296592/graphical-plot-of-words-similarity-given-by-word2vec
doc2vec How to cluster DocvecsArray,"<p>I've patched the following code from examples I've found over the web:</p>

<pre><code># gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec
from sklearn.cluster import KMeans

# random
from random import shuffle

# classifier

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
        return self.sentences

    def sentences_perm(self):
        shuffle(self.sentences)
        return self.sentences

sources = {'test.txt' : 'DOCS'}
sentences = LabeledLineSentence(sources)

model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())

for epoch in range(10):
    model.train(sentences.sentences_perm())

print(model.docvecs)
</code></pre>

<p>my test.txt file contains a paragraph per line.</p>

<p>The code runs fine and generates DocvecsArray for each line of text</p>

<p>my goal is to have an output like so:</p>

<p>cluster 1: [DOC_5,DOC_100,...DOC_N]<br>
cluster 2: [DOC_0,DOC_1,...DOC_N]</p>

<p>I have found the <a href=""https://stackoverflow.com/questions/27889873/clustering-text-documents-using-scikit-learn-kmeans-in-python"">following Answer</a>, but the output is:</p>

<p>cluster 1: [word,word...word]<br>
cluster 2: [word,word...word]</p>

<p>How can I alter the code and get document clusters?</p>
","python, machine-learning, k-means, word2vec, doc2vec","<p>So it looks like you're almost there.</p>

<p>You are outputting a set of vectors. For the sklearn package, you have to put those into a numpy array - using the numpy.toarray() function would probably be best. <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html"" rel=""noreferrer"">The documentation</a> for KMeans is really stellar and even across the whole library it's good.</p>

<p>A note for you is that I have had much better luck with <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#example-cluster-plot-dbscan-py"" rel=""noreferrer"">DBSCAN</a> than KMeans, which are both contained in the same sklearn library. DBSCAN doesn't require you to specify how many clusters you want to have on the output.</p>

<p>There are well-commented code examples in both links.</p>
",7,6,6152,2016-09-08 13:04:24,https://stackoverflow.com/questions/39391753/doc2vec-how-to-cluster-docvecsarray
sentiment analysis on IMDB data using tflearn (lstm -Tensorflow),"<p>I am working on tensorflow and some high level APIs on it like tflearn.</p>

<p>What I am trying to do here is to use lstm on IMDB data for sentiment analysis. There is a sample code in following link
<a href=""https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py"" rel=""nofollow"">https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py</a></p>

<p>However it uses a preprocessed data, but I wanna use my own IMDB raw data (downloaded from <a href=""http://ai.stanford.edu/~amaas/data/sentiment/"" rel=""nofollow"">http://ai.stanford.edu/~amaas/data/sentiment/</a>)</p>

<p>Here is the code that I updated for the sentiment analysis, all the intermediate steps seem correct but the accuracy is not stable (as you can see results below). When I print the predictions at the end, I see that the probabilities for each class is very very close (like [[0.4999946355819702, 0.5000053644180298], [0.5000001192092896, 0.49999988079071045], [0.49999362230300903, 0.5000064373016357], [0.49999985098838806, 0.5000001192092896]]).</p>

<p>I don't think that the problem is overfitting, since when I try to predict train data again, the result is as above. I think I am missing some point or doing something wrong.</p>

<p>Any help is appreciated,
Thanks</p>

<pre><code># -*- coding: utf-8 -*-
from __future__ import division, print_function, absolute_import

import tflearn
from tflearn.data_utils import to_categorical, pad_sequences
import string
import numpy as nm
import codecs
import re
import collections
import math
import tensorflow as tf
import random
import glob

allWords = []
allDocuments = []
allLabels = []

def readFile(fileName, allWords):


    file = codecs.open(fileName, encoding='utf-8')

    for line in file:
        line = line.lower().encode('utf-8')
        words = line.split()
        for word in words:
            word = word.translate(None, string.punctuation)
            if word != '':
                allWords.append(word)

    file.close()


def readFileToConvertWordsToIntegers(dictionary, fileName, allDocuments, allLabels, label):

    file = codecs.open(fileName, encoding='utf-8')
    document = []
    for line in file:
        line = line.lower().encode('utf-8')
        words = line.split()
        for word in words:
            word = word.translate(None, string.punctuation)
            if word in dictionary:
                index = dictionary[word]
            else:
                index = 0  # dictionary['UNK'] 
            document.append(index)
        allDocuments.append(document)
        allLabels.append(label)

    file.close()


vocabulary_size = 10000

def build_dataset(words):
  count = [['UNK', -1]]
  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
  dictionary = dict()
  for word, _ in count:
    dictionary[word] = len(dictionary)
  data = list()
  unk_count = 0
  for word in words:
    if word in dictionary:
      index = dictionary[word]
    else:
      index = 0  # dictionary['UNK']
      unk_count = unk_count + 1
    data.append(index)
  count[0][1] = unk_count
  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
  return dictionary, reverse_dictionary



fileList = glob.glob(""/Users/inanc/Desktop/aclImdb/train/neg/*.txt"")
for file in fileList:
    readFile(file, allWords)

fileList = glob.glob(""/Users/inanc/Desktop/aclImdb/test/train/*.txt"")
for file in fileList:
    readFile(file, allWords)

print(len(allWords))

dictionary, reverse_dictionary = build_dataset(allWords)
del allWords  # Hint to reduce memory.

print(len(dictionary))

fileList = glob.glob(""/Users/inanc/Desktop/aclImdb/train/neg/*.txt"")
for file in fileList:
    readFileToConvertWordsToIntegers(dictionary, file, allDocuments, allLabels, 0)

fileList = glob.glob(""/Users/inanc/Desktop/aclImdb/train/pos/*.txt"")
for file in fileList:
    readFileToConvertWordsToIntegers(dictionary, file, allDocuments, allLabels, 1)

print(len(allDocuments))
print(len(allLabels))

c = list(zip(allDocuments, allLabels)) # shuffle them partitioning

random.shuffle(c)

allDocuments, allLabels = zip(*c)

trainX = allDocuments[:22500]
testX = allDocuments[22500:]

trainY = allLabels[:22500]
testY = allLabels[22500:]


#counter=collections.Counter(trainY)
#print(counter)


trainY = to_categorical(trainY, nb_classes=2)
testY = to_categorical(testY, nb_classes=2)

trainX = pad_sequences(trainX, maxlen=100, value=0.)
testX = pad_sequences(testX, maxlen=100, value=0.)
# Converting labels to binary vectors
trainY = to_categorical(trainY, nb_classes=2)
testY = to_categorical(testY, nb_classes=2)

# Network building
net = tflearn.input_data([None, 100])
net = tflearn.embedding(net, input_dim=vocabulary_size, output_dim=128)
net = tflearn.lstm(net, 128, dropout=0.8)
net = tflearn.fully_connected(net, 2, activation='softmax')
net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,
                         loss='categorical_crossentropy')

# Training
model = tflearn.DNN(net, tensorboard_verbose=0)
model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,
          batch_size=32)
predictions = model.predict(trainX)
print(predictions)
</code></pre>

<p>Results:</p>

<pre><code>--
Training Step: 704  | total loss: 1.38629
Training Step: 704  | total loss: 1.38629: 0.4698 | val_loss: 1.38629 - val_acc:| Adam | epoch: 001 | loss: 1.38629 - acc: 0.4698 | val_loss: 1.38629 - val_acc: 0.4925 -- iter: 22500/22500
--
Training Step: 1408  | total loss: 1.38629
Training Step: 1408  | total loss: 1.38629 0.8110 | val_loss: 1.38629 - val_acc:| Adam | epoch: 002 | loss: 1.38629 - acc: 0.8110 | val_loss: 1.38629 - val_acc: 0.9984 -- iter: 22500/22500
--
Training Step: 1620  | total loss: 1.38629
Training Step: 2112  | total loss: 1.38629 0.8306 -- iter: 06784/22500
Training Step: 2112  | total loss: 1.38629 0.6303 | val_loss: 1.38629 - val_acc:| Adam | epoch: 003 | loss: 1.38629 - acc: 0.6303 | val_loss: 1.38629 - val_acc: 0.7382 -- iter: 22500/22500
--
Training Step: 2816  | total loss: 1.38629
Training Step: 2816  | total loss: 1.38629 0.5489 | val_loss: 1.38629 - val_acc:| Adam | epoch: 004 | loss: 1.38629 - acc: 0.5489 | val_loss: 1.38629 - val_acc: 0.2904 -- iter: 22500/22500
--
Training Step: 3520  | total loss: 1.38629
Training Step: 3520  | total loss: 1.38629 0.4848 | val_loss: 1.38629 - val_acc:| Adam | epoch: 005 | loss: 1.38629 - acc: 0.4848 | val_loss: 1.38629 - val_acc: 0.7828 -- iter: 22500/22500
--
Training Step: 4224  | total loss: 1.38629
Training Step: 4224  | total loss: 1.38629 0.5233 | val_loss: 1.38629 - val_acc:| Adam | epoch: 006 | loss: 1.38629 - acc: 0.5233 | val_loss: 1.38629 - val_acc: 0.9654 -- iter: 22500/22500
--
Training Step: 4928  | total loss: 1.38629
Training Step: 4928  | total loss: 1.38629 0.4400 | val_loss: 1.38629 - val_acc:| Adam | epoch: 007 | loss: 1.38629 - acc: 0.4400 | val_loss: 1.38629 - val_acc: 0.6725 -- iter: 22500/22500
--
Training Step: 5632  | total loss: 1.38629
Training Step: 5632  | total loss: 1.38629 0.4319 | val_loss: 1.38629 - val_acc:| Adam | epoch: 008 | loss: 1.38629 - acc: 0.4319 | val_loss: 1.38629 - val_acc: 0.5808 -- iter: 22500/22500
--
Training Step: 6336  | total loss: 1.38629
Training Step: 6336  | total loss: 1.38629 0.4765 | val_loss: 1.38629 - val_acc:| Adam | epoch: 009 | loss: 1.38629 - acc: 0.4765 | val_loss: 1.38629 - val_acc: 0.4833 -- iter: 22500/22500
--
Training Step: 7040  | total loss: 1.38629
Training Step: 7040  | total loss: 1.38629 0.5203 | val_loss: 1.38629 - val_acc:| Adam | epoch: 010 | loss: 1.38629 - acc: 0.5203 | val_loss: 1.38629 - val_acc: 0.2373 -- iter: 22500/22500
</code></pre>
","tensorflow, deep-learning, embedding, word2vec, lstm","<p>Ohh, it's my bad. I typed </p>

<pre><code>trainY = to_categorical(trainY, nb_classes=2) 
testY = to_categorical(testY, nb_classes=2) 
</code></pre>

<p>lines twice, so only one category exists afterwards. After I removed the repeated lines, problem has been solved.</p>
",1,0,1742,2016-09-11 16:47:22,https://stackoverflow.com/questions/39438386/sentiment-analysis-on-imdb-data-using-tflearn-lstm-tensorflow
Embedding lookup from multiple embeddings in tensorflow,"<p>Building a doc2Vec algorithm, there is a need for having multiple embeddings around. There are embeddings for the word vectors, while at the same time there are embeddings for the documents themselves. The way the algorithm works is similar to that of a CBOW model, but the document embedding is also used per each document being trained with a given window. So if we have a window of 5 words, we go ahead and go through those 5 words, but per each window we will always include the document embedding vector itself so that we can update it. </p>
","nlp, tensorflow, word2vec, doc2vec","<p>Just concat them:</p>

<pre><code>input_tensor = tf.concat(1, [wordembedding_tensor, documentembedding_tensor])
</code></pre>

<p>Then the input-tensors are your inputs.</p>
",2,0,775,2016-09-13 23:35:32,https://stackoverflow.com/questions/39480459/embedding-lookup-from-multiple-embeddings-in-tensorflow
How to load a pre-trained Word2vec MODEL File and reuse it?,"<p>I want to use a pre-trained <code>word2vec</code> model, but I don't know how to load it in python.</p>

<p>This file is a MODEL file (703 MB).
It can be downloaded here:<br>
<a href=""http://devmount.github.io/GermanWordEmbeddings/"" rel=""noreferrer"">http://devmount.github.io/GermanWordEmbeddings/</a></p>
","python, file, model, word2vec, gensim","<p>just for loading</p>

<pre><code>import gensim

# Load pre-trained Word2Vec model.
model = gensim.models.Word2Vec.load(""modelName.model"")
</code></pre>

<p>now you can train the model as usual. also, if you want to be able to save it and retrain it multiple times, here's what you should do</p>

<pre><code>model.train(//insert proper parameters here//)
""""""
If you don't plan to train the model any further, calling
init_sims will make the model much more memory-efficient
If `replace` is set, forget the original vectors and only keep the normalized
ones = saves lots of memory!
replace=True if you want to reuse the model
""""""
model.init_sims(replace=True)

# save the model for later use
# for loading, call Word2Vec.load()

model.save(""modelName.model"")
</code></pre>
",30,19,48836,2016-09-17 16:40:54,https://stackoverflow.com/questions/39549248/how-to-load-a-pre-trained-word2vec-model-file-and-reuse-it
doc2vec - Input Format for doc2vec training and infer_vector() in python,"<p>In gensim, when I give a string as input for training doc2vec model,  I get this error : </p>

<blockquote>
  <p>TypeError('don\'t know how to handle uri %s' % repr(uri))</p>
</blockquote>

<p>I referred to this question <a href=""https://stackoverflow.com/questions/36780138/doc2vec-taggedlinedocument"">Doc2vec : TaggedLineDocument()</a>
but still have a doubt about the input format. </p>

<p><code>documents = TaggedLineDocument('myfile.txt')</code></p>

<p>Should the myFile.txt have tokens as list of lists or separate list in each line for each document or a string? </p>

<p><code>For eg</code> - I have 2 documents.</p>

<p>Doc 1 : Machine learning is a subfield of computer science that evolved from the study of pattern recognition.</p>

<p>Doc 2 :  Arthur Samuel defined machine learning as a ""Field of study that gives computers the ability to learn"".</p>

<p>So, what should the <code>myFile.txt</code> look like?</p>

<p>Case 1 : simple text of each document in each line</p>

<p>Machine learning is a subfield of computer science that evolved from the study of pattern recognition</p>

<p>Arthur Samuel defined machine learning as a Field of study that gives computers the ability to learn</p>

<p>Case 2 : a list of lists having tokens of each document</p>

<p><code>[ [""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]</code>,</p>

<pre><code>[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""] ]
</code></pre>

<p>Case 3 : list of tokens of each document in a separate line</p>

<pre><code>[""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]

[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""]
</code></pre>

<p>And when I am running it on the test data, what should be the format of the sentence which i want to predict the doc vector for? Should it be like case 1 or case 2 below or something else?</p>

<p><code>model.infer_vector(testSentence, alpha=start_alpha, steps=infer_epoch)</code></p>

<p>Should the testSentence be :</p>

<p>Case 1 : string</p>

<pre><code>testSentence = ""Machine learning is an evolving field""
</code></pre>

<p>Case 2 : list of tokens</p>

<pre><code>testSentence = [""Machine"", ""learning"", ""is"", ""an"", ""evolving"", ""field""]
</code></pre>
","python, gensim, word2vec, doc2vec","<p><code>TaggedLineDocument</code> is a convenience class that expects its source file (or file-like object) to be space-delimited tokens, one per line. (That is, what you refer to as 'Case 1' in your 1st question.)</p>

<p>But you can write your own iterable object to feed to gensim <code>Doc2Vec</code> as the <code>documents</code> corpus, as long as this corpus (1) iterably-returns <code>next()</code> objects that, like TaggedDocument, have <code>words</code> and <code>tags</code> lists; and (2) can be iterated over multiple times, for the multiple passes <code>Doc2Vec</code> requires for both the initial vocabulary-survey and then <code>iter</code> training passes. </p>

<p>The <code>infer_vector()</code> method takes lists-of-tokens, similar to the <code>words</code> attribute of individual <code>TaggedDocument</code>-like objects. (That is, what you refer to as 'Case 2' in your 2nd question.)</p>
",5,2,2170,2016-09-21 11:33:05,https://stackoverflow.com/questions/39615420/doc2vec-input-format-for-doc2vec-training-and-infer-vector-in-python
Word2Vec: Using Gensim and Google-News dataset- Very Slow Execution Time,"<p>The Code is in python. I loaded up the binary model into gensim on python, &amp; used the ""init_sims"" option to make the execution faster. The OS is OS X.
It takes almost 50-60 seconds to load it up. And an equivalent time to find ""most_similar"". Is this normal? Before using the init_sims option, it took almost double the time! I have a feeling it might be an OS RAM allocation issue.</p>

<pre><code>model=Word2Vec.load_word2vec_format('GoogleNewsvectorsnegative300.bin',binary=True)
model.init_sims(replace=True)
model.save('SmallerFile')
#MODEL SAVED INTO SMALLERFILE &amp; NEXT LOAD FROM IT
model=Word2Vec.load('SmallerFile',mmap='r')
#GIVE RESULT SER!
print model.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>
","python, gensim, word2vec","<p>Note that the memory-saving effect of <code>init_sims(replace=True)</code> doesn't persist across save/load cycles, because saving always saves the 'raw' vectors (from which the unit-normalized vectors can be recalculated). So, even after your re-load, when you call <code>most_similar()</code> for the 1st time, <code>init_sims()</code> will be called behind the scenes, and the memory usage will be doubled.</p>

<p>And, the GoogleNews dataset is quite large, taking 3+ GB to load even before the unit-normalization possibly doubles the memory usage. So depending on what else you've got running and the machine's RAM, you might be using swap memory by the time the <code>most_similar()</code> calculations are running â€“ which is very slow for the calculate-against-every-vector-and-sort-results similarity ops. (Still, any <code>most_similar()</code> checks after the 1st won't need to re-fill the unit-normalized vector cache, so should go faster than the 1st call.)</p>

<p>Given that you've saved the model after <code>init_sims(replace=True)</code>, its raw vectors are already unit-normalized. So you can manually-patch the model to skip the recalculation, just after your <code>load()</code>:</p>

<pre><code>model.syn0norm = model.syn0
</code></pre>

<p>Then even your first <code>most_similar()</code> will just consult the (single, memory-mapped) set of vectors, without triggering an <code>init_sims()</code>.</p>

<p>If that's still too slow, you may need more memory or to trim the vectors to a subset. The GoogleNews vectors seem to be sorted to put the most-frequent words earliest, so throwing out the last 10%, 50%, even 90% may still leave you with a useful set of the most-common words. (You'd need to perform this trimming yourself by looking at the model object and source code.)</p>

<p>Finally, you can use a nearest-neighbors indexing to get faster top-N matches, but at a cost of extra memory and approximate results (that may miss some of the true top-N matches). There's an IPython notebook tutorial in recent gensim versions at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/annoytutorial.ipynb"" rel=""noreferrer"">annoytutorial.ipynb</a> IPython notebook of the demo IPython notebooks, in the gensim <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/"" rel=""noreferrer""><code>docs/notebooks</code></a> directory.</p>
",6,5,2587,2016-09-23 09:24:33,https://stackoverflow.com/questions/39657215/word2vec-using-gensim-and-google-news-dataset-very-slow-execution-time
Distributed Word2Vec Model Training using Apache Spark 2.0.0 and mllib,"<p>I have been experimenting with spark and mllib to train a word2vec model but I don't seem to be getting the performance benefits of distributed machine learning on large datasets. My understanding is that if I have w workers, then, if I create an RDD with n number of partitions where n>w and I try to create a Word2Vec Model by calling the fit function of Word2Vec with the RDD as parameter then spark would distribute the data uniformly to train separate word2vec models on these w workers and use some sort of a reducer function at the end to create a single output model from these w models. This would reduce the computation time as rather than 1 chunk, w chunks of data will be processed simultaneously. The trade-off would be that some loss of precision might happen depending upon the reducer function used at the end. Does Word2Vec in Spark actually work this way or not? I might need to play with the configurable parameters if this is indeed the case.</p>

<p><strong>EDIT</strong></p>

<p>Adding the reason behind asking this question. I ran java spark word2vec code on 10 worker machines and set suitable values for executor-memory, driver memory and num-executors, after going though the documentation, for a 2.5gb input text file which was mapped to rdd partitions which were then used as training data for an mllib word2vec model. The training part took multiple hours. The number of worker nodes doesn't seem to be having much of an effect on the training time. The same code runs successfully on smaller data files (of the order of 10s of MBs)</p>

<p><strong>Code</strong></p>

<pre><code>SparkConf conf = new SparkConf().setAppName(""SampleWord2Vec"");
conf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"");
conf.registerKryoClasses(new Class[]{String.class, List.class});
JavaSparkContext jsc = new JavaSparkContext(conf);
JavaRDD&lt;List&lt;String&gt;&gt; jrdd = jsc.textFile(inputFile, 3).map(new Function&lt;String, List&lt;String&gt;&gt;(){            
        @Override
        public List&lt;String&gt; call(String s) throws Exception {
            return Arrays.asList(s.split("",""));
        }        
});
jrdd.persist(StorageLevel.MEMORY_AND_DISK());
Word2Vec word2Vec = new Word2Vec()
      .setWindowSize(20)
      .setMinCount(20);

Word2VecModel model = word2Vec.fit(jrdd);
jrdd.unpersist(false);
model.save(jsc.sc(), outputfile);
jsc.stop();
jsc.close();
</code></pre>
","java, apache-spark, apache-spark-mllib, word2vec","<p>Judging from the comments, answers and downvotes I guess I wasn't able to frame my question correctly. But the answer to what I wanted to know is yes, it is possible to train your word2vec model in parallel on spark. The pull request for this feature was created long time back:</p>

<p><a href=""https://github.com/apache/spark/pull/1719"" rel=""nofollow"">https://github.com/apache/spark/pull/1719</a></p>

<p>In java, there is a setter method (setNumPartitions) for the Word2Vec object in spark mllib. This allows you to train your word2vec model on more than one executor in parallel. 
As per the comments on the pull request mentioned above:</p>

<p>""<em>To make our implementation more scalable, we train each partition separately and merge the model of each partition after each iteration. To make the model more accurate, multiple iterations may be needed.</em>""</p>

<p>Hope this helps someone.</p>
",4,1,2643,2016-09-28 18:44:08,https://stackoverflow.com/questions/39755358/distributed-word2vec-model-training-using-apache-spark-2-0-0-and-mllib
How can I tell if Gensim Word2Vec is using the C compiler?,"<p>I am trying to use Gensim's Word2Vec implementation. Gensim warns that if you don't have a C compiler, the training will be 70% slower.  Is there away to verify that Gensim is correctly using the C Compiler I have installed?</p>

<p>I am using Anaconda Python 3.5 on Windows 10.</p>
","python, compilation, installation, gensim, word2vec","<p>Apparently gensim offers a variable to detect this:</p>

<pre><code>assert gensim.models.doc2vec.FAST_VERSION &gt; -1
</code></pre>

<p>I found this line in this tutorial:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a></p>
",20,15,7179,2016-09-30 00:09:37,https://stackoverflow.com/questions/39781812/how-can-i-tell-if-gensim-word2vec-is-using-the-c-compiler
How to download word2vec?,"<p>When I'm trying to download Word2vec tool from official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">google code page</a>, but I got the following:</p>

<p>svn checkout <a href=""http://word2vec.googlecode.com/svn/trunk/"" rel=""noreferrer"">http://word2vec.googlecode.com/svn/trunk/</a></p>

<p>svn: E170013: Unable to connect to a repository at URL '<a href=""http://word2vec.googlecode.com/svn/trunk"" rel=""noreferrer"">http://word2vec.googlecode.com/svn/trunk</a>'</p>

<p>svn: E160013: '/svn/trunk' path not found</p>
","svn, word2vec","<p>You can download the original code from here: <a href=""https://github.com/tmikolov/word2vec"" rel=""noreferrer"">https://github.com/tmikolov/word2vec</a>, whereas I am not sure if there had been further updates to it since the time this snapshot was taken to github.</p>

<p>Or use a ""further maintained"" implementation of the algorithm, there are quite a few out there, in multiple languages. </p>
",10,18,30966,2016-10-15 11:53:34,https://stackoverflow.com/questions/40058693/how-to-download-word2vec
word2vec - get nearest words,"<p>Reading the tensorflow word2vec model output how can I output the words related to a specific word ?</p>

<p>Reading the src : <a href=""https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a> can view how the image is plotted.</p>

<p>But is there a data structure (e.g dictionary) created as part of training the model that allows to access nearest n words closest to given word ?
For example if word2vec generated image :</p>

<p><a href=""https://i.sstatic.net/Bg1eg.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Bg1eg.png"" alt=""enter image description here""></a></p>

<p>image src: <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html</a></p>

<p>In this image the words 'to , he , it' are contained in same cluster, is there a function which takes as input 'to' and outputs 'he , it' (in this case n=2) ?</p>
","tensorflow, word2vec","<p>This approach apply to word2vec in general. If you can save the word2vec in text/binary file like google/<a href=""http://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">GloVe</a> word vector. Then what you need is just the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#module-gensim.models.word2vec"" rel=""noreferrer"">gensim</a>.</p>

<p>To install:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim"" rel=""noreferrer"">Via github</a></p>

<p>Python code: </p>

<pre><code>from gensim.models import Word2Vec

gmodel=Word2Vec.load_word2vec_format(fname)
ms=gmodel.most_similar('good',10)
for x in ms:
    print x[0],x[1]
</code></pre>

<p>However this will search all the words to give the results, there are approximate nearest neighbor (ANN) which will give you the result faster but with a trade off in accuracy.  </p>

<p>In the latest gensim, <a href=""https://github.com/spotify/annoy"" rel=""noreferrer"">annoy</a> is used to perform the ANN, see this <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/annoytutorial.ipynb"" rel=""noreferrer"">notebooks</a> for more information.</p>

<p><a href=""https://github.com/mariusmuja/flann"" rel=""noreferrer"">Flann is another library for Approximate Nearest Neighbors.</a></p>
",14,12,20792,2016-10-16 19:18:01,https://stackoverflow.com/questions/40074412/word2vec-get-nearest-words
How do &quot;de-embed&quot; words in TensorFlow,"<p>I am trying to follow the tutorial for <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html#language-modeling"" rel=""nofollow noreferrer"">Language Modeling</a> on the TensorFlow site.  I see it runs and the cost goes down and it is working great, but I do not see any way to actually get the predictions from the model.  I tried following the instructions at <a href=""https://stackoverflow.com/questions/37179218/getting-word-from-id-at-tensorflow-rnn-sample"">this answer</a> but the tensors returned from session.run are floating point values like 0.017842259, and the dictionary maps words to integers so that does not work.</p>

<p>How can I get the predicted word from a tensorflow model? </p>

<p>Edit: I found this <a href=""https://github.com/tensorflow/tensorflow/issues/97"" rel=""nofollow noreferrer"">explanation</a> after searching around, I just am not sure what x and y would be in the context of this example.  They don't seem to use the same conventions for this example as they do in the explanation.</p>
","python, tensorflow, word2vec","<p>The tensor you are mentioning is the <code>loss</code>, which defines how the network is training. For prediction, you need to access the tensor <code>probabilities</code> which contain the probabilities for the next word. If this was classification problem, you'd just do <code>argmax</code> to get the top probability. But, to also give lower probability words a chance of being generated,some kind of sampling is often used.</p>

<p>Edit: I assume the code you used is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">this</a>. In that case, if you look at line 148 (<code>logits</code>) which can be converted into probabilities by simply applying the <code>softmax</code> function to it -- like shown in the pseudocode in tensorflow website. Hope this helps.</p>
",1,0,272,2016-10-17 19:22:37,https://stackoverflow.com/questions/40094097/how-do-de-embed-words-in-tensorflow
How to get the key value pairs in numpy.ndarray? (Gensim Word2vec),"<p>I am trying to get the keys as well as the vectors in the vector <code>model.syn0</code> which gives vectors by <code>model.syn0[""word""]</code> which gives an n-dim vector. Is there a better way to create a list of all the words in the model in the same order as the the vectors of <code>syn0</code> than this? I have 350000 words and this would take too long.</p>

<pre><code>from gensim.models import word2vec as wv
model = wv.Word2Vec.load('model')
lab=[]
for i in model.syn0:
    lab.append(model.similar_by_vector(i)[0])

print(type(model.syn0))
    &lt;type 'numpy.ndarray'&gt;
</code></pre>
","python, performance, gensim, word2vec","<p>At the direction of <a href=""https://groups.google.com/forum/#!searchin/gensim/$20List$20of$20words$20in$20the$20vocabulary$20(syn0$20word$20list)$20%7Csort:relevance/gensim/qkEZ-MFf-NI/jBHqH14pBwAJ"" rel=""nofollow noreferrer"">Gordon Mohr</a>, I found that the key value pairs are stored in <code>model.index2word</code>. </p>

<p>So the key-value pairs list can easily be obtained using:</p>

<pre><code>lab=model.index2word
</code></pre>
",0,1,1845,2016-10-21 16:59:07,https://stackoverflow.com/questions/40181943/how-to-get-the-key-value-pairs-in-numpy-ndarray-gensim-word2vec
Implement word2vec in Keras,"<p>I would like to implement word2vec algorithm in keras, Is this possible? 
How can I fit the model? Should I use custom loss function?</p>
","nlp, deep-learning, keras, theano, word2vec","<blockquote>
  <p>Is this possible?</p>
</blockquote>

<p>You've already answered it yourself: yes. In addition to <a href=""https://github.com/niitsuma/word2vec-keras-in-gensim"" rel=""noreferrer""><code>word2veckeras</code></a>, which uses <code>gensim</code>, here's another <a href=""https://github.com/abaheti95/Deep-Learning/tree/master/word2vec/keras"" rel=""noreferrer"">CBOW implementation</a> that doesn't have extra dependencies (just in case, I'm not affiliated with this repo). You can use them as examples.</p>

<blockquote>
  <p>How can I fit the model? </p>
</blockquote>

<p>Since the training data is the large corpus of sentences, the most convenient method is <a href=""https://keras.io/models/sequential/"" rel=""noreferrer""><code>model.fit_generator</code></a>, which ""fits the model on data generated batch-by-batch by a Python generator"". The generator runs indefinitely yielding <code>(word, context, target)</code> CBOW (or SG) tuples, but you manually specify <code>sample_per_epoch</code> and <code>nb_epoch</code> to limit the training. This way you decouple sentence analysis (tokenization, word index table, sliding window, etc) and actual keras model, plus <a href=""https://stackoverflow.com/questions/628903/performance-advantages-to-iterators"">save a lot of resources</a>.</p>

<blockquote>
  <p>Should I use custom loss function?</p>
</blockquote>

<p>CBOW minimizes the distance between the predicted and true distribution of the center word, so in the simplest form <code>categorical_crossentropy</code> will do it.
If you implement <a href=""https://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term"">negative sampling</a>, which is a bit more complex, yet much more efficient, the loss function changes to <a href=""https://keras.io/losses/"" rel=""noreferrer""><code>binary_crossentropy</code></a>. Custom loss function is unnecessary.</p>

<p>For anyone interested in details of math and probabilistic model, I highly recommend CS224D class by Stanford. <a href=""https://cs224d.stanford.edu/lecture_notes/notes1.pdf"" rel=""noreferrer"">Here is the lecture notes</a> about word2vec, CBOW and Skip-Gram.</p>

<p>Another useful reference: <a href=""https://github.com/danielfrg/word2vec"" rel=""noreferrer"">word2vec implementation</a> in pure <code>numpy</code> and <code>c</code>.</p>
",6,11,9665,2016-10-25 15:35:18,https://stackoverflow.com/questions/40244101/implement-word2vec-in-keras
Parallel version of t-SNE,"<p>Is there any Python library with parallel version of t-SNE algorithm?
Or does the multicore/parallel t-SNE algorithm exist?</p>

<p>I'm trying to reduce dimension (300d -> 2d) of all word2vecs in my vocabulary using t-SNE. </p>

<p>Problem: the size of vocabulary is about 130000 and it takes too long to proceed t-SNE for them.</p>
","python, parallel-processing, multiprocessing, word2vec, dimensionality-reduction","<p>Yes there is a parallel version of the barnes-hutt implementation of t-SNE.
<a href=""https://github.com/DmitryUlyanov/Multicore-TSNE"" rel=""noreferrer"">https://github.com/DmitryUlyanov/Multicore-TSNE</a></p>

<p>There is also now a new implementation of tSNE that uses a Fast-Fourier transform funciton to significantly speed up the convolution step. It also uses the ANNOY library to perform the nearest neighbours search, the default tree-based method is also there and both take advantage of parallel processing.</p>

<p>Original code is available here:
<a href=""https://github.com/KlugerLab/FIt-SNE"" rel=""noreferrer"">https://github.com/KlugerLab/FIt-SNE</a></p>

<p>and an R package version here:
<a href=""https://github.com/JulianSpagnuolo/FIt-SNE"" rel=""noreferrer"">https://github.com/JulianSpagnuolo/FIt-SNE</a></p>
",9,10,5851,2016-11-04 19:16:34,https://stackoverflow.com/questions/40430267/parallel-version-of-t-sne
word vector and paragraph vector query,"<p>I am trying to understand relation between word2vec and doc2vec vectors in Gensim's implementation. In my application, I am tagging multiple documents with same label (topic), I am training a doc2vec model on my corpus using dbow_words=1 in order to train word vectors as well. I have been able to obtain similarities between word and document vectors in this fashion which does make a lot of sense
For ex. getting documents labels similar to a word-
doc2vec_model.docvecs.most_similar(positive = [doc2vec_model[""management""]], topn = 50))</p>

<p>My question however is about theoretical interpretation of computing similarity between word2vec and doc2vec  vectors. Would it be safe to assume that when trained on the same corpus with same dimensionality (d = 200), word vectors and document vectors can always be compared to find similar words for a document label or similar document labels for a word. Any suggestion/ideas are most welcome.</p>

<p>Question 2: My other questions is about impact of high/low frequency of a word in final word2vec model. If wordA and wordB have similar contexts in a particular doc label(set) of documents but wordA has much higher frequency than wordB, would wordB have higher similarity score with the corresponding doc label or not. I am trying to train multiple word2vec models by sampling corpus in a temporal fashion and want to know if the hypothesis that as words get more and more frequent, assuming context relatively stays similar, similarity score with a document label would also increase. Am I wrong to make this assumption? Any suggestions/ideas are very welcome.</p>

<p>Thanks,
Manish</p>
","similarity, gensim, word2vec, temporal, doc2vec","<p>In a training mode where word-vectors and doctag-vectors are interchangeably used during training, for the same surrounding-words prediction-task, they tend to be meaningfully comparable. (Your mode, DBOW with interleaved skip-gram word-training, fits this and is the mode used by the paper '<a href=""https://arxiv.org/abs/1507.07998"" rel=""nofollow noreferrer"">Document Embedding with Paragraph Vectors</a>'.)</p>

<p>Your second question is abstract and speculative; I think you'd have to test those ideas yourself. The Word2Vec/Doc2Vec processes train the vectors to be good at certain mechanistic word-prediction tasks, subject to the constraints of the model and tradeoffs with other vectors' quality. That the resulting spatial arrangement happens to be then useful for other purposes â€“ ranked/absolute similarity, similarity along certain conceptual lines, classification, etc. â€“ is then just an observed, pragmatic benefit. It's a 'trick that works', and might yield insights, but many of the ways models change in response to different parameter choices or corpus characteristics haven't been theoretically or experimentally worked-out. </p>
",1,0,633,2016-11-07 18:30:20,https://stackoverflow.com/questions/40472070/word-vector-and-paragraph-vector-query
TensorFlow &#39;module&#39; object has no attribute &#39;global_variables_initializer&#39;,"<p>I'm new to Tensorflow
I'm running a Deep learning Assignment from Udacity on iPython notebook.
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"">link</a></p>

<p>And it has an error.</p>

<pre><code>AttributeError                            Traceback (most recent call last)
`&lt;ipython-input-18-3446420b5935&gt;` in `&lt;module&gt;`()
  2 
  3 with tf.Session(graph=graph) as session:
----&gt; 4   tf.global_variables_initializer().run()

AttributeError: 'module' object has no attribute 'global_variables_initializer'
</code></pre>

<p>Please help! How can I fix this? Thank you.</p>
","python, tensorflow, deep-learning, word2vec","<p>In older versions, it was called <code>tf.initialize_all_variables</code>.</p>
",42,39,34369,2016-11-09 16:20:16,https://stackoverflow.com/questions/40511562/tensorflow-module-object-has-no-attribute-global-variables-initializer
kaggleword2vec utility not found,"<p>I'm trying to follow along with some tutorials to complete some projects/competitions but I can't seem to find ""kaggleword2vec""; any suggestions?</p>

<pre><code>import os 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from KaggleWord2VecUtility import KaggleWord2VecUtility
import pandas as pd 
import numpy as np

Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/sentimentanalysis9"", line 4, in &lt;module&gt;
    from KaggleWord2VecUtility import KaggleWord2VecUtility
ImportError: No module named KaggleWord2VecUtility
</code></pre>

<p>Where can I find the KaggleWord2VecUtility? I have Word2Vec and a bunch of other toolkits I've been using but this one in particular isn't recognized. I'd like to continue following the tutorials but can't seem to without this module.</p>

<p>Thanks</p>
","python, scikit-learn, word2vec","<p>You can get everything you need here <a href=""https://github.com/wendykan/DeepLearningMovies"" rel=""nofollow noreferrer"">https://github.com/wendykan/DeepLearningMovies</a></p>
",2,1,2083,2016-11-15 18:25:37,https://stackoverflow.com/questions/40617106/kaggleword2vec-utility-not-found
use a.all() or a.any() error while trying to use gensim word2vec,"<p>I've been trying to run an example of how t use word2vec from the gensim library of python but I keep getting this error </p>

<pre><code>    ValueError: The truth value of an array with more than one element is   ambiguous. Use a.any() or a.all()
</code></pre>

<p>This is my code, it's just a simple example :</p>

<pre><code>    from gensim.models import Word2Vec
    sentences = [['first', 'sentence'], ['second', 'sentence']]
    # train word2vec on the two sentences
    model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Note: I've made sure that gensim is installed with all its dependencies.</p>
","python-2.7, gensim, word2vec","<p>I had the same exact problem too, what i did was installing python-dev package then re-installing gensim, somehow that worked, i'm on ubuntu so this is what i did :</p>

<pre><code>sudo apt-get install python-dev
sudo pip uninstall gensim
sudo pip install gensim
</code></pre>

<p>when i run this :</p>

<pre><code>model = gensim.models.Word2Vec(sentences=listSentence,min_count=2,window=3,size=20,workers=1)
print model['Brasil']
</code></pre>

<p>it worked and i got the result vector :</p>

<pre><code>[-0.01635483  0.02224622 -0.01865266  0.02168317 -0.01231722 -0.0207897
 -0.0014509   0.00264822 -0.01889374 -0.02109174 -0.00244757  0.00024959
 -0.00898884 -0.01826199 -0.01361686 -0.01770178 -0.02431025 -0.01903439
 -0.00775641  0.02353667]
</code></pre>
",0,2,1898,2016-11-18 06:53:42,https://stackoverflow.com/questions/40671057/use-a-all-or-a-any-error-while-trying-to-use-gensim-word2vec
Features of vector form of sentences for opinion finding.,"<p>I want to find the opinion of a sentence either positive or negative. For example talk about only one sentence.  </p>

<p><code>The play was awesome</code></p>

<p>If change it to vector form </p>

<p><code>[0,0,0,0]</code></p>

<p>After searching through the Bag of words</p>

<pre><code>bad
naughty
awesome
</code></pre>

<p>The vector form becomes</p>

<p><code>[0,0,0,1]</code></p>

<p>Same for other sentences. Now I want to pass it to the machine learning algorithm for training it. <strong>How can I train the network using these multiple vectors?</strong> (for finding the opinion of unseen sentences) Obviously not! Because the input is fix in neural network. Is there any way? The above procedure is just my thinking. Kindly correct me if I am wrong. Thanks in advance. </p>
","machine-learning, nlp, neural-network, word2vec","<p>Since your intuitive input format is ""Sentence"". Which is, indeed, a string of tokens with <strong>arbitrary</strong> length. Abstracting sentences as token series is not a good choice for many existing algorithms only works on determined format of inputs. </p>

<p>Hence, I suggest try using <strong>tokenizer</strong> on your entire training set. This will give you vectors of length of the dictionary, which is fixed for given training set. </p>

<p>Because when the length of sentences vary drastically, then size of the dictionary always keeps stable.</p>

<p>Then you can apply Neural Networks(or other algorithms) to the tokenized vectors.</p>

<p><strong>However</strong>, vectors generated by tokenizer is <strong>extremely sparse</strong> because you only work on sentences rather than articles.</p>

<p>You can try <a href=""https://en.wikipedia.org/wiki/Linear_discriminant_analysis"" rel=""nofollow noreferrer"">LDA</a> (supervised, not <a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow noreferrer"">PCA</a>), to reduce the dimension as well as amplify the difference.</p>

<p>That will keep the essential information of your training data as well as express your data at fixed size, while this ""size"" is not too large.</p>

<p>By the way, you may not have to label each word by its attitude since the opinion of a sentence also depends on other kind of words. </p>

<p>Simple arithmetics on number of opinion-expressing words many leave your model highly biased. Better label the sentences and leave the rest job to classifiers.</p>

<blockquote>
  <p>For the confusions</p>
  
  <p>PCA and LDA are Dimensional Reduction techniques.
  <a href=""https://www.quora.com/What-is-the-difference-between-LDA-and-PCA-for-dimension-reduction"" rel=""nofollow noreferrer"">difference</a></p>
  
  <p>Let's assume each tuple of sample is denoted as <code>x</code> (1-by-p vector).</p>
  
  <p>p is too large, we don't like that.</p>
  
  <p>Let's find a matrix A(p-by-k) in which k is pretty small. </p>
  
  <p>So we get <code>reduced_x</code> = <code>x*A</code>, and most importantly, <code>reduced_x</code> must
  be able to represent <code>x</code>'s characters.</p>
  
  <p>Given labeled data, LDA can provide proper A that can maximize
  distance between <code>reduced_x</code> of different classes, and also minimize
  the distance within identical classes.</p>
  
  <p>In simple words: compress data, keep information.   </p>
  
  <p>When you've got
  <code>reduced_x</code>, you can define training data: <code>(reduced_x|y)</code> where y is
  0 or 1.</p>
</blockquote>
",0,0,61,2016-11-21 06:23:18,https://stackoverflow.com/questions/40713967/features-of-vector-form-of-sentences-for-opinion-finding
Rename gensim Word2Vec words with mapping,"<p>I want to replace the words of my gensim Word2Vec model with a mapping.</p>

<p><strong>Example</strong></p>

<p>My current model has the word <code>'foo'</code> that maps to a vector: </p>

<pre><code>&gt;&gt;&gt; model['foo']
[1.0 0.0]
</code></pre>

<p>I have the mapping: <code>d = {'foo': 'bar', ...}</code></p>

<p>How can I rebuild the model with this new mapping such that </p>

<pre><code>&gt;&gt;&gt; model['bar']  # in place of 'foo'
[1.0 0.0]
</code></pre>
","python, gensim, word2vec","<p>One solution is to save the model in the C-based word2vec format and replace the original words with a mapping of the new words using <code>awk</code>.</p>

<p>Assume we have a file mapping of the form:</p>

<pre><code>$ cat map.txt
foo:bar
...
</code></pre>

<p>We can recreate the model via: </p>

<pre><code>import subprocess as sp
import shlex

from gensim.models import Word2Vec

model.save_word2vec_format('embeddings.txt', binary=False)

CMD = r""""""
awk -F'[ ]|:' 'FNR==NR {a[$1]=$2; next} FNR==1{print $0} FNR!=1{$1=a[$1]; print $0}' map.txt embeddings.txt
""""""

with open('new_embeddings.txt', 'w') as f:
    p = sp.Popen(shlex.split(CMD), stdout=f)

new_model = Word2Vec.load_word2vec_format('new_embeddings.txt')

new_model.create_binary_tree()
</code></pre>

<p>As an aside my mapping was actually an array where I was training on the index of the word in some array <code>arr</code>. I created the map file using numpy:</p>

<pre><code>import numpy as np

np.savetxt('map.txt', np.c_[np.arange(arr.size), arr], '%d:%s')
</code></pre>
",1,2,393,2016-12-02 15:56:07,https://stackoverflow.com/questions/40936197/rename-gensim-word2vec-words-with-mapping
Dumping spark word2vec vectors to a file,"<p>I am using spark mllib to generate word vectors. I wish to fit all my data and then get the trained word vectors and dump them to a file.</p>

<p>I am doing this :</p>

<pre><code>JavaRDD&lt;List&lt;String&gt;&gt; data = javaSparkContext.parallelize(streamingData, partitions);
Word2Vec word2vec = new Word2Vec();
Word2VecModel model = word2vec.fit(data);
</code></pre>

<p>So, if my training data had sentences like </p>

<pre><code>I love Spark
</code></pre>

<p>I want to save the output in files as :</p>

<pre><code>I       0.03 0.53 0.12...
love    0.31 0.14 0.12...
Spark   0.41 0.18 0.84...
</code></pre>

<p>After training, I am getting the vectors from the model object like this</p>

<pre><code>Map&lt;String, float[]&gt; wordMap = JavaConverters.mapAsJavaMapConverter(model.getVectors()).asJava();
List&lt;String&gt; wordvectorlist = Lists.newArrayList();
for (String s : wordMap.keySet()) {
    StringBuilder wordvector = new StringBuilder(s);
    for (float f : wordMap.get(s)) {
        wordvector.append("" "" + f);
    }
    wordvectorlist.add(wordvector.toString());
    if (wordvectorlist.size() &gt; 1000000) {
        writeToFile(wordvectorlist);
        wordvectorlist.clear();
    }

}
</code></pre>

<p>I will be generating these word vectors for a very huge data (~1.5 TB) and thus, I might not be able to save the returned object Word2VecModel in memory of my driver.
How can I store this wordvectors map as a rdd so that I can write to files without storing the full map in driver memory?</p>

<p>I looked into word2vec implementation of deeplearning4j but that implementation also requires loading all the vectors in driver memory.</p>
","java, apache-spark, rdd, apache-spark-mllib, word2vec","<p>Word2VecModel has a <a href=""https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/feature/Word2VecModel.html#save(org.apache.spark.SparkContext,%20java.lang.String)"" rel=""nofollow noreferrer"">save</a> function which saves it to disk in its own format
This will create a directory called data with parquet files of the data and a metadata file with human readable metadata.</p>

<p>You can now read the parquet file and convert it yourself or instead do spark.read.parquet to read it to dataframe. Each line would contain some of the map and you can write it any way you wish.</p>
",3,4,1758,2016-12-06 09:32:11,https://stackoverflow.com/questions/40991973/dumping-spark-word2vec-vectors-to-a-file
how to load a word2vec model and call its function into the mapper,"<p>I want to load a word2vec model and evaluate it by executing word analogy tasks (e.g. <em>a is to b as c is to something?</em>). To do this, first I load my w2v model:</p>



<pre class=""lang-py prettyprint-override""><code>model = Word2VecModel.load(spark.sparkContext, str(sys.argv[1]))
</code></pre>

<p>and then I call the mapper to evaluate the model:</p>

<pre class=""lang-py prettyprint-override""><code>rdd_lines = spark.read.text(""questions-words.txt"").rdd.map(getAnswers)
</code></pre>

<p>The <code>getAnswers</code> function reads one line per time from <em>questions-words.txt</em>, in which each line contains the question and the answer to evaluate my model (e.g. Athens Greece Baghdad Iraq, where a=Athens, b=Greece, c=Baghdad and something=Iraq). After reading the line, I create the <code>current_question</code> and the <code>actual_answer</code> (e.g.: <code>current_question=Athens Greece Baghdad</code> and <code>actual_answer=Iraq</code>). After that, I call the <code>getAnalogy</code> function that is used to compute the analogy (basically, given the question it computes the answer). Finally, after computing the analogy, I return the answer and write it to a text file.</p>

<p>The problem is that I get the following exception:</p>

<pre class=""lang-py prettyprint-override""><code>Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers.
</code></pre>

<p>and I think that it is thrown because I am using the model within the map function. This <a href=""https://stackoverflow.com/questions/34448456/using-word2vecmodel-transform-does-not-work-in-map-function"">question</a> is similar to my problem but I do not know how to apply that answer to my code. How can I solve this problem? The following is the full code:</p>

<pre class=""lang-py prettyprint-override""><code>def getAnalogy(s, model):
    try:
        qry = model.transform(s[0]) - model.transform(s[1]) - model.transform(s[2])    
        res = model.findSynonyms((-1)*qry,5) # return 5 ""synonyms""
        res = [x[0] for x in res]
        for k in range(0,3):
            if s[k] in res:
                res.remove(s[k])
        return res[0]
    except ValueError:
        return ""NOT FOUND""

def getAnswers (text):
    tmp = text[0].split(' ', 3)
    answer_list = []
    current_question = "" "".join(str(x) for x in tmp[:3])
    actual_answer = tmp[-1]

    model_answer = getAnalogy(current_question, model)
    if model_answer is ""NOT FOUND"":
        answer_list.append(""NOT FOUND\n"")
    elif model_answer is actual_answer:
        answer_list.append(""TRUE\n"")
    else:
        answer_list.append(""FALSE:\n"")
    return answer_list.append


if __name__ == ""__main__"":

    if len(sys.argv) != 3:
        print(""Usage: my_test &lt;file&gt;"", file=sys.stderr)
        exit(-1)


    spark = SparkSession\
    .builder\
    .appName(""my_test"")\
    .getOrCreate()


    model = Word2VecModel.load(spark.sparkContext, str(sys.argv[1]))

    rdd_lines = spark.read.text(""questions-words.txt"").rdd.map(getAnswers)

    dataframe = rdd_lines.toDF()

    dataframe.write.text(str(sys.argv[2]))

    spark.stop()
</code></pre>
","apache-spark, pyspark, apache-spark-mllib, word2vec","<p>As you have already suspected, you cannot use the model in a map function. On the other hand, the <code>questions-answers.txt</code> file is not that big (~ 20K lines), so you should better do the evaluation using vanilla Python list comprehensions (it is essentially the first suggested answer in the question you have linked); it is not fast, but it is just an one-off task. Here is a way, using <a href=""https://stackoverflow.com/questions/34172242/spark-word2vec-vector-mathematics/34298583#34298583"">my <code>getAnalogy</code> function</a> as you have augmented it for error handling (notice that I have already removed the 'comment' lines from <code>questions-answers.txt</code>, and that you should convert it to lowercase, something you don't seem to be doing in your code):
    </p>

<pre class=""lang-py prettyprint-override""><code>from pyspark.mllib.feature import Word2Vec, Word2VecModel
model = Word2VecModel.load(sc, ""word2vec/demo_200"") # model built with k=200
with open('/home/ctsats/word2vec/questions-words.txt') as f:
    lines = f.readlines()
lines2 = [x.lower() for x in lines] # all to lowercase
lines3 = [x.strip('\n') for x in lines2] # remove end-of-line characters
lines4 = [x.split(' ',3) for x in lines3]
lines4[0] # check:
# ['Athens', 'Greece', 'Baghdad', 'Iraq']

def getAnswers (text, model):
    actual_answer = text[-1]
    question = [text[0], text[1], text[2]]
    model_answer = getAnalogy(question, model)
    if model_answer == ""NOT FOUND"":
        correct_answer = ""NOT FOUND""
    elif model_answer == actual_answer:
        correct_answer = ""TRUE""
    else:
        correct_answer = ""FALSE""
    return text, model_answer, correct_answer
</code></pre>

<p>So, your evaluation list can now be built as</p>

<pre class=""lang-py prettyprint-override""><code>answer_list = [getAnswers(x, model) for x in lines4]    
</code></pre>

<p>Here's an example for the first 20 entries (with a model of <code>k=200</code>):</p>

<pre class=""lang-py prettyprint-override""><code>[(['athens', 'greece', 'baghdad', 'iraq'], u'turkey', 'FALSE'),
 (['athens', 'greece', 'bangkok', 'thailand'], u'turkey', 'FALSE'),
 (['athens', 'greece', 'beijing', 'china'], u'albania', 'FALSE'),
 (['athens', 'greece', 'berlin', 'germany'], u'germany', 'TRUE'),
 (['athens', 'greece', 'bern', 'switzerland'], u'liechtenstein', 'FALSE'),
 (['athens', 'greece', 'cairo', 'egypt'], u'albania', 'FALSE'),
 (['athens', 'greece', 'canberra', 'australia'], u'liechtenstein', 'FALSE'),
 (['athens', 'greece', 'hanoi', 'vietnam'], u'turkey', 'FALSE'),
 (['athens', 'greece', 'havana', 'cuba'], u'turkey', 'FALSE'),
 (['athens', 'greece', 'helsinki', 'finland'], u'finland', 'TRUE'),
 (['athens', 'greece', 'islamabad', 'pakistan'], u'turkey', 'FALSE'),
 (['athens', 'greece', 'kabul', 'afghanistan'], u'albania', 'FALSE'),
 (['athens', 'greece', 'london', 'england'], u'italy', 'FALSE'),
 (['athens', 'greece', 'madrid', 'spain'], u'portugal', 'FALSE'),
 (['athens', 'greece', 'moscow', 'russia'], u'russia', 'TRUE'),
 (['athens', 'greece', 'oslo', 'norway'], u'albania', 'FALSE'),
 (['athens', 'greece', 'ottawa', 'canada'], u'moldova', 'FALSE'),
 (['athens', 'greece', 'paris', 'france'], u'france', 'TRUE'),
 (['athens', 'greece', 'rome', 'italy'], u'italy', 'TRUE'),
 (['athens', 'greece', 'stockholm', 'sweden'], u'norway', 'FALSE')]
</code></pre>
",4,5,2994,2016-12-08 18:56:31,https://stackoverflow.com/questions/41046843/how-to-load-a-word2vec-model-and-call-its-function-into-the-mapper
Word2Vec - adding constraint to vector representation,"<p>I am trying to adapt the pre-trained Google News word2vec model to my specific domain. For the domain I am looking at, certain words are known to be similar to each other so in an ideal world, the Word2Vec representation of those words should represent that. I understand that I can train the pre-trained model on a corpus of domain-specific data to update the vectors. </p>

<p>However, if I know for certain that certain words are highly similar and should be together, is there a way for me to incorporate that constraint into the word2vec model? Mathematically, I would like to add a term to the loss function of word2vec that provides a penalty if two that I know to be similar are not positioned close to each other in the vector space. Does anyone have advice on how to implement this? Will this require me to unpack the word2vec model or is there a way for me to potentially add that additional term to the loss function?</p>
","nlp, stanford-nlp, word2vec","<p>One approach is to take pre-trained Google News word2vec and use this ""retrofitting"" tool:</p>

<p>Faruqui, Manaal, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. ""Retrofitting word vectors to semantic lexicons."" arXiv preprint arXiv:1411.4166 (2014). <a href=""https://arxiv.org/abs/1411.4166"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1411.4166</a></p>

<blockquote>
  <p>This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed.</p>
</blockquote>

<p>The code is available at <a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">https://github.com/mfaruqui/retrofitting</a> and is straightforward to use (I've personally used it for <a href=""https://arxiv.org/abs/1607.02802"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1607.02802</a>).</p>
",3,5,615,2016-12-11 11:41:51,https://stackoverflow.com/questions/41085755/word2vec-adding-constraint-to-vector-representation
Gensim Word2Vec model: Cut dimensions,"<p>I have a trained word2vec models in geinsim with 300 dimensions and would like to cut the dimensions to 100 (simply drop the last 200 dimensions). What is the easiest and most efficient way using python?</p>
","python, python-3.x, gensim, word2vec","<p>You could save the output model in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.save_word2vec_format"" rel=""nofollow noreferrer"">word2vec format</a>. Make sure to save it as a text file (.txt). The word2vec format is as follows</p>

<p>First line is <code>&lt;vocabulary_size&gt; &lt;embedding_size&gt;</code>. In your case the <code>&lt;embedding_size&gt;</code> will be <code>300</code>.
Rest of the lines will be <code>&lt;word&gt;&lt;TAB&gt;&lt;300 floating point numbers space separated&gt;</code>. Now you can easily parse this file in python and discard the last 200 floating points from each of the lines. Make sure to update the <code>&lt;embedding_size&gt;</code> in your first line. Save this as a new file (optional). Now you can load this new file as a fresh word2vec model using <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.load_word2vec_format"" rel=""nofollow noreferrer"">load_word2vec_format()</a>. </p>
",5,2,1576,2016-12-13 20:23:03,https://stackoverflow.com/questions/41129933/gensim-word2vec-model-cut-dimensions
Get weight matrices from gensim word2Vec,"<p>I am using gensim word2vec package in python.
I would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>
<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>
<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance <code>syn0</code> is not described as an attribute)</p>
","python, machine-learning, nlp, word2vec, gensim","<p>The <code>model.wv.syn0</code> contains the <em>input</em> embedding matrix. <em>Output</em> embedding is stored in <code>model.syn1</code> when it's trained with <a href=""http://building-babylon.net/2017/08/01/hierarchical-softmax/"" rel=""noreferrer"">hierarchical softmax</a> (<code>hs=1</code>) or in <code>model.syn1neg</code> when it uses negative sampling (<code>negative&gt;0</code>). That's it! When both hierarchical softmax and negative sampling are not enabled, <code>Word2Vec</code> uses a single weight matrix <code>model.wv.syn0</code> for training.</p>

<p>See also a related discussion <a href=""https://stackoverflow.com/q/42554289/712995"">here</a>.</p>
",14,16,9101,2016-12-15 11:19:11,https://stackoverflow.com/questions/41162876/get-weight-matrices-from-gensim-word2vec
What is the difference between gensim LabeledSentence and TaggedDocument,"<p>Please help me in understanding the difference between how <code>TaggedDocument</code> and <code>LabeledSentence</code> of <code>gensim</code> works. My ultimate goal is Text Classification using <code>Doc2Vec</code> model and any classifier. I am following this <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""noreferrer"">blog</a>!</p>

<pre><code>class MyLabeledSentences(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList


class MyTaggedDocument(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(TaggedDocument([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList

sentences = MyLabeledSentences(some_dir_name)
model_l = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5,     workers=7)
sentences_l = sentences.ToArray()
model_l.build_vocab(sentences_l )
for epoch in range(15): # 
    random.shuffle(sentences_l )
    model.train(sentences_l )
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha 

sentences = MyTaggedDocument(some_dir_name)
model_t = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)
sentences_t = sentences.ToArray()
model_l.build_vocab(sentences_t)
for epoch in range(15): # 
    random.shuffle(sentences_t)
    model.train(sentences_t)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha
</code></pre>

<p>My question is <code>model_l.docvecs['some_word']</code> is same as <code>model_t.docvecs['some_word']</code>?
Can you provide me weblink of good sources to get a grasp on how <code>TaggedDocument</code> or <code>LabeledSentence</code> works.</p>
","gensim, text-classification, word2vec, doc2vec","<p><code>LabeledSentence</code> is an older, deprecated name for the same simple object-type to encapsulate a text-example that is now called <code>TaggedDocument</code>. Any objects that have <code>words</code> and <code>tags</code> properties, each a list, will do. (<code>words</code> is always a list of strings; <code>tags</code> can be a mix of integers and strings, but in the common and most-efficient case, is just a list with a single id integer, starting at 0.)</p>

<p><code>model_l</code> and <code>model_t</code> will serve the same purposes, having trained on the same data with the same parameters, using just different names for the objects. But the vectors they'll return for individual word-tokens (<code>model['some_word']</code>) or document-tags (<code>model.docvecs['somefilename_NN']</code>) will likely be different â€“ there's randomness in Word2Vec/Doc2Vec initialization and training-sampling, and introduced by ordering-jitter from multithreaded training.  </p>
",7,8,4641,2016-12-16 10:33:16,https://stackoverflow.com/questions/41182372/what-is-the-difference-between-gensim-labeledsentence-and-taggeddocument
TensorFlow : How and where to specify save path in word2vec?,"<p>I am trying to implementing word2vec model of tensorflow and went through the tutorial of tensorflows <a href=""https://www.tensorflow.org/tutorials/word2vec/#scaling_up_with_noise-contrastive_training"" rel=""nofollow noreferrer"">Vector representation of words</a>.</p>

<p>After that I compiled the <a href=""https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py"" rel=""nofollow noreferrer"">word2vec code</a> in my system and I am getting this output.</p>

<pre><code>--train_data --eval_data and --save_path must be specified.
</code></pre>

<p>I am new to TensorFlow and I don't know where and how should I specify <code>train_data</code>, <code>eval_data</code> and <code>save_path</code>.</p>

<p>Also is there a more detailed tutorial available on TensorFlow vector representation of words to understand the <code>word2vec</code> model better. </p>
","python, machine-learning, tensorflow, word2vec","<p>When you run the script, you need to provide these as flags. Try running it like this,</p>

<pre><code>python word2vec.py --train_data &lt;path to training data&gt; --eval_data &lt;path to eval&gt; --save_path &lt;save directory&gt;
</code></pre>
",2,1,386,2016-12-22 06:24:14,https://stackoverflow.com/questions/41276935/tensorflow-how-and-where-to-specify-save-path-in-word2vec
How to use Word2Vec with two inputs in a loop?,"<p>I'm trying to create a similarity between two words using word2vec, I was successful, while doing it manually. but I have two big txt files. I want to create a loop. I tried a couple methods for looping but I was unsuccessful. so I decided to ask expert. </p>

<p>my code :</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
with open('myfile1.txt', 'r') as f:
    data1 = f.readlines()

with open('myfile2.txt', 'r') as f:
    data2 = f.readlines()

data = zip(data1, data2)

with open('myoutput.txt', 'a') as f:
    for x in data: 
        output = model.similarity(x[1], x[0])  # reading each word form each files
        out = '{} : {} : {}\n'.format(x[0].strip(), x[1].strip(),output)  
        f.write(out)
</code></pre>

<p>my input1, (text1)</p>

<pre><code>street 
spain 
ice
man
</code></pre>

<p>my input2 (text2)</p>

<pre><code>florist
paris 
cold 
kid
</code></pre>

<p>I want this output (output.txt)</p>

<pre><code>street florist 0.19991447551502498
spain paris 0.5380033328157873
ice cold 0.40968857572410483
man kid  0.42953233870042506
</code></pre>
","python, nlp, word2vec","<pre><code>import gensim

model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

 file1 = []
 file2 = []

 with open('myfile1.txt','rU') as f:
 for line in f:
 file1.append(line.rstrip())

 with open('myfile2.txt','rU') as f1:
 for line1 in f1:   
 file2.append(line1.rstrip())

 resutl=[]
 f=open('Output2.txt', ""w"") 
 for i in file1  :
 for g  in file2 :
        temp=[]
        temp.append(i)
        temp.append(g)
        w = model.similarity(i,g)
        temp.append(w)
        result=i+','+g+','+str(w)

        f.write(result)
        f.write('\n')

        f.close()
</code></pre>

<p>You had problem with the loop, the two loops should be together.  </p>
",0,2,482,2016-12-25 15:49:42,https://stackoverflow.com/questions/41322521/how-to-use-word2vec-with-two-inputs-in-a-loop
Tensorflow:Unable to visualize embeddings in word2vec_basic.py?,"<p>I am trying to implement tensorflows <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">word2vec_basic.py</a> model in my system.I went through this <a href=""https://www.tensorflow.org/tutorials/word2vec/"" rel=""nofollow noreferrer"">tutorial</a> to understand and implement it.</p>

<p>After I ran the word2vec code using python3 in my system it asked me to install sklearn, matplotlib, and scipy to visualize embeddings.</p>

<p>So I installed sklearn, matplotlib, and scipy using pip in my system and ran the code a second time but it still asks me to install the above 3 again.</p>

<p>No errors were there during installation of sklearn, matplotlib, and scipy. I am using Ubuntu 16.04 LTS.</p>

<p><a href=""https://i.sstatic.net/yVL4s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yVL4s.png"" alt=""Here is the screen shot""></a></p>

<p>UPDATE: Here is the screenshot after removing <code>try</code> and <code>except</code> - 
<a href=""https://ibin.co/377XsooeSdej.png"" rel=""nofollow noreferrer"">https://ibin.co/377XsooeSdej.png</a></p>
","python, ubuntu, installation, tensorflow, word2vec","<p>As evident from the error message, you are missing the <code>python3-tk</code> package. Try to install it using, </p>

<pre><code>sudo apt-get install python3-tk
</code></pre>
",1,0,445,2016-12-28 07:24:36,https://stackoverflow.com/questions/41357292/tensorflowunable-to-visualize-embeddings-in-word2vec-basic-py
Encoding issue in python while using w2v,"<p>I'm writing my first app in python to use word2vec model.
Here is my simple code</p>

<pre><code>import gensim, logging
import sys
import warnings
from gensim.models import Word2Vec

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def main(): 
    ####LOAD MODEL
    model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  
    model.similarity('man', 'women')

if __name__ == '__main__':
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")
        #warnings.simplefilter(""ignore"")
    main()
</code></pre>

<p>I getting this the following error:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte 
</code></pre>

<p>I tried solving it by adding these two lines, but I'm still getting the error. </p>

<pre><code>reload(sys)  # Reload does the trick!
sys.setdefaultencoding('UTF8') #UTF8 #latin-1
</code></pre>

<p>The w2v model was trained on English sentences.</p>

<p>EDIT: Here is the full stack:</p>

<pre><code>**%run ""...\getSimilarity.py""**
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
**...\getSimilarity.py in &lt;module&gt;()**
     64         warnings.simplefilter(""error"")
     65         #warnings.simplefilter(""ignore"")
---&gt; 66     main()

**...\getSimilarity.py in main()**
     30     ####LOAD MODEL
---&gt; 31     model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  # C binary format
     32     model.similarity('man', 'women')

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors)**
   1090             else:
   1091                 for line_no, line in enumerate(fin):
-&gt; 1092                     parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
   1093                     if len(parts) != vector_size + 1:
   1094                         raise ValueError(""invalid vector on line %s (is this really the text format?)"" % (line_no))

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\utils.pyc in any2unicode(text, encoding, errors)**
    215     if isinstance(text, unicode):
    216         return text
--&gt; 217     return unicode(text, encoding, errors=errors)
    218 to_unicode = any2unicode
    219 

**...\AppData\Local\Enthought\Canopy\App\appdata\canopy-1.6.2.3262.win-x86_64\lib\encodings\utf_8.pyc in decode(input, errors)**
     14 
     15 def decode(input, errors='strict'):
---&gt; 16     return codecs.utf_8_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

**UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte** 
</code></pre>

<p>Any hints how to solve the problem?
Thanks in advance.</p>
","python, gensim, word2vec","<p>I found the solution simply by reading this <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q10-loading-a-word2vec-model-fails-with-unicodedecodeerror-utf-8-codec-cant-decode-bytes-in-position-"" rel=""nofollow noreferrer"">FAQ</a> page. 
""The strings (words) stored in your model are not valid utf8. By default, gensim decodes the words using the strict encoding settings, which results in the above exception whenever an invalid utf8 sequence is encountered.""</p>
",0,1,3575,2017-01-02 16:55:15,https://stackoverflow.com/questions/41430565/encoding-issue-in-python-while-using-w2v
Python - Calculate Hierarchical clustering of word2vec vectors and plot the results as a dendrogram,"<p>I've generated a 100D word2vec model using my domain text corpus, merging common phrases, for example (good bye => good_bye). Then I've extracted 1000 vectors of desired words.</p>

<p>So I have a 1000 numpy.array like so:  </p>

<pre><code>[[-0.050378,0.855622,1.107467,0.456601,...[100 dimensions],
 [-0.040378,0.755622,1.107467,0.456601,...[100 dimensions],
 ...
 ...[1000 Vectors]
]
</code></pre>

<p>And words array like so:</p>

<pre><code>[""hello"",""hi"",""bye"",""good_bye""...1000]
</code></pre>

<p>I have ran K-Means on my data, and the results I got made sense:</p>

<pre><code>X = np.array(words_vectors)
kmeans = KMeans(n_clusters=20, random_state=0).fit(X)
for idx,l in enumerate(kmeans.labels_):
    print(l,words[idx])

--- Output ---
0 hello
0 hi
1 bye
1 good_bye
</code></pre>

<p>0 = greeting 1 = farewell</p>

<p>However, some words made me think that <strong>hierarchical clustering</strong> is more suitable for the task. I've tried using AgglomerativeClustering, Unfortunately ... for this Python nobee, things got complicated and I got lost.</p>

<p>How can I cluster my vectors, so the output would be a dendrogram, more or less, like the one found on <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""noreferrer"">this wiki</a> page?
<a href=""https://i.sstatic.net/4Whaq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/4Whaq.png"" alt=""enter image description here""></a></p>
","python, numpy, machine-learning, hierarchical-clustering, word2vec","<p>I had the same problem till now!
After finding always your post after searching it online (keyword = hierarchy clustering on word2vec).
I had to give you a perhaps valid solution.</p>

<pre><code>sentences = ['hi', 'hello', 'hi hello', 'goodbye', 'bye', 'goodbye bye']
sentences_split = [s.lower().split(' ') for s in sentences]

import gensim
model = gensim.models.Word2Vec(sentences_split, min_count=2)

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

l = linkage(model.wv.syn0, method='complete', metric='seuclidean')

# calculate full dendrogram
plt.figure(figsize=(25, 10))
plt.title('Hierarchical Clustering Dendrogram')
plt.ylabel('word')
plt.xlabel('distance')

dendrogram(
    l,
    leaf_rotation=90.,  # rotates the x axis labels
    leaf_font_size=16.,  # font size for the x axis labels
    orientation='left',
    leaf_label_func=lambda v: str(model.wv.index2word[v])
)
plt.show()
</code></pre>
",15,8,9140,2017-01-04 11:28:16,https://stackoverflow.com/questions/41462711/python-calculate-hierarchical-clustering-of-word2vec-vectors-and-plot-the-resu
word2vec toolkit distance script,"<p>I'm using the ""distance"" script to find similar words over a word2vec that I have built. It contains around 1.6M words and was trained by this command:</p>

<pre><code>./word2vec -train processed-text-2016.txt -output vec-cbow-neg.txt -debug 2 -threads 5 -size 300 -window 10 -sample 1e-3 -negative 10 -hs 0 -binary 0 -cbow 1 &gt; w2v-neg.log &amp;
</code></pre>

<p>My problem is that when I type any word, I get the following:
Enter word or sentence (EXIT to break): rt</p>

<p>Word: rt  Position in vocabulary: 658253</p>

<h2>                                              Word       Cosine distance</h2>

<pre><code>                                     -0.000451              0.494857
                                        356414              0.477918
                                             9              0.441466
                                            83              0.432876
                                            63              0.431347
                                     -0.020525              0.429472
                                       .047345              0.425791
                                            36              0.423420
                                           242              0.418320
                                         ...                   ...
</code></pre>

<p>Enter word or sentence (EXIT to break): nd</p>

<p>Word: nd  Position in vocabulary: 336527</p>

<h2>                                              Word       Cosine distance</h2>

<pre><code>                                             3              0.494377
                                           489              0.492153
                                           632              0.483827
                                      0.002335              0.462591
                                          0693              0.458801
                                        036869              0.452456
                                        036819              0.447690
                                            31              0.443887
                                         ...                   ...
</code></pre>

<p>Enter word or sentence (EXIT to break): and</p>

<p>Word: and  Position in vocabulary: 1600843</p>

<h2>                                              Word       Cosine distance</h2>

<pre><code>                                        080852              0.451752
                                            57              0.438413
                                         16577              0.437900
                                             4              0.433538
                                       .005464              0.429279
                                        003131              0.422587
                                         17380              0.420614
                                             9              0.419624
                                          5082              0.419569
                                      0.019322              0.417945
                                       .000435              0.417265
                                        115991              0.414139
                                         ...                   ...
</code></pre>

<p>Enter word or sentence (EXIT to break): happy</p>

<p>Word: happy  Position in vocabulary: -1
Out of dictionary word!
Enter word or sentence (EXIT to break): man</p>

<p>Word: man  Position in vocabulary: 470143</p>

<h2>                                              Word       Cosine distance</h2>

<pre><code>                                      0.055039              0.488181
                                          4793              0.455608
                                         90743              0.454786
                                        060493              0.453180
                                            36              0.451387
                                             6              0.450261
                                             4              0.445118
                                           830              0.442580
                                           490              0.439919
                                      0.025327              0.437766
                                      0.005571              0.436606
                                      0.001964              0.436544
                                     -0.012627              0.434358
                                         ...                   ...
</code></pre>

<p>Enter word or sentence (EXIT to break): women</p>

<p>Word: women  Position in vocabulary: -1
Out of dictionary word!
Enter word or sentence (EXIT to break): queen</p>

<p>Word: queen  Position in vocabulary: -1</p>

<p>If I grep these words from the model file (text file), I find them, so I'm not sure why this is happening or how to overcome this? Is it because of noise in data (I'm degugging this) or in params I used?</p>
","distance, word2vec","<p>The answer is simply I'm using text format of the model not the binary format...</p>
",0,0,173,2017-01-06 13:06:05,https://stackoverflow.com/questions/41506400/word2vec-toolkit-distance-script
How to get random word2vec vector for unknow word?,"<p>I train word2vec on train data, but there are some words from test data not in the train data, so how can I produce the word vector that match the data distribution or number range like original?</p>
","machine-learning, nlp, word2vec","<p>You could take the list of non-frequent words in your vocabulary and average them to get an approximate word vector for unknown word. </p>

<p>OR</p>

<p>Let's say your target unknown word is <code>w</code> and its within the context <code>c-2 c-1 w c1 c2</code> where <code>c-2</code>, <code>c-1</code>, <code>c1</code> and <code>c2</code> are the context words. You could take the average of embedding of all the context words as a good approximation for the unknown word.</p>
",1,0,766,2017-01-07 04:59:05,https://stackoverflow.com/questions/41517969/how-to-get-random-word2vec-vector-for-unknow-word
Is there any way to get the vocabulary size from doc2vec model?,"<p>I am using gensim <code>doc2vec</code>. I want know if there is any efficient way to know the vocabulary size from doc2vec. One crude way is to count the total number of words, but if the data is huge(1GB or more) then this won't be an efficient way.</p>
","gensim, word2vec, doc2vec","<p>If <code>model</code> is your trained Doc2Vec model, then the number of unique word tokens in the surviving vocabulary after applying your <code>min_count</code> is available from:</p>

<pre><code>len(model.wv.vocab)
</code></pre>

<p>The number of trained document tags is available from:</p>

<pre><code>len(model.docvecs)
</code></pre>
",14,8,7167,2017-01-12 08:07:07,https://stackoverflow.com/questions/41607976/is-there-any-way-to-get-the-vocabulary-size-from-doc2vec-model
How to pass 3d Tensor to tensorflow RNN embedding_rnn_seq2seq,"<p>I'm trying to feed sentences in which each world has word2vec representation.
How can I do it in tensorflow seq2seq models?</p>

<p>Suppose the variable </p>

<pre><code>enc_inp = [tf.placeholder(tf.int32, shape=(None,10), name=""inp%i"" % t)
       for t in range(seq_length)]
</code></pre>

<p>Which has dimensions [num_of_observations or batch_size x word_vec_representation x  sentense_lenght].</p>

<p>when I pass it to embedding_rnn_seq2seq</p>

<pre><code>decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(
    enc_inp, dec_inp, stacked_lstm, 
    seq_length, seq_length, embedding_dim)
</code></pre>

<p>error occurs</p>

<pre><code>ValueError: Linear is expecting 2D arguments: [[None, 10, 50], [None, 50]]
</code></pre>

<p>Also there is a more complex problem
How can i pas as input a vector, not a scalar to first cell of my RNN?</p>

<p>By now it looks like (when we are about any sequence)</p>

<ol>
<li>get first value of sequence (scalar)</li>
<li>compute First layer RNN First layer embedding cell output</li>
<li>compute First layer RNN Second layer embedding cell output</li>
<li>etc</li>
</ol>

<p>But this is needed:</p>

<ol>
<li>Get first value of sequence (vector) </li>
<li>compute First layer RNN First layer cell output (as ordinary computing simple perceptron when Input is a vector) </li>
<li>compute First layer RNN Second layer embedding cell output (as ordinary computing simple perceptron when Input is a vector)</li>
</ol>
","python, tensorflow, word2vec, recurrent-neural-network","<p>The main point is that: 
seq2seq make inside themself word embedding. 
<a href=""https://www.reddit.com/r/MachineLearning/comments/3t1887/use_word_embeddings_in_tensor_flows_seq_2_seq/"" rel=""nofollow noreferrer"">Here is reddit question and answer</a></p>

<p>Also, if smbd wants to use pretrained Word2Vec there are ways to do it,
see: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow"">stackoverflow 1</a> </li>
<li><a href=""https://stackoverflow.com/questions/33851950/initialising-seq2seq-embedding-with-pretrained-word2vec"">stackoverflow 2</a></li>
</ul>

<p>So this can be used no only for word embedding</p>
",1,2,1017,2017-01-12 14:27:34,https://stackoverflow.com/questions/41615654/how-to-pass-3d-tensor-to-tensorflow-rnn-embedding-rnn-seq2seq
Using word2vec to calculate sentence similarity,"<p>On a previous post I found some code that described a method for calculating the semantic similarity between 2 sentences.</p>

<p>My question is what libraries, modules, etc. (ex. from NAME import NAME) do I need in order to run this code on my computer.
<a href=""https://stackoverflow.com/posts/35092200/revisions"">Link to code</a></p>

<p>I was thinking maybe word2vec, numpy, scikit learn but I'm not sure.</p>
","python, vector, word2vec, cosine-similarity, sentence","<p>Basically what you need is:</p>

<pre><code>pretrained word vector
gensim
numpy
scipy 
</code></pre>

<p>For the semantic, you need word vector so that you can calculate the similarities between your sentences. </p>

<p>Here is a step by step tutorial: <a href=""https://bitbucket.org/yunazzang/aiwiththebest_byor"" rel=""nofollow noreferrer"">How to calculate phrase similarity between phrases</a></p>
",2,2,6355,2017-01-12 21:18:16,https://stackoverflow.com/questions/41623174/using-word2vec-to-calculate-sentence-similarity
gensim Getting Started Error: No such file or directory: &#39;text8&#39;,"<p>I am learning about word2vec and GloVe model in python so I am going through this available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""noreferrer"">here</a>. </p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>&gt;&gt;&gt;from gensim.models import word2vec
&gt;&gt;&gt;import logging
&gt;&gt;&gt;logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
&gt;&gt;&gt;sentences = word2vec.Text8Corpus('text8')
&gt;&gt;&gt;model = word2vec.Word2Vec(sentences, size=200)
</code></pre>

<p>I am getting this error : </p>

<pre><code>2017-01-13 11:15:41,471 : INFO : collecting all words and their counts
Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    model = word2vec.Word2Vec(sentences, size=200)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 469, in __init__
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 533, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 545, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1536, in __iter__
    with utils.smart_open(self.fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'text8'
</code></pre>

<p>How do I rectify this ?
Thanks in advance for your help.</p>
","python, python-3.x, error-handling, gensim, word2vec","<p>It seems you're missing the file used here. Specifically, it is trying to open <code>text8</code> and can't find it (hence the <code>FileNotFoundError</code>).</p>

<p>You could download the file itself from <a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">here</a> as is stated <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus"" rel=""nofollow noreferrer"">in the documentation for <code>Text8Corpus</code></a>:</p>

<pre><code>Docstring:      
Iterate over sentences from the ""text8"" corpus, unzipped from http://mattmahoney.net/dc/text8.zip .
</code></pre>

<p>and make it available. <em>Extract it</em> and then supply it as an argument to <code>Text8Corpus</code>:</p>

<pre><code>sentences = word2vec.Text8Corpus('/path/to/text8')
</code></pre>
",6,7,4729,2017-01-13 06:46:13,https://stackoverflow.com/questions/41628856/gensim-getting-started-error-no-such-file-or-directory-text8
How to use word2vec with keras CNN (2D) to do text classification?,"<p>There is Convolution1D example <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"" rel=""nofollow noreferrer"">https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py</a> without word2vec.</p>

<p>Currently, I am using gensim to train word2vec model.</p>

<p>I want to use word2vec and keras cnn(2D not 1D) to do document classifacation(Chinese Text). I learned the basic flow of text classification in cnn and want to do a test.</p>

<h2>For example(the steps I imagine):</h2>

<ol>
<li><p>Use a good Cinese Tokenized Text Set to train word2vec model</p>

<pre><code>model = gensim.models.Word2Vec(new_sentences, workers=10, size=200, min_count=2)
</code></pre></li>
<li><p>Tokenize my sentences dataset to words lists dataset(the longest sentence has over 8000 words, shortest is less 50)</p>

<pre><code>1     ['ä½ ä»¬', 'å¥½', 'ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½']
2     ['å—¯', 'å¯¹çš„']
...
9999  ['å¥½', 'å°±', 'è¿™æ ·']
</code></pre></li>
<li><p>Use a method to transform words lists dataset to word2vec dataset</p>

<p>transform every word in every sencence to a vec by trained model.</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200]]
...
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>Pad  word2vec dataset (with size=200 zero array)</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
....
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>go to the  CNN (using Convolution2D)</p></li>
</ol>

<hr>

<p>I search for a long time, but can't find any way to do the step 3 ( after step 3,  the parameter and layers setting in step 5 is hard to understand too). </p>
","neural-network, deep-learning, keras, gensim, word2vec","<p><strong>Transforming a single sentence to a 2D vector</strong></p>

<p>Assuming you have a list of words and a model you can do:</p>

<pre><code>import numpy as np
sentence_vec = None
for word in sentence:
    word_vec = np.expand_dims(model[word], axis=0)
    if sentence_vec is None:
        sentence_vec = word_vec
    else:
        sentence_vec = np.concatenate((sentence_vec, word_vec), axis=0)
</code></pre>

<p>As for step 5 - it would be helpful if you listed what you are having trouble with. Basically you only need to do is change both 1D operations (Convolution1D, GlobalMaxPooling1D) to their 2D counter-parts .</p>
",1,3,2109,2017-01-17 07:05:39,https://stackoverflow.com/questions/41690885/how-to-use-word2vec-with-keras-cnn-2d-to-do-text-classification
How to implement word2vec CBOW in keras with shared Embedding layer and negative sampling?,"<p>I want to create a word embedding pretraining network which adds something on top of word2vec CBOW. Therefore, I'm trying to implement word2vec CBOW first. Since I'm very new to <a href=""https://keras.io/"" rel=""nofollow noreferrer"">keras</a>, I'm unable to figure out how to implement CBOW in it.</p>

<p><strong>Initialization</strong>:</p>

<p>I have calculated the vocabulary and have the mapping of word to integers.</p>

<p><strong>Input to the (yet to be implemented) network</strong>:</p>

<p>A list of <code>2*k + 1</code> integers (representing the central word and <code>2*k</code> words in context)</p>

<p><strong>Network Specification</strong></p>

<p>A shared <code>Embedding</code> layer should take this list of integers and give their corresponding vector outputs. Further a mean of <code>2*k</code> context vector is to be taken (I believe this can be done using <code>add_node(layer, name, inputs=[2*k vectors], merge_mode='ave')</code>).</p>

<p>It will be very helpful if anyone can share a small code-snippet of this.</p>

<p><strong>P.S.</strong>: I was looking at <a href=""https://github.com/niitsuma/word2vec-keras-in-gensim"" rel=""nofollow noreferrer"">word2veckeras</a>, but couldn't follow its code because it also uses a gensim.</p>

<p><strong>UPDATE 1</strong>:</p>

<p>I want to share the embedding layer in the network. The embedding layer should be able to take context words (2*k) and the current word as well. I can do this by taking all 2*k + 1 word indices in the input and write a custom lambda function which will do the needful. But, after that I also want to add negative sampling network for which I'll have to take embedding of more words and dot product with the context vector. Can someone provide with an example where Embedding layer is a shared node in the <code>Graph()</code> network</p>
","keras, embedding, word2vec","<p><code>Graph()</code> has been deprecated from <code>keras</code></p>
<p>Any arbitrary network can be created by using <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">keras functional API</a>.
Following is the demo code which created a word2vec cbow model with negative sampling tested on randomized inputs</p>
<pre><code>from keras import backend as K
import numpy as np
from keras.utils.np_utils import accuracy
from keras.models import Sequential, Model
from keras.layers import Input, Lambda, Dense, merge
from keras.layers.embeddings import Embedding

k = 3 # context windows size
context_size = 2*k
neg = 5 # number of negative samples
# generate weight matrix for embeddings
embedding = []
for i in range(10):
    embedding.append(np.full(100, i))
embedding = np.array(embedding)
print embedding

# Creating CBOW model
word_index = Input(shape=(1,))
context = Input(shape=(context_size,))
negative_samples = Input(shape=(neg,))
shared_embedding_layer = Embedding(input_dim=10, output_dim=100, weights=[embedding])

word_embedding = shared_embedding_layer(word_index)
context_embeddings = shared_embedding_layer(context)
negative_words_embedding = shared_embedding_layer(negative_samples)
cbow = Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,))(context_embeddings)

word_context_product = merge([word_embedding, cbow], mode='dot')
negative_context_product = merge([negative_words_embedding, cbow], mode='dot', concat_axis=-1)

model = Model(input=[word_index, context, negative_samples], output=[word_context_product, negative_context_product])

model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])

input_context = np.random.randint(10, size=(1, context_size))
input_word = np.random.randint(10, size=(1,))
input_negative = np.random.randint(10, size=(1, neg))

print &quot;word, context, negative samples&quot;
print input_word.shape, input_word
print input_context.shape, input_context
print input_negative.shape, input_negative

output_dot_product, output_negative_product = model.predict([input_word, input_context, input_negative])
print &quot;word cbow dot product&quot;
print output_dot_product.shape, output_dot_product
print &quot;cbow negative dot product&quot;
print output_negative_product.shape, output_negative_product
</code></pre>
<p>Hope it helps!</p>
<h2>UPDATE 1:</h2>
<p>I've completed the code and uploaded it <a href=""https://github.com/abaheti95/Deep-Learning/tree/master/word2vec/keras"" rel=""nofollow noreferrer"">here</a></p>
",3,4,7028,2017-01-27 06:28:06,https://stackoverflow.com/questions/41888085/how-to-implement-word2vec-cbow-in-keras-with-shared-embedding-layer-and-negative
Importing and working with word2vec GoogleNews-vectors-negative300.bin.gz into R,"<p>I am big fan of word2vec algorithm. I had obtained vectors binary file made by google research team and I would like to make some analysis on that (which I had previously made on much smaller datasets than google had made).</p>

<p>I am not able to import the file GoogleNews-vectors-negative300.bin.gz into the R.</p>

<p>I had extracted that, and using rword2vec (found on github) transformed from bin to txt file.
There is a kind of searching function inside the package, but it is sooo slooow.</p>

<p>That is why I am now attempting to import the file inside R and transform it to dataframe , if possible, with structure:</p>

<pre><code>name | vec1 | ... | vec300
</code></pre>

<p>I had tried built in readBin (could not obtain names), also readLines with txt (did not finish) or readr package and read_lines (made only 12Mb big vector)</p>

<p>could you please point me in the right direction?</p>
","r, word2vec","<p>I finally found a way.</p>

<p>Using package <a href=""http://www.rpubs.com/mukul13/rword2vec"" rel=""nofollow noreferrer"">rword2vec</a>, it is possible to use either function bin_to_txt or framework provided in the package. For more information see the vignette provided.</p>

<pre><code>library(rword2vec)
dist=distance(file_name = ""GoogleNews-vectors-negative300.bin"",search_word = ""king"",num = 10)
dist
</code></pre>

<blockquote>
<pre><code>           word              dist
1         kings 0.713804960250854
2         queen 0.651095926761627
3       monarch 0.641319692134857
4  crown_prince 0.620422065258026
5        prince 0.615999639034271
6        sultan 0.586482524871826
7         ruler 0.579756796360016
8       princes 0.564655303955078
9  Prince_Paras 0.543294668197632
10       throne 0.542210519313812
</code></pre>
</blockquote>
",1,0,2168,2017-01-27 21:40:54,https://stackoverflow.com/questions/41903454/importing-and-working-with-word2vec-googlenews-vectors-negative300-bin-gz-into-r
how to get tf-id from w2v on gensim,"<p>I have a textual dataset on which I trained a <code>gensim</code> w2v model. Now I want to use those vectors to recive the tf-idf values for the words and documents in my data set. What is the right way to do it? I tried to followe the <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tutorial</a> on gensim's site.</p>

<p>I expect something like <code>models.tfidfmodel(model.wv[model.wv.index2word])</code>
 but this fail since </p>

<p><code>File ""&lt;ipython-input-229-7946418f8a82&gt;"", line 1, in &lt;module&gt;
    models.tfidfmodel(model.wv[model.wv.index2word])
TypeError: 'module' object is not callable</code></p>

<p>does what I want makes since? Is BOW the only way to do that?</p>
","python-3.x, machine-learning, nlp, gensim, word2vec","<p>The tutorial you have linked to the model is given the corpus, i.e. the text (or transformed text) as a whole.</p>

<p>What you have <strong>tried</strong> to do is give the model the dictionary that the w2v model learned.</p>

<p>If what you want is </p>

<blockquote>
  <p>to recive the tf-idf values for the words and documents in my data
  set.</p>
</blockquote>

<p>Then you should simply pass it as such:</p>

<pre><code>tfidf = models.TfidfModel(corpus)
</code></pre>

<p>If what you actually want is to run the TF-IDF model on the <em>transformed</em> corpus, then you should first use your w2v to transform the corpus and then pass the transformed corpus to the tfidfmodel.</p>

<hr>

<p>Note that as the tfidf model simply calculates the word frequency there is nothing to be gained by giving it the transformed corpus and not the original one.</p>
",3,0,1374,2017-01-31 14:31:00,https://stackoverflow.com/questions/41960099/how-to-get-tf-id-from-w2v-on-gensim
How to calculate the cosine similarity between two words (word2vec in matlab)?,"<p>I have this parameters form word2vec_matlab, and I want to calculate the cosine similarity distance</p>

<pre><code>wordvecs_norm - Normalized word vectors
word2Index    - Map of words to indeces
input         - Input word (string)
k             - Number of words to return 
</code></pre>

<p>I tried </p>

<pre><code> word1 = ('king');
 word2 = ('queen');
 cosine = dot(wordvecs(word1)/ wordvecs_norm(word1), wordvecs(word2)/ wordvecs_norm(word2));        
</code></pre>
","matlab, word2vec","<p>Wikipedia gives this as formula: <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Cosine_similarity</a>
<a href=""https://i.sstatic.net/o1AOB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/o1AOB.png"" alt=""enter image description here""></a></p>

<p>I think this translates in MATLAB to: </p>

<p><code>wv1 = wordvecs(word1)
wv2 = wordvecs(word2)
cosine = dot(wv1,wv2)/ (sqrt(dot(wv1,wv1))*sqrt(dot(wv2,wv2)))
</code>
Let me know if it works!</p>
",0,0,1165,2017-02-03 10:56:28,https://stackoverflow.com/questions/42022615/how-to-calculate-the-cosine-similarity-between-two-words-word2vec-in-matlab
Does word2vec supports multiple languages?,"<p>I want to know if we can use word2vec algorithm to train models for languages other than English like Spanish, Chinese,Italian ?</p>
",word2vec,"<p>Yes! In fact one of Google's original word2vec papers highlighted its potential for use in machine-translation between language pairs:</p>

<p><em><a href=""https://arxiv.org/abs/1309.4168"" rel=""noreferrer"">Exploiting Similarities among Languages for Machine Translation</a></em></p>

<p>Note that as with English, you'll need to break example texts into word-tokens before feeding them to the Word2Vec algorithm, which may be harder in some languages.</p>
",6,5,8726,2017-02-06 10:09:28,https://stackoverflow.com/questions/42065113/does-word2vec-supports-multiple-languages
Startegies to deal with new terms in test data set,"<p>I'm using word2vec model to build a classifier on training data set and wonder what are technics to deal with unseen terms (words) in the test data.</p>

<p>Removing new terms doesn't seem like a best approach.
My current thought is to recalculate word2vec on combined data set (training + test) and replace new terms with nearest word from training data set (or maybe some linear combination of 2-3 nearest). Sounds a bit tricky, but should be doable.</p>

<p>Have you come across similar problem? Any idea/suggestion how to deal with unseen terms?</p>
","machine-learning, nlp, word2vec","<p>First, are you sure removing unknown terms is so bad? (Have you tried it?) If there were no examples of those terms in your training data, they may not be that common or important â€“ at least not until you have more training data with many varied examples of their use. And even if they are relevant, until you have many examples up front, you can't know much about their significance. </p>

<p>As you note, you could re-train a Word2Vec model including the new examples, and thus get some vague idea of where the unknown words belong. You could conceivably then re-train any downstream classifier using all the new word-vectors, or project the new-word back into the original space. (This could use a method similar to that described in section 4 of the <em><a href=""https://arxiv.org/abs/1309.4168"" rel=""nofollow noreferrer"">Exploiting Similarities among Languages for Machine Translation</a></em> paper, except now your two 'languages' are the models before and after the new word(s) are added.) But if you're only working from a few such word-occurrences, or perhaps in a single new text, everything you'll learn about that word is already a function of the surrounding words already available to your classifier, so the gains might be quite small. (That is, it's only the heftier meaning that comes from many diverse examples elsewhere that's likely to add to understanding of a new text, beyond its existing context.)</p>

<p>Some variants of word2vec, like Facebook's fastText, also learn vectors for fragments of words. Those are combined to make the full word vectors. Then when new out-of-original-vocabulary words are encountered later, they can synthesize word-vectors from the shared fragments. When the new word is morphologically related to a known word, this strategy can do OK. So you may want to take a look at FastText. (It also has a mode where classification-labels are mixed into word-vector training, which can serve to make the word-vectors better for later classification among the same labels.)</p>
",0,1,1554,2017-02-07 11:28:34,https://stackoverflow.com/questions/42088715/startegies-to-deal-with-new-terms-in-test-data-set
SpaCy: how to load Google news word2vec vectors?,"<p>I've tried several methods of loading the google news word2vec vectors (<a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">https://code.google.com/archive/p/word2vec/</a>):</p>

<pre><code>en_nlp = spacy.load('en',vector=False)
en_nlp.vocab.load_vectors_from_bin_loc('GoogleNews-vectors-negative300.bin')
</code></pre>

<p>The above gives:</p>

<pre><code>MemoryError: Error assigning 18446744072820359357 bytes
</code></pre>

<p>I've also tried with the .gz packed vectors; or by loading and saving them with gensim to a new format:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
model.save_word2vec_format('googlenews2.txt')
</code></pre>

<p>This file then contains the words and their word vectors on each line.
I tried to load them with:</p>

<pre><code>en_nlp.vocab.load_vectors('googlenews2.txt')
</code></pre>

<p>but it returns ""0"".</p>

<p>What is the correct way to do this?</p>

<p><strong>Update:</strong></p>

<p>I can load my own created file into spacy.
I use a test.txt file with ""string 0.0 0.0 ...."" on each line. Then zip this txt with .bzip2 to test.txt.bz2.
Then I create a spacy compatible binary file:</p>

<pre><code>spacy.vocab.write_binary_vectors('test.txt.bz2', 'test.bin')
</code></pre>

<p>That I can load into spacy:</p>

<pre><code>nlp.vocab.load_vectors_from_bin_loc('test.bin')
</code></pre>

<p>This works!
However, when I do the same process for the googlenews2.txt, I get the following error:</p>

<pre><code>lib/python3.6/site-packages/spacy/cfile.pyx in spacy.cfile.CFile.read_into (spacy/cfile.cpp:1279)()

OSError: 
</code></pre>
","python, nlp, word2vec, spacy","<p>For spacy 1.x, load Google news vectors into gensim and convert to a new format (each line in .txt contains a single vector: string, vec):</p>

<pre><code>from gensim.models.word2vec import Word2Vec
from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
model.wv.save_word2vec_format('googlenews.txt')
</code></pre>

<p>Remove the first line of the .txt:</p>

<pre><code>tail -n +2 googlenews.txt &gt; googlenews.new &amp;&amp; mv -f googlenews.new googlenews.txt
</code></pre>

<p>Compress the txt as .bz2:</p>

<pre><code>bzip2 googlenews.txt
</code></pre>

<p>Create a SpaCy compatible binary file:</p>

<pre><code>spacy.vocab.write_binary_vectors('googlenews.txt.bz2','googlenews.bin')
</code></pre>

<p>Move the googlenews.bin to /lib/python/site-packages/spacy/data/en_google-1.0.0/vocab/googlenews.bin of your python environment.</p>

<p>Then load the wordvectors:</p>

<pre><code>import spacy
nlp = spacy.load('en',vectors='en_google')
</code></pre>

<p>or load them after later:</p>

<pre><code>nlp.vocab.load_vectors_from_bin_loc('googlenews.bin')
</code></pre>
",26,23,20936,2017-02-07 15:50:16,https://stackoverflow.com/questions/42094180/spacy-how-to-load-google-news-word2vec-vectors
tensorflow Word2Vec error,"<p>I downloaded source code of word2vec in github below.
<code>https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py</code>
I am using tensorflow on pycharm.
I'm using windows 10.
I installed tensorflow, python, numpy which are needed to use tensorflow on windows. 
In word2vec.py source code, I set the <code>savepath</code>, <code>trainpath</code>, and <code>evalpath</code>.
I downloaded the training text file from <code>http://mattmahoney.net/dc/text8.zip</code>
which the source code recommended.
But when I ran the code I get the error below:</p>

<blockquote>
  <p>C:\Users\Sungjin\AppData\Local\Programs\Python\Python35\python.exe
  C:/Users/Sungjin/PycharmProjects/untitled/ImpW2V.py Traceback (most
  recent call last):   File
  ""C:/Users/Sungjin/PycharmProjects/untitled/ImpW2V.py"", line 43, in
  
      word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(<strong>file</strong>)),
  'word2vec_ops.so'))   File
  ""C:\Users\Sungjin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\load_library.py"",
  line 64, in load_op_library
      None, None, error_msg, error_code) tensorflow.python.framework.errors_impl.NotFoundError:
  C:\Users\Sungjin\PycharmProjects\untitled\word2vec_ops.so not found</p>
  
  <p>Process finished with exit code 1</p>
</blockquote>

<p>I checked Readme.md file on github but there is nothing mentioned about the file.
It seems I have to make <code>word2Vec_ops</code>.so file but I don't know how.</p>
","python, tensorflow, deep-learning, word2vec","<p>You're using windows and .so files are (almost) the equivalent of DLL files but under unix systems. So, even if you could find the .so file, that wouldn't work under windows.</p>

<p>If you're willing to install linux and do everything from there, in the same place where you found word2vec.py you can also find how to build the .so file.
( <a href=""https://github.com/tensorflow/models/tree/master/tutorials/embedding"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/tutorials/embedding</a> , it says <code>You will need to compile the ops as follows</code> ). If you don't know how linux and g++ / gcc work, you should probably switch to another project that is windows specific.</p>
",1,0,1813,2017-02-08 06:20:47,https://stackoverflow.com/questions/42106032/tensorflow-word2vec-error
load pre-trained word2vec model for doc2vec,"<p>I'm using gensim to extract feature vector from a document.
I've downloaded the pre-trained model from Google named <code>GoogleNews-vectors-negative300.bin</code> and I loaded that model using the following command:</p>

<pre><code>model = models.Doc2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>My purpose is to get a feature vector from a document. For a word, it's very easy to get the corresponding vector:</p>

<pre><code>vector = model[word]
</code></pre>

<p>However, I don't know how to do it for a document. Could you please help?</p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>A set of word vectors (such as <code>GoogleNews-vectors-negative300.bin</code>) is neither necessary nor sufficient for the kind of text vectors (Le/Mikolov 'Paragraph Vectors') created by the Doc2Vec class. It instead expects to be trained with example texts to learn per-document vectors. Then, also, the trained model can be used to 'infer' vectors for other new documents.</p>

<p>(The Doc2Vec class only supports the <code>load_word2vec_format()</code> method because it inherits from the Word2Vec class â€“ not because it needs that functionality.)</p>

<p>There's another simple kind of text vector that can be created by simply averaging all the words in the document, perhaps also according to some per-word significance weighting. But that's not what Doc2Vec provides.</p>
",1,3,1870,2017-02-08 16:58:43,https://stackoverflow.com/questions/42119237/load-pre-trained-word2vec-model-for-doc2vec
How to find similar words with FastText?,"<p>I am playing around with <code>FastText</code>, <a href=""https://pypi.python.org/pypi/fasttext"" rel=""noreferrer"">https://pypi.python.org/pypi/fasttext</a>,which is quite similar to <code>Word2Vec</code>. Since it seems to be a pretty new library with not to many built in functions yet, I was wondering how to extract morphological similar words. </p>

<p>For eg: <code>model.similar_word(""dog"")</code> -> dogs. But there is no function built-in.</p>

<p>If I type 
<code>model[""dog""]</code> </p>

<p>I only get the vector, that might be used to compare cosine similarity.
<code>model.cosine_similarity(model[""dog""], model[""dogs""]])</code>. </p>

<p>Do I have to make some sort of loop and do <code>cosine_similarity</code> on all possible pairs in a text? That would take time ...!!!</p>
","python, nlp, word2vec, fasttext","<p>Use Gensim, load fastText trained .vec file with load.word2vec models and use most_similiar() method to find similar words!</p>
",17,15,27577,2017-02-13 14:33:31,https://stackoverflow.com/questions/42206557/how-to-find-similar-words-with-fasttext
AttributeError: type object &#39;Word2Vec&#39; has no attribute &#39;load_word2vec_format&#39;,"<p>I am trying to implement word2vec model and getting Attribute error </p>

<blockquote>
  <p>AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'</p>
</blockquote>

<p>Below is the code :</p>

<pre><code>wv = Word2Vec.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
wv.init_sims(replace=True)
</code></pre>

<p>Please let me know the issue ?</p>
","python, nlp, gensim, word2vec","gojomo's answer is right

<p><code>gensim.models.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)</code></p>

<p>try to upgrade all dependencies of gensim(e.g. smart_open), if you still have errors as follows</p>

<p><code>pip install --upgrade gensim</code></p>

<p><i>File ""/home/liangn/PythonProjects/DeepRecommendation/Algorithm/Word2Vec.py"", line 18, in <strong>init</strong>
    self.model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 191, in load_word2vec_format with utils.smart_open(fname) as fin:</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 138, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 642, in file_smart_open
    return compression_wrapper(open(fname, mode), fname, mode)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 630, in compression_wrapper
    return make_closing(GzipFile)(file_obj, mode)</p>

<p>File ""/usr/lib64/python2.7/gzip.py"", line 94, in <strong>init</strong>
    fileobj = self.myfileobj = <strong>builtin</strong>.open(filename, mode or 'rb')</p>

<p><strong>TypeError: coercing to Unicode: need string or buffer, file found</strong></i></p>
",4,7,16424,2017-02-21 09:49:33,https://stackoverflow.com/questions/42363897/attributeerror-type-object-word2vec-has-no-attribute-load-word2vec-format
Interpreting negative Word2Vec similarity from gensim,"<p>E.g. we train a word2vec model using <code>gensim</code>:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models.word2vec import Word2Vec

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
w2v_model = Word2Vec(texts, size=500, window=5, min_count=1)
</code></pre>

<p>And when we query the similarity between words, we find negative similarity scores:</p>

<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')
0.046929569156789336
&gt;&gt;&gt; w2v_model.similarity('graph', 'system')
0.063683518562347399
&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')
-0.040026775040430063
&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')
-0.0072684112978664561
</code></pre>

<p><strong>How do we interpret the negative scores?</strong> </p>

<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>

<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>

<p>Looking at the Python wrapper code, there isn't much too: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>

<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>
","python, nlp, similarity, gensim, word2vec","<p>Cosine similarity ranges from -1 to 1, same as a regular cosine wave.</p>

<p><a href=""https://i.sstatic.net/5hD3g.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5hD3g.png"" alt=""Cosine Wave""></a></p>

<p>As for the source:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ba1ce894a5192fc493a865c535202695bb3c0424/gensim/models/word2vec.py#L1511"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ba1ce894a5192fc493a865c535202695bb3c0424/gensim/models/word2vec.py#L1511</a></p>

<pre><code>def similarity(self, w1, w2):
    """"""
    Compute cosine similarity between two words.
    Example::
      &gt;&gt;&gt; trained_model.similarity('woman', 'man')
      0.73723527
      &gt;&gt;&gt; trained_model.similarity('woman', 'woman')
      1.0
    """"""
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2])
</code></pre>
",14,21,11700,2017-02-22 03:00:46,https://stackoverflow.com/questions/42381902/interpreting-negative-word2vec-similarity-from-gensim
What is the `null_word` parameter in gensim Word2Vec?,"<p>The <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">Word2Vec</a> object in <code>gensim</code> has a <code>null_word</code> parameter that isn't explained in the docs. </p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)</p>
</blockquote>

<p><strong>What is the <code>null_word</code> parameter used for?</strong></p>

<p>Checking the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680</a>, it states:</p>

<pre><code>    if self.null_word:
        # create null pseudo-word for padding when using concatenative L1 (run-of-words)
        # this word is only ever input â€“ never predicted â€“ so count, huffman-point, etc doesn't matter
        word, v = '\0', Vocab(count=1, sample_int=0)
        v.index = len(self.wv.vocab)
        self.wv.index2word.append(word)
        self.wv.vocab[word] = v
</code></pre>

<p><strong>What is ""concatenative L1""?</strong></p>
","python, null, deep-learning, gensim, word2vec","<p>The <code>null_word</code> is only used if using the PV-DM with concatenation mode â€“ parameters <code>dm=1, dm_concat=1</code> in model initialization. </p>

<p>In this non-default mode, the doctag-vector and the vectors of the neighboring words within <code>window</code> positions of a target word are <em>concatenated</em> into a very-wide input layer, rather than the more typical averaging. </p>

<p>Such models are much larger and slower than other modes. In the case of target words near the beginning or end of a text example, there might not be enough neighboring words to create this input layer â€“ but the model requires values for those slots. So the <code>null_word</code> is essentially used as padding. </p>

<p>While the original <code>Paragraph Vectors</code> paper mentioned using this mode in some of their experiments, this mode is not sufficient to reproduce their results. (No one that I know of has been able to reproduce those results, and other comments from one of the authors imply that the original paper has some error or omission in its process.)</p>

<p>Additionally, I haven't found cases where this mode offers a clear benefit to justify the added time/memory. (It might require very-large datasets or very-long training times to show any benefit.)</p>

<p>So you shouldn't be too concerned about this model property unless you're doing advanced experiments with this less-common mode â€“ in which case you can review the source for all the fine details about how it's used as padding.</p>
",1,0,1836,2017-02-22 03:32:50,https://stackoverflow.com/questions/42382207/what-is-the-null-word-parameter-in-gensim-word2vec
Save gensim Word2vec model in binary format .bin with save_word2vec_format,"<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = gensim.models.word2vec.LineSentence('dati.txt')
model = gensim.models.Word2Vec(
sentences, size=300, window=5, min_count=5, workers=5,
sg=1, hs=1, negative=0
)
model.save_word2vec_format('model.bin', binary=True)
</code></pre>

<p>The last method, save_word2vec_format, gives me this error:</p>

<p><code>
AttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'
</code></p>

<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=""https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56"" rel=""noreferrer"">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>

<p>Thank you in advance!</p>
","python, attributes, nlp, gensim, word2vec","<p>Are you using a pre-release release candidate version of gensim, or code directly from the <code>develop</code> branch?</p>

<p>In those versions <code>save_word2vec_format()</code> has moved to a utility class called <code>KeyedVectors</code>. </p>

<p>You won't yet (as of February 2017) get these versions from the usual way of installing gensim, <code>pip install gensim</code> â€“ and it's likely that by the time this change is in the official distribution, the error message for trying the older call will be improved.</p>

<p>I recommend using the version that comes via plain <code>pip install gensim</code> unless you are a relatively expert user who is also carefully following the project <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md"" rel=""noreferrer"">CHANGELOG.md</a>.</p>
",6,5,18734,2017-02-22 18:33:14,https://stackoverflow.com/questions/42399565/save-gensim-word2vec-model-in-binary-format-bin-with-save-word2vec-format
Python File iterator running multiple times,"<p>I am using gensim to create a word2vec model of a sample file I have in a directory. I followed a tutorial online, which reads files in a directory and processes it line by line. My sample file has 9 lines in it. But this code gives my the same lines 9 times. Can someone please explain what's happening.</p>

<pre><code> class MySentences(object):
     def __init__(self, dirname):
         self.dirname = dirname   

     def __iter__(self): 
         for fname in os.listdir(self.dirname):
             for line in open(os.path.join(self.dirname, fname)):
                 print os.path.join(self.dirname, fname)
                 yield line.split() 

 sentences = MySentences('/fakepath/Folder')
</code></pre>

<p>Details:
Suppose filename contains 3 lines like</p>

<pre><code>hi how are you.
I am fine.
I am good.
</code></pre>

<p><code>line.split()</code> should give me: <code>['hi','how','are','you']</code> only once. But this happens 3 times so I get the above list thrice instead of once. If the total sentences are 5, then it returns the line 5 times.</p>
","python, python-2.7, iterator, word2vec, listiterator","<p>First you should figure out what you are trying to do. The <code>class MySentences</code> takes a directory as parameter and create a object <code>sentences</code> with a generator in it. So the <code>sentences</code> has a generator contains all lines in all the files in the directory. </p>

<p>For example:</p>

<pre><code>for line in sentences:
    print(line)
</code></pre>

<p>you will get a lot of lists with words as a element(I have removed the print statement that prints path) . Which is:</p>

<pre><code>['hi', 'how', 'are', 'you.']
</code></pre>

<p><code>['I', 'am', 'fine.']</code></p>

<p><code>['I', 'am', 'good.']</code></p>
",0,-1,124,2017-02-23 06:36:51,https://stackoverflow.com/questions/42408666/python-file-iterator-running-multiple-times
Reduce Google&#39;s Word2Vec model with Gensim,"<p>Loading the complete pre-trained word2vec model by <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""noreferrer"">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>

<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>

<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>
","nlp, gensim, word2vec","<p>The GoogleNews word-vectors file format doesn't include frequency info. But, it does seem to be sorted in roughly more-frequent to less-frequent order. </p>

<p>And, <code>load_word2vec_format()</code> offers an optional <code>limit</code> parameter that only reads that many vectors from the given file. </p>

<p>So, the following should do roughly what you've requested:</p>

<pre><code>goognews_wordecs = KeyedVectors.load_word2vec_format(`GoogleNews-vectors-negative300.bin.gz`, binary=True, limit=200000)
</code></pre>
",8,9,4123,2017-02-25 17:38:06,https://stackoverflow.com/questions/42459373/reduce-googles-word2vec-model-with-gensim
Quantification of non-determinism in CS experiments,"<p>Heyall,</p>

<p>I'm working on my MSc thesis in computer science. More specifically, I am doing research on the effects of tuning the hyperparameters of distributional semantic models when used as features in statistical dependency parsers. I am using word2vec, a non-deterministic neural net-based word embedding software. In order to be able to validate my results, I have to quantify the degree of non-determinism in my models.</p>

<p>I do however think that this question can be asked on a more abstract level -- what test can I use to quantify the degree of non-determinism in a statistical model? Say for instance that I get the following results when performing the same experiment five times:</p>

<pre><code>89.5, 89.7, 89.4, 89.5, 89.9
</code></pre>

<p>How can I determine ""how non-deterministic"" my system is and use this as a basis for accepting or discarding the outcome of future experiments?</p>
","statistics, word2vec, non-deterministic, dependency-parsing","<p>If by test you mean a significance or hypothesis test, those tests are useless and you can ignore them.</p>

<p>The appropriate way to quantify uncertainty in language parsing or anything else is to express uncertainty as probability. In the context of language parsing, that means constructing a probability distribution over possible ways to parse a given sentence. </p>

<p>If you need to make decisions, you need to supply additional data which express preferences over outcomes (i.e. utility functions). Probability and utility are combined via the so-called expected utility hypothesis: the best action is the one which maximizes expected utility. </p>

<p>A useful introduction to these concepts, using example from many fields, is ""Making Hard Decisions"" by Robert Clemen. More specific to your problem, a web search for probabilistic language parsing turns up many hits.</p>

<p>You might get more interest in this question on stats.stackexchange.com. There might already answers to related questions there.</p>
",2,0,44,2017-02-26 15:41:29,https://stackoverflow.com/questions/42470555/quantification-of-non-determinism-in-cs-experiments
Questions about word embedding(word2vec),"<p>I am trying to understand word2vec(word embedding) architecture, and I have few questions about it:</p>

<ul>
<li>first, why is word2vec model considered a log-linear model? Is it because it uses a soft max at output layer?</li>
<li>second, why does word2vec remove hidden layer? Is it just because of computational complexity?</li>
<li>third, why does word2vec not use activation function? (as compared to NNLM(Neural Network Language Model).</li>
</ul>
","neural-network, word2vec, word-embedding","<blockquote>
  <p>first, why word2vec model is log-linear model? because it uses a soft max at output layer?</p>
</blockquote>

<p>Exactly, softmax is a log-linear classification model. The intent is to obtain values at the output that can be considered a posterior probability distribution</p>

<blockquote>
  <p>second, why word2vec removes hidden layer? it just because of
  computational complexity?
  third, why word2ved don't use activation function? compare for
  NNLM(Neural Network Language Model).</p>
</blockquote>

<p>I think your second and third question are linked in the sense that an extra hidden layer and an activation function would make the model more complex than necessary. Note that while no activation is explicitly formulated, we could consider it to be a linear classification function. It appears that the dependencies that the word2vec models try to model can be achieved with a linear relation between the input words.</p>

<p>Adding a non-linear activation function allows the neural network to map more complex functions, which could in turn lead to fit the input onto something more complex that doesn't retain the dependencies word2vec seeks.</p>

<p>Also note that linear outputs don't saturate which facilitates gradient-based learning. </p>
",4,1,1036,2017-02-28 05:42:33,https://stackoverflow.com/questions/42501035/questions-about-word-embeddingword2vec
Product merge layers with Keras functionnal API for Word2Vec model,"<p>I am trying to implement a Word2Vec CBOW with negative sampling with Keras, following the code found <a href=""https://github.com/abaheti95/Deep-Learning/blob/master/word2vec/keras/cbow_model.py"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>EMBEDDING_DIM = 100

sentences = SentencesIterator('test_file.txt')
v_gen = VocabGenerator(sentences=sentences, min_count=5, window_size=3,
                       sample_threshold=-1, negative=5)

v_gen.scan_vocab()
v_gen.filter_vocabulary()
reverse_vocab = v_gen.generate_inverse_vocabulary_lookup('test_lookup')

# Generate embedding matrix with all values between -1/2d, 1/2d
embedding = np.random.uniform(-1.0 / (2 * EMBEDDING_DIM),
                              1.0 / (2 * EMBEDDING_DIM),
                              (v_gen.vocab_size + 3, EMBEDDING_DIM))

# Creating CBOW model
# Model has 3 inputs
# Current word index, context words indexes and negative sampled word indexes
word_index = Input(shape=(1,))
context = Input(shape=(2*v_gen.window_size,))
negative_samples = Input(shape=(v_gen.negative,))

# All inputs are processed through a common embedding layer
shared_embedding_layer = (Embedding(input_dim=(v_gen.vocab_size + 3),
                                    output_dim=EMBEDDING_DIM,
                                    weights=[embedding]))

word_embedding = shared_embedding_layer(word_index)
context_embeddings = shared_embedding_layer(context)
negative_words_embedding = shared_embedding_layer(negative_samples)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
              output_shape=(EMBEDDING_DIM,))(context_embeddings)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
negative_context_product = merge([negative_words_embedding, cbow],
                                 mode='dot',
                                 concat_axis=-1)

# The dot products are outputted
model = Model(input=[word_index, context, negative_samples],
              output=[word_context_product, negative_context_product])

# Binary crossentropy is applied on the output
model.compile(optimizer='rmsprop', loss='binary_crossentropy')
print(model.summary())

model.fit_generator(v_gen.pretraining_batch_generator(reverse_vocab),
                    samples_per_epoch=10,
                    nb_epoch=1)
</code></pre>

<p>However, I get an  error during the merge part because Embedding layer is a 3D tensor while cbow is only 2 dimensions. I assume I need to reshape the embedding (which is [?, 1, 100]) to [1, 100] but I can't find how to reshape with the functional API.
I am using the Tensorflow backend.</p>

<p>Also, if someone can point to an other implementation of CBOW with Keras (Gensim free), I would love to have a look to it!</p>

<p>Thank you!</p>

<p>EDIT: Here is the error</p>

<pre><code>Traceback (most recent call last):
  File ""cbow.py"", line 48, in &lt;module&gt;
    word_context_product = merge([word_embedding, cbow], mode='dot')
    .
    .
    .
ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>
","python, nlp, keras, word2vec, word-embedding","<pre><code>ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>

<p>Indeed you need to reshape the <code>word_embedding</code> tensor. Two ways to do it :</p>

<ul>
<li><p>Either you use the <code>Reshape()</code> layer, imported from <code>keras.layers.core</code>, this is done like :</p>

<pre><code>word_embedding = Reshape((100,))(word_embedding)
</code></pre>

<p>the argument of <code>Reshape</code> is a tuple with the target shape.</p></li>
<li><p>Or you can use <code>Flatten()</code> layer, also imported from <code>keras.layers.core</code>, used like this :</p>

<pre><code>word_embedding = Flatten()(word_embedding)
</code></pre>

<p>taking nothing as an argument, it will just remove ""empty"" dimensions.</p></li>
</ul>

<p>Does this help you? </p>

<p><strong>EDIT :</strong></p>

<p>Indeed the second <code>merge()</code> is a bit more tricky. The <code>dot</code> merge in Keras only accepts tensors of the same rank, so same <code>len(shape)</code>.
So what you will do is use a <code>Reshape()</code> layer to add back that 1 empty dimension, then use the feature <code>dot_axes</code> instead of <code>concat_axis</code> which is not relevant for a <code>dot</code> merge.
This is what I propose you for the solution :</p>

<pre><code>word_embedding = shared_embedding_layer(word_index)
# Shape output = (None,1,emb_size)
context_embeddings = shared_embedding_layer(context)
# Shape output = (None, 2*window_size, emb_size)
negative_words_embedding = shared_embedding_layer(negative_samples)
# Shape output = (None, negative, emb_size)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
                     output_shape=(EMBEDDING_DIM,))(context_embeddings)
# Shape output = (None, emb_size)
cbow = Reshape((1,emb_size))(cbow)
# Shape output = (None, 1, emb_size)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
# Shape output = (None, 1, 1)
word_context_product = Flatten()(word_context_product)
# Shape output = (None,1)
negative_context_product = merge([negative_words_embedding, cbow], mode='dot',dot_axes=[2,2])
# Shape output = (None, negative, 1)
negative_context_product = Flatten()(negative_context_product)
# Shape output = (None, negative)
</code></pre>

<p>Is it working? :)</p>

<p>The problem comes from the rigidity of TF regarding the matrix multiplication. Merge with ""dot"" mode calls the backend <code>batch_dot()</code> function and, as opposed to Theano, TensorFlow requires the matrix to have the same rank : <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">read here</a>. </p>
",2,2,641,2017-02-28 17:18:24,https://stackoverflow.com/questions/42514986/product-merge-layers-with-keras-functionnal-api-for-word2vec-model
Gensim word2vec in python3 missing vocab,"<p>I'm using gensim implementation of Word2Vec. I have the following code snippet:</p>

<pre><code>print('training model')
model = Word2Vec(Sentences(start, end))
print('trained model:', model)
print('vocab:', model.vocab.keys())
</code></pre>

<p>When I run this in python2, it runs as expected. The final print is all the words in the vocabulary.</p>

<p>However, if I run it in python3, I get an error:</p>

<pre><code>trained model: Word2Vec(vocab=102, size=100, alpha=0.025)
Traceback (most recent call last):
  File ""learn.py"", line 58, in &lt;module&gt;
    train(to_datetime('-4h'), to_datetime('now'), 'model.out')
  File ""learn.py"", line 23, in train
    print('vocab:', model.vocab.keys())
AttributeError: 'Word2Vec' object has no attribute 'vocab'
</code></pre>

<p>What is going on? Is gensim word2vec not compatible with python3?</p>
","python, gensim, word2vec","<p>Are you using the same version of gensim in both places? Gensim 1.0.0 moves <code>vocab</code> to a helper object, so whereas in pre-1.0.0 versions of gensim (in Python 2 or 3), you can use:</p>

<pre><code>model.vocab
</code></pre>

<p>...in gensim 1.0.0+ you should instead use (in Python 2 or 3)...</p>

<pre><code>model.wv.vocab
</code></pre>
",40,20,20974,2017-02-28 19:43:33,https://stackoverflow.com/questions/42517435/gensim-word2vec-in-python3-missing-vocab
How can I access output embedding(output vector) in gensim word2vec?,"<p>I want to use output embedding of word2vec such as in <a href=""http://www2016.net/proceedings/companion/p83.pdf"" rel=""noreferrer"">this paper (Improving document ranking with dual word embeddings)</a>.</p>

<p>I know input vectors are in syn0, output vectors are in syn1 and syn1neg if negative sampling.</p>

<p>But when I calculated most_similar with output vector, I got same result in some ranges because of removing syn1 or syn1neg.</p>

<p>Here is what I got.</p>

<pre><code>IN[1]: model = Word2Vec.load('test_model.model')

IN[2]: model.most_similar([model.syn1neg[0]])

OUT[2]: [('of', -0.04402521997690201),
('has', -0.16387106478214264),
('in', -0.16650712490081787),
('is', -0.18117375671863556),
('by', -0.2527652978897095),
('was', -0.254993200302124),
('from', -0.2659570872783661),
('the', -0.26878535747528076),
('on', -0.27521973848342896),
('his', -0.2930959463119507)]
</code></pre>

<p>but another syn1neg numpy vector is already similar output.</p>

<pre><code>IN[3]: model.most_similar([model.syn1neg[50]])

OUT[3]: [('of', -0.07884830236434937),
('has', -0.16942456364631653),
('the', -0.1771494299173355),
('his', -0.2043554037809372),
('is', -0.23265135288238525),
('in', -0.24725285172462463),
('by', -0.27772971987724304),
('was', -0.2979024648666382),
('time', -0.3547973036766052),
('he', -0.36455872654914856)]
</code></pre>

<p>I want to get output numpy arrays(negative or not) with preserved during training.</p>

<p>Let me know how can I access pure syn1 or syn1neg, or code, or some word2vec module can get output embedding.</p>
","python, numpy, gensim, word2vec","<p>With negative-sampling, <code>syn1neg</code> weights are per-word, and in the same order as <code>syn0</code>. </p>

<p>The mere fact that your two examples give similar results doesn't necessarily indicate anything is wrong. The words are by default sorted by frequency, so the early words (including those in position 0 and 50) are very-frequent words with very-generic cooccurrence-based meanings (that may all be close to each other).</p>

<p>Pick a medium-frequency word with a more distinct meaning, and you may get more meaningful results (if your corpus/settings/needs are sufficiently like those of the 'dual word embeddings' paper). For example, you might want to compare:</p>

<pre><code>model.most_similar('cousin')
</code></pre>

<p>...with...</p>

<pre><code>model.most_similar(positive=[model.syn1neg[model.vocab['cousin'].index])
</code></pre>

<p>However, in all cases the existing <code>most_similar()</code> method only looks for similar-vectors in <code>syn0</code> â€“ the 'IN' vectors of the paper's terminology. So I believe the above code would only really be computing what the paper might call 'OUT-IN' similarity: a list of which IN vectors are most similar to a given OUT vector. They actually seem to tout the reverse, 'IN-OUT' similarity, as something useful. (That'd be the OUT vectors most similar to a given IN vector.)</p>

<p>The latest versions of gensim introduce a <code>KeyedVectors</code> class for representing a set of word-vectors, keyed by string, separate from the specific Word2Vec model or other training method. You could potentially create an extra <code>KeyedVectors</code> instance that replaces the usual <code>syn0</code> with <code>syn1neg</code>, to get lists of OUT vectors similar to a target vector (and thus calculate top-n 'IN-OUT' similarities or even 'OUT-OUT' similarities). </p>

<p>For example, this <em>might</em> work (I haven't tested it): </p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab  # same
outv.index2word = model.wv.index2word  # same
outv.syn0 = model.syn1neg  # different
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p><code>syn1</code> only exists when using hierarchical-sampling, and it's less clear what an ""output embedding"" for an individual word would be there. (There are multiple output nodes corresponding to predicting any one word, and they all need to be closer to their proper respective 0/1 values to predict a single word. So unlike with `syn1neg, there's no one place to read a vector that means a single word's output. You might have to calculate/approximate some set of hidden->output weights that would drive those multiple output nodes to the right values.)</p>
",8,7,8099,2017-03-02 11:31:07,https://stackoverflow.com/questions/42554289/how-can-i-access-output-embeddingoutput-vector-in-gensim-word2vec
tensoflow word2vec_basic input-output placeholders,"<p>I am a  newbie in tensorflow. I am trying to understand the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">word2vec_basic</a> script.</p>

<p>At the beggining it defined input and output.</p>

<pre><code>train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
</code></pre>

<p>If I understood correctly the shape of train_inputs - <code>[batch_size]</code> is an array of integers, and the length of the arrays is <code>batch_size</code>. Then the shape of <code>train_labels</code> is <code>[batch_size, 1]</code>, which is the matrix  with a single row. Correct? If yes, I don't understand why it's so, and what should be stored in those placeholders. According to the theory, the <code>label</code> is a int and the <code>input</code> is an array of the sliding window of context, so why <code>batch_size</code> shows up there?</p>

<p>It seems like I missed something fundamental in the theory. I would appreciate an explanation. </p>
","python, machine-learning, tensorflow, deep-learning, word2vec","<p>I happened to be debugging word2vec_basic.py recently and for a simple data set example ""the dog saw a cat the dog chased the cat the cat climbed a tree"", <code>dictionary</code> is <code>{'the': 0, 'cat': 1, 'dog': 2, 'a': 3, 'saw': 4, 'chased': 5, 'climbed': 6, 'tree': 7}</code>, <code>reverse_dictionary</code> is <code>{0: 'the', 1: 'cat', 2: 'dog', 3: 'a', 4: 'saw', 5: 'chased', 6: 'climbed', 7: 'tree'}</code>. </p>

<p>When batch_size=8, skip_window=1, embedding_size=1, and num_skips=2, calling <code>generate_batch</code> returns:
<code>
batch: [2 2 4 4 3 3 1 1]
labels: [[0] [4] [2] [3] [4] [1] [0] [3]]
</code></p>

<p>Translated to words, they are:
<code>
batch: [dog, dog, saw, saw, a, a, cat, cat]
labels: [[the], [saw], [dog], [a], [saw], [cat], [the], [a]]
</code></p>

<p>For the first three words ""the dog saw"" in the data set, as skip_window is 1, the expected (target, context) pairs should be (dog, the) and (dog, saw) - see ""The Skip-gram Model"" in <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec</a> for more detail if needed.</p>

<p>As for why 1 is used in <code>[batch_size, 1]</code>, the <code>nce_loss</code> documentation at <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss</a> says that ""labels: A Tensor of type int64 and shape [batch_size, num_true]. The target classes."" and ""num_true: An int. The number of target classes per training example."" (num_true has a default value of 1)</p>

<p>And the <code>nce_loss</code> in the code is defined:</p>

<pre><code>      loss = tf.reduce_mean(
      tf.nn.nce_loss(weights=nce_weights,
                     biases=nce_biases,
                     labels=train_labels,
                     inputs=embed,
                     num_sampled=num_sampled,
                     num_classes=vocabulary_size))
</code></pre>

<p>So now it should be easy to see the shapes of <code>train_inputs</code> and <code>train_labels</code> make sense.</p>
",1,1,83,2017-03-08 04:29:38,https://stackoverflow.com/questions/42663047/tensoflow-word2vec-basic-input-output-placeholders
Gensim - Memory error using GoogleNews-vector model,"<p>I  getting memory error, when I use <strong>GoogleNews-vectors-negative300.bin</strong> or try to train a model with Gensim with wikipedia dataset corpus.(1 GB). I have 4GB RAM in my system. Is there any way to bypass this.</p>

<p>Can we host it on cloud service like AWS to get better speed ?</p>
","nlp, gensim, word2vec","<p>4GB is very tight for that vector set; you should have 8GB or more to load the full set. Alternatively you could use the optional <code>limit</code> argument to <code>load_word2vec_format()</code> to just load some of the vectors. For example, <code>limit=500000</code> would load just the first 500,000 (instead of the full 3 million). As the file appears to put the more-frequently-appearing tokens first, that may be sufficient for many purposes. </p>
",6,2,1961,2017-03-08 14:11:29,https://stackoverflow.com/questions/42673590/gensim-memory-error-using-googlenews-vector-model
word2vec word to color association?,"<p>I'm trying to get color associations like so:</p>

<pre><code>apple -&gt; red

banana -&gt; yellow

grass -&gt; green

sky -&gt; blue
</code></pre>

<p>using the GoogleNews-vectors-negative300.bin vectors, I first tried</p>

<pre><code>wv.similarity('apple',color)
</code></pre>

<p>where color is a primary color, eg 'red','yellow','blue' etc.</p>

<p>with fruits 'orange' is always the highest color association, probably because it conflates the color and the fruit. When I remove orange the results are still strange:</p>

<pre><code>apple:

[('violet', 0.24978276994901127), ('green', 0.20656763297902447), ('red', 0.19834849929308024), ('yellow', 0.18963902211016806), ('cyan', 0.17945308073294569), ('blue', 0.13687176308102386)]

cherry:
[('violet', 0.27348741504236473), ('red', 0.25540695681746473), ('yellow', 0.24285150471329794), ('blue', 0.20400566489159569), ('green', 0.18741563150077917), ('cyan', 0.12736182067644364)]

banana:
[('yellow', 0.27708333668133234), ('green', 0.25977272141145935), ('red', 0.24736077659820707), ('violet', 0.23909913025940599), ('cyan', 0.16519069493338848), ('blue', 0.15660144725154587)]
</code></pre>

<p>so apparently 'violet' aligns with 'apple' and 'cherry' along some other dimension (maybe they're plants?).</p>

<p>I tried formulating it as an analogy. This worked for some objects but doesn't generalize well:</p>

<pre><code>wv.most_similar(restrict_vocab=100000, positive=['apple','yellow'], negative=['banana'])
[(u'red', 0.5296207666397095), (u'orange', 0.501822829246521), (u'bright_yellow', 0.49562686681747437), (u'purple', 0.4909234642982483), (u'blue', 0.465557336807251), (u'pink', 0.43768370151519775), (u'colored', 0.4296746551990509), (u'brown', 0.4290006756782532), (u'bright_orange', 0.4261433482170105), (u'yellows', 0.4199957549571991)]

wv.most_similar(restrict_vocab=100000, positive=['grass','yellow'], negative=['banana'])
[(u'bright_yellow', 0.4722655713558197), (u'blue', 0.45448029041290283), (u'red', 0.43442922830581665), (u'lawns', 0.4275570809841156), (u'maroon', 0.4197036325931549), (u'bright_orange', 0.41167205572128296), (u'brown', 0.4110153317451477), (u'purple', 0.4074830412864685), (u'grassy', 0.4017237722873688), (u'striped', 0.40009182691574097)]
</code></pre>

<p>I also tried the facebook fasttext embeddings but results were worse. How should I approach this problem and isolate the vector for ""common color of object""?</p>
","word2vec, word-embedding","<p>This is an interesting thing to try, but whether word-vectors would successfully encode these relationships at all seems to me an open question, and any level of success would depend on a lot of particular training choices, including corpus and parameters. </p>

<p>In particular, the <code>GoogleNews</code> vector set, trained on published news-stories, might not be as strong in associating fruits-with-colors as other choices. (For example, I'd guess the text of children's books, or transcripts of educational programs, would induce stronger vector-arrangements for this purpose.)</p>

<p>A <code>violet</code> is also a flower, so that partially explains its closeness to other fruits. And as you note the prominence of 'orange' as both a fruit and color creates issues when it's involved. There is no disambiguation between multiple word-senses in plain Word2Vec, or the <code>GoogleNews</code> vectors, although there's been some research using or creating word-vectors to distinguish alternate word-senses. (Even words like 'blue', 'green', 'yellow', 'cherry', and 'grass' have alternate meanings that may be affecting vector-positioning.)</p>

<p>I do suspect the analogy/directional approach may have more luck than pure-similarity. (That is, asking ""which colorword is in the direction learned from these other objectword->colorword examples?"", rather than ""which colorword is absolutely closest to this objectword?"")</p>

<p>You might want try objectword->colorword example pairs from a larger domain, or try additional vector-math to see if other definitions/composites better match the answers you expect. </p>

<p>For example. maybe your ""learn-the-direction"" examples should include non-fruits â€“ sky->blue, coal->black, etc. </p>

<p>And I recall seeing once the suggestion that analogy-solving could be improved if many-known-good analogies of the same-relationship were used together, rather than just one. (That is, compose a direction from all of ""England:London"", ""Russia:Moscow"", ""France:Paris"" before probing ""Germany:<em>?</em>"", rather than just one. I'm not sure if adding more vectors to the gensim <code>most_similar()</code> <code>positive</code>/<code>negative</code> lists has the same effect or you need to do the differencing/averaging/norming yourself.)</p>

<p>An interesting paper on interpretation and improvement of analogy results is Levy &amp; Goldberg's ""<a href=""http://www.aclweb.org/anthology/W14-1618"" rel=""nofollow noreferrer"">Linguistic Regularities in Sparse and Explicit Word Representations</a>"".</p>

<p>There's other work that tries to train or skew word (or concept/entity) vectors to be better at question-answering, which might be relevant, but other than suggesting that as a search term, I don't know any technique that's especially appropriate or ready-to-use in available libraries. </p>
",2,2,1064,2017-03-10 16:41:23,https://stackoverflow.com/questions/42723465/word2vec-word-to-color-association
How do I get a single vector for a single word using Word2Vec?,"<p>I'm trying to solve a Deep Learning text classification problem, so I have to vectorize the text input with Word2Vec to feed it into a neural network.</p>

<p>So I downloaded a Google pre trained Word2Vec model: <a href=""https://github.com/3Top/word2vec-api"" rel=""nofollow noreferrer"">https://github.com/3Top/word2vec-api</a></p>

<p>And load it using gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('Word2Vec.bin', binary=True)
</code></pre>

<p>When I try to print a specific word:</p>

<pre><code>print(model['cat'])
# =&gt; expected output: 0.47385435 (or something)
# =&gt; actual output: array with hundreds of floats between -1 and 1
</code></pre>

<p>Why don't I just get one vector for one word? Isn't that the point?</p>

<p>Bonus Question: Can I load the 3M word vectors in the Google pre trained Word2Vec model into a MongoDB database? (Columns: id - word(string) - vector(float)). Because loading the model from a .bin or .txt file takes over a minute.</p>
","python, deep-learning, word2vec","<pre><code>When I try to print a specific word:

print(model['cat'])
# =&gt; expected output: 0.47385435 (or something)
# =&gt; actual output: array with hundreds of floats between -1 and 1
Why don't I just get one vector for one word? Isn't that the point?
</code></pre>

<p>""array with hundreds of floats between -1 and 1"" IS a word vector.</p>

<p>Why do you expect a scala (0.47385435) when you want to call a vector?</p>

<p>You need to read this: <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec</a></p>
",1,4,2538,2017-03-18 20:29:11,https://stackoverflow.com/questions/42879491/how-do-i-get-a-single-vector-for-a-single-word-using-word2vec
Load pretrained word embedding into Tensorflow model,"<p>I'm trying to modify this <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""nofollow noreferrer"">Tensorflow LSTM model</a> to load this pre-trained GoogleNews word ebmedding <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">GoogleNews-vectors-negative300.bin</a> (or a tensorflow Word2Vec embedding would be just as good).</p>

<p>I've been reading examples on how to load a pre-trained word embedding into tensorflow (eg. <a href=""https://gist.github.com/j314erre/b7c97580a660ead82022625ff7a644d8#file-train-py-L133"" rel=""nofollow noreferrer"">1: here</a>, <a href=""https://github.com/Conchylicultor/DeepQA/blob/master/chatbot/chatbot.py#L385"" rel=""nofollow noreferrer"">2: here</a>, <a href=""https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow"">3: here</a> and <a href=""https://stackoverflow.com/questions/36370677/injecting-pre-trained-word2vec-vectors-into-tensorflow-seq2seq"">4: here</a>).</p>

<p>In the first linked example they can easily <a href=""https://gist.github.com/j314erre/b7c97580a660ead82022625ff7a644d8#file-train-py-L157"" rel=""nofollow noreferrer"">assign the embedding to the graph</a>: </p>

<pre><code>sess.run(cnn.W.assign(initW))
</code></pre>

<p>In the second linked example they <a href=""https://github.com/Conchylicultor/DeepQA/blob/master/chatbot/chatbot.py#L392"" rel=""nofollow noreferrer"">create an embedding-wrapper variable</a>:</p>

<pre><code>with tf.variable_scope(""embedding_rnn_seq2seq/rnn/embedding_wrapper"", reuse=True):
        em_in = tf.get_variable(""embedding"")
</code></pre>

<p>then they <a href=""https://github.com/Conchylicultor/DeepQA/blob/master/chatbot/chatbot.py#L448"" rel=""nofollow noreferrer"">initialize the embedding wrapper</a>:</p>

<pre><code>sess.run(em_in.assign(initW))    
</code></pre>

<p>Both those examples make sense, but it's not obvious to me how I can assign the unpacked embedding initW to the TF graph in my case. (I'm a TF beginner).</p>

<p>I can prepare initW like the first two examples:</p>

<pre><code>def loadEmbedding(self, word_to_id):
    # New model, we load the pre-trained word2vec data and initialize embeddings
    with open(os.path.join('GoogleNews-vectors-negative300.bin'), ""rb"", 0) as f:
        header = f.readline()
        vocab_size, vector_size = map(int, header.split())
        binary_len = np.dtype('float32').itemsize * vector_size
        initW = np.random.uniform(-0.25,0.25,(len(word_to_id), vector_size))
        for line in range(vocab_size):
            word = []
            while True:
                ch = f.read(1)
                if ch == b' ':
                    word = b''.join(word).decode('utf-8')
                    break
                if ch != b'\n':
                    word.append(ch)
            if word in word_to_id:
                initW[word_to_id[word]] = np.fromstring(f.read(binary_len), dtype='float32')
            else:
                f.read(binary_len)
    return initW
</code></pre>

<p>From the solution in <a href=""https://stackoverflow.com/questions/36370677/injecting-pre-trained-word2vec-vectors-into-tensorflow-seq2seq"">example 4</a>, I thought I should be able to do something like  </p>

<pre><code>session.run(tf.assign(embedding, initW)).
</code></pre>

<p>If I try to add the line here like this <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L364"" rel=""nofollow noreferrer"">when the session is initialized </a>:</p>

<pre><code>with sv.managed_session() as session:
        initializer = tf.random_uniform_initializer(-config.init_scale,
                                                    config.init_scale)
        session.run(tf.assign(m.embedding, initW))
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: Fetch argument &lt;tf.Tensor 'Assign:0' shape=(10000, 300) dtype=float32_ref&gt; cannot be interpreted as a Tensor. (Tensor Tensor(""Assign:0"", shape=(10000, 300), dtype=float32_ref, device=/device:CPU:0) is not an element of this graph.)
</code></pre>

<hr>

<p>Update: I updated the code following Nilesh Birari's suggestion: <a href=""https://gist.github.com/KristenMoore/3f3097201d47def6451c1d3a62f3d3cc"" rel=""nofollow noreferrer"">Full code</a>. It results in no improvement in validation or test set perplexity, it only improves training set perplexity. </p>
","tensorflow, lstm, word2vec","<p>Correct me if I am wrong, trying to answer with my limited understanding of tensorflow.</p>

<pre><code>ValueError: Fetch argument &lt;tf.Tensor 'Assign:0' shape=(10000, 300) dtype=float32_ref&gt; cannot be interpreted as a Tensor. (Tensor Tensor(""Assign:0"", shape=(10000, 300), dtype=float32_ref, device=/device:CPU:0) is not an element of this graph.)
</code></pre>

<p>This simply states you are trying to initialize element of different graph, so I guess you need to be in same scope in which your graph is define. Just adjusting your embedding initialization code in same scope can solve the problem.</p>

<pre><code>with tf.Graph().as_default():
    initializer = tf.random_uniform_initializer(-config.init_scale,
                                                config.init_scale)
    with tf.name_scope(""Train""):
        train_input = PTBInput(config=config, data=train_data, name=""TrainInput"")
        with tf.variable_scope(""Model"", reuse=None, initializer=initializer):
            m = PTBModel(is_training=True, config=config, input_=train_input)
        tf.summary.scalar(""Training Loss"", m.cost)
        tf.summary.scalar(""Learning Rate"", m.lr)

    with tf.name_scope(""Valid""):
        valid_input = PTBInput(config=config, data=valid_data, name=""ValidInput"")
        with tf.variable_scope(""Model"", reuse=True, initializer=initializer):
            mvalid = PTBModel(is_training=False, config=config, input_=valid_input)
        tf.summary.scalar(""Validation Loss"", mvalid.cost)

    with tf.name_scope(""Test""):
        test_input = PTBInput(config=eval_config, data=test_data, name=""TestInput"")
        with tf.variable_scope(""Model"", reuse=True, initializer=initializer):
            mtest = PTBModel(is_training=False, config=eval_config,
                             input_=test_input)

    sv = tf.train.Supervisor(logdir=FLAGS.save_path)
    with sv.managed_session() as session:
        word2vec = loadEmbedding(word_to_id)
        session.run(tf.assign(m.embedding, word2vec))
        print(""WORKED!!!"")
</code></pre>

<p>I guess this should be only problem, as you can see in your <a href=""https://gist.github.com/j314erre/b7c97580a660ead82022625ff7a644d8#file-train-py-L133"" rel=""nofollow noreferrer"">first</a> example, initialization are under same scope.</p>
",1,1,2257,2017-03-22 07:01:36,https://stackoverflow.com/questions/42944787/load-pretrained-word-embedding-into-tensorflow-model
How to train Word2Vec model on Wikipedia page using gensim?,"<p>After reading <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">this article</a>, I start to train my own model. The problem is that the author does not make it clear what the <code>sentences</code> in  <code>Word2Vec</code> should be like. </p>

<p>I download the text from a Wikipedia page, as it is written is the article, and I make a list of sentences from it:</p>

<pre><code>sentences = [word for word in wikipage.content.split('.')]
</code></pre>

<p>So, for example, <code>sentences[0]</code> looks like:</p>

<pre><code>'Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed'
</code></pre>

<p>Then I try to train a model with this list:</p>

<pre><code>model = Word2Vec(sentences, min_count=2, size=50, window=10,  workers=4)
</code></pre>

<p>But the dictionary of the model consists of letters! For example, the output of <code>model.wv.vocab.keys()</code> is:</p>

<pre><code>dict_keys([',', 'q', 'D', 'B', 'p', 't', 'o', '(', ')', '0', 'V', ':', 'j', 's', 'R', '{', 'g', '-', 'y', 'c', '9', 'I', '}', '1', 'M', ';', '`', '\n', 'i', 'r', 'a', 'm', 'â€“', 'v', 'N', 'h', '/', 'P', 'F', '8', '""', 'â€™', 'W', 'T', 'u', 'U', '?', ' ', 'n', '2', '=', 'w', 'C', 'O', '6', '&amp;', 'd', '4', 'S', 'J', 'E', 'b', 'L', '$', 'l', 'e', 'H', 'â‰ˆ', 'f', 'A', ""'"", 'x', '\\', 'K', 'G', '3', '%', 'k', 'z'])
</code></pre>

<p>What am I doing wrong? Thanks in advance!</p>
","python, nlp, gensim, word2vec","<p>The input to the <code>Word2Vec</code> model object could be a list of list of words, using the tokenization function in <code>nltk</code>:</p>

<pre><code>&gt;&gt;&gt; import wikipedia
&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; page = wikipedia.page('machine learning')
&gt;&gt;&gt; sentences = [word_tokenize(sent) for sent in sent_tokenize(page.content)]
&gt;&gt;&gt; sentences[0]
['Machine', 'learning', 'is', 'the', 'subfield', 'of', 'computer', 'science', 'that', 'gives', 'computers', 'the', 'ability', 'to', 'learn', 'without', 'being', 'explicitly', 'programmed', '.']
</code></pre>

<p>And feed it in:</p>

<pre><code>&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; model = Word2Vec(sentences, min_count=2, size=50, window=10,  
&gt;&gt;&gt; list(model.wv.vocab.keys())[:10]
['sparsely', '(', 'methods', 'their', 'typically', 'information', 'assessment', 'False', 'often', 'problems']
</code></pre>

<p>But in general, an generator (of sentence) that contains a generator (of words) would work too, i.e.:</p>

<pre><code>&gt;&gt;&gt; from gensim.utils import tokenize
&gt;&gt;&gt; paragraphs = map(tokenize, page.content.split('\n')) # paragraphs
&gt;&gt;&gt; model = Word2Vec(paragraphs, min_count=2, size=50, window=10,  workers=4)
&gt;&gt;&gt; list(model.wv.vocab.keys())[:10]
['sparsely', 'methods', 'their', 'typically', 'information', 'assessment', 'False', 'often', 'problems', 'symptoms']
</code></pre>
",6,1,1655,2017-03-23 13:05:13,https://stackoverflow.com/questions/42976912/how-to-train-word2vec-model-on-wikipedia-page-using-gensim
Does or will H2O provide any pretrained vectors for use with h2o word2vec?,"<p>H2O recently added word2vec in its API.  It is great to be able to easily train your own word vectors on a corpus you provide yourself. </p>

<p>However even greater possibilities exist from using big data and big computers, of the type that software vendors like Google or H2O.ai, but not so many end-users of H2O, may have access to, due to network bandwidth and compute power limitations.</p>

<p>Word embeddings can be seen as a type of unsupervised learning. As such, great value can be had in a data science pipeline by using pretrained word vectors that were built on a very large corpus as infrastructure in specific applications. Using general purpose pretrained word vectors can be seen as a form of transfer learning.  Reusing word vectors is analogous to computer vision deep learning generic lowest layers that learn to detect edges in photographs. Higher layers detect specific kinds of objects composed from the edge layers below them.</p>

<p>For example Google provides some pretrained word vectors with their word2vec package.  The more examples the better is often true with unsupervised learning.  Further, sometimes it's practically difficult for an individual data scientist to download a giant corpus of text on which to train your own word vectors.  And there is no good reason for every user to recreate the same wheel by training word vectors themselves on the same general purpose corpuses (corpi?) like wikipedia.</p>

<p>Word embeddings are very important and have the potential to be the bricks and mortar of a galaxy of possible applications.  TF-IDF, the old basis for many natural language data science applications, stands to be made obsolete by using word embeddings instead.</p>

<p>Three questions:</p>

<p>1 - Does H2O currently provide any general purpose pretrained word embeddings (word vectors), for example trained on text found at legal or other public-owned (government) websites, or wikipedia or twitter or craigslist, or other free or Open Commons sources of human-written text?</p>

<p>2 - Is there a community site where H2O users can share their trained word2vec word vectors that are built on more specialized corpuses, such as medicine and law?</p>

<p>3 - Can H2O import Google's pretrained word vectors from their word2vec package?</p>
","word2vec, h2o, unsupervised-learning","<p>thank you for your questions.</p>

<p>You are absolutely right, there are many situations when you don't need a custom model and pre-trained model will work well. I assume people will mostly build their own models on smaller problems in their specific domain and use pre-trained models to complement the custom model.</p>

<p>You can import 3rd party pre-trained models into H2O as long as they are in a CSV-like format. This is true for many available GloVe models.</p>

<p>To do that import the model into a Frame (just like with any other dataset):</p>

<pre><code>w2v.frame &lt;- h2o.importFile(""pretrained.glove.txt"")
</code></pre>

<p>And then convert it to a regular H2O word2vec model:</p>

<pre><code>w2v.model &lt;- h2o.word2vec(pre_trained = w2v.frame, vec_size = 100)
</code></pre>

<p>Please note that you need to provide the size of the embeddings.</p>

<p>H2O doens't plan to provide a model exchange/model market for w2v model as far as I know. You can use models that are available on-line: <a href=""https://github.com/3Top/word2vec-api"" rel=""nofollow noreferrer"">https://github.com/3Top/word2vec-api</a></p>

<p>We currently do not support importing Google's binary format of word embeddings, however the support is on our road map as it makes a lot of sense for our users.</p>
",4,3,1046,2017-03-23 16:46:03,https://stackoverflow.com/questions/42982176/does-or-will-h2o-provide-any-pretrained-vectors-for-use-with-h2o-word2vec
How to speed up Gensim Word2vec model load time?,"<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>
","python, nlp, gensim, word2vec","<p>In recent gensim versions you can load a subset starting from the front of the file using the optional <code>limit</code> parameter to <code>load_word2vec_format()</code>. (The GoogleNews vectors seem to be in roughly most- to least- frequent order, so the first N are usually the N-sized subset you'd want. So use <code>limit=500000</code> to get the most-frequent 500,000 words' vectors â€“ still a fairly large vocabulary â€“ saving 5/6ths of the memory/load-time.)</p>

<p>So that may help a bit. But if you're re-loading for every web-request, you'll still be hurting from loading's IO-bound speed, and the redundant memory overhead of storing each re-load. </p>

<p>There are some tricks you can use in combination to help. </p>

<p>Note that after loading such vectors in their original word2vec.c-originated format, you can re-save them using gensim's native <code>save()</code>. If you save them uncompressed, and the backing array is large enough (and the GoogleNews set is definitely large enough), the backing array gets dumped in a separate file in a raw binary format. That file can later be memory-mapped from disk, using gensim's native <code>[load(filename, mmap='r')][1]</code> option.</p>

<p>Initially, this will make the load seem snappy â€“ rather than reading all the array from disk, the OS will just map virtual address regions to disk data, so that some time later, when code accesses those memory locations, the necessary ranges will be read-from-disk. So far so good!</p>

<p>However, if you are doing typical operations like <code>most_similar()</code>, you'll still face big lags, just a little later. That's because this operation requires both an initial scan-and-calculation over all the vectors (on first call, to create unit-length-normalized vectors for every word), and then another scan-and-calculation over all the normed vectors (on every call, to find the N-most-similar vectors). Those full-scan accesses will page-into-RAM the whole array â€“ again costing the couple-of-minutes of disk IO. </p>

<p>What you want is to avoid redundantly doing that unit-normalization, and to pay the IO cost just once. That requires keeping the vectors in memory for re-use by all subsequent web requestes (or even multiple parallel web requests). Fortunately memory-mapping can also help here, albeit with a few extra prep steps.</p>

<p>First, load the word2vec.c-format vectors, with <code>load_word2vec_format()</code>. Then, use <code>model.init_sims(replace=True)</code> to force the unit-normalization, destructively in-place (clobbering the non-normalized vectors). </p>

<p>Then, save the model to a new filename-prefix: model.save('GoogleNews-vectors-gensim-normed.bin'`. (Note that this actually creates multiple files on disk that need to be kept together for the model to be re-loaded.)</p>

<p>Now, we'll make a short Python program that serves to both memory-map load the vectors, <em>and</em> force the full array into memory. We also want this program to hang until externally terminated (keeping the mapping alive), <em>and</em> be careful not to re-calculate the already-normed vectors. This requires another trick because the loaded KeyedVectors actually don't know that the vectors are normed. (Usually only the raw vectors are saved, and normed versions re-calculated whenever needed.)</p>

<p>Roughly the following should work:</p>

<pre><code>from gensim.models import KeyedVectors
from threading import Semaphore
model = KeyedVectors.load('GoogleNews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
model.most_similar('stuff')  # any word will do: just to page all in
Semaphore(0).acquire()  # just hang until process killed
</code></pre>

<p>This will still take a while, but only needs to be done once, before/outside any web requests. While the process is alive, the vectors stay mapped into memory. Further, unless/until there's other virtual-memory pressure, the vectors should stay loaded in memory. That's important for what's next.</p>

<p>Finally, in your web request-handling code, you can now just do the following:</p>

<pre><code>model = KeyedVectors.load('GoogleNews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
# â€¦ plus whatever else you wanted to do with the model
</code></pre>

<p>Multiple processes can share read-only memory-mapped files. (That is, once the OS knows that file X is in RAM at a certain position, every other process that also wants a read-only mapped version of X will be directed to re-use that data, at that position.). </p>

<p>So this web-reqeust <code>load()</code>, <em>and any subsequent accesses</em>, can all re-use the data that the prior process already brought into address-space and active-memory. Operations requiring similarity-calcs against every vector will still take the time to access multiple GB of RAM, and do the calculations/sorting, but will no longer require extra disk-IO and redundant re-normalization. </p>

<p>If the system is facing other memory pressure, ranges of the array may fall out of memory until the next read pages them back in. And if the machine lacks the RAM to ever fully load the vectors, then every scan will require a mixing of paging-in-and-out, and performance will be frustratingly bad not matter what. (In such a case: get more RAM or work with a smaller vector set.)</p>

<p>But if you do have enough RAM, this winds up making the original/natural load-and-use-directly code ""just work"" in a quite fast manner, without an extra web service interface, because the machine's shared file-mapped memory functions as the service interface. </p>
",66,28,30073,2017-03-23 20:30:58,https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time
Python Gensim word2vec vocabulary key,"<p>I want to make word2vec with gensim. I heard that vocabulary corpus should be unicode so I converted it to unicode.</p>

<pre><code># -*- encoding:utf-8 -*-
# !/usr/bin/env python
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
from gensim.models import Word2Vec
import pprint

with open('parsed_data.txt', 'r') as f:
    corpus = map(unicode, f.read().split('\n'))

model = Word2Vec(size=128, window=5, min_count=5, workers=4)
model.build_vocab(corpus,keep_raw_vocab=False)
model.train(corpus)
model.save('w2v')

pprint.pprint(model.most_similar(u'ë„ˆ'))
</code></pre>

<p>Above is my source code. It seems like work well. However there are problem with vocabulary key. I want to make korean word2vec which use unicode. For example word <code>ì‚¬ê³¼</code> which means apology in english and it's unicode is <code>\xC0AC\xACFC</code> If I try to find <code>ì‚¬ê³¼</code> in word2vec, key error occur...<br>
Instead of <code>\xc0ac\xacfc</code> <code>\xc0ac</code> and <code>\xacfc</code> stores separately. 
What's the reason and how to solve it?</p>
","python, unicode, gensim, word2vec","<p>Word2Vec requires text examples that are broken into word-tokens. It appears you are simply providing strings to Word2Vec, so when it iterates over them, it will only be seeing single-characters as words. </p>

<p>Does Korean use spaces to delimit words? If so, break your texts by spaces before handing the list-of-words as a text example to Word2Vec. </p>

<p>If not, you'll need to use some external word-tokenizer (not part of gensim) before passing your sentences to Word2Vec.</p>
",3,1,1765,2017-03-28 09:32:41,https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key
How word2vec retrieves result from binary files?,"<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p>the code loads the google_news binary file to model. 
my question is, how the line 3 computes the output from a binary file ( Since Binary files contains 0's and 1's).</p>
","neural-network, nlp, semantics, text-mining, word2vec","<p>I'm not sure exactly what the question is here, but I assume you're asking how to load the binary into your Python app? You can use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> for example which has built-in tools to decode the binary:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p><strong>EDIT</strong></p>

<p>I feel your question is more about binary files in general? This does not seem related to word2vec specifically. Anyways, in a word2vec binary file each line is a pair of word and weights in binary format. First the word is decoded into a string by looping the characters until it meets the binary character for ""space"". Then the rest is decoded from binary into floats. We know the number of floats since word2vec binary files have a header, such as ""3000000 300"", which tells us there are 3m words, each word is a 300D vector.</p>

<p>A binary file is organized as a series of bytes, each 8 bits. Read more about binary on the <a href=""https://en.wikipedia.org/wiki/Binary_file"" rel=""nofollow noreferrer"">wiki page</a>. </p>

<p>The number 0.0056 in decimal format, becomes in binary:</p>

<pre><code>00111011 10110111 10000000 00110100
</code></pre>

<p>So here there are 4 bytes that make up a float. How do we know this? Because we assume the binary encodes 32 bit float.</p>

<p>What if the binary file represents 64 bit precision floats? Then the decimal 0.0056 in binary becomes:</p>

<pre><code>00111111 01110110 11110000 00000110 10001101 10111000 10111010 11000111
</code></pre>

<p>Yes, twice the length because twice the precision. So when we decode the word2vec file, if the weights are 300d, and 64 bit encoding, then there should be 8 bytes to represent each number. So a word embedding would have 300*64=19,200 binary digits in each line of the file. Get it?</p>

<p>You can google ""how binary digits"" work, millions of examples.</p>
",1,0,956,2017-03-28 16:25:57,https://stackoverflow.com/questions/43074949/how-word2vec-retrieves-result-from-binary-files
Saving output (context) embeddings in word2vec (gensim implementation) as a final model,"<p>I have studied <code>word2vec</code> implementation in gensim, I am aware that input vectors are in <code>syn0</code>, output vectors are in <code>syn1</code> and <code>syn1neg</code> if negative sampling.</p>

<p>I know I can access similarity between input and output embeddings like this:</p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab
outv.index2word = model.wv.index2word  
outv.syn0 = model.syn1neg 
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p>My question is, if it is possible to save output embeddings (from <code>syn1</code> or <code>syn1neg</code> matrix) as final model. For example, when <code>model.save()</code>, so that it outputs output embeddings (or where exactly in the code of <code>word2vec.py</code> I could access and modify that). I need this in order to use these output embeddings as input to classifier. I have done it previously in brute-force approach, so I would like to access output embeddings easily.</p>
","python, gensim, word2vec, word-embedding","<p>Your object <code>outv</code>, as an instance of <code>KeyedVectors</code>, has its own <code>save()</code> method (inherited from the <code>SaveLoad</code> superclass defined in <code>gensim/utils.py</code>) and <code>save_word2vec_format()</code> method. Each would save them in a manner you could reload into Python code again later. </p>
",2,1,1609,2017-03-29 15:57:20,https://stackoverflow.com/questions/43098535/saving-output-context-embeddings-in-word2vec-gensim-implementation-as-a-fina
Word2Vec: Is it possible to train with respect to weight in NLP?,"<p>I used <code>Gensim</code>'s <code>Word2Vec</code> for training most similar words.</p>

<p>My dataset is all posts from my college community site.</p>

<p>Each dataset consists of like this:</p>

<pre><code>(title) + (contents) + (all comments)  // String
</code></pre>

<p>For example, </p>

<pre><code>data[0] =&gt; ""This is title. Contents is funny. What so funny?. Not funny for me""
</code></pre>

<p>So, I have around 400,000 datas like above and make them as a vector and try to train these data via <code>Word2Vec</code>. </p>

<p>I wonder that whether it is possible to make <code>Word2Vec</code> consider WEIGHT, which means, if I give an weight to certain data vector, <code>Word2Vec</code> train this data in a way that each word in this data vector has more strong relationship(similarity).</p>

<p>For example, If I gave a weight 5 to dataset, <code>I like Pizza, Chicken</code>, the word <code>Pizza</code> and <code>Chicken</code> (or <code>like</code> and <code>Pizza</code> etc) has strong relations than other data vector's words.</p>

<p>Would that be possible?</p>

<p>Sorry for poor explanation but I'm not native english speaker. If need more detailed info, please post comment.</p>
","nlp, gensim, word2vec","<p>There's no such configurable weighting in the definition of the word2vec algorithm, or the gensim implementation. </p>

<p>You could try repeating those text examples that you want to have more influence. (Ideally, such repetitions wouldn't be back-to-back, but shuffled among the entire dataset.) </p>

<p>As a result, those examples will affect the underlying model's training more often, for a greater proportion of the total training time â€“ shifting the relative positioning of the involved words, compared to less-repeated examples. That <em>might</em> have the end result you're seeking. </p>
",2,2,1405,2017-04-02 07:25:25,https://stackoverflow.com/questions/43165724/word2vec-is-it-possible-to-train-with-respect-to-weight-in-nlp
What is relation between tsne and word2vec?,"<p>As I know of, <code>tsne</code> is reducing dimension of word vector. </p>

<p><code>Word2vec</code> is generate word embedding model with huge amount of data.</p>

<p>What is the relation between two?</p>

<p>Does <code>Word2vec</code> use <code>tsne</code> inside? </p>

<p>(I use <code>Word2vec</code> from <code>Gensim</code>)</p>
","nlp, gensim, word2vec","<p>Internally they both use <em>gradient-descent</em> to reach their final optimized states. And both can be considered dimensionality-reduction operations. But, word2vec does not internally use t-SNE (or vice-versa). </p>

<p><strong>t-SNE</strong> (""t-distributed stochastic neighbor embedding"") typically reduces many-dimensional data to 2- or 3-dimensions, for the purposes of plotting a visualization. It involves learning a mapping from the original dimensionality, to the fewer dimensions, which still keeps similar points near each other.  </p>

<p><strong>word2vec</strong> takes many text examples and learns a shallow neural-network that's good at predicting words from nearby words. A particular layer of that neural-network's weights, which represent individual words, then becomes the learned N-dimensional word-vectors, with the value of N often 100 to 600. </p>

<p>(There's an alternative way to create word-vectors called <em>GLoVE</em> that works a little more like t-SNE, in that it trains directly from the high-dimensional co-occurrence matrix of words, rather than from the many in-context co-occurrence examples. But it's still not t-SNE itself.)</p>

<p>You could potentially run t-SNE with a target dimensionality of 100-400. But since that end-result wouldn't yet yield nice plots, the maintenance of 'nearness' that's central to t-SNE won't have delivered its usual intended benefit. </p>

<p>You could potentially learn word2vec (or GLoVE) vectors of just 2- or 3-dimensions, but most of the useful similarities/arrangements that people seek from word-vectors would be lost in the crowding. And in a plot, you'd probably not see as strong visual 'clumping' of related-word categories, because t-SNE's specific high-to-low dimensionality nearness-preservation goal wasn't applied. </p>
",3,3,1030,2017-04-02 09:29:02,https://stackoverflow.com/questions/43166762/what-is-relation-between-tsne-and-word2vec
can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?,"<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
","python, nlp, semantics, gensim, word2vec","<p><a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">From the gensim documentation:</a></p>

<blockquote>
  <p><code>sg</code> defines the training algorithm. By default (<code>sg=0</code>), CBOW is used. Otherwise (<code>sg=1</code>), skip-gram is employed.</p>
</blockquote>
",1,2,411,2017-04-02 17:55:28,https://stackoverflow.com/questions/43171573/can-anyone-tell-me-about-the-model-skipgram-cbow-used-by-gensim
Pyspark - Load trained model word2vec,"<p>I want to use word2vec with PySpark to process some data.
I was previously using Google trained model <code>GoogleNews-vectors-negative300.bin</code> with <code>gensim</code> in Python.</p>

<p>Is there a way I can load this bin file with <code>mllib.word2vec</code> ?
Or does it make sense to export the data as a dictionary from Python <code>{word : [vector]}</code> (or <code>.csv</code> file) and then load it in <code>PySpark</code>? </p>

<p>Thanks</p>
","python, load, pyspark, gensim, word2vec","<p><a href=""https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html"" rel=""nofollow noreferrer"">Binary import is supported in Spark 3.x</a>:</p>
<pre><code>spark.read.format(&quot;binaryFile&quot;).option(&quot;pathGlobFilter&quot;, &quot;*.png&quot;).load(&quot;/path/to/data&quot;)
</code></pre>
<p>However, this would require processing the binary data. Hence a <code>gensim</code> <a href=""https://stackoverflow.com/questions/49471037/save-results-of-word2vec-model-query-in-a-csv-file"">export</a> is rather recommended:</p>
<pre><code># Save gensim model
filename = &quot;stored_model.csv&quot; 
trained_model.save(filename) 
</code></pre>
<p>Then <a href=""https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"" rel=""nofollow noreferrer"">load the model</a> in pyspark:</p>
<pre><code>df = spark.read.load(&quot;stored_model.csv&quot;,
                     format=&quot;csv&quot;, 
                     sep=&quot;;&quot;, 
                     inferSchema=&quot;true&quot;, 
                     header=&quot;true&quot;)
</code></pre>
",2,8,1313,2017-04-06 08:27:18,https://stackoverflow.com/questions/43249717/pyspark-load-trained-model-word2vec
using word2vec with heroku cloud platform,"<p>how can i use word2vec with heroku or any other cloud platform. I want to store my trained data set on cloud platform. so that every query can be retrieved through there.</p>
","heroku, semantics, word2vec","<p>I think you might want to take a look at <a href=""https://www.floydhub.com/"" rel=""nofollow noreferrer"">Floydhub</a>, which is a Deep Learning PaaS (dubbed by its creators as ""Heroku for Deep Learning"").
Specifically, I think you may want to take a look at this <a href=""https://github.com/floydhub/fastText"" rel=""nofollow noreferrer"">fastText</a> example.</p>

<p>See <a href=""https://medium.com/towards-data-science/stock2vec-from-ml-to-p-e-2e6ba407c24"" rel=""nofollow noreferrer"">Stock2Vec</a> for an example of a Word2Vec model trained on Floydhub. </p>
",1,0,284,2017-04-15 09:30:13,https://stackoverflow.com/questions/43424455/using-word2vec-with-heroku-cloud-platform
Update spaCy Vocabulary,"<p>I was wondering if it is possible to update spacys default vocabulary. What I am trying doing is this:</p>

<ul>
<li>run word2vec on my own corpus with gensim</li>
<li>load the vectors into my model with <code>nlp.vocab.load_vectors_from_bin_loc(\path)</code></li>
</ul>

<p>But since a lot of words in my corpus aren't in spacys default vocabulary I can't make use of the imported vectors. Is there an (easy) way to add those missing types?  </p>

<p><strong>Edit:</strong><br>
I realize it might be problematic to mix vectors. So my question is:<br>
How can I import a custom vocabulary into spacy?</p>
","python, word2vec, spacy","<p>This is much easier in the next version, which should be out this week --- I'm just finishing testing it. For now:</p>

<p>By default spaCy loads a data/vocab/vec.bin file, where the ""data"" directory is within the spacy.en module directory
Create the vec.bin file from a bz2 file using spacy.vocab.write_binary_vectors
Either replace spaCy's vec.bin file, or call nlp.vocab.load_rep_vectors at run-time, with the path to the binary file.
The above is a bit inconvenient at first, but the binary file format is much smaller and faster to load, and the vectors files are fairly big. Note that GloVe distributes in gzip format, not bzip.</p>

<p>Out of interest: are you using the GloVe vectors, or something you trained on your own data? If your own data, did you use Gensim? I'd like to make this much easier, so I'd appreciate suggestions for what work-flow you'd like to see.</p>

<p><strong>Load new vectors at run-time, optionally converting them</strong></p>

<pre><code>    import spacy.vocab

    def set_spacy_vectors(nlp, binary_loc, bz2_loc=None):
        if bz2_loc is not None:
            spacy.vocab.write_binary_vectors(bz2_loc, binary_loc)
        write_binary_vectors(bz2_input_loc, binary_loc)

        nlp.vocab.load_rep_vectors(binary_loc)
</code></pre>

<p><strong>Replace the vec.bin, so your vectors will be loaded by default</strong></p>

<pre><code>from spacy.vocab import write_binary_vectors
    import spacy.en

    from os import path

    def main(bz2_loc):
        bin_loc = path.join(path.dirname(spacy.en.__file__), 'data', 'vocab', 'vec.bin')
        write_binary_vectors(bz2_loc, bin_loc)

if __name__ == '__main__':
    plac.call(main)
</code></pre>
",3,3,2920,2017-04-20 15:48:51,https://stackoverflow.com/questions/43524301/update-spacy-vocabulary
String similarity TF-IDF Bag of words or Word2vec,"<p>I am trying to create an application that computes the similarity between 2 strings.
The strings are not long. 3 Sentences long at maximum.
I did some research and I came across some possible solution paths.</p>

<p>First one use bag of words: count words and compare the 2 produced vectors ( cosine similarity)</p>

<p>The second use TF-IDF and compare produced vectors.</p>

<p>The third is use word2vec and compare vectors.</p>

<p>Now for the questions.</p>

<p>Performance wise is word2vec performance better that TF-IDF for short sentences?</p>

<p>What is the best way to train word2vec model? Should I use a large amount of text ( wikipedia dump for example) or train it using just the sentences that are being compared.</p>

<p>How to get sentence similarity from word2vec. should I average the words in each sentence or is there a better solution?</p>
","python, nlp, tf-idf, word2vec, sentence-similarity","<ul>
<li><p>With good train data, word2vec must have better performance. (I got good results from it)</p></li>
<li><p>You must have large amount of data for good model. The best way is using pre-trained data if you are working on English. There are good models in <a href=""https://github.com/3Top/word2vec-api"" rel=""nofollow noreferrer"">this link</a> you can use. Google News pre-trained model is working perfect as I know.</p></li>
<li><p>It is common to use Average of words in part of text like sentence. The better way can be Weighted Average like tf-idf weighting average. Also there is a hot research on semantic textual similarity you can follow it from it's <a href=""http://ixa2.si.ehu.es/stswiki/index.php/Main_Page"" rel=""nofollow noreferrer"">Wiki Page</a></p></li>
</ul>
",2,1,2367,2017-04-21 15:24:32,https://stackoverflow.com/questions/43546510/string-similarity-tf-idf-bag-of-words-or-word2vec
creating word2vec model syn1neg.npy extension,"<p>When creating model,there is not any more model with extension finish </p>

<blockquote>
  <p>.syn1neg.npy </p>
  
  <p>syn0.npy</p>
</blockquote>

<p>My code is below:</p>

<pre><code>corpus= x+y
tok_corp= [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus]
model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)
model.save('/home/Desktop/test_model')

model = gensim.models.Word2Vec.load('/home/kafein/Desktop/chatbot/test_model')
</code></pre>

<p>There is only 1 model file</p>

<pre><code>test_model
</code></pre>

<p>Which part i am wrong ?</p>
","python, python-3.x, deep-learning, word2vec, doc2vec","<p>Gensim's native <code>.save()</code> only saves off parts of the model into such separate files (like <code>test_model.syn1neg.npy</code> etc) if they are larger than a certain threshold. When they're small, they get ""pickled"" up into the single model save file.</p>

<p>So there's no problem/error here. If you start training a larger model with more words, you may see those other files re-appear. (When you do, be sure to keep them alongside the main <code>test_model</code> file, if copying/moving them elsewhere â€“ all the files together are needed to re-<code>load()</code> the model.)</p>
",19,7,4968,2017-04-24 12:37:39,https://stackoverflow.com/questions/43588290/creating-word2vec-model-syn1neg-npy-extension
Is it possible to use gensim word2vec model in deeplearning4j.word2vec?,"<p>I'm new to deeplearning4j, i want to make sentence classifier using words vector as input for the classifier. 
I was using python before, where the vector model was generated using gensim, and i want to use that model for this new classifier. 
Is it possible to use gensim's word2vec model in deeplearning4j.word2vec and how i can do that?</p>
","java, gensim, word2vec, deeplearning4j","<p>Yes, it's possible since Word2Vec implementation defines a standard to structure its model.</p>

<p>To do this:</p>

<ol>
<li><p>Using <em>gensim</em>, save the model <strong>compatible with Word2Vec implementation</strong>:</p>

<pre><code>w2v_model.wv.save_word2vec_format(""path/to/w2v_model.bin"", binary=True)
</code></pre></li>
<li><p>From <em>DL4J</em>, load the same pre-trained model:</p>

<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(""path/to/w2v_model.bin"");
</code></pre></li>
</ol>

<p>In fact, you could test the model in both codes and you should see the same results, for instance:</p>

<p>With gensim:</p>

<pre><code>print(w2v_model.most_similar(""love""))
print(w2v_model.n_similarity([""man""], [""king""]))
</code></pre>

<p>And with DL4J:</p>

<pre><code>System.out.println(w2vModel.wordsNearest(""love"", 10));
System.out.println(w2vModel.similarity(""man"", ""king""));
</code></pre>
",10,6,3893,2017-04-26 11:37:02,https://stackoverflow.com/questions/43633092/is-it-possible-to-use-gensim-word2vec-model-in-deeplearning4j-word2vec
how to acquire word frequency from Word2Vec model,"<p>I have already trained a Word2Vec model with gensim.models.Word2Vec.
by which means can I acquire the frequency of each word in this model?</p>
","python, word2vec","<pre><code> model = gensim.models.Word2Vec.load('Word2VecModel/word2vec')
 for w in model.vocab:
     print (w, model.vocab[w].count)
</code></pre>
",5,3,3635,2017-04-27 02:40:34,https://stackoverflow.com/questions/43647749/how-to-acquire-word-frequency-from-word2vec-model
Load Pretrained Word2Vec Embedding in Tensorflow,"<p>I am trying to load a pretrained Word2Vec (or Glove) embedding in my Tensorflow code, however I have some problems understanding it as I cannot find many examples. The question is not about getting and loading the embedding matrix, which I understand, but about looking up the word ids.
Currently I am using the code from <a href=""https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/"" rel=""nofollow noreferrer"">https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/</a>.
There, first the embedding matrix is loaded (understood).
Then, a vocabulary processor is used to convert a sentence x to a list of word IDs:</p>

<pre><code>vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
#fit the vocab from glove
pretrain = vocab_processor.fit(vocab)
#transform inputs
x = np.array(list(vocab_processor.transform(your_raw_input)))
</code></pre>

<p>This works and gives me a list of word ids, but I do not know if this is correct. What bothers me most is the question how the vocabulary processor gets the correct word ids from the embedding I just read (since otherwise the result of the embedding would be wrong).
Does the fit step do this?</p>

<p>Or is there another way, how do you do this lookup?</p>

<p>Thanks!
Oliver</p>
","tensorflow, lstm, embedding, word2vec","<p>Yes, the <code>fit</code> step tells the <code>vocab_processor</code> the <em>index</em> of each word (starting from 1) in the <code>vocab</code> array. <code>transform</code> just reversed this lookup and produces the index from the words and uses <code>0</code> to pad the output to the <code>max_document_size</code>.</p>

<p>You can see that in a short example here:</p>

<pre class=""lang-py prettyprint-override""><code>vocab_processor = learn.preprocessing.VocabularyProcessor(5)
vocab = ['a', 'b', 'c', 'd', 'e']
pretrain = vocab_processor.fit(vocab)

pretrain == vocab_processor
# True

np.array(list(pretrain.transform(['a b c', 'b c d', 'a e', 'a b c d e'])))

# array([[1, 2, 3, 0, 0],
#        [2, 3, 4, 0, 0],
#        [1, 5, 0, 0, 0],
#        [1, 2, 3, 4, 5]])
# 
</code></pre>
",1,1,1492,2017-04-27 12:49:31,https://stackoverflow.com/questions/43658327/load-pretrained-word2vec-embedding-in-tensorflow
Analogies from Word2Vec in TensorFlow?,"<p>I implemented Word Embeddings in Tensor Flow similarly to the code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">here</a> I was able to get the final embeddings (final_embeddings), but I would like to evaluate the embeddings using the analogies typical of this exercise. How can I identify which term corresponds to which row in the final embeddings array? Alternatively, is there an implementation in Tensor Flow for this? Any help would be greatly appreciated (specifics and resources would be a plus;) ). Thanks!</p>
","tensorflow, word2vec, word-embedding","<p>Recommend this <a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">conceptual tutorial</a> to you. 
If you are using skip-gram, the input is one-hot encoding. So the index of the <code>1</code> is the index of the vector of the word.</p>

<p><a href=""https://i.sstatic.net/7tZsZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7tZsZ.png"" alt=""enter image description here""></a></p>

<p>The implementation in tensorflow is quite simple. You may want to see this function: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer"">tf.nn.embedding_lookup</a></p>

<p>For example:</p>

<pre><code>embed = tf.nn.embedding_lookup(embedding, inputs)
</code></pre>

<p>The embed is the vector you are looking for.</p>
",1,2,316,2017-05-03 02:17:36,https://stackoverflow.com/questions/43750010/analogies-from-word2vec-in-tensorflow
Visualise word2vec generated from gensim using t-sne,"<p>I have trained a doc2vec and corresponding word2vec on my own corpus using gensim. I want to visualise the word2vec using t-sne with the words. As in, each dot in the figure has the ""word"" also with it.</p>

<p>I looked at a similar question here : <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">t-sne on word2vec</a></p>

<p>Following it, I have this code : </p>

<p>import gensim
import gensim.models as g</p>

<pre><code>from sklearn.manifold import TSNE
import re
import matplotlib.pyplot as plt

modelPath=""/Users/tarun/Desktop/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin""
model = g.Doc2Vec.load(modelPath)

X = model[model.wv.vocab]
print len(X)
print X[0]
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X[:1000,:])

plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.show()
</code></pre>

<p>This gives a figure with dots but no words. That is I don't know which dot is representative of which word. How can I display the word with the dot?</p>
","scikit-learn, data-visualization, gensim, word2vec","<p>Two parts to the answer: how to get the word labels, and how to plot the labels on a scatterplot.</p>
<p><strong>Word labels in gensim's word2vec</strong></p>
<p><code>model.wv.vocab</code> is a dict of {word: object of numeric vector}. To load the data into <code>X</code> for t-SNE, I made one change.</p>
<pre><code>vocab = list(model.wv.key_to_index)
X = model.wv[vocab]
</code></pre>
<p>This accomplishes two things: (1) it gets you a standalone <code>vocab</code> list for the final dataframe to plot, and (2) when you index <code>model</code>, you can be sure that you know the order of the words.</p>
<p>Proceed as before with</p>
<pre><code>tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
</code></pre>
<p>Now let's put <code>X_tsne</code> together with the <code>vocab</code> list. This is easy with pandas, so <code>import pandas as pd</code> if you don't have that yet.</p>
<pre><code>df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])
</code></pre>
<p>The vocab words are the <em>indices</em> of the dataframe now.</p>
<p>I don't have your dataset, but in the <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">other SO</a> you mentioned, an example <code>df</code> that uses sklearn's newsgroups would look something like</p>
<pre><code>                        x             y
politics    -1.524653e+20 -1.113538e+20
worry        2.065890e+19  1.403432e+20
mu          -1.333273e+21 -5.648459e+20
format      -4.780181e+19  2.397271e+19
recommended  8.694375e+20  1.358602e+21
arguing     -4.903531e+19  4.734511e+20
or          -3.658189e+19 -1.088200e+20
above        1.126082e+19 -4.933230e+19
</code></pre>
<p><strong>Scatterplot</strong></p>
<p>I like the object-oriented approach to matplotlib, so this starts out a little different.</p>
<pre><code>fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(df['x'], df['y'])
</code></pre>
<p>Lastly, the <code>annotate</code> method will label coordinates. The first two arguments are the text label and the 2-tuple. Using <code>iterrows()</code>, this can be very succinct:</p>
<pre><code>for word, pos in df.iterrows():
    ax.annotate(word, pos)
</code></pre>
<p>[Thanks to Ricardo in the comments for this suggestion.]</p>
<p>Then do <code>plt.show()</code> or <code>fig.savefig()</code>. Depending on your data, you'll probably have to mess with <code>ax.set_xlim</code> and <code>ax.set_ylim</code> to see into a dense cloud. This is the newsgroup example without any tweaking:</p>
<p><a href=""https://i.sstatic.net/OSKOJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OSKOJ.png"" alt=""scatterplot"" /></a></p>
<p>You can modify dot size, color, etc., too. Happy fine-tuning!</p>
",49,22,22551,2017-05-04 07:31:22,https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim-using-t-sne
Word2vec classification and clustering tensorflow,"<p>I am trying to cluster some sentences using similarity (maybe cosine) and then maybe use a classifier to put text in predefined classes. </p>

<p>My idea is to use tensorflow to generate the word embedding then average them for each sentence. Next use a clustering/classification algorithm.</p>

<p>Does tensorflow provide ready to use word2vec generation algorithm?</p>

<p>Would a bag of words model generate a good output?</p>
","tensorflow, nlp, word2vec, text-classification","<ul>
<li><p><strong>No</strong>, tensorflow does not provide a <strong>ready-to-use word2vec</strong> but it does have <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">a tutorial on word2vec</a>.</p></li>
<li><p><strong>Yes</strong>, a bag of words can generate surprisingly good output (but not <em>state-of-the-art</em>), and has the benefit of being <strong>amazingly faster</strong>.  I have a small amount of data (tens of thousands of sentences) and have achieved F1 scores of >0.90 for classification.</p></li>
</ul>
",1,0,947,2017-05-04 14:17:45,https://stackoverflow.com/questions/43785438/word2vec-classification-and-clustering-tensorflow
Unpickling Error while using Word2Vec.load(),"<p>I am trying to load a binary file using <code>gensim.Word2Vec.load(fname)</code> but I get the error:</p>

<blockquote>
  <p>File ""file.py"", line 24, in 
      model = gensim.models.Word2Vec.load('ammendment_vectors.model.bin')   </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1396, in load
      model = super(Word2Vec, cls).load(*args, **kwargs) </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 271, in load
      obj = unpickle(fname)  </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 933, in unpickle
      return _pickle.load(f, encoding='latin1')</p>
  
  <p>_pickle.UnpicklingError: could not find MARK</p>
</blockquote>

<p>I googled but I am unable to figure out why this error is coming up. Please let me know if any other information is required.</p>
","python, gensim, word2vec","<p>This would normally work, if the file was created by gensim's native <code>.save()</code>. </p>

<p>Are you sure the file <code>'ammendment_vectors.model.bin'</code> is complete and uncorrupted? </p>

<p>Was it created using the same Python/gensim versions as in use where you're trying to <code>load()</code> it? </p>

<p>Can you try re-creating the file? </p>
",4,2,7060,2017-05-17 10:23:45,https://stackoverflow.com/questions/44022180/unpickling-error-while-using-word2vec-load
Gensim Word2Vec: poor training performance.,"<p>that might actually be a dumb question but I just can't figure out why my script with gensim.models.word2vec is not working. Here is the thing, I'm using the stanford sentiment analysis databank dataset (~11000 reviews), and i'm trying to build word2vec using gensim, this is my script: </p>

<pre><code>import gensim as gs 
import sys 

# open the datas
sentences = gs.models.word2vec.LineSentence('../processedWords.txt')
print(""size in RAM of the sentences: {}"".format(sys.getsizeof(sentences)))

# transform them
# bigram_transformer = gs.models.Phrases(sentences)

model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5)
model.save('firstModel')
print(model.similarity('film', 'test'))
print(model.similarity('film', 'movie'))
</code></pre>

<p>Now, my problem is that the script runs in 2s, and gives only huge similarity between every pair of words. In addition, some words which are in the sentences are not in the built vocabulary. </p>

<p>I must be doing something obviously wrong, but can't figure what. </p>

<p>Thank you for your help. </p>
","python-3.x, dataset, text-mining, gensim, word2vec","<p>I'm almost certain that this is because you haven't specified a number of training iterations; I think <code>iter</code> defaults to 1, which is basically useless for training a neural net. Add the <code>iter=&lt;int&gt;</code> flag to your model declaration, e.g. <code>model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5, iter=1000)</code>.
Kind of a face-palmer but I did the same exact thing.</p>
",3,3,1355,2017-05-20 20:26:35,https://stackoverflow.com/questions/44090503/gensim-word2vec-poor-training-performance
Does it make sense to talk about skip-gram and cbow when using The Glove method?,"<p>I'm trying different word embeddings methods, in order to pick the approache that works the best for me. I tried word2vec and FastText. Now, I would like to try Glove. In both word2vec and FastText, there is two versions: Skip-gram (predict context from word) and CBOW (predict word from context). But in Glove python package, there is no parameter that enables you to choose whether you want to use skipg-gram or Cbow. </p>

<p>Given that Glove does not work the same way as w2v, I'm wondering: Does it make sense to talk about skip-gram and cbow when using The Glove method ?</p>

<p>Thanks in Advance</p>
","python-3.x, word2vec, word-embedding","<p>Not really, skip-gram and CBOW are simply the names of the two Word2vec models. They are shallow neural networks that generate word embeddings by predicting a context from a word and vice versa, and then treating the output of the hidden layer as the vector/representation. GloVe uses a different approach, making use of the global statistics of a corpus by training on a co-occurrence matrix, rather than local context windows.</p>
",3,2,906,2017-05-22 12:40:34,https://stackoverflow.com/questions/44113128/does-it-make-sense-to-talk-about-skip-gram-and-cbow-when-using-the-glove-method
Code for gensim Word2vec as an HTTP service &#39;KeyedVectors&#39; Attribute error,"<p>I am using the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> code from the word2vec HTTP server running at <a href=""https://rare-technologies.com/word2vec-tutorial/#bonus_app"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/#bonus_app</a>. I changed the loaded file to a file of vectors trained with the original C version of word2vec. I load the file with </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)
</code></pre>

<p>and it seems to load without problems. But when I test the HTTP service with, let's say </p>

<pre><code>curl 'http://127.0.0.1/most_similar?positive%5B%5D=woman&amp;positive%5B%5D=king&amp;negative%5B%5D=man' 
</code></pre>

<p>I got an empty result with only the execution time.</p>

<pre><code>{""taken"": 0.0003361701965332031, ""similars"": [], ""success"": 1}
</code></pre>

<p>I put a <code>traceback.print_exc()</code> on the except part of the related method, which is in this case <code>def most_similar(self, *args, **kwargs):</code> and I got: </p>

<pre><code>Traceback (most recent call last):
  File ""./w2v_server.py"", line 114, in most_similar
    topn=5)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 304, in most_similar
    self.init_sims()
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 817, in init_sims
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>

<p>Any idea on why this might happens? </p>

<p>Note: I use python 2.7 and I installed gensim using pip, which gave me gensim 2.1.0.</p>
","python, gensim, word2vec","<p>FYI that demo code was baed on gensim 0.12.3 (from 2015, as listed in its <code>requirements.txt</code>), and would need updating to work with the latest gensim. </p>

<p>It might be sufficient to add a line to <code>w2v_server.py</code> at line 70 (just after the <code>load_word2vec_format()</code>), to force the creation of the needed <code>syn0norm</code> property (which in older gensims was auto-created on load), before deleting the raw <code>syn0</code> values. Specifically:</p>

<pre><code>self.model.init_sims(replace=True)
</code></pre>

<p>(You would leave out the <code>replace=True</code> if you were going to be doing operations other than <code>most_similar()</code>, that might require raw vectors.)</p>

<p>If this works to fix the problem for you, a pull-request to the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> repo would be favorably received!</p>
",2,2,515,2017-05-23 19:26:36,https://stackoverflow.com/questions/44143441/code-for-gensim-word2vec-as-an-http-service-keyedvectors-attribute-error
Pandas dataframe to doc2vec.LabeledSentence,"<p>I have this dataframe :</p>

<pre><code>    order_id    product_id  user_id          
    2           33120       u202279  
    2           28985       u202279  
    2           9327        u202279  
    4           39758       u178520  
    4           21351       u178520  
    5           6348        u156122  
    5           40878       u156122  
</code></pre>

<p>Type user_id : String<br>
Type product_id : Integer  </p>

<p>I would like to use this dataframe to create a Doc2vec corpus. So, I need to use the LabeledSentence function to create a dict :<br>
 {tags : user_id, words:
all product ids ordered by each user_id}   </p>

<p>But the the dataframe shape is (32434489, 3), so I should avoid to use a loop to create my labeledSentence. </p>

<p>I try to run this function (below) with multiprocessing but is too long.  </p>

<p>Have you any idea to transform my dataframe in the good format for a Doc2vec corpus where the tag is the user_id and the words is the list of products by user_id?</p>

<pre><code>def append_to_sequences(i):
     user_id = liste_user_id.pop(0)
     liste_produit_userID = data.ix[data[""user_id""]==user_id, ""product_id""].astype(str).tolist()
     return doc2vec.LabeledSentence(words=prd_user_list, tags=user_id )

pool = multiprocessing.Pool(processes=3)
result = pool.map_async(append_to_sequences, np.arange(len_liste_unique_user))
pool.close()
pool.join()
sentences = result.get()
</code></pre>
","python, pandas, dataframe, word2vec, doc2vec","<p>Using multiprocessing is likely overkill. The forking of processes can wind up duplicating all existing memory, and involve excess communication marshalling results back into the master process. </p>

<p>Using a loop should be OK. 34 million rows (and far fewer unique <code>user_id</code>s) isn't that much, depending on your RAM. </p>

<p>Note that in recent versions of gensim <code>TaggedDocument</code> is the preferred class for Doc2Vec examples. </p>

<p>If we were to assume you have a list of all unique <code>user_id</code>s in <code>liste_user_id</code>, and a (new, not shown) function that gets the list-of-words for a <code>user_id</code> called <code>words_for_user()</code>, creating the documents for Doc2Vec in memory could be as simple as:</p>

<pre><code>documents = [TaggedDocument(words=words_for_user(uid), tags=[uid])
             for uid in liste_user_id]
</code></pre>

<p>Note that <code>tags</code> should be a list of tags, not a single tag â€“ even though in many common cases each document only has a single tag. (If you provide a single string tag, it will see <code>tags</code> as a list-of-characters, which is not what you want.)</p>
",2,1,2280,2017-05-24 09:37:13,https://stackoverflow.com/questions/44154572/pandas-dataframe-to-doc2vec-labeledsentence
"Regex replace is taking time for millions of documents, how to make it faster?","<p>I have documents like:</p>

<pre><code>documents = [
    ""I work on c programing."",
    ""I work on c coding."",
]
</code></pre>

<p>I have synonym file such as:</p>

<pre><code>synonyms = {
    ""c programing"": ""c programing"",
    ""c coding"": ""c programing""
}
</code></pre>

<p>I want to replace all synonyms for which I wrote this code:</p>

<pre><code># added code to pre-compile all regex to save compilation time. credits alec_djinn

compiled_dict = {}
for value in synonyms:
    compiled_dict[value] = re.compile(r'\b' + re.escape(value) + r'\b')

for doc in documents:
    document = doc
    for value in compiled_dict:
        lowercase = compiled_dict[value]
        document = lowercase.sub(synonyms[value], document)
    print(document)
</code></pre>

<p>Output:</p>

<pre><code>I work on c programing.
I work on c programing.
</code></pre>

<p>But since the number of documents is a few million and the number of synonym terms are in 10s of thousands, the expected time for this code to finish is 10 days approx.</p>

<p>Is there a faster way to do this?</p>

<p>PS: with the output I want to train word2vec model.</p>

<p>Any help is greatly appreciated. I was thinking of writing some cpython code and putting it in parallel threads.</p>
","python, parallel-processing, word2vec, cpython","<p>I have done string replacement jobs like this before, also for training word2vec models on very large text corpora.  When the number of terms to replace (your ""synonym terms"") is very large, it can make sense to do string replacement using the <a href=""https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm"" rel=""noreferrer"">Aho-Corasick algorithm</a> instead of looping over many single string replacements.  You can take a look at my <a href=""https://github.com/wroberts/fsed"" rel=""noreferrer"">fsed</a> utility (written in Python), which might be useful to you.</p>
",20,12,4550,2017-05-25 10:38:56,https://stackoverflow.com/questions/44178449/regex-replace-is-taking-time-for-millions-of-documents-how-to-make-it-faster
Why are almost all cosine similarities positive between word or document vectors in gensim doc2vec?,"<p>I have calculated document similarities using Doc2Vec.docvecs.similarity() in gensim. Now, I would either expect the cosine similarities to lie in the range [0.0, 1.0] if gensim used the absolute value of the cosine as the similarity metric, or roughly half of them to be negative if it does not.</p>

<p>However, what I am seeing is that <em>some</em> similarities are negative, but they are very rare â€“ less than 1% of pairwise similarities in my set of 30000 documents.</p>

<p>Why are almost all of the similarities positive?</p>
","python, gensim, word2vec, doc2vec","<p>There's no inherent guarantee in Word2Vec/Doc2Vec that the generated set of vectors is symmetrically distributed around the origin point. They could be disproportionately in some directions, which would yield the results you've seen. </p>

<p>In a few tests I just did on the toy-sized dataset ('lee corpus') used in the bundled gensim <code>docs/notebooks/doc2vec-lee.ipynb</code> notebook, checking the cosine-similarities of all documents against the first document, it vaguely seems that:</p>

<ol>
<li>using hierarchical-softmax rather than negative sampling (<code>hs=1, negative=0</code>) yields a balance between >0.0 and &lt;0.0 cosine-similarities that is closer-to (but not yet quite) half and half</li>
<li>using a smaller number of negative samples (such as <code>negative=1</code>) yields a more balanced set of results; using a larger number (such as <code>negative=10</code>) yields relatively more >0.0 cosine-similarities</li>
</ol>

<p>While not conclusive, this is mildly suggestive that the arrangement of vectors may be influenced by the <code>negative</code> parameter. Specifically, typical negative-sampling parameters, such as the default <code>negative=5</code>, mean words will be trained more times as non-targets, than as positive targets. That <em>might</em> push the preponderance of final coordinates in one direction. (More testing on larger datasets and modes, and more analysis of how the model setup could affect final vector positions, would be necessary to have more confidence in this idea.)</p>

<p>If for some reason you wanted a more balanced arrangement of vectors, you could consider transforming their positions, post-training. </p>

<p>There's an interesting recent paper in the word2vec space, <a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">""All-but-the-Top: Simple and Effective Postprocessing for Word Representations""</a>, that found sets of trained word-vectors don't necessarily have a 0-magnitude mean â€“ they're on average in one direction from the origin. And further, this paper reports that subtracting the common mean (to 're-center' the set), and also removing a few other dominant directions, can improve the vectors' usefulness for certain tasks. </p>

<p>Intuitively, I suspect this 'all-but-the-top' transformation might serve to increase the discriminative 'contrast' in the resulting vectors. </p>

<p>A similar process <em>might</em> yield similar benefits for doc-vectors â€“ and would likely make the full set of cosine-similarities, to any doc-vector, more balanced between >0.0 and &lt;0.0 values.</p>
",4,3,1885,2017-06-03 15:29:31,https://stackoverflow.com/questions/44345576/why-are-almost-all-cosine-similarities-positive-between-word-or-document-vectors
Tensorflow tf.constant_initializer is very slow,"<p>Trying to use pre trained word2vec embeddings of 100 dim for training a LSTM </p>

<pre><code>@staticmethod
def load_embeddings(pre_trained_embeddings_path, word_embed_size):
    embd = []
    import time
    start_time = time.time()
    cnt = 4
    with codecs.open(pre_trained_embeddings_path, mode=""r"", encoding='utf-8') as f:
        for line in f.readlines():
            values = line.strip().split(' ')
            embd.append(values[1:])
            cnt += 1
            if cnt % 100000 == 0:
                print(""word-vectors loaded: %d"" % cnt)

    embedding, vocab_size, embed_dim = embd, len(embd), len(embd[0])

    load_end_time = time.time()
    print(""word vectors loaded from and start initialising, cnt: %d, time taken: %d secs "" % (vocab_size, load_end_time - start_time))

    embedding_init = tf.constant_initializer(embedding, dtype=tf.float16)
    src_word_embedding = tf.get_variable(shape=[vocab_size, embed_dim], initializer=embedding_init, trainable=False, name='word_embedding', dtype=tf.float16)

    print(""word-vectors loaded and initialised, cnt: %d, time taken: %d secs"" % (vocab_size, time.time() - load_end_time))

    return src_word_embedding
</code></pre>

<p>And the output of this when running this method is like : </p>

<pre><code>word vectors loaded from and start initialising, cnt: 2419080, time taken: 74 secs
word-vectors loaded and initialised, cnt: 2419080, time taken: 1647 secs
</code></pre>

<p>system info: <code>tensorflow 1.1.0, tcmalloc, python 3.6, ubuntu 14.04</code></p>

<p>HALF an hour to initialize seems to be very slow or is it a normal behavior ? Any idea what could be the issue or is there one ?</p>

<p>UPDATE: using @sirfz method of supplying the embeddings made it really fast to load the embeddings <code>Initialization Done in 85 secs</code></p>
","python, tensorflow, lstm, word2vec","<p>Loading large constants into a graph is not only slower, it also leaks lots of memory. I had a similar issue which <a href=""https://github.com/tensorflow/tensorflow/issues/9742"" rel=""nofollow noreferrer"">I reported not long ago</a> and the best workaround for me was:</p>

<pre><code># placeholder for loading your saved embeddings
embedding_init = tf.placeholder(tf.float16, shape=[vocab_size, embed_dim])
src_word_embedding = tf.get_variable(initializer=embedding_init, trainable=False, name='word_embedding', dtype=tf.float16)

# run initialization with the value of embeddings placeholder
session.run(tf.global_variables_initializer(), feed_dict={embedding_init: embedding})
</code></pre>
",0,4,2009,2017-06-04 11:20:57,https://stackoverflow.com/questions/44353509/tensorflow-tf-constant-initializer-is-very-slow
Spark word2vec example explanation and how to get similarity between strings,"<p>I followed the example in the Spark documentation page to use word2vec, <a href=""https://spark.apache.org/docs/2.1.0/ml-features.html#word2vec"" rel=""nofollow noreferrer"">link</a>. It worked but I didn't quite understand what it is trying to compute. </p>

<p>Are the output vectors the output strings representation?</p>

<p>If yes, I tried to compute the cosine similarity between them but I got negative values because the vectors are not positive. </p>

<p>Can Spark word2vec create positive only vectors?</p>

<p>How to compute similarity between a list of strings using Spark word2vec?</p>
","java, apache-spark, word2vec, cosine-similarity","<blockquote>
  <p>The output vector(by using transform on dataset) is a representation of the document(possibly sentence or sentences) which is supplied to the model .So; in essence this output is a combination of all the vector representation of each of the words in the given document(most likely a simple vector sum).</p>
  
  <p>You can use findSynonyms to get ""num"" number of words closest in similarity to the given word. findSynonyms is based on cosine similarity only.
     Currently I am using it to generate feature Vectors which I am using as input to  another model.</p>
  
  <p>In order to compute similarity between two strings as some kind of a no. you would need to implement some variation of findSynonyms method.The current implementation generates a cosVec corresponding to input string and then tries to find the word Vecs which are closest to this vec .</p>
  
  <p>I am not sure about the part whether it can create only positive vectors and whether it is at all required/(makes sense) to generate only positive vectors.</p>
</blockquote>
",1,0,1384,2017-06-05 09:53:41,https://stackoverflow.com/questions/44366249/spark-word2vec-example-explanation-and-how-to-get-similarity-between-strings
Importing word vectors from tensorflow into gensim,"<p>I want to import word vecters created from tensorflow and utilize it at gensim.</p>

<p>there is a method <code>gensim.models.KeyedVectors.load_word2vec_format</code></p>

<p>so I tried this method by following exactly the same way in <a href=""https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim"">Training wordvec in Tensorflow, importing to Gensim</a></p>

<p>Example:</p>

<blockquote>
  <p>2 3</p>
  
  <p>word0 -0.000737 -0.002106 0.001851</p>
  
  <p>word1 -0.000878 -0.002106 0.002834</p>
</blockquote>

<p>Save the file and then load with kwarg binary=False:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)
</code></pre>

<p>but error like</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    model=gensim.models.KeyedVectors.load_word2vec_format('test.w2v')
  File ""C:\Users\cbj\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 243, in load_word2vec_format
    raise EOFError(""unexpected end of input; is count incorrect or file otherwise damaged?"")
EOFError: unexpected end of input; is count incorrect or file otherwise damaged?
</code></pre>

<p>raised</p>

<p>how can I solve this problem?</p>
","python, tensorflow, gensim, word2vec, word-embedding","<p>This error is raised when the number of vector data doesn't match the number you provided at the first line.</p>

<p>If the first line wrote <code>2 3</code>, you should have exactly <code>2</code> lines below. Make sure that there's no empty line at the end of your file and of course some where in your file.</p>
",1,1,2488,2017-06-05 15:00:06,https://stackoverflow.com/questions/44371835/importing-word-vectors-from-tensorflow-into-gensim
Pairwise Earth Mover Distance across all documents (word2vec representations),"<p>Is there a library that will take a list of documents and en masse compute the nxn matrix of distances - where the word2vec model is supplied? I can see that genism allows you to do this between two documents - but I need a fast comparison across all docs. like sklearns cosine_similarity.</p>
","python, scikit-learn, distance, word2vec","<p>The ""Word Mover's Distance"" (earth-mover's distance applied to groups of word-vectors) is a fairly involved optimization calculation dependent on every word in each document. </p>

<p>I'm not aware of any tricks that would help it go faster when calculating many at once â€“ even many distances to the same document. </p>

<p>So the only thing needed to calculate pairwise distances are nested loops to consider each (order-ignoring unique) pairing. </p>

<p>For example, assuming your list of documents (each a list-of-words) is <code>docs</code>, a gensim word-vector model in <code>model</code>, and <code>numpy</code> imported as <code>np</code>, you could calculate the array of pairwise distances D with:</p>

<pre><code>D = np.zeros((len(docs), len(docs)))
for i in range(len(docs)):
    for j in range(len(docs)):
        if i == j:
            continue  # self-distance is 0.0
        if i &gt; j:
            D[i, j] = D[j, i]  # re-use earlier calc
        D[i, j] = model.wmdistance(docs[i], docs[j])
</code></pre>

<p>It may take a while, but you'll then have all pairwise distances in array D. </p>
",3,3,1256,2017-06-06 01:50:14,https://stackoverflow.com/questions/44380199/pairwise-earth-mover-distance-across-all-documents-word2vec-representations
Getting error while using gensim model in python,"<p>I have made doc2vec file by training data using gensim model now while processing it. I am getting an error.
I am running the below code:-</p>

<p>model = Doc2Vec.load('sentiment140.d2v')</p>

<pre><code>if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>I am getting a Keyerror - ""TEST_POS_72""<a href=""https://i.sstatic.net/z9TEW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z9TEW.png"" alt=""ERROR""></a></p>

<p>I want to know what I am doing wrong.</p>
","python, gensim, word2vec","<p>The error means quite literally that no doc-vector with the key ('tag') <code>TEST_POS_72</code> is part of the model. There mustn't have been any documents with that tag presented during training. </p>

<p>You can see a list of all known doc-tags in the model in <code>model.docvecs.offset2doctag</code>. If <code>TEST_POS_72</code> isn't there, you can't access a doc-vector via <code>model.docvecs['TEST_POS_72']</code>. (If that list is empty, then the doc-vectors were trained to be accessed by plain int keys â€“ and <code>model.docvecs[72]</code> would be a more appropriate way to access a doc-vector.)</p>

<p>(Separately, Doc2Vec won't work well with tiny corpuses of a few hundred documents, and the warning in your screenshot ""Slow version of gensim.models.doc2vec is being used"" means that gensim's optimized C-compiled routines weren't part of the installation, and training will be 100x or more slower.)</p>
",0,2,250,2017-06-09 04:05:11,https://stackoverflow.com/questions/44449132/getting-error-while-using-gensim-model-in-python
What method should I use to convert words into features for Machine Learning applications?,"<p>I am planning on building a gender classifier. I know the two popular models are tf-idf and word2vec. 
While tf-idf focuses on the importance of a word in a document and similarity of documents, word2vec focuses more on the relationship between words and similarity between them. </p>

<p>However none of theme seem to be perfect for building vector features to be used for gender classification. Is there any other alternative vectorization model that might suit this task? </p>
","python, nlp, tf-idf, word2vec","<p>Yes, there is another alternative to w2v: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">GloVe</a>.</p>

<p>GloVe stands for <em>Global Vector Embeddings</em>. 
As someone who has used this technique before to good effect, I would recommend GloVe. </p>

<p>GloVe optimally trains neural word embeddings not just by looking at local windows but considering a much larger width (30+ size), thereby embedding a much deeper level of semantics to the embedding.</p>

<p>With glove, it is easy to model relationships such as: <code>X[man] - X[woman] = X[king] - X[queen]</code>, where these are all vectors.</p>

<p><a href=""https://i.sstatic.net/AcZzd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AcZzd.png"" alt=""enter image description here""></a></p>

<p>Credits: GloVe GitHub page (linked below).</p>

<p>You can train your own GloVe embeddings, or you may use their retrained models available. Even for specific domains, the general models seem to work reasonably well, although you would get a lot more out of your models if you trained them yourself. Please look at the GitHub page for instructions on how to train your own models. It is very easy.</p>

<p>Additional reading:</p>

<ul>
<li><a href=""https://nlp.stanford.edu/pubs/glove.pdf"" rel=""nofollow noreferrer"">GloVe: Global Vectors for Word Representation</a></li>
<li><a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">GloVe repository</a> </li>
</ul>
",2,2,514,2017-06-11 22:32:22,https://stackoverflow.com/questions/44489357/what-method-should-i-use-to-convert-words-into-features-for-machine-learning-app
Does Word2Vec with high iterations work for very small toy datasets?,"<p>I'm trying to run Word2Vec first on a very tiny toy dataset that I made up by hand -- just to convince myself I'm doing it correctly before I go for my main dataset. But despite doing 99000 iterations the results weren't great. (Tiger and Lion didn't have as high a similarity as I thought they would). </p>

<p>Toy dataset:</p>

<pre><code>s= [['Tiger', 'Zebra'], ['Tiger', 'Lion', 'Cheetah'],
     ['Orangutan', 'Bonobo', 'Orangutan', 'Chimpanzee'],
     ['Dog', 'Cat', 'Mouse'], ['Tiger', 'Rhino'],
     ['House', 'Car'], ['Antelope', 'Gazelle'],
     ['Zebra', 'Horse'], ['Tiger', 'Lion', 'Leopard'],
     ['Cat', 'Mouse'], ['Mouse', 'Hampster', 'Gerbil'],
     ['Rhino', 'Zebra'], ['Zebra', 'Antelope'],
     ['Tiger', 'Lion'], ['Lion', 'Tiger', 'Giraffe'],
     ['Leopard', 'Lion'], ['Leopard', 'Tiger', 'Lion'],
     ['Tiger', 'Lion'], ['Tiger', 'Lion'],
     ['Car', 'Van'], ['Car', 'Lorry'],
     ['Car', 'Van'], ['Car', 'Lorry'],
     ['Car', 'Van'], ['Car', 'Lorry']
     ]
</code></pre>

<p>In theory should I expect a toy dataset like this to show amazing results if I did large amount of iterations?</p>

<p>Here is the code I'm using:</p>

<pre><code>model = gensim.models.Word2Vec(s, min_count=0, iter=iterations,size=100)
</code></pre>

<p>Ps. <a href=""https://stats.stackexchange.com/questions/215637/word2vec-vector-quality-vs-number-of-training-iterations/284803#284803"">See here</a> for related discussion.</p>
",word2vec,"<p>In my experience, Word2Vec does not work well on tiny or contrived datasets. Sometimes, more iterations (or making the model much smaller in <code>size</code> dimensionality) can eke out some hints of meaningfulness â€“ but nothing like the results from real multi-million-word training sets. </p>

<p>The true power of the algorithm relies on the balance-of-influences learned from large, diverse, naturally-varying text examples. </p>

<p>(As your synthetic dataset isn't even comprehensible language, I'm not sure what ""amazing results"" would be possible â€“ what's the generalizable patterns that these short, repetitive lists-of-animals should be teaching a model?)</p>
",2,1,712,2017-06-12 05:33:04,https://stackoverflow.com/questions/44492006/does-word2vec-with-high-iterations-work-for-very-small-toy-datasets
Do I still need to load word2vec model at model testing?,"<p>This may sound like a naive question, but i am quite new on this. Let's say I use the Google pre-trained word2vector model (<a href=""https://github.com/dav/word2vec"" rel=""nofollow noreferrer"">https://github.com/dav/word2vec</a>) to train a classification model. I save my classification model. Now I load back the classification model into memory for testing new instances. Do I need to load the Google word2vector model again? Or is it only used for training my model? </p>
","machine-learning, word2vec","<p>It depends on how your corpuses and test examples are structured and pre-processed. </p>

<p>You are probably using the pre-trained word-vectors to turn text into numerical features. At first, text examples are vectorized to train the classifier. Later, other (test/production) text examples will be vectorized in the same, and presented to get the classifier to get its judgements. </p>

<p>So you will need to use the same text-to-vectors process for test/production text examples as was used during training. Perhaps you've done that in a separate earlier bulk step, in which case you already have the features in the vector form the classifier uses. But often your classifier pipeline will itself take raw text, and vectorize it â€“ in which case it will need the same pre-trained (word)->(vector) mappings available at test time as were available during training.  </p>
",0,0,66,2017-06-13 07:11:51,https://stackoverflow.com/questions/44514609/do-i-still-need-to-load-word2vec-model-at-model-testing
word prediction with rnn using word2vec,"<p>I'm trying to predict word with recurrent neural network.
I'm training network by putting independently pre-trained <code>word2vec</code> of words as input.</p>

<p>And I wonder if I can use <code>word2vec</code> of target word to calculate error cost.
It seems not working and I've never seen such examples or papers.
Is it possible to use word2vec as a target value for calculating error cost?
If so, what kind of cost function should I use?
If not, please explain the reason mathematically.</p>

<p>And how should I set input and target?
Now I'm using architecture like below :</p>

<pre><code>input : word1, word2, word3, target : word4
input : word1, word2, word3, word4, target : word5
</code></pre>

<p>Maybe I can use another option like :</p>

<pre><code>input : word1, word2 target : word2, word3
input : word1, word2, word3, target : word2, word3, word4
</code></pre>

<p>Which one is better? Or is there another option?</p>

<p>If there's any reference let me know.</p>
","nlp, recurrent-neural-network, word2vec","<p>The prediction is usually made through an output softmax layer that gives the probabilities for all words in the vocabulary.</p>

<p>However a recent paper suggests tying the input word vectors with the output word classifiers and training them end-to-end. This significantly reduces the number of parameters. <a href=""https://arxiv.org/abs/1611.01462"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1611.01462</a></p>

<p>With regards to architectures, atleast for training I would prefer the second option since the first one loses information about the second and third word that can also be used for training.</p>
",0,0,606,2017-06-13 14:39:12,https://stackoverflow.com/questions/44524630/word-prediction-with-rnn-using-word2vec
Bring Word2Vec models efficiently into Production Service,"<p>This is kind of a long shot, but I am hoping that someone has been in a similar situation as I am looking for some advice how to efficiently bring a set of large word2vec models into a production environment.</p>

<p>We have a range of trained w2v models with a dimensionality of 300. Due to the underlying data - huge corpus with POS tagged words; specialized vocabularies with up to 1 mio words - these models became quite large and we are currently looking into effective ways how to expose these to our users w/o paying a too high price in infrastructure.</p>

<p>Besides trying to better control the vocabulary size, obviously, dimensionality reduction on the feature vectors would be an option. Is anyone aware of publications around that, particularly on how this would affect model quality, and how to best measure this?</p>

<p>Another option is to pre-calculate the top X most similar words to each vocabulary word and to provide a lookup table. With the model size being that big, this is currently also very inefficient. Are there any heuristics known that could be used reduce the number of necessary distance calculations from n x n-1 to a lower number? </p>

<p>Thank you very much!</p>
","performance, production, data-science, word2vec, dimensionality-reduction","<p>There are pre-indexing techniques for similarity-search in high-dimensional spaces which can speed nearest-neighbor discovery, but usually at a cost of absolute accuracy. (They also need more memory for the index.)</p>

<p>An example is the <a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">ANNOY library</a>. The gensim project includes a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/annoytutorial-text8.ipynb"" rel=""nofollow noreferrer"">demo notebook showing its use with Word2Vec</a>.</p>

<p>I once did some experiments using just 16-bit (rather than 32-bit) floats in a Word2Vec model. It saved memory in the idle state, and nearest-neighbor top-N results were nearly unchanged. But, perhaps because some behind-the-scenes up-conversion to 32-bit floats was still occurring during the one-against-all distance-calculations, speed of operations was actually reduced. (And this suggests that each distance-calculation may have caused a temporary memory expansion offsetting any idle-state savings.) So it's not a quick fix, but further research here â€“ perhaps involving finding/implementing the right routines for float16 array operations â€“ could maybe mean 50% model-size savings and equivalent or even better speed. </p>

<p>For many applications, discarding the least-frequent words doesn't hurt much â€“ or even, when done before training, can improve the quality of the remaining vectors. As many implementations, including gensim, sort the word-vector array in most-to-least-frequent order, you can discard the tail-end of the array to save memory, or limit <code>most_similar()</code> searches to the first-N entries to speed calculations. </p>

<p>Once you've minimized the vocabulary size, you want to be sure the full set is in RAM, and no swapping is triggered during the (typical) full-sweep distance-calculations. If you need multiple processes to serve answers from the same vector set, as in a web service on a multicore machine, gensim's memory-mapping operations can prevent each process from loading its own redundant copy of the vectors. You can see a discussion of this technique in this <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">answer about speeding gensim Word2Vec loading time</a>. </p>

<p>Finally, while precomputing top-N neighbors for a larger vocabulary is both time-consuming and memory-intensive, if your pattern of access is such that some tokens are checked far more than others, a cache of the N most-recently or M most-frequently requested top-N could improve perceived performance a lot â€“ making only less-frequently-requested neighbor-lists require the full distance calculations to every other token. </p>
",1,4,1277,2017-06-14 15:28:26,https://stackoverflow.com/questions/44548920/bring-word2vec-models-efficiently-into-production-service
How do I use the word vector returned by word2vec as features?,"<p>I am planning to use Multi Layer Perceptron Classifier from Scikit Learn for this purpose.<br>
Output is the Gender of that word which shall be represented in a one-hot encoding like [1,0,0] for male, [0, 1, 0] for female and [0, 0, 1] for female. 
Now one of the inputs is the word vector for the word. Each of these vectors has 20 dimensions.
The other features are it's Part Of Speech Tags and Singularity(0)/Plurality(1) state. 
My question is how do I use the word vector which is an array as a feature in MLPClassifier?</p>
","python, scikit-learn, neural-network, gensim, word2vec","<p>Your w2v vector captures some semantic similarity with respect to the word. This vector must be considered a whole - it is a feature in itself. </p>

<p>One nice attribute of neural networks are their capability of extracting and learning patterns on their own. As input, you could consider concatenating the word vector along with a vectorised/numerical equivalent of the POS tag, and finally the singularity state:</p>

<pre><code>------------------- ----  -   
\_________________/ \__/  |     } ------ 25d vector input to the MLP (assuming your POS takes 4 spaces)
     w2v vector      POS state
</code></pre>

<p>As long as you follow a consistent scheme with the training, testing, and unseen data, your MLP will use the entire input to automatically extract features from the input as it learns.   </p>
",1,0,800,2017-06-14 19:34:58,https://stackoverflow.com/questions/44553278/how-do-i-use-the-word-vector-returned-by-word2vec-as-features
How can I improve the cosine similarity of two documents(sentences) in doc2vec model?,"<p>I am building a NLP chat application in Python using <code>gensim</code> library through <code>doc2vec</code> model. I have hard coded documents and given a set of training examples, I am testing the model by throwing a user question and then finding most similar documents as a first step. In this case my test question is an exact copy of a document from training example. </p>

<pre><code>import gensim
from gensim import models
sentence = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'tell',u'me',u'about'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'what',u'is',u'my',u'limit',u'how',u'much',u'can',u'I',u'claim'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'I',u'am',u'retiring',u'how',u'much',u'can',u'claim',u'have', u'resigned'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'what',u'is',u'my',u'eligibility',u'post',u'my',u'promotion'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'what',u'is', u'my',u'eligibility' u'post',u'my',u'promotion'], tags=[""SENT_4""])
sentences = [sentence, sentence1, sentence2, sentence3, sentence4]
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])
model = models.Doc2Vec(alpha=0.03, min_alpha=.025, min_count=2)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)
    model.alpha -= 0.002  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model_loaded.docvecs.most_similar([""SENT_4""]))
</code></pre>

<p>Result:</p>

<pre><code>[('SENT_1', 0.043695494532585144), ('SENT_2', 0.0017897281795740128), ('SENT_0', -0.018954679369926453), ('SENT_3', -0.08253869414329529)]
</code></pre>

<p>Similarity of <code>SENT_4</code> and <code>SENT_3</code> is only <code>-0.08253869414329529</code> when it should be 1 since they are exactly same. How should I improve this accuracy? Is there a specific way of training documents and I am missing something out?  </p>
","python, nlp, gensim, word2vec, doc2vec","<p>Word2Vec/Doc2Vec don't work well on toy-sized examples (such as few texts, short texts, and few total words). Many of the desirable properties are only reliably achieved with training sets of millions of words, or tens-of-thousands of documents. </p>

<p>In particular, with only 5 examples, and only a dozen or two words, but 100-dimensions of modeling vectors, the training isn't forced to do the main thing which makes word-vectors/doc-vectors useful: compress representations into dense embeddings, where similar items <em>need</em> to be incrementally nudged near each other in vector space, because there's no way to retain all the original variation in a sort-of-giant-lookup-table.  With more dimensions than corpus variation, your identical-tokens <code>SENT_3</code> and <code>SENT_4</code> can adopt wildly different doc-vectors, and the model is still large enough to do great on its training task (essentially, 'overfit'), without the desired end-state of similar-texts having similar-vectors being forced. </p>

<p>You can sometimes squeeze a little more meaning out of small datasets with more training iterations, and a much-smaller model (in terms of vector <code>size</code>), but really: these vectors need large, varied datasets to become meaningful.</p>

<p>That's the main issue. Some other inefficiencies or errors in your example code:</p>

<ul>
<li><p>Your code doesn't use the class <code>LabeledLineSentence</code>, so there's no need to include it here â€“ it's irrelevant boilerplate. (Also, <code>TaggedDocument</code> is the preferred name for the <code>words</code>+<code>tags</code> document class in recent gensim versions, rather than <code>LabeledSentence</code>.)</p></li>
<li><p>Your custom-management of <code>alpha</code> and <code>min_alpha</code> is unlikely to do anything useful. These are best left at their defaults unless you already have something working, understand the algorithm well, and then want to try subtle optimizations. </p></li>
<li><p><code>train()</code> will do its own iterations, so you don't need to call it many times in an outer loop. (This code as written does in its first loop 5 <code>model.iter</code> iterations at <code>alpha</code> values gradually descending from 0.03 to 0.025, then 5 iterations at a fixed alpha of 0.028, then 5 more at 0.026, then 27 more sets of 5 iterations at decreasing alpha, ending on the 30th loop at a fixed alpha of -0.028. That's a nonsense ending value â€“ the learning-rate should never be negative â€“ at the end of a nonsense progression. Even with a big dataset, these 150 iterations, about half happening at negative <code>alpha</code> values, would likely yield weird results.)</p></li>
</ul>
",1,1,2362,2017-06-16 06:07:00,https://stackoverflow.com/questions/44581914/how-can-i-improve-the-cosine-similarity-of-two-documentssentences-in-doc2vec-m
Why does word2vec only take one task for mapPartitionsWithIndex at Word2Vec.scala:323,"<p>I am running <em>word2vec</em> in spark and when it comes to <code>fit()</code>, only one task is observed in UI as in image:</p>

<p><img src=""https://i.sstatic.net/nYV3g.png"" alt=""enter image description here"">.</p>

<p>As per the configuration, <code>num-executors = 1000, executor-cores = 2</code>. And the RDD coalesces to 2000 partitions. It takes quite a long time for <code>mapPartitionsWithIndex</code>. Can it be distributed to multiple executors or tasks?</p>
","scala, apache-spark, apache-spark-mllib, word2vec","<p><code>setNumPartitions(numPartitions: Int)</code> solves my problem. I did not check the default value.
Sets number of partitions (default: 1).</p>
",7,4,668,2017-06-16 07:38:06,https://stackoverflow.com/questions/44583529/why-does-word2vec-only-take-one-task-for-mappartitionswithindex-at-word2vec-scal
Loading pre-trained word2vec to initialise embedding_lookup in the Estimator model_fn,"<p>I am solving a text classification problem. I defined my classifier using the <code>Estimator</code> class with my own <code>model_fn</code>. I would like to use Google's pre-trained <code>word2vec</code> embedding as initial values and then further optimise it for the task at hand. </p>

<p>I saw this post: <a href=""https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow"">Using a pre-trained word embedding (word2vec or Glove) in TensorFlow</a><br>
which explains how to go about it in 'raw' TensorFlow code. However, I would really like to use the <code>Estimator</code> class. </p>

<p>As an extension, I would like to then train this code on Cloud ML Engine, is there a good way of passing in the fairly large file with initial values?  </p>

<p>Let's say we have something like:</p>

<pre><code>def build_model_fn():
    def _model_fn(features, labels, mode, params):
        input_layer = features['feat'] #shape=[-1, params[""sequence_length""]]
        #... what goes here to initialize W

        embedded = tf.nn.embedding_lookup(W, input_layer)
        ...
        return predictions

estimator = tf.contrib.learn.Estimator(
    model_fn=build_model_fn(),
    model_dir=MODEL_DIR,
    params=params)
estimator.fit(input_fn=read_data, max_steps=2500)
</code></pre>
","tensorflow, word2vec, google-cloud-ml-engine","<p>Embeddings are typically large enough that the only viable approach is using them to initialize a <code>tf.Variable</code> in your graph. This will allow you to take advantage of param servers in distributed, etc. </p>

<p>For this (and anything else), I would recommend you use the new ""core"" estimator, <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a> as this will make things much easier.</p>

<p>From the answer in the link you provided, and knowing that we want a variable not a constant, we can either take approach:</p>

<p>(2) Initialize the variable using a feed dict, or
 (3) Load the variable from a checkpoint</p>

<hr>

<p>I'll cover option (3) first since it's much easier, and better:</p>

<p>In your <code>model_fn</code>, simply initialize a variable using the <code>Tensor</code> returned by a <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/framework/load_variable"" rel=""nofollow noreferrer""><code>tf.contrib.framework.load_variable</code></a> call. This requires:</p>

<ol>
<li>That you have a valid TF checkpoint with your embeddings</li>
<li>You know the <em>fully qualified</em> name of the embeddings variable within the checkpoint.</li>
</ol>

<p>The code is pretty simple:</p>

<pre><code>def model_fn(mode, features, labels, hparams):
  embeddings = tf.Variable(tf.contrib.framework.load_variable(
      'gs://my-bucket/word2vec_checkpoints/',
      'a/fully/qualified/scope/embeddings'
  ))
  ....
  return tf.estimator.EstimatorSpec(...)
</code></pre>

<p>However this approach won't work for you if your embeddings weren't produced by another TF model, hence option (2).</p>

<hr>

<p>For (2), we need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Scaffold"" rel=""nofollow noreferrer""><code>tf.train.Scaffold</code></a> which is essentially a configuration object that holds all the options for starting a <code>tf.Session</code> (which estimator intentionally hides for lots of reasons).</p>

<p>You may specify a <code>Scaffold</code> in the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec"" rel=""nofollow noreferrer""><code>tf.train.EstimatorSpec</code></a> you return in your <code>model_fn</code>. </p>

<p>We create a placeholder in our model_fn, and make it the 
initializer operation for our embedding variable, then pass an <code>init_feed_dict</code> via the <code>Scaffold</code>.  e.g.</p>

<pre><code>def model_fn(mode, features, labels, hparams):
  embed_ph = tf.placeholder(
      shape=[hparams.vocab_size, hparams.embedding_size], 
      dtype=tf.float32)
  embeddings = tf.Variable(embed_ph)
  # Define your model
  return tf.estimator.EstimatorSpec(
      ..., # normal EstimatorSpec args
      scaffold=tf.train.Scaffold(init_feed_dict={embed_ph: my_embedding_numpy_array})
  )
</code></pre>

<p>What's happening here is the <code>init_feed_dict</code> will populate the values of the <code>embed_ph</code> placeholder at runtime, which will then allow the <code>embeddings.initialization_op</code> (assignment of the placeholder), to run.</p>

<hr>
",9,5,2106,2017-06-21 15:46:10,https://stackoverflow.com/questions/44680769/loading-pre-trained-word2vec-to-initialise-embedding-lookup-in-the-estimator-mod
How to extract a word vector from the Google pre-trained model for word2vec?,"<p>The file <code>GoogleNews-vectors-negative300.bin</code> contains 300 million word-vectors. I think (not sure) this file is loaded when the following line is written:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
</code></pre>

<p>I want to download the vectors for words that I give externally in a list called <code>words</code>. This is my code to do this:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport', 'amenity', 'area', 'atm', 'barrier', 'bay', 'bench', 'boundary', 'bridge', 'building', 'bus', 'cafe', 'car', 'coast', 'continue', 'created', 'defibrillator', 'drinking', 'ele', 'embankment', 'entrance', 'ferry', 'foot', 'fountain', 'fuel', 'gate', 'golf', 'gps', 'grave', 'highway', 'horse', 'hospital', 'house', 'landuse', 'layer', 'leisure', 'man', 'manmade', 'market', 'marketplace', 'maxheight', 'name', 'natural', 'noexit', 'oneway', 'park', 'parking', 'pgs', 'place', 'worship', 'playground', 'police', 'police station', '', 'post', 'post box or mail', 'power', 'powerstation', 'private', 'public', 'railway', 'ref', 'residential', 'restaurant', 'road', 'route', 'school', 'shelter', 'shop', 'source', 'sport', 'toilet', 'toilets', 'tourism', 'unknown', 'vehicle', 'vending', 'vending machine', 'village', 'wall', 'waste', 'water', 'waterway', 'worship'];

model = gensim.models.KeyedVectors.load_word2vec_format(words, binary=True)

M = len(words)
count = 0
for i in range(1,M):
    wi = id2word[words[i]]
    if wi in word2vec.vocab:
        vector[:,count] = model[:,i]
        count = count+1

f = open('word_vectors.csv', 'w')
print(vector, file=f)
f.close()
</code></pre>

<p>But when I run the code, it just freezes up my system. Is it because it is loading the whole of the binary file before searching for the words in <code>words</code>? If yes, how do I get around this issue? I think of this as I get the following warning, which is why I use the <code>warning</code> package to suppress it:</p>

<pre><code>c:\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
</code></pre>

<p>And the error it gives is:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vec.py"", line 18, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format(topic, binary=True) 
  File ""c:\Python35\lib\site-packages\gensim\models\keyedvectors.py"", line 196, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""c:\Python35\lib\site-packages\smart_open\smart_open_lib.py"", line 208, in smart_open
    raise TypeError('don\'t know how to handle uri %s' % repr(uri))
TypeError: don't know how to handle uri [['access'], ['aeroway'], ['airport'], ['amenity'], ['area'], ['atm'], ['barrier'], ['bay'], ['bench'], ['boundary'], ['bridge'], ['building'], ['bus'], ['cafe'], ['car'], ['coast'], ['continue'], ['created'], ['defibrillator'], ['drinking'], ['ele'], ['embankment'], ['entrance'], ['ferry'], ['foot'], ['fountain'], ['fuel'], ['gate'], ['golf'], ['gps'], ['grave'], ['highway'], ['horse'], ['hospital'], ['house'], ['landuse'], ['layer'], ['leisure'], ['man'], ['manmade'], ['market'], ['marketplace'], ['maxheight'], ['name'], ['natural'], ['noexit'], ['oneway'], ['park'], ['parking'], ['pgs'], ['place'], ['worship'], ['playground'], ['police'], ['police station'], [''], ['post'], ['post box or mail'], ['power'], ['powerstation'], ['private'], ['public'], ['railway'], ['ref'], ['residential'], ['restaurant'], ['road'], ['route'], ['school'], ['shelter'], ['shop'], ['source'], ['sport'], ['toilet'], ['toilets'], ['tourism'], ['unknown'], ['vehicle'], ['vending'], ['vending machine'], ['village'], ['wall'], ['waste'], ['water'], ['waterway'], ['worship']]
</code></pre>

<p>This I guess means that the program is not able to search for the words in the binary file. So, how to solve it? </p>
","python, file-handling, gensim, word2vec","<p>Use the following code to extract the word vector from the Google trained model for word2vec:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

# this line doesn't load the trained model 
from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport']

# this is how you load the model
model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)

# to extract word vector
print(model[words[0]])  #access
</code></pre>

<p>Result vector:</p>

<pre><code>[ -8.74023438e-02  -1.86523438e-01 .. ]
</code></pre>

<p>Your system is freezing because of the large size of model. Try using system with more memory or you can limit the size of model you are loading. </p>

<p><strong>Limit model size while loading</strong></p>

<pre><code>model = KeyedVectors.load_word2vec_format(path_to_model, binary=True, limit=20000)
</code></pre>
",7,2,4014,2017-06-22 07:44:56,https://stackoverflow.com/questions/44693241/how-to-extract-a-word-vector-from-the-google-pre-trained-model-for-word2vec
How to remove stop words from documents in gensim?,"<p>I'm building a NLP chat application using Doc2Vec technique in Python using its <code>gensim</code> package. I have already done tokenizing and stemming. I want to remove the stop words (to test if it works better) from both the training set as well as the question which user throws. </p>

<p>Here is my code.</p>

<pre><code>import gensim
import nltk
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampl',u'what',u'is'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampl',u'tell',u'me',u'about'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'i',u'am',u'how',u'much',u'can',u'elig',u'claim'],tags=[""SENT_5""])
sentence6 = models.doc2vec.LabeledSentence(words=[u'resign',u'i',u'have',u'how',u'much',u'can',u'i',u'claim',u'elig'],tags=[""SENT_6""])
sentence7 = models.doc2vec.LabeledSentence(words=[u'promot',u'what',u'is',u'my',u'elig',u'post',u'my'],tags=[""SENT_7""])
sentence8 = models.doc2vec.LabeledSentence(words=[u'claim',u'can,',u'i',u'for'],tags=[""SENT_8""])
sentence9 = models.doc2vec.LabeledSentence(words=[u'product',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_9""])
sentence10 = models.doc2vec.LabeledSentence(words=[u'hotel',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_10""])
sentence11 = models.doc2vec.LabeledSentence(words=[u'onlin',u'product',u'can',u'i',u'for',u'bought',u'through',u'claim',u'sampl'],tags=[""SENT_11""])
sentence12 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'guidelin',u'where',u'do',u'i',u'apply',u'form',u'sampl'],tags=[""SENT_12""])
sentence13 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'procedur',u'rule',u'and',u'regul',u'what',u'is',u'the',u'for'],tags=[""SENT_13""])
sentence14 = models.doc2vec.LabeledSentence(words=[u'can',u'i',u'submit',u'expenditur',u'on',u'behalf',u'of',u'my',u'friend',u'and',u'famili',u'claim',u'and',u'reimburs'],tags=[""SENT_14""])
sentence15 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'procedur',u'can',u'i',u'submit',u'from',u'shopper stop',u'claim'],tags=[""SENT_15""])
sentence16 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'can',u'i',u'submit',u'from',u'pantaloon',u'claim'],tags=[""SENT_16""])
sentence17 = models.doc2vec.LabeledSentence(words=[u'invoic',u'procedur',u'can',u'i',u'submit',u'invoic',u'from',u'spencer',u'claim'],tags=[""SENT_17""])

# User asks a question.

document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
#print(type(tokenized_document))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))
sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

# Building vocab.
sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence19]

#I tried to remove the stop words but it didn't work out as LabeledSentence object has no attribute lower.
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
          for document in sentences]
..
</code></pre>

<p>Is there a way I can remove stop words from <code>sentences</code> directly and get a new set of vocab without stop words ?</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Your <code>sentences</code> object is a already a list of <code>LabeledSentence</code> objects. You construct these above; they include a list-of-strings in <code>words</code> and a list-of-strings in <code>tags</code>. </p>

<p>So each item in that list (<code>document</code> in your list-comprehension) can't have a string method like <code>.lower()</code> applied to it. (Nor would it need to be <code>.split()</code>, as its <code>words</code> are already separate tokens.)</p>

<p>The cleanest approach would be to remove stop-words from the lists-of-words <em>before</em> they're used to construct <code>LabeledSentence</code> objects. For example, you could make a function <code>without_stopwords()</code>, defined at the top. Then your lines creating <code>LabeledSentence</code> objects could instead be like:</p>

<pre><code>sentence0 = LabeledSentence(words=remove_stopwords([u'sampl', u'what', u'is']), 
                            tags=[""SENT_0""])
</code></pre>

<p>Alternatively, you could <em>mutate</em> the existing <code>LabeledSentence</code> objects so that each of their <code>words</code> attributes now lack stop-words. This would replace your last line with something more like:</p>

<pre><code>for doc in sentences:
    doc.words = [word for word in doc.words if word not in stoplist]
texts = sentences
</code></pre>

<p>Separately, things you didn't ask but should know:</p>

<ul>
<li><p><code>TaggedDocument</code> is now the preferred example-class name for Doc2Vec text objects â€“ but in fact any object that has the two required properties <code>words</code> and <code>tags</code> will work fine.</p></li>
<li><p>Doc2Vec doesn't show many of the desired properties on tiny, toy-sized datasets â€“ don't be surprised if a model built on dozens of sentences does not do anything useful, or misleads about what preprocessing/meta-parameter options are best. (Tens of thousands of texts, and texts at least tens-of-words long, are much better for meaningful results.)</p></li>
<li><p>Much Word2Vec/Doc2Vec work doesn't bother with stemming or stop-word removal, but it may sometimes be helpful.</p></li>
</ul>
",4,1,8646,2017-06-22 12:02:19,https://stackoverflow.com/questions/44698910/how-to-remove-stop-words-from-documents-in-gensim
How does the tensorflow word2vec tutorial update embeddings?,"<p>This thread comes close: <a href=""https://stackoverflow.com/questions/37982478/what-is-the-purpose-of-weights-and-biases-in-tensorflow-word2vec-example?noredirect=1&amp;lq=1"">What is the purpose of weights and biases in tensorflow word2vec example?</a> </p>

<p>But I am still missing something from my interpretation of this: <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>

<p>From what I understand, you feed the network the indices of target and context words from your dictionary.</p>

<pre><code>_, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
average_loss += loss_val
</code></pre>

<p>The batch inputs are then looked up to return the vectors that are randomly generated at the beginning</p>

<pre><code>    embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_inputs)
</code></pre>

<p>Then an optimizer adjusts the weights and biases to best predict the label as opposed to num_sampled random alternatives</p>

<pre><code> loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))

  # Construct the SGD optimizer using a learning rate of 1.0.
  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
</code></pre>

<p>My questions are as follows:</p>

<ol>
<li><p><strong>Where do the embeddings variable get updated?</strong>. It appears to me that I could get the final result by either running the index of a word through the neural network, or by just taking the <code>final_embeddings</code> vectors and using that. But I do not understand where <code>embeddings</code> is ever changed from its random initialization.</p></li>
<li><p>If I were to draw this computation graph, what would it look like (or better yet, <strong>what is the best way to actually do so</strong>)?</p></li>
<li><p>Is this running all of the context/target pairs in the batch at once? Or one by one?</p></li>
</ol>
","python, tensorflow, word2vec, word-embedding","<p><strong>Embeddings:</strong> Embeddings is a variable. It gets updated every time you do backprop (while running optimizer with loss)</p>

<p><strong>Grpah:</strong> Did you try saving the graph and displaying it in tensorboard ? Is this what you're looking for ?</p>

<p><strong>Batching:</strong> Atleast in the example you linked, he is doing batch processing using the function at line 96. <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L96"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L96</a></p>

<p>Please correct me if I misunderstood your question. </p>
",2,2,508,2017-06-22 16:22:32,https://stackoverflow.com/questions/44704751/how-does-the-tensorflow-word2vec-tutorial-update-embeddings
Word2vec saved model is not UTF-8 encoded but the sentence input to the Word2vec model is UTF-8 encoded,"<p>I trained a word2vec model using gensim package and saved it with the following name. </p>

<pre><code>model_name = ""300features_1minwords_10context""
model.save(model_name)
</code></pre>

<p>I got these log message info. while the model was getting trained and saved.</p>

<pre><code>INFO : not storing attribute syn0norm
INFO : not storing attribute cum_table
</code></pre>

<p>Then, I tried to load the model using this, </p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(""300features_1minwords_10context"")
</code></pre>

<p>I got the following error. </p>

<pre><code>2017-06-22 21:27:14,975 : INFO : loading Word2Vec object from 300features_1minwords_10context
2017-06-22 21:27:15,496 : INFO : loading wv recursively from 300features_1minwords_10context.wv.* with mmap=None
2017-06-22 21:27:15,497 : INFO : setting ignored attribute syn0norm to None
2017-06-22 21:27:15,498 : INFO : setting ignored attribute cum_table to None
2017-06-22 21:27:15,499 : INFO : loaded 300features_1minwords_10context
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-9d90db0f07c0&gt; in &lt;module&gt;()
      1 from gensim.models import Word2Vec
      2 model = Word2Vec.load(""300features_1minwords_10context"")
----&gt; 3 model.syn0.shape

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Also, in the file ""300features_1minwords_10context"", it shows that </p>

<pre><code>""300features_1minwords_10context"" is not UTF-8 encoded
Saving disabled.
Open console for more details 
</code></pre>

<p>To fix the above attribute error, I have also tried the following from the google forum, </p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(""300features_1minwords_10context"")
model.syn0.shape
</code></pre>

<p>It resulted in another error which is </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The model is trained with UTF-8 encoded sentences. I am not sure why is it throwing this error ?</p>

<p>More info : </p>

<pre><code>df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)
features = ['MaterialDescription']
temp_features = df[features]
temp_features.to_csv('materialDescription', encoding='UTF-8')
X = pd.read_csv('materialDescription',encoding='UTF-8')
</code></pre>

<p>Here, I had to use 'mac_roman' encoding in order to access it using pandas dataframe. Since the text in the dataframe has to be in UTF-8 while training the model, I have saved that particular feature in a separate csv file by encoding it with UTF-8 and later, I have the accessed that particular column.</p>

<p>Any help is appreciable </p>
","python-3.x, utf-8, nlp, gensim, word2vec","<p>Are you using the latest gensim? If not, be sure to try it â€“ there have sometimes been <code>save()</code>/<code>load()</code> bugs in older versions. </p>

<p>The INFO ""not storing"" log lines are normal â€“ they're not indicative of any problem (and thus could be deleted from your question.) </p>

<p>Are you getting the ""has no attribute"" error directly upon the <code>load()</code>? (A full error stack here would be useful, and clarify this.)</p>

<hr>

<p><strong>UPDATE:</strong> From the now-shown error-stack, the error is <em>not</em> occurring in the <code>load()</code> line, but on the following line, when you attempt to access <code>model.syn0.shape</code>. Recent versions of gensim no longer have a <code>syn0</code> as a property of Word2Vec class objects â€“ that info is moved to a constituent <code>KeyedVectors</code> object, in the <code>wv</code> property. So <code>model.wv.syn0.shape</code> is likely to access what you're seeking, without an error. </p>

<hr>

<p>When your model is largish, <code>save()</code> can generate multiple files on the side, with extra extensions, for the model's large array properties (like <code>syn0</code>). These files must be kept alongside the main filename for the model to be re-<code>load()</code>ed. Is it possible you've moved the <code>300features_1minwords_10context</code> file, but not any such accompanying files, to a new location where the <code>load()</code> is then incomplete? </p>

<p>You can't <code>load_word2vec_format()</code> a file that was native-gensim <code>save()</code>d â€“ their different formats entirely, so the encoding error is just an artifact of trying to read a binary Python pickle file (from <code>save()</code>) as another format entirely. </p>
",2,1,2064,2017-06-22 23:06:43,https://stackoverflow.com/questions/44710644/word2vec-saved-model-is-not-utf-8-encoded-but-the-sentence-input-to-the-word2vec
how to preserve number of records in word2vec?,"<p>I have 45000 text records in my dataframe. I wanted to convert those 45000 records into word vectors so that I can train a classifier on the word vector. I am not tokenizing the sentences. I just split the each entry into list of words.</p>

<p>After training word2vec model with 300 features, the shape of the model resulted in only 26000. How can I preserve all of my 45000 records ? </p>

<p>In the classifier model, I need all of those 45000 records, so that it can match 45000 output labels.</p>
","python-3.x, nlp, word2vec","<p>If you are splitting each entry into a list of words, that's essentially 'tokenization'. </p>

<p>Word2Vec just learns vectors for each word, not for each text example ('record') â€“ so there's nothing to 'preserve', no vectors for the 45,000 records are ever created. But if there are 26,000 unique words among the records (after applying <code>min_count</code>), you will have 26,000 vectors at the end. </p>

<p>Gensim's Doc2Vec (the '
Paragraph Vector' algorithm) can create a vector for each text example, so you may want to try that. </p>

<p>If you only have word-vectors, one simplistic way to create a vector for a larger text is to just add all the individual word vectors together. Further options include choosing between using the unit-normed word-vectors or raw word-vectors of many magnitudes; whether to then unit-norm the sum; and whether to otherwise weight the words by any other importance factor (such as TF/IDF). </p>

<p>Note that unless your documents are very long, this is a quite small training set for either Word2Vec or Doc2Vec. </p>
",2,0,357,2017-06-24 19:25:27,https://stackoverflow.com/questions/44740161/how-to-preserve-number-of-records-in-word2vec
converting from a list to dictionary for a word2vec model,"<p>i have a huge corpus of data in my my text file that i want to train for skip gram model.
i have split  the data from file into list
now i want to count the words with their number of occurrence and make a dictionary ,give the word as key to the dictionary and frequency as the value.here is a snippet of my code</p>

<pre><code>with open(""enwik8"",""r"") as data:
    words=data.read().split()   

vocabulary_size = 5000


  count = [['UNK', -1]]
  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
</code></pre>

<p>i have succesfully made a list with the words and their frequency upto first most common 50000 words,now i need to feed them to dictionary,key as a word and value as freq.</p>

<pre><code>dictionary = dict()
for word, _ in count:
</code></pre>

<p>can anyone help me through??</p>
","python, dictionary, nlp, word2vec","<p>Assuming you have already a list of words, here is how you draw dictionary out of it as per your need:</p>

<pre><code>word_dict = dict()
for word_count in words:
    if word_count[0] not in word_dict:
        word_dict[word_count[0]] = word_count[1]
</code></pre>

<p>your list contains tuples, so <code>word_dict[word_count[0]]</code>, so I am placing first item of tuple that is word as a <code>key</code> in dictionary and second item <code>word_count[1]</code> in tuple which is count as <code>value</code> to that <code>key</code></p>
",1,0,506,2017-06-26 08:53:31,https://stackoverflow.com/questions/44756052/converting-from-a-list-to-dictionary-for-a-word2vec-model
Determining canonical classes with text data,"<p>I have a unique problem and I'm not aware of any algorithm that can help me. Maybe someone on here does.</p>

<p>I have a dataset compiled from many different sources (teams). One field in particular is called ""type"". Here are some example values for type:</p>

<blockquote>
  <p>aple, apples, appls, ornge, fruits, orange, orange z, pear,
  cauliflower, colifower, brocli, brocoli, leeks, veg, vegetables.</p>
</blockquote>

<p>What I would like to be able to do is to group them together into e.g. fruits, vegetables, etc.</p>

<p>Put another way I have multiple spellings of various permutations of a parent level variable (fruits or vegetables in this example) and I need to be able to group them as best I can.</p>

<p>The only other potentially relevant feature of the data is the team that entered it, assuming some consistency in the way each team enters their data.</p>

<p>So, I have several million records of multiple spellings and short spellings (e.g. apple, appls) and I want to group them together in some way. In this example by fruits and vegetables.</p>

<p>Clustering would be challenging since each entry is most often 1 or two words, making it tricky to calculate a distance between terms.</p>

<p>Short of creating a massive lookup table created by a human (not likely with millions of rows), is there any approach I can take with this problem?</p>
","cluster-analysis, text-mining, word2vec","<p>You will need to first solve the spelling problem, unless you have Google scale data that could allow you to learn fixing spelling with Google scale statistics.</p>

<p>Then you will still have the problem that ""Apple"" could be a fruit or a computer. Apple and ""Granny Smith"" will be completely different. You best guess at this second stage is something like word2vec trained on <em>massive</em> data. Then you get high dimensional word vectors, and can finally try to solve the clustering challenge, if you ever get that far with decent results. Good luck.</p>
",1,-1,28,2017-06-27 06:33:47,https://stackoverflow.com/questions/45300866/determining-canonical-classes-with-text-data
"Will Word2Vec be more efficient in text based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText etc?","<p>I am a beginner in learning Word2Vec and just started to do some study on Word2vec from the Internet. I have gone through almost all the questions in Quora and StackOverflow but didn't get my answer anywhere from the previous questions. So my question is-</p>
<ol>
<li>Is it possible to apply word2vec in plagiarism detection?</li>
<li>If yes, then will Word2Vec be more efficient in text-based Plagiarism detection than WordNet or any other word embeddings like GloVe, fastText, etc?</li>
</ol>
<p>Thanks in advance.</p>
","nlp, wordnet, word2vec, word-embedding, plagiarism-detection","<p>Yes, these ""dense embedding"" models of word meaning like word2vec may be useful in plagiarism detection. (They're also likely useful in obfuscating plagiarism from simple detectors, as they can assist automated transforms on existing text that change the words while keeping the meaning similar.)</p>

<p>Only by testing within a particular system and with respect to quantitative evaluations will you know for sure how well it can work, or whether a particular embedding is better or worse than something like WordNet. </p>

<p>Among word2vec, fastttext, and GloVE, results will probably be very similar â€“ they all use roughly the same info (word co-occurrences within a sliding context window) to make maximally-predictive word-vectors â€“ so they behave very similarly with similar training data. </p>

<p>Any differences are subtle â€“ the non-GLoVe options might work better for very larger vocabularies; fasttext is essentially the word2vec in some modes, but adds new options for either modeling subword ngrams (which can then help to create better-than-random vectors for future out-of-vocabulary words) or optimizing the vectors for classification problems. </p>

<p>But the vectors for known words, which can be trained with plentiful training data, are going to be very similar in capabilities if the training processes are similarly meta-optimized for your task.  </p>
",3,1,1254,2017-06-27 17:56:54,https://stackoverflow.com/questions/44786888/will-word2vec-be-more-efficient-in-text-based-plagiarism-detection-than-wordnet
"How to use word embeddings/word2vec .. differently? With an actual, physical dictionary","<p>If my title is incorrect/could be better, please let me know.</p>

<p>I've been trying to find an existing paper/article describing the problem that I'm having: I'm trying to create vectors for words so that they are equal to the sum of their parts.
For example: Cardinal(the bird) would be equal to the vectors of: red, bird, and ONLY that.
In order to train such a model, the input might be something like a dictionary, where each word is defined by it's attributes.
Something like: </p>

<p>Cardinal: bird, red, ....</p>

<p>Bluebird: blue, bird,....</p>

<p>Bird: warm-blooded, wings, beak, two eyes, claws....</p>

<p>Wings: Bone, feather....</p>

<p>So in this instance, each word-vector is equal to the sum of the word-vector of its parts, and so on.</p>

<p>I understand that in the original word2vec, semantic distance was preserved, such that Vec(Madrid)-Vec(Spain)+Vec(Paris) = approx Vec(Paris).</p>

<p>Thanks!</p>

<p>PS: Also, if it's possible, new words should be able to be added later on.</p>
","machine-learning, nlp, deep-learning, embedding, word2vec","<p>If you're going to be building a dictionary of the components you want, you don't really need word2vec at all. You've already defined the dimensions you want specified: just use them, e.g. in Python:</p>

<pre><code>kb = {""wings"": {""bone"", ""feather""}, 
      ""bird"":  {""wings"", ""warm-blooded"", ...}, ...}
</code></pre>

<p>Since the values are sets, you can do set intersection:</p>

<pre><code>kb[""bird""] | kb[""reptile""]
</code></pre>

<p>You'll need to do find some ways decompose the elements recursively for comparisons, simplifications, etc. These are decisions you'll have to make based on what you expect to happen during such operations. </p>

<p>This sort of manual dictionary development is quite an old fashioned approach. Folks like Schank and Abelson used to do stuff like this in the 1970's. The problem is, as these dictionaries get more complex, they become intractable to maintain and more inaccurate in their approximations. You're welcome to try as an exercise---it can be kind of fun!---but keep your expectations low. </p>

<p>You'll also find aspects of meaning lost in these sorts of decompositions.  One of word2vec's remarkable properties is its sensitives to the <em>gestalt</em> of words---words may have meaning that is composed of parts, but there's a piece in that composition that makes the whole greater than the sum of the parts. In a decomposition, the gestalt is lost. </p>

<p>Rather than trying to build a dictionary, you might be best off exploring what W2V gives you anyway, from a large corpus, and seeing how you can leverage that information to your advantage. The linguistics of what exactly W2V renders from text aren't wholly understood, but in trying to do something specific with the embeddings, you might learn something new about language. </p>
",1,0,431,2017-06-28 14:57:09,https://stackoverflow.com/questions/44806078/how-to-use-word-embeddings-word2vec-differently-with-an-actual-physical-dic
Word2vec: add external word to every context,"<p>I'm looking for a simple ""hack"" to implement the following idea: I want to have a specific word appear artificially in the context of every word (the underlying goal is to try and use word2vec for supervised sentence classification).</p>

<p>An example is best:</p>

<p>Say I have the sentence: <code>""The dog is in the garden""</code>, and a window of 1.</p>

<p>So we would get the following pais of (target, context):</p>

<pre><code>(dog, The), (dog, is), (is, dog), (is, in), etc.
</code></pre>

<p>But what I would like to feed to the word2vec algo is this:</p>

<pre><code>(dog, The), (dog, is), **(dog, W)**, (is, dog), (is, in), **(is, W)**, etc.,
</code></pre>

<p>as if my word W was in the context of every word.</p>

<p>where W is a word of my choosing, not in the existing vocabulary.</p>

<p>Is there an easy way to do this in R or python ?</p>
","nlp, word2vec","<p>I imagined you have list of sentences and list of labels for each sentence:</p>

<pre><code>sentences = [
    [""The"", ""dog"", ""is"", ""in"", ""the"", ""garden""],
    [""The"", ""dog"", ""is"", ""not"", ""in"", ""the"", ""garden""],
]
</code></pre>

<p>Then you created the word-context pairs:</p>

<pre><code>word_context = [(""dog"", ""The""), (""dog"", ""is""), (""is"", ""dog""), (""is"", ""in"") ...]
</code></pre>

<p>Now if for each sentence you have a label, you can add labels to context of all words:</p>

<pre><code>labels = [
    ""W1"",
    ""W2"",
]

word_labels = [
    (word, label)
    for sent, label in zip(sentences, labels)
    for word in sent
]

word_context += word_labels
</code></pre>

<p>Unless you want to keep the order in word-context pairs!</p>
",1,1,318,2017-06-28 15:31:57,https://stackoverflow.com/questions/44806896/word2vec-add-external-word-to-every-context
Use of fasttext Pre-trained word vector as embedding in tensorflow script,"<p>Can I use fasttext word vector like the ones here:
<a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a>
in a tensorflow script as an embedding vector instead of word2vec or glove without using the library fasttext</p>
","python, tensorflow, word2vec, fasttext","<p>When you use pre-trained word vector, you can use gensim libarary. </p>

<p>For your reference.
<a href=""https://blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27"" rel=""noreferrer"">https://blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27</a></p>

<pre class=""lang-py prettyprint-override""><code>In [1]: from gensim.models import KeyedVectors

In [2]: jp_model = KeyedVectors.load_word2vec_format('wiki.ja.vec')

In [3]: jp_model.most_similar('car')
Out[3]: 
[('cab', 0.9970724582672119),
 ('tle', 0.9969051480293274),
 ('oyc', 0.99671471118927),
 ('oyt', 0.996662974357605),
 ('è»Š', 0.99665766954422),
 ('s', 0.9966464638710022),
 ('æ–°è»Š', 0.9966358542442322),
 ('hice', 0.9966053366661072),
 ('otg', 0.9965877532958984),
 ('è»Šä¸¡', 0.9965814352035522)]
</code></pre>

<h2>EDIT</h2>

<p>I created a new branch forked from <a href=""https://github.com/dennybritz/cnn-text-classification-tf"" rel=""noreferrer"">cnn-text-classification-tf</a>. Here is the link.
<a href=""https://github.com/satojkovic/cnn-text-classification-tf/tree/use_fasttext"" rel=""noreferrer"">https://github.com/satojkovic/cnn-text-classification-tf/tree/use_fasttext</a></p>

<p>In this branch, there are three modifications to use fasttext.</p>

<ol>
<li>Extract the vocab and the word_vec from fasttext. (util_fasttext.py)</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>model = KeyedVectors.load_word2vec_format('wiki.en.vec')
vocab = model.vocab
embeddings = np.array([model.word_vec(k) for k in vocab.keys()])

with open('fasttext_vocab_en.dat', 'wb') as fw:
    pickle.dump(vocab, fw, protocol=pickle.HIGHEST_PROTOCOL)
np.save('fasttext_embedding_en.npy', embeddings)
</code></pre>

<ol start=""2"">
<li><p>Embedding layer</p>

<p>W is initialized by zeros, and then an embedding_placeholder is set up to receive the word_vec, and finally W is assigned. (text_cnn.py)</p></li>
</ol>

<pre class=""lang-py prettyprint-override""><code>W_ = tf.Variable(
    tf.constant(0.0, shape=[vocab_size, embedding_size]),
    trainable=False,
    name='W')

self.embedding_placeholder = tf.placeholder(
    tf.float32, [vocab_size, embedding_size],
    name='pre_trained')

W = tf.assign(W_, self.embedding_placeholder)
</code></pre>

<ol start=""3"">
<li><p>Use the vocab and the word_vec</p>

<p>The vocab is used to build the word-id maps, and the word_vec is fed into the embedding_placeholder.</p></li>
</ol>

<pre class=""lang-py prettyprint-override""><code>with open('fasttext_vocab_en.dat', 'rb') as fr:
    vocab = pickle.load(fr)
embedding = np.load('fasttext_embedding_en.npy')

pretrain = vocab_processor.fit(vocab.keys())
x = np.array(list(vocab_processor.transform(x_text)))
</code></pre>

<pre class=""lang-py prettyprint-override""><code>feed_dict = {
    cnn.input_x: x_batch,
    cnn.input_y: y_batch,
    cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,
    cnn.embedding_placeholder: embedding
}
</code></pre>

<p>Please try it out. </p>
",9,3,9948,2017-06-29 15:33:09,https://stackoverflow.com/questions/44829438/use-of-fasttext-pre-trained-word-vector-as-embedding-in-tensorflow-script
Word2vec gensim - Calculating similarity between word isn&#39;t working when using phrases,"<p>Using <code>gensim</code> <code>word2vec</code> model in order to calculate similarities between two words. Training the model with a 250mb Wikipedia text gave a good result - about 0.7-0.8 similarity score for a related pair of words.</p>

<p>The problem is when I am using the <code>Phraser</code> model to add up phrases the similarity score drops to nearly zero for the same exact words.</p>

<p><strong>Results with the phrase model:</strong></p>

<pre><code>speed - velocity - 0.0203503432178
high - low - -0.0435703782446
tall - high - -0.0076987978333
nice - good - 0.0368784716958
computer - computational - 0.00487748035808
</code></pre>

<p>That probably means I am not using the Phraser model correctly.</p>

<p><strong>My Code:</strong></p>

<pre><code>    data_set_location = **
    sentences = SentenceIterator(data_set_location)

    # Train phrase locator model
    self.phraser = Phraser(Phrases(sentences))

    # Renewing the iterator because its empty
    sentences = SentenceIterator(data_set_location)

    # Train word to vector model or load it from disk
    self.model = Word2Vec(self.phraser[sentences], size=256, min_count=10, workers=10)



class SentenceIterator(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
                yield line.lower().split()
</code></pre>

<p>Trying the pharser model alone looks like it worked fine:</p>

<p><code>&gt;&gt;&gt;vectorizer.phraser['new', 'york', 'city', 'the', 'san', 'francisco']
['new_york', 'city', 'the', 'san_francisco']</code></p>

<p>What can cause such behavior?</p>

<p><strong>Trying to figure out the solution:</strong></p>

<p>according to gojomo answer, I've tried to create a <code>PhraserIterator</code>:</p>

<pre><code>import os

class PhraseIterator(object):
def __init__(self, dirname, phraser):
    self.dirname = dirname
    self.phraser = phraser

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
            yield self.phraser[line.lower()]
</code></pre>

<p>using this iterator I've tried to train my <code>Word2vec</code> model.</p>

<pre><code>phrase_iterator = PhraseIterator(text_dir, self.phraser)
self.model = Word2Vec(phrase_iterator, size=256, min_count=10, workers=10
</code></pre>

<p>Word2vec training log:</p>

<pre><code>    Using TensorFlow backend.
2017-06-30 19:19:05,388 : INFO : collecting all words and their counts
2017-06-30 19:19:05,456 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
2017-06-30 19:20:30,787 : INFO : collected 6227763 word types from a corpus of 28508701 words (unigram + bigrams) and 84 sentences
2017-06-30 19:20:30,793 : INFO : using 6227763 counts as vocab in Phrases&lt;0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000&gt;
2017-06-30 19:20:30,793 : INFO : source_vocab length 6227763
2017-06-30 19:21:46,573 : INFO : Phraser added 50000 phrasegrams
2017-06-30 19:22:22,015 : INFO : Phraser built with 70065 70065 phrasegrams
2017-06-30 19:22:23,089 : INFO : saving Phraser object under **/Models/word2vec/phrases_model, separately None
2017-06-30 19:22:23,441 : INFO : saved **/Models/word2vec/phrases_model
2017-06-30 19:22:23,442 : INFO : collecting all words and their counts
2017-06-30 19:22:29,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-06-30 19:33:06,667 : INFO : collected 143 word types from a corpus of 163438509 raw words and 84 sentences
2017-06-30 19:33:06,677 : INFO : Loading a fresh vocabulary
2017-06-30 19:33:06,678 : INFO : min_count=10 retains 95 unique words (66% of original 143, drops 48)
2017-06-30 19:33:06,679 : INFO : min_count=10 leaves 163438412 word corpus (99% of original 163438509, drops 97)
2017-06-30 19:33:06,683 : INFO : deleting the raw counts dictionary of 143 items
2017-06-30 19:33:06,683 : INFO : sample=0.001 downsamples 27 most-common words
2017-06-30 19:33:06,683 : INFO : downsampling leaves estimated 30341972 word corpus (18.6% of prior 163438412)
2017-06-30 19:33:06,684 : INFO : estimated required memory for 95 words and 256 dimensions: 242060 bytes
2017-06-30 19:33:06,685 : INFO : resetting layer weights
2017-06-30 19:33:06,724 : INFO : training model with 10 workers on 95 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-06-30 19:33:14,974 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:23,229 : INFO : PROGRESS: at 0.24% examples, 607 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:31,445 : INFO : PROGRESS: at 0.48% examples, 810 words/s, 
...
2017-06-30 20:19:00,864 : INFO : PROGRESS: at 98.57% examples, 1436 words/s, in_qsize 0, out_qsize 1
2017-06-30 20:19:06,193 : INFO : PROGRESS: at 99.05% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:11,886 : INFO : PROGRESS: at 99.29% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:17,648 : INFO : PROGRESS: at 99.52% examples, 1438 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:22,870 : INFO : worker thread finished; awaiting finish of 9 more threads
2017-06-30 20:19:22,908 : INFO : worker thread finished; awaiting finish of 8 more threads
2017-06-30 20:19:22,947 : INFO : worker thread finished; awaiting finish of 7 more threads
2017-06-30 20:19:22,947 : INFO : PROGRESS: at 99.76% examples, 1439 words/s, in_qsize 0, out_qsize 8
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 6 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 5 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 4 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-06-30 20:19:22,949 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-06-30 20:19:22,949 : INFO : training on 817192545 raw words (4004752 effective words) took 2776.2s, 1443 effective words/s
2017-06-30 20:19:22,950 : INFO : saving Word2Vec object under **/Models/word2vec/word2vec_model, separately None
2017-06-30 20:19:22,951 : INFO : not storing attribute syn0norm
2017-06-30 20:19:22,951 : INFO : not storing attribute cum_table
2017-06-30 20:19:22,958 : INFO : saved **/Models/word2vec/word2vec_model
</code></pre>

<p>After this training - any of two similarity calculation produce zero:</p>

<pre><code>speed - velocity - 0
high - low - 0
</code></pre>

<p>So it seems that the iterator is not working well so I've checked it using gojomo trick:</p>

<pre><code>print(sum(1 for _ in s))
1

print(sum(1 for _ in s))
1
</code></pre>

<p>And its working. </p>

<p>What may be the problem?</p>
","python, deep-learning, gensim, word2vec, phrases","<p>First, if your iterable class is working properly â€“ and it looks OK to me â€“ you won't need to ""renew the iterator because it's empty"". Rather, it will be capable of being iterated over multiple times. You can test if it's working properly as an iterable-object, rather than a single iteration, with code like:</p>

<pre><code>sentences = SentencesIterator(mypath)
print(sum(1 for _ in sentences))
print(sum(1 for _ in sentences))
</code></pre>

<p>If the same length prints twice, congratulations, you have a true iterable object. (You might want to update the class name to reflect that.) If the second length is <code>0</code>, you've only got an iterator: it can be consumed once, and then is empty on subsequent attempts. (If so, adjust the class code so that each call to <code>__iter__()</code> starts fresh. But as noted above, I think your code is already correct.)</p>

<p>That digression was important, because what's the true cause of your problem is that <code>self.phraser[sentences]</code> is just returning a one-time iterator object, <em>not</em> a repeatable iterable object. Thus, Word2Vec's 1st vocabulary-discovery step consumes the whole corpus in its one pass, then all training passes just see nothing â€“ and no training occurs. (If you have INFO-level logging on, this should be evident in the output showing instant training over no examples.)</p>

<p>Try making a <code>PhraserIterable</code> class, which takes a <code>phraser</code> and a <code>sentences</code>, and upon each call to <code>__iter__()</code> starts a new, fresh pass over the setences. Supply a (confirmed-restartable) instance of that as the corpus for Word2Vec. You should see training take longer as it does its default 5 passes â€“ and then see real results on later token-comparisons. </p>

<p>Separately: the on-the-fly upgrading of original <code>sentences</code> unigrams to phraser-calculated bigrams can be computationally expensive. The approach suggested above means that happens 6 times â€“ the vocabulary-scan then the 5 training passes. Where running-time is a concern, it can be beneficial to perform the phraser-combination once, saving the results to an in-memory object (if your corpus easily fits in RAM) or a new simply-space-delimited interim-results file, then use that file as input to the Word2Vec model. </p>
",1,0,1067,2017-06-29 17:25:57,https://stackoverflow.com/questions/44831480/word2vec-gensim-calculating-similarity-between-word-isnt-working-when-using-p
"Word Embedding, LookupTable, Word Embedding Visualizations","<p>I need to ask few questions regarding word embeddings.....could be basic.</p>
<ol>
<li>When we convert a one-hot vector of a word for instance king <code>[0 0 0 1 0]</code> into an embedded vector <code>E = [0.2, 0.4, 0.2, 0.2]</code>.... is there any importance for each index in resultant word vector? For instance <code>E[1]</code> which is 0.2.... what specifically <code>E[1]</code> defines (although I know its basically a transformation into another space).... or word vector collectively defines context but not individually...</li>
<li>How the dimension (reduced or increased) of a word vector matters as compared to the original one-hot vector?</li>
<li>How can we define a lookup table in terms of an embedding layer?</li>
<li>is lookup table a kind of random generated table or it already been trained separately with respect to data instance in data and we just use it later on in Neural Network operations?</li>
<li>Is there any method to visualize an embedded vector at Hidden Layer (as we do have in Image based Neural Network Processing)?</li>
</ol>
<p>Thanks in advance</p>
","deep-learning, text-mining, word2vec, word-embedding","<p>1: Each element (or a group of element) in embedding vector have some meaning, but mostly unknown for human. Depend on what algorithm you use, a word embedding vector may have different meaning, but usually useful. 
For example, <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>, similar word 'frog', 'toad' stay near each other in vector space. King - man result in vector similar to Queen. </p>

<ol start=""3"">
<li><p>Turn vocab into index. For example, you have a vocabulary list:
[dog, cat, mouse, feed, play, with]
Then the sentences: Dog play with cat => 0, 4, 5, 1
While, you have embedding matrix as follow</p>

<p>[0.1, 0.1, 0]  # comment: this is dog <br>
[0.2, 0.5, 0.1] # this is cat <br>
[...] <br>
[...] <br>
[...] <br>
[...] <br></p></li>
</ol>

<p>where first row is embedding vector of dog, second row is cat, then so on
Then, you use the index (0, 4, 5, 1) after lookup would become a matrix [[0.1, 0.1, 0][...][...][0.2, 0.5, 0.1]]</p>

<ol start=""4"">
<li>either or both

<ul>
<li>You can randomly init embedding vector and training it with gradient descent</li>
<li>You can take pretrained word vector and keep it fixed (i.e: read-only, no change). 
You can train your word vector in model and use it in another model. Our you can download pretrained word vector online. Example Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip on <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a></li>
<li>You can init with pretrained word vector and train with your model by  gradient descent</li>
</ul></li>
</ol>

<p>Update:
<strong>One-hot vector</strong> does not contain any information. You can think that one-hot vector is index of that vector in vocabulary. 
For example, Dog =>  [1, 0, 0, 0, 0, 0] and cat =>  [0, 1, 0, 0, 0, 0]. There are some different between one-hot vs index: </p>

<ul>
<li><p>if you input a list of index: [0, 4, 5, 1] to your multi-layer perceptron, it cannot learn anything (I tried...).But if you input a matrix of one-hot vector [[...1][1...][...][...]], it learn something. But it costly in term of RAM and CPU. </p></li>
<li><p>One-hot cost a lot of memory to store zeros. Thus, I suggest randomly init embedding matrix if you don't have one. Store dataset as index, and use index to look up embedding vector</p></li>
</ul>

<blockquote>
  <p>""its mean that lookup table is just a matrix of embedded vectors
  (already been trained seperately via word2vec or...) for each word in
  the vocabulary. and while in the process of neural network either we
  can use an Embedding Layer or we can just refer to embedded vector in
  lookup table for that particular embedded vector against particular
  one-hot vector.""</p>
</blockquote>

<p>Use the ""INDEX"" to look-up in lookup table. Turn dog into 0, cat into 1. One-hot vector and index contain same information, but one-hot cost more memory to store. Moreover, a lot of deeplearning framework accept index as input to embedding layer (which, output is a vector represent for a word in that index.)</p>

<blockquote>
  <p>"". How we get this embedding vector...""</p>
</blockquote>

<p>=> read paper. Here is paper about <a href=""https://arxiv.org/abs/1301.3781"" rel=""noreferrer"">Word2vec</a> and <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>. Ask your lecturers for more detail, they are willing to help you.  </p>
",7,7,9805,2017-07-03 09:24:22,https://stackoverflow.com/questions/44881999/word-embedding-lookuptable-word-embedding-visualizations
How can I load word2vec vectors?,"<p>I have tried lots of different pre-trained models, most of them have 0 documentation the few that have some sort of documentation say that it is possible to load it with</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

word_vectors = KeyedVectors.load_word2vec_format('model102938_that_cannot_be_loaded.bin', binary=True)
</code></pre>

<p>but it never works and I always get a random error.
Is there actually a way to load it? Or am I better off buying a couple hundred server racks and training it myself?</p>
","nlp, word2vec","<p>You can load w2v models directly using the <code>gensim.models.Word2Vec</code> class, like this:</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(...)
</code></pre>
",1,1,1532,2017-07-03 10:04:28,https://stackoverflow.com/questions/44882814/how-can-i-load-word2vec-vectors
Visualize embedding in tensorboard,"<p>I used tensorflow script <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">word2vec_basic.py</a> and I saved the model with tf.summary :
    saver = tf.train.Saver()
    save_path = saver.save(sess, ""./w2v/model.ckpt"")</p>

<p>I visualize the embedding with tensorboard succesfully but I get indexes of words in the vector
<a href=""https://i.sstatic.net/dwPnJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dwPnJ.jpg"" alt=""enter image description here""></a>
How can I get the words in the embedding instead of indexes in the vocabulary</p>
","python, tensorflow, word2vec","<p>I used this answer:
<a href=""https://stackoverflow.com/questions/41708106/linking-tensorboard-embedding-metadata-to-checkpoint"">linking-tensorboard-embedding-metadata-to-checkpoint</a></p>

<p>the problem was I tried o call tensorboard with logdir : ""./w2v/model.ckpt""
I should called it only with ""w2v/""</p>
",0,1,1144,2017-07-05 00:22:01,https://stackoverflow.com/questions/44915464/visualize-embedding-in-tensorboard
Word2Vec Vocabulary not definded error,"<p>I am new to python and word2vec and keep getting a ""you must first build vocabulary before training the model"" error. What is wrong with my code?</p>

<p>Here is my code:</p>

<pre><code>file_object=open(""SupremeCourt.txt"",""w"")
from gensim.models import word2vec

data = word2vec.Text8Corpus('SupremeCourt.txt')
model = word2vec.Word2Vec(data, size=200)

out=model.most_similar()

print(out[1])
print(out[2])
</code></pre>
","python, word2vec","<p>I could see some wrong things in your code like the file is opened in write mode and the model which you have loaded doesn't contain the word which you want to find the most similar words. 
I would like to suggest to use the predefined models like <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""nofollow noreferrer"">google_news_vectors</a> to load in the gensim or to build your own word2vec <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">model</a> so that you won't get the error.
the usage of most_similar in gensim is <code>out = model.most_similar(""word-name"")</code></p>

<pre><code>file_object=open(""SupremeCourt.txt"",""r"")
from gensim.models import word2vec

data = word2vec.Text8Corpus('SupremeCourt.txt')
model = word2vec.Word2Vec(data, size=200)#use google news vectors here 

out=model.most_similar(""word"")
print(out)
</code></pre>
",1,1,297,2017-07-06 12:17:46,https://stackoverflow.com/questions/44948661/word2vec-vocabulary-not-definded-error
word2vec guesing word embeddings,"<p>can word2vec be used for guessing words with just context?
having trained the model with a large data set e.g. Google news how can I use word2vec to predict a similar word with only context  e.g. with input "", who dominated chess for more than 15 years, will compete against nine top players in St Louis, Missouri.""  The output should be Kasparov or maybe Carlsen.</p>

<p>I'ven seen only the similarity apis but I can't make sense how to use them for this?  is this not how word2vec was intented to use? </p>
",word2vec,"<p>It is not the intended use of word2vec. The word2vec algorithm internally tries to predict exact words, using surrounding words, as a roundabout way to learn useful vectors for those surrounding words. </p>

<p>But even so, it's not forming exact predictions during training. It's just looking at a single narrow training example â€“ context words and target word â€“ and performing a very simple comparison and internal nudge to make its conformance to that one example slightly better. Over time, that self-adjusts towards useful vectors â€“ even if the predictions remain of wildly-varying quality. </p>

<p>Most word2vec libraries don't offer a direct interface for showing ranked predictions, given context words. The Python gensim library, for the last few versions (as of current version 2.2.0 in July 2017), has offered a <code>predict_output_word()</code> method that roughly shows what the model would predict, given context-words, for some training modes. See:</p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word</a></p>

<p>However, considering your fill-in-the-blank query (also called a 'cloze deletion' in related education or machine-learning contexts):</p>

<pre><code>_____, who dominated chess for more than 15 years, will compete against nine top players in St Louis, Missouri
</code></pre>

<p>A vanilla word2vec model is unlikely to get that right. It has little sense of the relative importance of words (except when some words are more narrowly predictive of others). It has no sense of grammar/ordering, or or of the compositional-meaning of connected-phrases (like 'dominated chess' as opposed to the separate words 'dominated' and 'chess'). Even though words describing the same sorts of things are usually near each other, it doesn't know categories to be able to determine that the blank must be a 'person' and a 'chess player', and the fuzzy-similarities of word2vec don't guarantee words-of-a-class will necessarily all be nearer-each-other than other words.</p>

<p>There has been a bunch of work to train word/concept vectors (aka 'dense embeddings') to be better at helping at such question-answering tasks. A random example might be <a href=""https://arxiv.org/abs/1609.08097"" rel=""nofollow noreferrer"">""Creating Causal Embeddings for Question Answering with Minimal Supervision""</a> but queries like [word2vec question answering] or [embeddings for question answering] will find lots more. I don't know of easy out-of-the-box libraries for doing this, with or without a core of word2vec, though. </p>
",4,2,1029,2017-07-06 14:21:13,https://stackoverflow.com/questions/44951605/word2vec-guesing-word-embeddings
Gensim: Loss of Words/Tokens while Training,"<p>I have a corpus built out of Wikimedia Dump files stored at <strong><em>sentences.txt</em></strong>
I have a sentence say 'à¤¨à¥€à¤°à¤œà¤ƒ à¤¹à¤¾à¤ à¤®à¤¾à¤¤à¤¾ à¤œà¥€! à¤¸à¥à¤•à¥‚à¤² à¤–à¤¼à¤¤à¥à¤® à¤¹à¥‹à¤¤à¥‡ à¤¸à¥€à¤§à¤¾ à¤˜à¤° à¤†à¤Šà¤à¤—à¤¾'</p>

<p>Now when I try to extract the word vectors there is always one or two words which have been missed out while training (despite being included in the list to be trained upon) and I get the KeyError.
Is there any way to improve the training so that it doesn't miss out words that frequently? </p>

<p>Here is a proof that it does happen. <code>tok.wordtokenize</code> is a word tokenizer. <code>sent.drawlist()</code> as well as <code>sents.drawlist()</code> returns a list of sentences from the corpus stored inside <strong><em>sentences.txt</em></strong>. </p>

<hr>

<pre><code>&gt;&gt;&gt; sentence = 'à¤¨à¥€à¤°à¤œà¤ƒ à¤¹à¤¾à¤ à¤®à¤¾à¤¤à¤¾ à¤œà¥€! à¤¸à¥à¤•à¥‚à¤² à¤–à¤¼à¤¤à¥à¤® à¤¹à¥‹à¤¤à¥‡ à¤¸à¥€à¤§à¤¾ à¤˜à¤° à¤†à¤Šà¤à¤—à¤¾'
&gt;&gt;&gt; sentence = tok.wordtokenize(sentence) #tok.wordtokenize() is simply a word tokenizer.
&gt;&gt;&gt; sentences = sent.drawlist()
&gt;&gt;&gt; sentences = [tok.wordtokenize(i) for i in sentences]
&gt;&gt;&gt; sentences2 = sents.drawlist()
&gt;&gt;&gt; sentences2 = [tok.wordtokenize(i) for i in sentences2]
&gt;&gt;&gt; sentences = sentences2 + sentences + sentence
&gt;&gt;&gt; ""à¤¨à¥€à¤°à¤œà¤ƒ"" in sentences #proof that the word is present inside sentences
True
&gt;&gt;&gt; sentences[0:10] #list of tokenized sentences.
[['à¤µà¤¿à¤¶à¥à¤µ', 'à¤­à¤°', 'à¤®à¥‡à¤‚', 'à¤•à¤°à¥‹à¥œà¥‹à¤‚', 'à¤Ÿà¥€à¤µà¥€', 'à¤¦à¤°à¥à¤¶à¤•à¥‹à¤‚', 'à¤•à¥€', 'à¤‰à¤¤à¥à¤¸à¥à¤•à¤¤à¤¾', 'à¤­à¤°à¥€', 'à¤¨à¤¿à¤—à¤¾à¤¹', 'à¤•à¥‡', 'à¤¬à¥€à¤š', 'à¤®à¤¿à¤¸', 'à¤‘à¤¸à¥à¤Ÿà¥à¤°à¥‡à¤²à¤¿à¤¯à¤¾', 'à¤œà¥‡à¤¨à¤¿à¤«à¤°', 'à¤¹à¥‰à¤•à¤¿à¤‚à¤¸', 'à¤•à¥‹', 'à¤®à¤¿à¤¸', 'à¤¯à¥‚à¤¨à¤¿à¤µà¤°à¥à¤¸-à¥¨à¥¦à¥¦à¥ª', 'à¤•à¤¾', 'à¤¤à¤¾à¤œ', 'à¤ªà¤¹à¤¨à¤¾à¤¯à¤¾', 'à¤—à¤¯à¤¾'], ['à¤•à¤°à¥€à¤¬', 'à¤¦à¥‹', 'à¤˜à¤‚à¤Ÿà¥‡', 'à¤šà¤²à¥‡', 'à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®', 'à¤®à¥‡à¤‚', 'à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨', 'à¤¦à¥‡à¤¶à¥‹à¤‚', 'à¤•à¥€', 'à¥®à¥¦', 'à¤¸à¥à¤‚à¤¦à¤°à¤¿à¤¯à¥‹à¤‚', 'à¤•à¥‡', 'à¤¬à¥€à¤š', 'à¥¨à¥¦', 'à¤µà¤°à¥à¤·à¥€à¤¯', 'à¤¹à¥‰à¤•à¤¿à¤‚à¤¸', 'à¤•à¥‹', 'à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤ ', 'à¤†à¤‚à¤•à¤¾', 'à¤—à¤¯à¤¾'], ['à¤®à¤¿à¤¸', 'à¤…à¤®à¥‡à¤°à¤¿à¤•à¤¾', 'à¤¶à¥ˆà¤‚à¤¡à¥€', 'à¤«à¤¿à¤¨à¥‡à¤œà¥€', 'à¤•à¥‹', 'à¤ªà¥à¤°à¤¥à¤®', 'à¤‰à¤ª', 'à¤µà¤¿à¤œà¥‡à¤¤à¤¾', 'à¤”à¤°', 'à¤®à¤¿à¤¸', 'à¤ªà¥à¤¯à¥‚à¤°à¥‡à¤Ÿà¥‹', 'à¤°à¤¿à¤•à¥‹', 'à¤…à¤²à¥à¤¬à¤¾', 'à¤°à¥‡à¤‡à¤œ', 'à¤¦à¥à¤µà¤¿à¤¤à¥€à¤¯', 'à¤‰à¤ª', 'à¤µà¤¿à¤œà¥‡à¤¤à¤¾', 'à¤šà¥à¤¨à¥€', 'à¤—à¤ˆ'], ['à¤­à¤¾à¤°à¤¤', 'à¤•à¥€', 'à¤¤à¤¨à¥à¤¶à¥à¤°à¥€', 'à¤¦à¤¤à¥à¤¤à¤¾', 'à¤…à¤‚à¤¤à¤¿à¤®', 'à¥§à¥¦', 'à¤ªà¥à¤°à¤¤à¤¿à¤­à¤¾à¤—à¤¿à¤¯à¥‹à¤‚', 'à¤®à¥‡à¤‚', 'à¤¹à¥€', 'à¤¸à¥à¤¥à¤¾à¤¨', 'à¤¬à¤¨à¤¾', 'à¤ªà¤¾à¤ˆ'], ['à¤¹à¥‰à¤•à¤¿à¤‚à¤¸', 'à¤¨à¥‡', 'à¤•à¤¹à¤¾', 'à¤•à¤¿', 'à¤œà¥€à¤¤', 'à¤•à¥‡', 'à¤¬à¤¾à¤°à¥‡', 'à¤®à¥‡à¤‚', 'à¤‰à¤¸à¤¨à¥‡', 'à¤¸à¤ªà¤¨à¥‡', 'à¤®à¥‡à¤‚', 'à¤­à¥€', 'à¤¨à¤¹à¥€à¤‚', 'à¤¸à¥‹à¤šà¤¾', 'à¤¥à¤¾'], ['à¤¸à¥Œà¤‚à¤¦à¤°à¥à¤¯', 'à¤•à¥€', 'à¤¯à¤¹', 'à¤¶à¥€à¤°à¥à¤·', 'à¤ªà¥à¤°à¤¤à¤¿à¤¯à¥‹à¤—à¤¿à¤¤à¤¾', 'à¤•à¥à¤µà¤¿à¤Ÿà¥‹', 'à¤•à¥‡', 'à¤•à¤¨à¥à¤µà¥‡à¤‚à¤¶à¤¨', 'à¤¸à¥‡à¤‚à¤Ÿà¤°', 'à¤®à¥‡à¤‚', 'à¤®à¤‚à¤—à¤²à¤µà¤¾à¤°', 'à¤¦à¥‡à¤°', 'à¤°à¤¾à¤¤', 'à¤¶à¥à¤°à¥‚', 'à¤¹à¥à¤ˆ'], ['à¤•à¤°à¥€à¤¬', 'à¥­à¥«à¥¦à¥¦', 'à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ', 'à¤¦à¤°à¥à¤¶à¤•à¥‹à¤‚', 'à¤•à¥€', 'à¤®à¥Œà¤œà¥‚à¤¦à¤—à¥€', 'à¤®à¥‡à¤‚', 'à¤µà¤¿à¤¶à¥à¤µ', 'à¤•à¥€', 'à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤ ', 'à¤¸à¥à¤‚à¤¦à¤°à¥€', 'à¤•à¥‡', 'à¤šà¤¯à¤¨', 'à¤•à¥€', 'à¤•à¤µà¤¾à¤¯à¤¦', 'à¤¶à¥à¤°à¥‚', 'à¤¹à¥à¤ˆ'], ['à¤¹à¤°', 'à¤šà¤°à¤£', 'à¤•à¥‡', 'à¤¬à¤¾à¤¦', 'à¤²à¥‹à¤—à¥‹à¤‚', 'à¤•à¥€', 'à¤¸à¤¾à¤‚à¤¸à¥‡', 'à¤¥à¤®à¤¨à¥‡', 'à¤²à¤—à¤¤à¥€à¤‚'], ['à¤Ÿà¥€à¤µà¥€', 'à¤ªà¤°', 'à¤²à¥à¤¤à¥à¤«', 'à¤‰à¤ à¤¾', 'à¤°à¤¹à¥‡', 'à¤¦à¤°à¥à¤¶à¤•', 'à¤…à¤ªà¤¨à¥‡', 'à¤¦à¥‡à¤¶', 'à¤µ', 'à¤•à¥à¤·à¥‡à¤¤à¥à¤°', 'à¤•à¥€', 'à¤¸à¥à¤‚à¤¦à¤°à¥€', 'à¤•à¥€', 'à¤ªà¥à¤°à¤¤à¤¿à¤¯à¥‹à¤—à¤¿à¤¤à¤¾', 'à¤®à¥‡à¤‚', 'à¤¸à¥à¤¥à¤¿à¤¤à¤¿', 'à¤•à¥‡', 'à¤¬à¤¾à¤°à¥‡', 'à¤®à¥‡à¤‚', 'à¤µà¥à¤¯à¤—à¥à¤°', 'à¤°à¤¹à¥‡'], ['à¤«à¤¾à¤‡à¤¨à¤²', 'à¤®à¥‡à¤‚', 'à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥‡', 'à¤µà¤¾à¤²à¥€', 'à¤ªà¤¾à¤‚à¤š', 'à¤ªà¥à¤°à¤¤à¤¿à¤­à¤¾à¤—à¤¿à¤¯à¥‹à¤‚', 'à¤®à¥‡à¤‚', 'à¤®à¤¿à¤¸', 'à¤ªà¥‡à¤°à¤¾à¤—à¥à¤µà¥‡', 'à¤¯à¤¾à¤¨à¤¿à¤¨à¤¾', 'à¤—à¥‹à¤‚à¤œà¤¾à¤²à¥‡à¤œ', 'à¤”à¤°', 'à¤®à¤¿à¤¸', 'à¤¤à¥à¤°à¤¿à¤¨à¤¿à¤¦à¤¾à¤¦', 'à¤µ', 'à¤Ÿà¥‹à¤¬à¥ˆà¤—à¥‹', 'à¤¡à¥‡à¤¨à¤¿à¤¯à¤²', 'à¤œà¥‹à¤‚à¤¸', 'à¤­à¥€', 'à¤¶à¤¾à¤®à¤¿à¤²', 'à¤¥à¥€à¤‚']]
&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences, size =10,  min_count=1) 
&gt;&gt;&gt; pred = []
&gt;&gt;&gt; for word in sentence:
...         pred.append(model.wv[word].tolist())
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 574, in __getitem__
    return self.word_vec(words)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 273, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'à¤¨à¥€à¤°à¤œà¤ƒ' not in vocabulary""
</code></pre>

<hr>

<p>As you can see, I check for the word ""à¤¨à¥€à¤°à¤œà¤ƒ"" inside the list of tokenized sentences. It is present in the list that I feed into the Word2Vec trainer and yet after training it is not in the vocabulary. </p>
","python, gensim, word2vec","<p>It should never 'miss' words that were included in the tokenized corpus, and had at least <code>min_count</code> occurrences. So if you get a <code>KeyError</code>, you can be confident that the associated word-token was never supplied during training. </p>

<p>In your example code to reproduce, take a close look at:</p>

<pre><code>sentence = ""Jack and Jill went up the Hill""
sentence = [word_tokenize(i) for i in sentence]
</code></pre>

<p><code>i in sentence</code> will be each <em>character</em> of the string. It's unlikely your unshown <code>word_tokenize()</code> function does anything useful with the individual characters <code>['J', 'a', 'c', 'k', ' ', ...]</code> - probably just leaving them as a list of letters. Then <code>+</code>-appending that to your other <code>sentences</code> makes <code>sentences</code> 30 items longer, rather than the single extra tokenized example you expect.</p>

<p>I suspect your real issue is different but related: something wrong with tokenization and composition. So check every step individually for the expected results and nested-types. (Using unique variables per step, like <code>sentences_tokenized</code> or <code>sentence_tokenized</code> instead of clobber-reusing variables like <code>sentences</code> and <code>sentence</code>, can help debug.)</p>

<p><strong>Update as you suggested as edit:</strong> The issue with your latest code is that the line where you <code>+</code>-append is still wrong; it's appending each word in sentence as if it were its own new sentence. Looking at the results of each step â€“ in the variable contents and lengths â€“ should help make this clear. Also I again recommend not reusing variables for multiple steps while debugging. The line <code>""à¤¨à¥€à¤°à¤œà¤ƒ"" in sentences #proof that the word is present inside sentences</code> is actually proving sentences is wrong; that single word should not be an item in <code>sentences</code>, but in its single last list-of-tokens, <code>sentences[-1]</code>.</p>
",1,0,1023,2017-07-07 06:56:08,https://stackoverflow.com/questions/44964380/gensim-loss-of-words-tokens-while-training
"After loading a pretrained Word2Vec model, how do I get word2vec representations of new sentences?","<p>I loaded a word2vec model using Google News dataset. Now I want to get the Word2Vec representations of a list of sentences that I wish to cluster. After going through the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a> I found this <code>gensim.models.word2vec.LineSentence</code>but I'm not sure this is what I am looking for. </p>

<p>There should be a way to get word2vec representations of a list of sentences from a pretrained model right? None of the links I searched had anything about it. Any leads would be appreciated. </p>
","cluster-analysis, gensim, word2vec","<p>Word2Vec only offers vector representations for words, not sentences. </p>

<p>One crude but somewhat effective (for some purposes) way to go from word-vectors to vectors for longer texts (like sentences) is to average all the word-vectors together. This isn't a function of the gensim <code>Word2Vec</code> class; you have to code this yourself. </p>

<p>For example, with the word-vectors already loaded as <code>word_model</code>, you'd roughly do:</p>

<pre><code>import numpy as np

sentence_tokens = ""I do not like green eggs and ham"".split()
sum_vector = np.zeros(word_model.vector_size)
for token in sentence_tokens:
    sum_vector += word_model[token]
sentence_vector = sum_vector / len(sentence_tokens)
</code></pre>

<p>Real code might add handling for when the tokens aren't all known to the model, or other ways of tokenizing/filtering the text, and so forth.  </p>

<p>There are other more sophisticated ways to get the vector for a length-of-text, such as the 'Paragraph Vectors' algorithm implemented by gensim's <code>Doc2Vec</code> class. These don't necessarily start with pretrained word-vectors, but can be trained on your own corpus of texts. </p>
",1,0,544,2017-07-13 00:47:01,https://stackoverflow.com/questions/45069715/after-loading-a-pretrained-word2vec-model-how-do-i-get-word2vec-representations
Predict middle word word2vec,"<p>I have the predict_output_word method from the official github repository. which takes only wod2vec models trained with skip-gram and tries to predict the middle word by summing the vectors of all the input word's indices and 
divids this by the length of np_sum of the input word indices. Then you consider output and take softmax to get probabilities of the predicted word after you sum all these probabilities to get the most likely word. Is there a better way to approach this in other to get better words since this gives very bad results for shorter sentences.
below is the code from github.</p>

<pre><code>def predict_output_word(model, context_words_list, topn=10):

from numpy import exp,  dtype, float32 as REAL,\
ndarray, empty, sum as np_sum,
from gensim import utils, matutils 

""""""Report the probability distribution of the center word given the context words as input to the trained model.""""""
if not model.negative:
    raise RuntimeError(""We have currently only implemented predict_output_word ""
        ""for the negative sampling scheme, so you need to have ""
        ""run word2vec with negative &gt; 0 for this to work."")

if not hasattr(model.wv, 'syn0') or not hasattr(model, 'syn1neg'):
    raise RuntimeError(""Parameters required for predicting the output words not found."")

word_vocabs = [model.wv.vocab[w] for w in context_words_list if w in model.wv.vocab]
if not word_vocabs:
    warnings.warn(""All the input context words are out-of-vocabulary for the current model."")
    return None


word2_indices = [word.index for word in word_vocabs]

#sum all the indices
l1 = np_sum(model.wv.syn0[word2_indices], axis=0)

if word2_indices and model.cbow_mean:
    #l1 = l1 / len(word2_indices)
    l1 /= len(word2_indices)

prob_values = exp(dot(l1, model.syn1neg.T))     # propagate hidden -&gt; output and take softmax to get probabilities
prob_values /= sum(prob_values)
top_indices = matutils.argsort(prob_values, topn=topn, reverse=True)

return [(model.wv.index2word[index1], prob_values[index1]) for index1 in top_indices]   #returning the most probable output words with their probabilities
</code></pre>
","machine-learning, word2vec","<p>While the word2vec algorithm trains word-vectors by trying to predict words, and then those word-vectors may be useful for other purposes, it is not likely to be the ideal algorithm if word-prediction is your real goal. </p>

<p>Most word2vec implementations haven't even offered a specific interface for individual word-predictions. In gensim, <code>predict_output_word()</code> was only added recently. It only works for some modes. It doesn't quite treat the <code>window</code> the same as during training â€“ there's no effective weighting-by-distance. And, it is fairly expensive â€“ essentially checking the model's prediction for every word, then reporting the top-N. (The 'prediction' that occurs during training is 'sparse' and much more efficient - just running enough of the model to nudge it to be better at a single example.)</p>

<p>If word-prediction is your real goal, you may get better results from other methods, including just calculating a big lookup-table of how-often words appear near each-other or near other n-grams. </p>
",2,1,1700,2017-07-14 12:01:35,https://stackoverflow.com/questions/45102484/predict-middle-word-word2vec
word2vec vocab vs char,"<p>I'm using word2vec to represent my words as vectors.</p>

<pre><code>text = np.loadtxt(""file.txt"", dtype=str, delimiter="" "")
word2vec = w2v.Word2Vec(text, size=100, window=5, min_count=5, workers=4)
print(len(word2vec.wv.vocab))
</code></pre>

<p>text is a list of words(strings). Instead of printing the number of words, this code prints 26, # English letters. In order to train word2vec to my model, I need to be dealing with words, not letters. I've tried converting text to a string, but it wasn't successful. What am I doing wrong?</p>
","python, word2vec","<p>I believe you need to pass a list of lists of words:</p>

<pre><code>word2vec = w2v.Word2Vec(text.reshape(-1, 1), size=100, window=5, min_count=5, workers=4)
</code></pre>
",0,0,611,2017-07-14 15:18:21,https://stackoverflow.com/questions/45106468/word2vec-vocab-vs-char
python word2vec context similarity using surrounding words,"<p>I would like to use embeddings made by w2v in order to obtain the most likely substitute words GIVEN a context (surrounding words), rather than supplying an individual word.</p>

<p>Example:
sentence = 'I would like to go to the park tomorrow after school'</p>

<p>If I want to find candidates similar to ""park"", typically I would just leverage the similarity function from the Gensim model</p>

<pre><code>model.most_similar('park')
</code></pre>

<p>and obtain semantically similar words. However this could give me similar words to the verb 'park' instead of the noun 'park', which I was after.</p>

<p>Is there any way to query the model and give it surrounding words as context to provide better candidates?</p>
","python, gensim, word2vec, word-embedding","<p>Word2vec is not, primarily, a word-prediction algorithm. Internally it tries to do semi-predictions, to train its word-vectors, but usually these training-predictions aren't the end-use for which word-vectors are wanted.</p>

<p>That said, recent versions of gensim added a <code>predict_output_word()</code> method that (for some model modes) approximates the predictions done during training. It might be useful for your purposes. </p>

<p>Alternatively, checking for the words <code>most_similar()</code> to your initial target word that are <em>also</em> somewhat-similar to the context words might help. </p>

<p>There have been some research papers about ways to disambiguate multiple word senses (like 'to /park/ a car' versus 'walk in a /park/') during word-vector training, but I haven't seen them implemented in open source libraries. </p>
",3,1,855,2017-07-14 17:05:14,https://stackoverflow.com/questions/45108291/python-word2vec-context-similarity-using-surrounding-words
word2vec models consist of characters instead of words,"<p>I am trying to make a word2vec model by Gensim on Persian language which has ""space"" as the character delimiter, I use python 3.5. The problem that I encounter was I gave a text file as input and it returns a model which only consists of each character separately instead of words. I also gave the input as a list of words which is recommended on :</p>

<p><a href=""https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key]"">Python Gensim word2vec vocabulary key</a></p>

<p>It doesn't work for me and I think it doesn't consider sequence of words in a sentence so it wouldn't be correct.</p>

<p>I did some preprocessing on my input which consist of:</p>

<p>collapse multiple whitespaces into a single one<br>
tokenize by splitting on whitespace<br>
remove words less than 3 characters long
remove stop words</p>

<p>I gave the text to word2vec which gave me result correctly, but I need it on python so my choice is limited to use Gensim.</p>

<p>Also I tried to load the model which made by word2vec source on gensim I get error so I need create the word2vec model by Gensim.</p>

<p>my code is:</p>

<pre><code>  wfile = open('aggregate.txt','r')    
  wfileRead = wfile.read()    
  model = word2vec.Word2Vec(wfileRead , size=100)   
  model.save('Word2Vec.txt')
</code></pre>
","gensim, word2vec","<p>The gensim Word2Vec model does not expect <em>strings</em> as its text examples (sentences), but <em>lists-of-tokens</em>. Thus, it's up to your code to tokenize your text, before passing it to Word2Vec. </p>

<p>Your code as shown just passes raw data from 'aggregate.txt' file into Word2Vec as <code>wFileRead</code>.</p>

<p>Look at examples in the gensim documentation, including the <code>LineSentence</code> class included with gensim, for ideas </p>
",8,3,4898,2017-07-18 06:59:03,https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words
Gensim Word2Vec Error: ValueError: missing section header before line #0,"<p>I am new to Gensim Word2Vec. I was trying to use Word2Vec to build word vectors for some raw html files. So I first convert the html file into txt file.</p>

<h3>My First Question:</h3>

<p>When I train the word2vec model, everything is fine. But when I want to test the accuracy of the model by doing</p>

<pre><code>model.accuracy(file_name)
</code></pre>

<p>it produced error: </p>

<pre><code>Traceback (most recent call last):
  File ""build_w2v.py"", line 82, in &lt;module&gt;
    main()
  File ""build_w2v.py"", line 77, in main
    gen_w2v_model()
  File ""build_w2v.py"", line 71, in gen_w2v_model
    accuracy = model.accuracy(target)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1330, in accuracy
    return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 679, in accuracy
    raise ValueError(""missing section header before line #%i in %s"" % (line_no, questions))
ValueError: missing section header before line #0
</code></pre>

<p>Below is the sample file:</p>

<pre><code>zGR='ca-about-health_js';var ziRfw=0;zobt="" Vision Ads"";zOBT="" Ads"";function zIpSS(u){zpu(0,u,280,375,""ssWin"")}function zIlb(l,t,f){zT(l,'18/1Pp/wX')}


zWASL=1;zGRH=1
#rs{margin:0 0 10px}#rs #n5{font-weight:bold}#rs a{padding:7px;text-transform:capitalize}Poking Eyelashes - Poking Eyelashes Problem


&lt;!--
zGOW=0;xd=0;zap="""";zAth='25752';zAthG='25752';zTt='11';zir='';zBTS=0;zBT=0;zSt='';zGz=''
ch='health';gs='vision';xg=""Vision"";zcs=''
zFDT='0'
zFST='0'
zOr='BA15WT26OkWA0O1b';zTbO=zRQO=1;zp0=zp1=zp2=zp3=zfs=0;zDc=1;
zSm=zSu=zhc=zpb=zgs=zdn='';zFS='BA110BA0110B00101';zFD='BA110BA0110B00101'
zDO=zis=1;zpid=zi=zRf=ztp=zpo=0;zdx=20;zfx=100;zJs=0;
zi=1;zz=';336280=2-1-1299;72890=2-1-1299;336155=2-1-12-1;93048=2-1-12-1;30050=2-1-12-1';zx='100';zde=15;zdp=1440;zds=1440;zfp=0;zfs=66;zfd=100;zdd=20;zaX=new Array(11, new Array(100,1051,8192,2,'336,300'),7, new Array(100,284,8196,12,'336,400'));zDc=1;;zDO=1;;zD336=1;zhc='';;zGTH=1;
zGo=0;zG=17;zTac=2;zDot=0;
zObT=""Vision"";zRad=5;var tp="" primedia_""+(zBT?"""":""non_"")+""site_targeting"";if(!this.zGCID)zGCID=tp
else zGCID+=tp;
if(zBT&gt;0){zOBR=1}
if(!this.uy)uy='about.com';if(typeof document.domain!=""undefined"")document.domain=uy;//--&gt;


function zob(p){if(!this.zOfs)return;var a=zOfs,t,i=0,l=a.length;if(l){w('&lt;div id=""oF""&gt;&lt;b&gt;'+(this.zobt?zobt:xg+' Ads')+'&lt;/b&gt;&lt;ul&gt;');while((i&lt;l)&amp;&amp;i&lt;zRad){t=a[i++].line1;w('&lt;li&gt;&lt;a href=""/z/js/o'+(p?p:'')+'.htm?k='+zUriS(t.toLowerCase())+(this.zobr?zobr:'')+'&amp;d='+zUriS(t)+'&amp;r='+zUriS(zWl)+'"" target=""_'+(this.zOBNW?'new'+zr(9999):'top')+'""&gt;'+t+'&lt;/a&gt;&lt;/li&gt;');}w('&lt;/ul&gt;&lt;/div&gt;')}}function rb600(){if(gEI('bb'))gEI('bb').height=600}zJs=10
zJs=11
zJs=12
zJs=13
zc(5,'jsc',zJs,9999999,'')
zDO=0
</code></pre>

<p>So This file actually begins with many (I don't know) space or \n. When I open in the vim.<a href=""https://i.sstatic.net/jOz4T.png"" rel=""nofollow noreferrer"">It looks like this</a>.</p>

<p><strong>So what is the problem here?</strong></p>

<h3>My second question:</h3>

<p>Also, I am doing text classification of some biomedical papers. The files I was given are all raw html files in either Japanese or English. After I do the ascii conversion and some stop_words cleaning, there are still many HTML code left in the file. </p>

<p>When I try to clean these files and restrict the characters to [a-zA-Z0-9], I found some medical terms like [4protein...] or something get not properly cleaned as well.</p>

<p><strong>Are there any suggestions in how to clean up these files?</strong></p>
","python, html, nlp, gensim, word2vec","<p>The argument to <code>accuracy()</code> should be a set of analogies to test the model against, in the format of the <code>questions-words.txt</code> file available from the original word2vec.c distribution. (It should <em>not</em> be your own file.)</p>
",0,0,741,2017-07-18 10:36:34,https://stackoverflow.com/questions/45164340/gensim-word2vec-error-valueerror-missing-section-header-before-line-0
How to make word2vec model&#39;s loading time and memory use more efficient?,"<p>I want to use Word2vec in a web server (production) in two different variants where I fetch two sentences from the web and compare it in real-time. For now, I am testing it on a local machine which has 16GB RAM. </p>

<p>Scenario:
w2v = load w2v model </p>

<pre><code>If condition 1 is true:
   if normalize:
      reverse normalize by w2v.init_sims(replace=False) (not sure if it will work)
   Loop through some items:
   calculate their vectors using w2v
else if condition 2 is true:
   if not normalized:
       w2v.init_sims(replace=True)
   Loop through some items:
   calculate their vectors using w2v
</code></pre>

<p>I have already read the solution about reducing the vocabulary size to a small size but I would like to use all the vocabulary. </p>

<p>Are there new workarounds on how to handle this? Is there a way to initially load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary? </p>
","python, nlp, nltk, gensim, word2vec","<p>As a one-time delay that you should be able to schedule to happen before any service-requests, I would recommend against worrying too much about the first-time <code>load()</code> time. (It's going to inherently take a lot of time to load a lot of data from disk to RAM â€“ but once there, if it's being kept around and shared between processes well, the cost is not spent again for an arbitrarily long service-uptime.)</p>

<p>It doesn't make much sense to ""load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary"" â€“ as soon as any similarity-calc is needed, the whole set of vectors need to be accessed for any top-N results. (So the ""half-loaded"" state isn't very useful.) </p>

<p>Note that if you do <code>init_sims(replace=True)</code>, the model's original raw vector magnitudes are clobbered with the new unit-normed (all-same-magnitude) vectors. So looking at your pseudocode, the only difference between the two paths is the explicit <code>init_sims(replace=True)</code>. But if you're truly keeping the same shared model in memory between requests, as soon as <code>condition 2</code> occurs, the model is normalized, and thereafter calls under <code>condition 1</code> are also occurring with normalized vectors. And further, additional calls under <code>condition 2</code> just redundantly (and expensively) re-normalize the vectors in-place. So if normalized-comparisons are your only focus, best to do one in-place <code>init_sims(replace=True)</code> at service startup - not at the mercy of order-of-requests. </p>

<p>If you've saved the model using gensim's native <code>save()</code> (rather than <code>save_word2vec_format()</code>), and as uncompressed files, there's the option to 'memory-map' the files on a future re-load. This means rather than immediately copying the full vector array into RAM, the file-on-disk is simply marked as providing the addressing-space. There are two potential benefits to this: (1) if you only even access some limited ranges of the array, only those are loaded, on demand; (2) many separate processes all using the same mapped files will automatically reuse any shared ranges loaded into RAM, rather than potentially duplicating the same data. </p>

<p>(1) isn't much of an advantage as soon as you need to do a full-sweep over the whole vocabulary â€“ because they're all brought into RAM then, and further at the moment of access (which will have more service-lag than if you'd just pre-loaded them). But (2) is still an advantage in multi-process webserver scenarios. There's a lot more detail on how you might use memory-mapped word2vec models efficiently in a prior answer of mine, at <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",2,1,2450,2017-07-19 09:16:06,https://stackoverflow.com/questions/45186094/how-to-make-word2vec-models-loading-time-and-memory-use-more-efficient
Gensim Doc2Vec generating huge file for model,"<p>I am trying to run doc2vec library from gensim package. My problem is that when I am training and saving the model the model file is rather large(2.5 GB) I tried using this line :</p>

<pre><code>model.estimate_memory()
</code></pre>

<p>But it didn't change anything. I also have tried to change max_vocab_size to decrease the space. But there was not luck. Can somebody help me with this matter?</p>
","python, semantics, gensim, word2vec, doc2vec","<p>Doc2Vec models can be large. In particular, any word-vectors in use will use 4 bytes per dimension, times two layers of the model. So a 300-dimension model with a 200,000 word vocabulary will use just for the vectors array itself:</p>

<pre><code>200,000 vectors * 300 dimensions * 4 bytes/float * 2 layers = 480MB
</code></pre>

<p>(There will be additional overhead for the dictionary storing vocabulary information.)</p>

<p>Any doc-vectors will also use 4 bytes per dimnsion. So if you train a vectors for a million doc-tags, the model will use just for the doc-vectors array:</p>

<pre><code>1,000,000 vectors * 300 dimensions * 4 bytes/float = 2.4GB
</code></pre>

<p>(If you're using arbitrary string tags to name the doc-vectors, there'll be additional overhead for that.)</p>

<p>To use less memory when loaded (which will also result in a smaller store file), you can use a smaller vocabulary, train fewer doc-vecs, or use a smaller vector size. </p>

<p>If you'll only need the model for certain narrow purposes, there may be other parts you can throw out after training â€“ but that requires knowledge of the model internals/source-code, and your specific needs, and will result in a model that's broken (and likely to throw errors) for many other usual operations.</p>
",7,5,1155,2017-07-19 15:37:04,https://stackoverflow.com/questions/45195169/gensim-doc2vec-generating-huge-file-for-model
How are Words converted to Vectors in Stanford NER,"<p>I am looking at Stanford NER and want to know how the words are represented. Are they converted to vectors using Word2Vec or Glove when training the model using linear CRF. </p>

<p>A Little more study shows me that the data is stored into a CRFDatum structure. Can anyone please elaborate on this?</p>
","stanford-nlp, word2vec","<p>Well, now I know how the old-school AI people feel...</p>

<p>Back in the Old Days (including when the NER system was built), before neural networks took off, statistical ML converted discrete outputs into vectors using custom-built featurizers. For language, this usually resulted in a very long but sparse vector of one-hot features. For example, a featurizer might assign each word a one-hot representation: 1 at the index corresponding to the word, and zero elsewhere. For NER, these features were usually things like the characters in the word (one-hot encoded), prefixes and suffixes of length $k$, word shape, part-of-speech tag, etc.</p>

<p>In Stanford's code, these sparse vectors are usually represented as <code>Counter</code> objects of one form or another, which then get passed into a <code>Datum</code> object and converted into a more densely packed <code>Dataset</code> object, which is fed into the optimizer (usually, <code>QNMinimizer</code>, implementing L-BFGS).</p>
",2,0,339,2017-07-20 21:06:28,https://stackoverflow.com/questions/45225180/how-are-words-converted-to-vectors-in-stanford-ner
Tensorboard Embeddings: &quot;parsing metadata&quot; hangs,"<p>I am trying to visualize embeddings using tensorboard but the embedding tab seems to hang on ""parsing metadata"".</p>

<p>I checked the code, metadata tsv file, and the <code>projector_config.ptxt</code> against the <a href=""https://www.tensorflow.org/get_started/embedding_viz"" rel=""nofollow noreferrer"">tensorboard embedding visualization tutorial</a> ; everything seems to be correct and tensorboard is not giving me any messages in the terminal.</p>

<p>The code I am using to generate the embeddings and visualization can be found <a href=""https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/examples/04_word2vec_visualize.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I am running tensorflow 1.2.1 for python 2.7 with gpu support.</p>
","python, tensorflow, word2vec, tensorboard","<p>It turns out that the <a href=""https://www.tensorflow.org/get_started/embedding_viz#setup"" rel=""noreferrer"">Tensorboard Embedding Visualization Tutorial</a> is incorrect; the <code>metadata_path</code> property in <code>projector_config.pbtxt</code> needs to be set <strong>relative</strong> to the log directory. Otherwise, TensorBoard (the embedding projector frontend) will misleadingly halt at the ""Parsing metadata"" step as it searches for the metadata file in a path that does not exist.</p>

<p>For more on this, see the corresponding <a href=""https://github.com/tensorflow/tensorboard/issues/247"" rel=""noreferrer"">issue on github.</a></p>
",5,2,861,2017-07-20 21:12:33,https://stackoverflow.com/questions/45225276/tensorboard-embeddings-parsing-metadata-hangs
How to represent Word2Vec model to graph? (or convert a 1x300 numpy array to just 1x2 array),"<p>I have a 1x300 numpy array from my Word2Vec model which is returns like this:</p>

<pre><code>[ -2.55022556e-01   1.06162608e+00  -5.86191297e-01  -4.43067521e-01
   4.46810514e-01   4.31743741e-01   2.16610283e-01   9.27684903e-01
  -4.47879761e-01  -9.11142007e-02   3.27048987e-01  -8.05553675e-01
  -8.54483843e-02  -2.85595834e-01  -2.70745698e-02  -3.08014955e-02
   1.53204888e-01   3.16114485e-01  -2.82659411e-01  -2.98218042e-01
  -1.03240972e-02   2.12806061e-01   1.63605273e-01   9.42423999e-01
   1.20789325e+00   4.11570221e-01  -5.46323597e-01   1.95108235e-01
  -4.53743488e-01  -1.28625661e-01  -7.43277609e-01   1.11551750e+00
  -4.51873302e-01  -1.14495361e+00  -6.69551417e-02   6.88364863e-01
  -6.01781428e-01  -2.36386538e-01  -3.64305973e-01   1.18274912e-01
   2.03438237e-01  -1.01153564e+00   6.67958856e-01   1.80363625e-01
   1.26524955e-01  -2.96024203e-01  -9.93479714e-02  -4.93405871e-02
   1.02504417e-01   7.63318688e-02  -3.68398607e-01   3.03587675e-01
  -2.90227026e-01   1.51891649e-01  -6.93689287e-03  -3.99766594e-01
  -1.86124116e-01  -2.86920428e-01   2.04880714e-01   1.39914978e+00
   1.84370011e-01  -4.58923727e-01   3.91094625e-01  -7.52937734e-01
   3.05261135e-01  -4.55163687e-01   7.22679734e-01  -3.76093656e-01
   6.05900526e-01   3.26470852e-01   4.72957864e-02  -1.18182398e-01
   3.51043999e-01  -3.07209432e-01  -6.10330477e-02   4.14131492e-01
   7.57511556e-02  -6.48704231e-01   1.42518353e+00  -9.20495167e-02
   6.36665523e-01   5.48510313e-01   5.92754841e-01  -6.29535854e-01
  -4.47180003e-01  -8.99413109e-01  -1.52441502e-01  -1.98326513e-01
   4.74154204e-01  -2.07036674e-01  -6.70400202e-01   6.67807996e-01
  -1.04234733e-01   7.16163218e-01   3.32825005e-01   8.20083246e-02
   5.88186264e-01   4.06852067e-01   2.66174138e-01  -5.35981596e-01
   3.26077454e-02  -4.04357493e-01   2.19569445e-01  -2.74264365e-01
  -1.65187627e-01  -4.06753153e-01   6.12065434e-01  -1.89857081e-01
  -5.56927800e-01  -6.78636551e-01  -7.52498448e-01   1.04564428e+00
   5.32510102e-01   5.05628288e-01   1.95120305e-01  -6.40793025e-01
   5.73082231e-02  -1.58281475e-02  -2.62718409e-01   1.74351722e-01
  -6.95129633e-02   3.44214857e-01  -4.24746841e-01  -2.75907904e-01
  -6.60992935e-02  -1.19041657e+00  -6.01056278e-01   5.67718685e-01
  -6.47478551e-02   1.55902460e-01  -2.48480186e-01   5.56753576e-01
   1.29889056e-01   3.91534269e-01   1.28707469e-01   1.29670590e-01
  -6.98880851e-01   2.43386969e-01   7.70289376e-02  -1.14947490e-01
  -4.31593180e-01  -6.16873622e-01   6.03831768e-01  -2.07050622e-01
   1.23276520e+00  -1.67524610e-02  -4.67656374e-01   1.00281858e+00
   5.17916441e-01  -7.99495637e-01  -4.22653735e-01  -1.45487636e-01
  -8.71369673e-04   1.25453219e-01  -1.25869447e-02   4.66426492e-01
   5.07026255e-01  -6.53024793e-01   7.53435045e-02   8.33864748e-01
   3.37398499e-01   7.50920832e-01  -4.80326146e-01  -4.52838868e-01
   5.92808545e-01  -3.57870340e-01  -1.07011057e-01  -1.13945460e+00
   3.97635132e-01   1.23554178e-01   4.81683850e-01   5.47445454e-02
  -2.18614921e-01  -2.00085923e-01  -3.73975009e-01   8.74632657e-01
   6.71471596e-01  -4.01738763e-01   4.76147681e-01  -5.79257011e-01
  -1.51511624e-01   1.43170074e-01   5.00052273e-01   1.46719962e-01
   2.43085429e-01   5.89158475e-01  -5.25088668e-01  -2.65306592e-01
   2.18211919e-01   3.83228660e-01  -2.51622144e-02   2.32621357e-01
   8.06669474e-01   1.37254462e-01   4.59401071e-01   5.63044667e-01
  -5.79878241e-02   2.68106610e-01   5.47239482e-01  -5.05441546e-01]
</code></pre>

<p>It's so frustrating to read because I just want to get a 1x2 array like <code>[12,19]</code> so I can represent it to graph and make a cosine distance measurement to the 1x2 array.</p>

<p>How to do it? Or how to represent the 1x300 Word2Vec model to a 2D graph?</p>
","python, arrays, numpy, graph, word2vec","<p>There are many ways to apply ""dimensionality reduction"" to high-dimensional data, for aid in interpretation or graphing. </p>

<p>One super-simple way to reduce your 300-dimensions to just 2-dimensions, for plotting on a flat screen/paper: just discard 298 of the dimensions! You'll have something to plot â€“ such as the point <code>(-0.255022556, 1.06162608)</code> if taking just the 1st 2 dimensions of your example vector.</p>

<p>However, starting from word2vec vectors, those won't likely be very interesting points, individually or when you start plotting multiple words. The exact axes dimensions of such vectors are unlikely to be intuitively meaningful to humans, and you're throwing 99.7% of all the meaning per vector away â€“ and quite likely the dimensions which (in concert with each other) capture semantically-meaningful relationships.</p>

<p>So you'd be more likely to do some more thoughtful dimensionality-reduction. A super-simple technique would be to pick two vector-directions that are thought to be meaningful as your new X and Y axes. In the word2vec world, these wouldn't necessarily be existing vectors in the set â€“ though they could be â€“ but might be the difference between two vectors. (The analogy-solving power of word2vec vectors essentially comes from discovering the difference-between two vectors A and B, then applying that difference to a third vector C to find a 4th vector D, at which point D often has the same human-intuitive analogical-relationship to C as B had to A.) </p>

<p>For example, you might difference the word-vectors for 'man' and 'woman', to get a vector which bootstraps your new X-axis. Then difference the word-vectors for 'parent' and 'worker', to get vector which bootstraps your new Y-axis. Then, for every candidate 300-dimensional vector you want to plot, find that candidate vector's ""new X"" by calculating the magnitude of its projection onto your X-direction-vector. Then, find that  candidate vector's ""new Y"" by calculating the magnitude of its projection onto your Y-direction-vector. This <em>might</em> result in a set of relative values that, on a 2-D chart, vaguely match human intuitions about often-observed linguisti relationships between gender and familial/workplace roles. </p>

<p>As @poorna-prudhvi's comment mentions, PCA and t-SNE are other techniques â€“ which may specifically do better at preserving certain interesting qualities of the full-dimensional data. <a href=""https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"" rel=""nofollow noreferrer"">t-SNE</a>, especially, was invented for to support machine-learning and plotting, and tries to keep the distance-relationships that existed in the higher-number-of-dimensions similar in the lower-number-of-dimensions. </p>
",1,0,853,2017-07-22 01:37:42,https://stackoverflow.com/questions/45249799/how-to-represent-word2vec-model-to-graph-or-convert-a-1x300-numpy-array-to-jus
Why doesn&#39;t gensim&#39;s Word2Vec recognize &#39;compute_loss&#39; keyword?,"<p>According to the <strong>gensim.models.Word2Vec</strong> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">API reference</a>, ""compute_loss"" is a valid keyword. However, I get an error that says it's an <code>unexpected keyword</code>.</p>

<p><strong>UPDATE</strong>:</p>

<p>The Word2Vec class on GitHub <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">does have</a> the 'compute_loss' keyword, but my local library does not.
I see that the gensim documentation and library deviate from each other.
I found that the <code>win-64/gensim-2.2.0-np113py35_0.tar.bz2</code>-file in <a href=""https://anaconda.org/anaconda/gensim/files"" rel=""nofollow noreferrer"">conda repository</a> is not up to date.</p>

<p>However after uninstalling gensim with conda, <code>pip install gensim</code> did not change anything as it still doesn't work.</p>

<p>Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p><strong>/END OF UPDATE</strong></p>

<p>I followed and downloaded the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">tutorial notebook on Word2Vec</a>.</p>

<p>In input [25], first cell after ""Training Loss Computation"" headline, I get an error in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> class' initializer. </p>

<p>Input:</p>

<pre><code># instantiating and training the Word2Vec model
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, 
compute_loss=True, hs=0, sg=1, seed=42)

# getting the training loss value
training_loss = model_with_loss.get_latest_training_loss()
print(training_loss)
</code></pre>

<p>Output:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-c2933abf4b08&gt; in &lt;module&gt;()
      1 # instantiating and training the Word2Vec model
----&gt; 2 model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)
      3 
      4 # getting the training loss value
      5 training_loss = model_with_loss.get_latest_training_loss()

TypeError: __init__() got an unexpected keyword argument 'compute_loss'
</code></pre>

<p>I have gensim 2.2.0 installed via conda and have a new new clone from the gensim repository (with the tutorial notebook). I'm using 64-bit Python 3.5.3 on windows 10. (Anaconda)</p>

<p>I've tried to search for others with same encounter, but I haven't been successful. </p>

<p>Do you know the reason for this, and how to fix this? Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p>I've also previously <a href=""https://groups.google.com/forum/#!topic/gensim/J1J2zTZwD7Q"" rel=""nofollow noreferrer"">posted the question</a> in the official mailing list.</p>
","python, gensim, word2vec","<p><strong>UPDATE:</strong> <code>compute_loss</code> was added in version 2.3.0, on July 25th. <strong>/UPDATE</strong></p>

<p>The notebook referenced in the question is on the <strong>develop</strong> branch. The <strong>master</strong> branch has a <a href=""https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">notebook</a> that is consistent with the latest distribution.</p>

<p>The <code>compute_loss</code> parameter was added in <a href=""https://github.com/RaRe-Technologies/gensim/commit/cdc5944a15dc3fecbd80eb61145b7a0ef838c7fd"" rel=""nofollow noreferrer"">this commit</a>, June 19. The <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">last upload</a> to PYPI was June 21, only two days later. (As of today). The <code>compute_loss</code> is not included in the distribution. (Last commit in v2.2.0 is <a href=""https://github.com/RaRe-Technologies/gensim/commit/dfd1f8ea084179a56080b2bfc53b6a6f30a9cc59"" rel=""nofollow noreferrer"">this</a>.)</p>

<p>I assume that the solution is to wait for the next version of gensim, and download code from repository in the mean time.</p>

<p>However, this might cause challenges to get gensim FAST version to work, at least on Windows. See <a href=""https://stackoverflow.com/questions/44490739/using-gensim-shows-slow-version-of-gensim-models-doc2vec-being-used"">Using Gensim shows &quot;Slow version of gensim.models.doc2vec being used&quot;</a>.</p>

<p>How to install gensim from GitHub is explained in their <a href=""https://radimrehurek.com/gensim/install.html#install-gensim"" rel=""nofollow noreferrer"">install documentation</a>.</p>
",2,3,1796,2017-07-24 08:47:11,https://stackoverflow.com/questions/45276029/why-doesnt-gensims-word2vec-recognize-compute-loss-keyword
Using a Word2Vec model pre-trained on wikipedia,"<p>I need to use gensim to get vector representations of words, and I figure the best thing to use would be a word2vec module that's pre-trained on the english wikipedia corpus. Does anyone know where to download it, how to install it, and how to use gensim to create the vectors?</p>
","wikipedia, gensim, word2vec","<p>@imanzabet provided useful links with pre-trained vectors, but if you want to train the models yourself using genism than you need to do two things:</p>
<ol>
<li><p>Acquire the Wikipedia data, which you can access <a href=""https://dumps.wikimedia.org/"" rel=""nofollow noreferrer"">here</a>.  Looks like the most recent snapshot of English Wikipedia was on the 20th, and it can be found <a href=""https://dumps.wikimedia.org/enwiki/20170720/"" rel=""nofollow noreferrer"">here</a>. I believe the other English-language &quot;wikis&quot; e.g. <a href=""https://en.wikiquote.org"" rel=""nofollow noreferrer"">quotes</a> are captured separately, so if you want to include them you'll need to download those as well.</p>
</li>
<li><p>Load the data and use it to generate the models. That's a fairly broad question, so I'll just link you to the excellent <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">genism documentation</a> and <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">word2vec tutorial</a>.</p>
</li>
</ol>
",2,18,21396,2017-07-25 17:51:48,https://stackoverflow.com/questions/45310409/using-a-word2vec-model-pre-trained-on-wikipedia
similarity measurement among names?,"<p>I have a list of names with me and iam trying to find the most similar 5 names from the list of any given name as a query.
I thought of applying word2vec or else using Text.similar() from nltk.
but iam not sure whether these will work for names as well.</p>

<p>any similarity measure would work for me.
any suggestions?
this not for any project but just i wanted to learn new things.</p>
","nlp, nltk, stanford-nlp, word2vec","<p>Since you added NLTK, I assume you are fine working in Python.<br>
Check out the <a href=""https://github.com/jamesturk/jellyfish"" rel=""nofollow noreferrer"">Jellyfish</a> library which contains 10 different algorithms for comparing strings. Some of them will compare just the characters while others will try to guess how a string would be pronounced and help you identify other phrases that are very differently spelt but would sound similar.<br>
The actual algorithms are all written in C and so this library is pretty efficient!<br>
I think you will find the Jaro-Winkler distance to be most useful. Also check out <a href=""http://users.cecs.anu.edu.au/~Peter.Christen/publications/tr-cs-06-02.pdf"" rel=""nofollow noreferrer"">this paper</a>.</p>
",4,0,1893,2017-07-26 09:35:02,https://stackoverflow.com/questions/45322996/similarity-measurement-among-names
How to obtain the word list from pyspark word2vec model?,"<p>I am trying to generate word vectors using PySpark. Using gensim I can see the words and the closest words as below:</p>



<pre class=""lang-python prettyprint-override""><code>sentences = open(os.getcwd() + ""/tweets.txt"").read().splitlines()
w2v_input=[]
for i in sentences:
    tokenised=i.split()
    w2v_input.append(tokenised)
model = word2vec.Word2Vec(w2v_input)
for key in model.wv.vocab.keys():
    print key
    print model.most_similar(positive=[key])
</code></pre>

<p>Using PySpark</p>

<pre class=""lang-python prettyprint-override""><code>inp = sc.textFile(""tweet.txt"").map(lambda row: row.split("" ""))
word2vec = Word2Vec()
model = word2vec.fit(inp)
</code></pre>

<p>How can I generate the words from the vector space in model? That is the pyspark equivalent of the gensim <code>model.wv.vocab.keys()</code>?</p>

<p>Background: I need to store the words and the synonyms from the model in a map so I can use them later for finding the sentiment of a tweet. I cannot reuse the word-vector model in the map functions in pyspark as the model belongs to the spark context (error pasted below). I want the pyspark word2vec version instead of gensim because it provides better synonyms for certain test words.</p>

<pre class=""lang-python prettyprint-override""><code> Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation.SparkContext can only be used on the driver, not in code that it run on workers.
</code></pre>

<p>Any alternative solution is also welcome.</p>
","apache-spark, nlp, pyspark, apache-spark-mllib, word2vec","<p>The equivalent command in Spark is <code>model.getVectors()</code>, which again returns a dictionary. Here is a quick toy example with only 3 words (<code>alpha, beta, charlie</code>), adapted from the <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.Word2Vec"" rel=""noreferrer"">documentation</a>:</p>



<pre class=""lang-python prettyprint-override""><code>sc.version
# u'2.1.1'

from pyspark.mllib.feature import Word2Vec
sentence = ""alpha beta "" * 100 + ""alpha charlie "" * 10
localDoc = [sentence, sentence]
doc = sc.parallelize(localDoc).map(lambda line: line.split("" ""))
word2vec = Word2Vec()
model = word2vec.fit(doc)

model.getVectors().keys()
#  [u'alpha', u'beta', u'charlie']
</code></pre>

<p>Regarding finding synonyms, you may find <a href=""https://stackoverflow.com/questions/34172242/spark-word2vec-vector-mathematics/34298583#34298583"">another answer of mine</a> useful.</p>

<p>Regarding the error you mention and a possible workaround, have a look at <a href=""https://stackoverflow.com/questions/41046843/how-to-load-a-word2vec-model-and-call-its-function-into-the-mapper/41190031#41190031"">this answer</a> of mine.</p>
",5,4,7193,2017-07-27 09:17:59,https://stackoverflow.com/questions/45346418/how-to-obtain-the-word-list-from-pyspark-word2vec-model
Tensorflow: pre-trained embeddings initialization issue during retraining,"<p>My goal is (1) Load a pre-trained word embedding matrix from a file as the initial value; (2) Fine tune the word embedding instead of keeping it fixed; (3) Each time I restore the model, load the fine-tuned word embedding instead of the pre-trained one.</p>

<p>I have tried sth like:</p>

<pre><code>class model():
    def __init__(self):
    # ...
    def _add_word_embed(self):
        W = tf.get_variable('W', [self._vsize, self._emb_size], 
                 initializer=tf.truncated_normal_initializer(stddev=1e-4))
        W.assign(load_and_read_w2v())
        # ...
    def _add_seq2seq(self):
        # ...
    def build_graph(self):
        self._add_word_embed()
        self._add_seq2seq()
</code></pre>

<p>But this approach would cover the fine-tuned word embedding whenever I stop the training and restart it. I also tried <code>sess.run(W.assign())</code> after calling <code>model.build_graph</code>. But it threw an error that the graph has been finalized and I can not change it anymore. Could you please tell me the right way to achieve it? Thanks in advance!</p>

<p>EDIT:</p>

<p>This question is not duplicated as IT HAS A NEW REQUIREMENT: USE THE PRE-TRAINED WORD EMBEDDING AT THE BEGINNING OF TRAINING AND FIND-TUNE IT AFTERWARDS. I ALSO ASK HOW TO DO THIS EFFICIENTLY. THE ACCEPTED ANSWER IN THAT QUESTION IS NOT FXXKING SATISFIED WITH THIS REQUIREMENT. CAN YOU THINK TWICE BEFORE YOU MARK ANY QUESTION AS DUPLICATED ???????????</p>
","python, tensorflow, deep-learning, word2vec","<p>Here is a toy example on how to do it:</p>

<pre><code># The graph

# Inputs
vocab_size = 2
embed_dim = 2
embedding_matrix = np.ones((vocab_size, embed_dim))

#The weight matrix to initialize with embeddings
W = tf.get_variable(initializer=tf.zeros([vocab_size, embed_dim]), name='embed', trainable=True)

# global step used to take care of the weight initialization 
# for the first time will be loaded from numpy array and not during retraining.
global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')

# Initialiazation of weights based on global_step
initW = tf.cond(tf.equal(global_step, 0), lambda:W.assign(embedding_matrix), lambda: W)
inc = tf.assign_add(W,[[1, 1],[1, 1]])

# Update global step
update = tf.assign_add(global_step, 1)
op = tf.group(inc, update)

# init_fn 
def init_embed(sess):
  sess.run(initW)
</code></pre>

<p>Now if we run the above in a session:</p>

<pre><code>sv = tf.train.Supervisor(logdir='tmp',init_fn=init_embed)
with sv.managed_session() as sess:
   print('global step:', sess.run(global_step))
   print('Initial weight:')
   print(sess.run(W))
   for i in range(2):  
      sess.run([op])
    _ W, g_step= sess.run([W, global_step])
   print('Final weight:')        
   print(_W)
   sv.saver.save(sess,sv.save_path, global_step=g_step)

# Output at first run
   Initial weight:
   [[ 1.  1.]
   [ 1.  1.]]

   Final weight:
   [[ 3.  3.]
   [ 3.  3.]]

#Output at second run
   Initial weight:
   [[ 3.  3.]
   [ 3.  3.]]
   Final weight:
   [[ 5.  5.]
   [ 5.  5.]]
</code></pre>
",3,0,875,2017-07-30 15:19:37,https://stackoverflow.com/questions/45401394/tensorflow-pre-trained-embeddings-initialization-issue-during-retraining
Gensim: KeyError: &quot;word not in vocabulary&quot;,"<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>

<pre><code>b = ['let',
 'know',
 'buy',
 'someth',
 'featur',
 'mashabl',
 'might',
 'earn',
 'affili',
 'commiss',
 'fifti',
 'year',
 'ago',
 'graduat',
 '21yearold',
 'dustin',
 'hoffman',
 'pull',
 'asid',
 'given',
 'one',
 'piec',
 'unsolicit',
 'advic',
 'percent',
 'buy']
</code></pre>

<p><strong>Model</strong></p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)
print(model) 
### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####
</code></pre>

<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>

<blockquote>
  <p>KeyError: ""word 'buy' not in vocabulary""</p>
</blockquote>

<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>
","python, nlp, gensim, word2vec, topic-modeling","<p>The first parameter passed to <code>gensim.models.Word2Vec</code> is an iterable of sentences. Sentences themselves are a list of words. From the docs:</p>

<blockquote>
  <p>Initialize the model from an iterable of <code>sentences</code>. Each sentence is a
  list of words (unicode strings) that will be used for training.</p>
</blockquote>

<p>Right now, it thinks that each word in your list <code>b</code> is a sentence and so it is doing <code>Word2Vec</code> for each <strong>character</strong> in each word, as opposed to each word in your <code>b</code>. Right now you can do:</p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)

print(model['a'])
array([  7.42487283e-03,  -5.65282721e-03,   1.28707094e-02, ... ]
</code></pre>

<p>To get it to work for words, simply wrap <code>b</code> in another list so that it is interpreted correctly:</p>

<pre><code>model = gensim.models.Word2Vec([b],min_count=1,size=32)

print(model['buy'])
array([-0.01331611,  0.00496594, -0.00165093, -0.01444992,  0.01393849, ... ]
</code></pre>
",39,26,45918,2017-07-31 15:59:08,https://stackoverflow.com/questions/45420466/gensim-keyerror-word-not-in-vocabulary
Python: clustering similar words based on word2vec,"<p>This might be the naive question which I am about to ask. I have a tokenized corpus on which I have trained Gensim's Word2vec model. The code is as below</p>

<pre><code>site = Article(""http://www.datasciencecentral.com/profiles/blogs/blockchain-and-artificial-intelligence-1"")
site.download()
site.parse()

def clean(doc):
    stop_free = "" "".join([i for i in word_tokenize(doc.lower()) if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    snowed = "" "".join(snowball.stem(word) for word in normalized.split())
    return snowed   

b = clean(site.text)
model = gensim.models.Word2Vec([b],min_count=1,size=32)
print(model) ### Prints: Word2Vec(vocab=643, size=32, alpha=0.025) ####
</code></pre>

<p>To cluster similar words, I am using PCA to visualize the clusters of similar words. But the problem is that it is forming only big cluster as seen in the image.</p>

<p><strong>PCA &amp; scatter plot Code:</strong></p>

<pre><code>vocab = list(model.wv.vocab)
X = model[vocab]
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

df = pd.concat([pd.DataFrame(X_pca),
                pd.Series(vocab)],
               axis=1)
df.columns = ['x','y','word']

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.scatter(df['x'],df['y'])
plt.show()
</code></pre>

<p>So, I have three questions here:</p>

<p>1) Is just one article enough to have the clear segregation of the clusters?</p>

<p>2) If I have a model trained with huge corpus and I want to predict the similar words in the new article and visualize them (i.e. words in the article I'm predicting) in the form of clusters, is there a way to do that?</p>

<p>I highly appreciate your suggestions. Thank you.</p>
","python, nlp, cluster-analysis, word2vec, topic-modeling","<ol>
<li>No, not really. For reference, common word2vec models which are trained on wikipedia (in english) consists around 3 billion words.</li>
<li>You can use KNN (or something similar). Gensim has the <code>most_similar</code> function to get the closest words. Using a dimensional reduction (like PCA or tsne) you can get yourself a nice cluster. (Not sure if gensim has tsne module, but sklearn has, so you can use it)</li>
</ol>

<p>btw you're referring to some image, but it's not available.</p>
",1,0,3819,2017-07-31 20:50:44,https://stackoverflow.com/questions/45425070/python-clustering-similar-words-based-on-word2vec
Word Embedding Relations,"<p>I want to learn more about the algebra function I can perform over the word embedding vectors. I know that by cosine similarity I can get the most similar word. But I need to do one more level of inference and get the relations below:</p>

<p>The relation of X1 to X2 is like relation of X3 to X4.</p>

<p>As and example I can say the relation of princess to prince is like women to men. I have X1 to X3 and my problem is how efficiently I can figure out what X4 can be. I tried cosine to absolute difference of vectors but it is not working.</p>
","numpy, matrix, word2vec, algebra, word-embedding","<p>You can look at exactly how the original Google-released <code>word2vec</code> code solves analogies in its <code>word-analogy.c</code> code: </p>

<p><a href=""https://github.com/tmikolov/word2vec/blob/master/word-analogy.c"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/word-analogy.c</a></p>

<p>If you're more familiar with Python, you can look how the gensim Word2Vec implementation tests analogies, in its <code>accuracy()</code> method, by reading the analogy ""a:b :: c:expected"" from the <code>questions-words.txt</code> file (as provided in the original Google word2vec package), then using <code>b</code> and <code>c</code> as positive (added) examples, and <code>a</code> as a negative example (subtracted), to then find words near the resulting vector:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L697"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L697</a></p>

<p>The operation of the used <code>most_similar()</code> function, which accepts multiple <code>positive</code> and <code>negative</code> examples before returning a list of closest vectors, is seen at: </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L290"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/keyedvectors.py#L290</a></p>
",0,0,382,2017-08-01 18:02:56,https://stackoverflow.com/questions/45444821/word-embedding-relations
Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class,"<p>I have been struggling to understand the use of <code>size</code> parameter in the <code>gensim.models.Word2Vec</code></p>

<p>From the Gensim documentation, <code>size</code> is the dimensionality of the vector. Now, as far as my knowledge goes, word2vec creates a vector of the probability of closeness with the other words in the sentence for each word. So, suppose if my <code>vocab</code> size is 30 then how does it create a vector with the dimension greater than 30? Can anyone please brief me on the optimal value of <code>Word2Vec</code> size? </p>

<p>Thank you.</p>
","python, gensim, word2vec","<p><code>size</code> is, as you note, the dimensionality of the vector.</p>

<p>Word2Vec needs large, varied text examples to create its 'dense' embedding vectors per word. (It's the competition between many contrasting examples during training which allows the word-vectors to move to positions that have interesting distances and spatial-relationships with each other.)</p>

<p>If you only have a vocabulary of 30 words, word2vec is unlikely an appropriate technology. And if trying to apply it, you'd want to use a vector size much lower than your vocabulary size â€“ ideally <em>much</em> lower. For example, texts containing many examples of each of tens-of-thousands of words might justify 100-dimensional word-vectors.</p>

<p>Using a higher dimensionality than vocabulary size would more-or-less guarantee 'overfitting'. The training could tend toward an idiosyncratic vector for each word â€“ essentially like a 'one-hot' encoding â€“ that would perform better than any other encoding, because there's no cross-word interference forced by representing a larger number of words in a smaller number of dimensions. </p>

<p>That'd mean a model that does about as well as possible on the Word2Vec internal nearby-word prediction task â€“ but then awful on other downstream tasks, because there's been no generalizable relative-relations knowledge captured. (The cross-word interference is what the algorithm <em>needs</em>, over many training cycles, to incrementally settle into an arrangement where similar words <em>must</em> be similar in learned weights, and contrasting words different.) </p>
",21,9,21079,2017-08-01 18:12:40,https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class
Loadiing a trained Word2Vec model in Spark,"<p>I am trying to load google's Pre-trained vectors 'GoogleNews-vectors-negative300.bin.gz' <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google-word2vec</a> into spark. </p>

<p>I converted the bin file to txt and created a smaller chunk for testing that I called 'vectors.txt'. I tried loading it as the following:</p>

<pre><code>      val sparkSession = SparkSession.builder
  .master(""local[*]"")
  .appName(""Word2VecExample"")
  .getOrCreate()

  val model2= Word2VecModel.load(sparkSession.sparkContext, ""src/main/resources/vectors.txt"")

  val synonyms = model2.findSynonyms(""the"", 5)

  for((synonym, cosineSimilarity) &lt;- synonyms) {
    println(s""$synonym $cosineSimilarity"")
  }
</code></pre>

<p>and to my surprise I am faced with the following error:</p>

<pre><code>Exception in thread ""main"" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/elievex/Repository/ARCANA/src/main/resources/vectors.txt/metadata
</code></pre>

<p>I'm not sure where did the 'metadata' after 'vectors.txt' came from.
I am using Spark, Scala and Scala IDE for Eclipse.</p>

<p>What am I doing wrong? is there a different way to load a pre-trained model in spark? Would appreciate any tips.</p>
","scala, apache-spark, word2vec","<p>How exactly did you get vector.txt? If you read JavaDoc for <a href=""https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/mllib/feature/Word2VecModel.html#save(org.apache.spark.SparkContext,%20java.lang.String)"" rel=""nofollow noreferrer"">Word2VecModel.save</a> you may see that:</p>

<blockquote>
  <p>This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ 
  <br/>The model may be loaded using Loader.load.</p>
</blockquote>

<p>So what you need is model in <a href=""https://parquet.apache.org/"" rel=""nofollow noreferrer"">Parquet</a> format which is standard for Spark ML models.</p>

<p>Unfortunately load from Google's native format has not been implemented yet (see <a href=""https://issues.apache.org/jira/browse/SPARK-9484"" rel=""nofollow noreferrer"">SPARK-9484</a>).</p>
",0,0,2065,2017-08-03 18:46:39,https://stackoverflow.com/questions/45492239/loadiing-a-trained-word2vec-model-in-spark
What is the effect of adding new word vector embeddings onto an existing embedding space for Neural networks,"<p>In Word2Vector, the word embeddings are learned using co-occurrence and updating the vector's dimensions such that words that occur in each other's context come closer together.</p>

<p>My questions are the following:</p>

<p>1) If you already have a pre-trained set of embeddings, let's say a 100 dimensional space with 40k words, can you add 10 additional words onto this embedding space without changing the existing word embeddings. So you would only be updating the dimensions of the new words using the existing word embeddings. I'm thinking of this problem with respect to the  ""word 2 vector"" algorithm, but if people have insights on how GLoVe embeddings work in this case, I am still very interested.</p>

<p>2) Part 2 of the question is; Can you then use the NEW word embeddings in a NN that was trained with the previous embedding set and expect reasonable results. For example, if I had trained a NN for sentiment analysis, and the word ""nervous"" was previously not in the vocabulary, then would ""nervous"" be correctly classified as ""negative"".</p>

<p>This is a question about how sensitive (or robust) NN are with respect to the embeddings. I'd appreciate any thoughts/insight/guidance.</p>
","neural-network, word2vec, word-embedding","<p>The initial training used info about known words to plot them in a useful N-dimensional space. </p>

<p>It is of course theoretically possible to then use new information, about new words, to also give them coordinates in the same space. You would want lots of varied examples of the new words being used together with the old words. </p>

<p>Whether you want to freeze the positions of old words, or let them also drift into new positions based on the new examples, could be an important choice to make. If you've already trained a pre-existing classifier (like a sentiment classifier) using the older words, and didn't want to re-train that classifier, you'd probably want to lock the old words in place, and force the new words into compatible positioning (even if the newer combined text examples would otherwise change the relative positions of older words). </p>

<p>Since after an effective train-up of the new words, they should generally be near similar-meaning older words, it would be reasonable to expect classifiers that worked on the old words to still do something useful on the new words. But how well that'd work would depend on lots of things, including how well the original word-set covered all the generalizable 'neighborhoods' of meaning. (If the new words bring in shades of meaning of which there were no examples in the old words, that area of the coordinate-space may be impoverished, and the classifier may have never had a good set of distinguishing examples, so performance could lag.)</p>
",2,1,2602,2017-08-03 23:05:02,https://stackoverflow.com/questions/45495885/what-is-the-effect-of-adding-new-word-vector-embeddings-onto-an-existing-embeddi
What is the difference between wmd (word mover distance) and wmd based similarity?,"<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>

<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)
</code></pre>

<p>Reference: <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>

<p>Reference: 
<a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>

<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>

<pre><code>n_queries = len(query)
result = []
for qidx in range(n_queries):
    # Compute similarity for each query.
    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
    qresult = numpy.array(qresult)
    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.

    # Append single query result to list of all results.
    result.append(qresult)
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>
","nlp, nltk, gensim, word2vec, word-embedding","<p>I think with the 'update' you'e more-or-less answered your own question. </p>

<p>That one is a distance, and the other a similarity, is the only difference between the two calculations. As the notebook you link notes in the <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html#Part-2:-Similarity-queries-using-WmdSimilarity"" rel=""noreferrer"">relevant section</a>:</p>

<blockquote>
  <p>WMD is a measure of distance. The similarities in WmdSimilarity are simply the negative distance. Be careful not to confuse distances and similarities. Two similar documents will have a high similarity score and a small distance; two very different documents will have low similarity score, and a large distance.</p>
</blockquote>

<p>As the code you've excerpted shows, the similarity measure being used there is not exactly the 'negative' distance, but scaled so all similarity values are from 0.0 (exclusive) to 1.0 (inclusive). (That is, a zero distance becomes a 1.0 similarity, but ever-larger distances become ever-closer to 0.0.)</p>
",5,4,2262,2017-08-04 08:59:17,https://stackoverflow.com/questions/45502464/what-is-the-difference-between-wmd-word-mover-distance-and-wmd-based-similarit
How to calculate a One-Hot Encoding value into a real-valued vector?,"<p>In Word2Vec, i've learned that both of CBOW and Skip-gram produce a one-hot encoding value to create a vector (cmiiw), I wonder how to calculate or represents a One-Hot Encoding value into a real-valued vector, for example (source: <a href=""http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations"" rel=""nofollow noreferrer"">DistrictDataLab's Blog about Distributed Representations</a>)
from this:
<img src=""https://s1.postimg.org/naavbr5lr/Lev_Konstantinovskiy_Next_generation_of_word_embeddings_in_Gensim_2.jpg"" alt=""One-Hot Encoding&#39;s example"">
into:
<img src=""https://s1.postimg.org/734kvoylb/Lev_Konstantinovskiy_Next_generation_of_word_embeddings_in_Gensim_2.jpg"" alt=""One-Hot Encoding&#39;s example"">
please help, I was struggling on finding this information.</p>
","nlp, deep-learning, word2vec, word-embedding","<p>The word2vec algorithm itself is what incrementally learns the real-valued vector, with varied dimension values. </p>

<p>In contrast to the one-hot encoding, these vectors are often called ""dense embeddings"". They're ""dense"" because unlike the one-hot encoding, which is ""sparse"" with many dimensions and mostly zero values, they have fewer dimensions and (usually) no zero-values. They're an ""embedding"" because they've ""embed"" a discrete set-of-words into another continuous-coordinate-system.</p>

<p>You'd want to read the <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">original word2vec paper</a> for a full formal description of how the dense embeddings are made. </p>

<p>But the gist is that the dense vectors start totally random, and so at first the algorithm's internal neural network is useless for predicting neighboring words. But each (context)->(target) word training example from a text corpus is tried against the network, and each time the difference from the desired prediction is used to apply a tiny nudge, towards a better prediction, to both word-vector and internal-network-weight values. </p>

<p>Repeated many times, initially with larger nudges (higher learning-rate) then with ever-smaller nudges, the dense vectors rearrange their coordinates from their initial randomness to a useful relative-arrangement â€“ one that's about-as-good as possible for predicting the training text, given the limits of the model itself. (That is, any further nudge that improves predictions on some examples, worsens it on others â€“ so you might as well consider training done.)</p>

<p>You then read the resulting dense embedding real-valued vectors out of the model, and use them for purposes other than just nearby-word prediction.  </p>
",4,2,945,2017-08-06 11:18:31,https://stackoverflow.com/questions/45531476/how-to-calculate-a-one-hot-encoding-value-into-a-real-valued-vector
Is there a semantic similarity method that outperforms word2vec approach for semantic accuracy?,"<p>I am looking at various semantic similarity methods such as word2vec, word mover distance (WMD), and fastText. fastText is not better than Word2Vec as for as semantic similarity is concerned. WMD and Word2Vec have almost similar results. </p>

<p>I was wondering if there is an alternative which has outperformed the Word2Vec model for semantic accuracy? </p>

<p><strong>My use case:</strong>
<em>Finding word embeddings for two sentences, and then use cosine similarity to find their similarity.</em> </p>
","nlp, nltk, gensim, word2vec, fasttext","<p>Whether any technique ""outperforms"" another will depend highly on your training data, the specific metaparameter options you choose, and your exact end-task. (Even ""semantic similarity"" may have many alternate aspects depending on the application.)</p>

<p>There's no one way to go from word2vec word-vectors to a sentence/paragraph vector. You could add the raw vectors. You could average the unit-normalized vectors. You could perform some other sort of weighted-average, based on other measures of word-significance. So your implied baseline is unclear. </p>

<p>Essentially you have to try a variety of methods and parameters, for your data and goal, with your custom evaluation. </p>

<p>Word Mover's Distance <em>doesn't</em> reduce each text to a single vector, and the pairwise calculation between two texts can be expensive, but it has reported very good performance on some semantic-similarity tasks. </p>

<p>FastText is essentially word2vec with some extra enhancements and new modes. Some modes with the extras turned off are exactly the same as word2vec, so using FastText word-vectors in some wordvecs-to-textvecs scheme should closely approximate using word2vec word-vectors in the same scheme. Some modes might help the word-vector quality for some purposes, but make the word-vectors less effective inside a wordvecs-to-textvecs scheme. Some modes might make the word-vector better for sum/average composition schemes â€“ you should look especially at the 'classifier' mode, which trains word-vecs to be good, when averaged, at a classification task. (To the extent you may have any semantic labels for your data, this might make the word-vecs more composable for semantic-similarity tasks.)</p>

<p>You may also want to look at the 'Paragraph Vectors' technique (available in gensim as <code>Doc2Vec</code>), or other research results that go by the shorthand names 'fastSent' or 'sent2vec'. </p>
",4,2,1618,2017-08-08 10:05:15,https://stackoverflow.com/questions/45565389/is-there-a-semantic-similarity-method-that-outperforms-word2vec-approach-for-sem
Setting max length of char n-grams for fastText,"<p>I want to compare word2vec and fasttext model based on this comparison tutorial. 
<a href=""https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb"" rel=""nofollow noreferrer"">https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a></p>

<p>According to this, the semantic accuracy of fastText model increase when we set the max length of char n-grams to zero, such that fastText starts to behave almost like to word2vec. It ignores the ngrams. </p>

<p>However, I can not find any formation on how to set this parameter while loading a fastText model. Any ideas on how to do this?</p>
","nlp, nltk, gensim, word2vec, fasttext","<p>The parameter is set at training time â€“ and then the model is built using that parameter, and dependent on that parameter for interpretation. So you wouldn't typically change it upon loading an already-trained model, and there's no API in gensim (or the original FastText) to change the setting on an already-trained model. </p>

<p>(By looking at the source and tampering with the loaded model state directly, you <em>might</em> be able to approximate the effect of ignoring char-ngrams that had been trained â€“ but that'd be a novel mode, not at all like the no-ngrams-trained mode evaluated in the notebook you've linked. It might generate interesting, or awful, results â€“ no way to tell without trying it.)</p>
",2,0,1255,2017-08-08 13:03:45,https://stackoverflow.com/questions/45569142/setting-max-length-of-char-n-grams-for-fasttext
Semantic Similarity across multiple languages,"<p>I am using word embeddings for finding similarity between two sentences. Using word2vec, I also get a similarity measure if one sentence is in English and the other one in Dutch (though not very good). </p>

<p>So I started wondering if it's possible to compute the similarity between two sentences in two different languages (without an explicit translation), especially if the languages have some similarities (Englis/Dutch)?</p>
","nlp, nltk, gensim, word2vec","<p>Let's assume that your sentence-similarity scheme uses only word-vectors as an input â€“ as in simple word-vector averaging schemes, or Word Mover's Distance. </p>

<p>It should be possible to do what you've suggested, provided that:</p>

<ul>
<li>you have good sets of word-vectors for each language's words</li>
<li>the coordinate spaces of the word-vectors are compatible, meaning the words for the exact-same things in both languages have nearly-identical coordinates (and other words with similar meanings have close coordinates)</li>
</ul>

<p>That second quality is not automatically assured. In fact, given the random initialization of word2vec models, and other randomization introduced by the algorithm/implementation, even subsequent training runs on the exact same data won't place words into the exact same places. So word-vectors trained on totally-separate English/Dutch corpuses won't likely place equivalent words at the same coordinates. </p>

<p>But, you can learn an algebraic-transformation between two spaces, based on certain anchor/reference word-pairs (that you know should have similar vectors). You can then apply that transformation to all words in one of the two sets, which results in you having vectors for those 'foreign' words within the comparable coordinate-space of the 'canonical' word-set. </p>

<p>In fact this very idea was used in one of the first word2vec papers:</p>

<p>""<a href=""https://arxiv.org/abs/1309.4168"" rel=""nofollow noreferrer"">Exploiting Similarities among Languages for Machine Translation</a>""</p>

<p>If you were to apply a similar transformation on one of your language word-vector sets, then use those transformed vectors as inputs to your sentence-vector scheme, those sentence-vectors would likely have some useful comparability to sentence-vectors in the other language, bootstrapped from word-vectors in the same coordinate-space. </p>

<p><strong>Update:</strong> There's a very interesting <a href=""https://arxiv.org/abs/1601.02502"" rel=""nofollow noreferrer"">recent paper</a> that manages to train word-vectors in multiple languages simultaneously, using a corpus that includes both raw sentences in each single language, and a (smaller) set of aligned-sentences that are known to mean the same in both languages. Gensim doesn't yet support this mode, but there's <a href=""https://groups.google.com/d/msg/gensim/zksGwKHnIUA/7lde13FbAgAJ"" rel=""nofollow noreferrer"">discussion of supporting it</a> in a future refactor.  </p>
",7,3,3702,2017-08-08 14:38:45,https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages
Reading large files into memory for word2vec conversion,"<p>I'm trying to read a large .log file (having more than sixty thousand lines) into memory. I want to apply Word2Vec algorithm implemented in gensim. I have tried number of solutions, but none of them seems to be working. Any help would be appreciated.</p>

<p>Code1:</p>

<pre><code>def file_reader(file_obj):
    return [word for line in open(file_obj, 'r') for word in line.split()]
</code></pre>

<p>Code2:</p>

<pre><code>for i,line in enumerate(open(file_obj,'r')):
       print(i,line)
       sentences += line
</code></pre>
","python, word2vec","<p>You need to chunk the file somehow.  Since your file is line based, you can use python's normal line chunking (ie <code>for line in file</code>).  The problem you're running into is that this</p>

<pre><code>def file_reader(file_obj):
    return [word for line in open(file_obj, 'r') for word in line.split()]
</code></pre>

<p>loads the whole file into your return statement.</p>

<p>Instead of doing this, you need to assemble the vector at the same time you read the line.  As you encounter each word, do your stop-word removal and lemitization right there, and if there's anything left, add it to your vector.</p>

<p>Or, process it sentence-by-sentence if you need more context to each word.  In either case, do the processing in your reader as it's read in rather than gathering all the data from the file and then processing it.</p>
",0,0,420,2017-08-08 15:34:42,https://stackoverflow.com/questions/45572515/reading-large-files-into-memory-for-word2vec-conversion
Extract more meaningful words from publicly available word embedding,"<p>I have two publicly available word embeddings such as Glove and Google Word2vec.
However, in their vocabulary, there are too many misspelling words or garbage words(e.g., ##AA##, adirty, etc). To avoid this words, I would like to extract frequent word(e.g., top 50000 words) since I think relatively high frequent words has normal forms.</p>

<p>So, I wonder if there is a way to find word frequency in above two pretrained word embeddings. If not, I want to know if there are some techniques to exclude this words. </p>
","machine-learning, word2vec, word-embedding","<p>The GoogleNews vector set does not contain frequency information, but does seem to be sorted from most-frequent to least-frequent. So, if you change the code that loads it to only load the first N words, you should get the N most-frequent words.</p>

<p>(The Python gensim library for training or working with word-vectors includes this as a <code>limit</code> option on the <code>load_word2vec_format()</code> function.)</p>

<p>GLoVe may follow the same convention â€“ a look over the order-of-words in the file should give a good idea.</p>
",3,3,314,2017-08-11 09:43:53,https://stackoverflow.com/questions/45631962/extract-more-meaningful-words-from-publicly-available-word-embedding
Evaluating Word2Vec model by finding linear algebraic structure of words,"<p>I have built Word2Vecmodel using gensim library in python.I want to evaluate my word embedding as follows</p>

<blockquote>
  <p>If A is related to B and C is related to D, then A-C+B should be equal to D. For example, embedding vector arithmetic of ""India""-""Rupee""+""Japan"" should be equal to the embedding of ""Yen"".</p>
</blockquote>

<p>I have used in built functions of gensim like predict_output_word,most_similar but couldn't get desired results.</p>

<pre><code>new_model.predict_output_word(['india','rupee','japan'],topn=10)
new_model.most_similar(positive=['india', 'rupee'], negative=['japan'])
</code></pre>

<p>Kindly help me in evaluating my model as per the criteria above.</p>
","nlp, word2vec, word-embedding","<p>You should you the <code>most_similar()</code> method's <code>positive</code> and <code>negative</code> arguments in the same manner as the <code>accuracy()</code> method:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/718b1c6bd1a8a98625993d73b83d98baf385752d/gensim/models/keyedvectors.py#L697"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/718b1c6bd1a8a98625993d73b83d98baf385752d/gensim/models/keyedvectors.py#L697</a></p>

<p>Specifically, if you have a analogy of the form ""A is to B as C is to [expected]"", you should look at:</p>

<pre><code>results = model.most_similar(positive=[word_b, word_c], negative=[word_a])
</code></pre>

<p>Or in your example:</p>

<pre><code>results = model.most_similar(positive=['rupee', 'japan'], negative=['india'])
</code></pre>
",2,0,378,2017-08-12 06:53:55,https://stackoverflow.com/questions/45647439/evaluating-word2vec-model-by-finding-linear-algebraic-structure-of-words
Word2Vec word embeddings on H2O Deep Water with GPU,"<p>We followed the text categorization process with iterating the items below:</p>

<ol>
<li>Create a Word2Vec word embedding model with text documents.</li>
<li>Do a Grid Search and tree depth parameters.</li>
<li>Select the best performed Final GBM model.</li>
</ol>

<p>As we iterate through the list CPU cores are working at %100 load. Are there any procedure or solution iterating the above process with H2O Deep Water GPU capabilities ?</p>
","deep-learning, word2vec, h2o, grid-search, gbm","<p>No, no and maybe.</p>

<p>The maybe is that you could switch from GBM to xgboost, which does have a GPU option (I believe only single-node is supported, and only in Linux currently). xgboost is apparently slightly quicker on small data sets, h2o.gbm slightly quicker on large data sets. If you have a GPU going free, and are using the latest version of H2O, it should be easy to swap <code>h2o.gbm</code> with <code>h2o.xgboost</code> (<code>H2OXGBoostEstimator</code> if using Python API) and see for yourself.</p>

<p>I'd be interested to hear the relative timings!</p>

<p>(BTW, the 2nd ""no"" is for GPU use specifically for grids; but all the effort is in the models, not the grid itself, so the 2nd ""no"" could just as well be ""N/A"")</p>
",1,1,158,2017-08-12 10:51:04,https://stackoverflow.com/questions/45649375/word2vec-word-embeddings-on-h2o-deep-water-with-gpu
Word embedding training,"<p>I have one corpus for word embedding. Using this corpus, I trained my word embedding. However, whenever I train my word embedding, the results are quite different(this results are based on K-Nearest Neighbor(KNN)). For example, in the first training, '<strong>computer</strong>' nearest neighbor words are '<strong>laptops</strong>', '<strong>computerized</strong>' ,'<strong>hardware</strong>'. But, in the second training, this knn words are '<strong>software</strong>', '<strong>machine</strong>',...('laptops' is low ranked!) - all training are performed independently 20 epochs, and hyper-parameters are all the same.</p>

<p>I want to train my word embedding very similar(e.g., 'laptops' is high ranked). How should i do? Should I modulate hyper-parameters(learning rate, initializing, etc)?</p>
","machine-learning, deep-learning, word2vec, word-embedding","<p>You didn't say what word2vec software you're using, which might change the relevant factors. </p>

<p>The word2vec algorithm inherently uses randomness, in both initialization and several aspects of its training (like the selection of negative-examples, if using negative-sampling, or random downsampling of very-frequent words). Additionally, if you're doing multithreaded training, the essentially-random jitter in the OS thread scheduling will change the order of training examples, introducing another source of randomness. So you shouldn't necessarily expect subsequent runs, even with the exact same parameters and corpus, to give identical results. </p>

<p>Still, with enough good data, suitable parameters, and a proper training loop, the relative-neighbors results <em>should</em> be fairly similar from run-to-run. If it's not, more data or more iterations might help. </p>

<p>Wildly-different results would be most likely if the model is overlarge (too many dimensions/words) for your corpus â€“ and thus prone to overfitting. That is, it finds a great configuration for the data, through essentially memorizing its idiosyncracies, without achieving any generalization power. And if such overfitting is possible, there are typically many equally-good such memorizations â€“ so they can be very different from run-to-tun. Meanwhile, a right-sized model with lots of data will instead be capturing true generalities, and <em>those</em> would be more consistent from run-to-run, despite any randomization.</p>

<p>Getting more data, using smaller vectors, using more training passes, or upping the minimum-count of word-occurrences to retain/train a word all might help. (Very-infrequent words don't get high-quality vectors, so wind up just interfering with the quality of other words, and then randomly intruding in most-similar lists.)</p>

<p>To know what else might be awry, you should clarify in your question things like:</p>

<ul>
<li>software used</li>
<li>modes/metaparameters used</li>
<li>corpus size, in number of examples, average example size in words, and unique-words count (both in the raw corpus, and after any minumum-count is applied)</li>
<li>methods of preprocessing</li>
<li>code you're using for training (if you're managing the multiple training-passes yourself)</li>
</ul>
",2,3,590,2017-08-18 01:30:31,https://stackoverflow.com/questions/45747170/word-embedding-training
How to create a model using trained models?,"<p>I have created two models using gensim word2vec. Now I want to merge these two models in a way that I get the union of these two models.</p>

<p>Eg: </p>

<ul>
1. Model one has following vocabulary
</ul>

<pre><code>{""Hi"", ""Hello"", ""World""}
</code></pre>

<ul>
2. Model two has the following vocabulary
</ul>

<pre><code>{""Hi"", ""King"", ""Hello"", ""Human""}
</code></pre>

<p>Now I want to use these two models and create a new model which will have the following vocabulary</p>

<pre><code>{""Hi"", ""Hello"", ""World"", ""King"", ""Human""}
</code></pre>
","machine-learning, nlp, deep-learning, gensim, word2vec","<p>Common word2vec libraries, like gensim, do not provide such a facility for merging models. Inherently, the coordinates of words within-a-model are only comparable, in terms of distances and directions, with other words in-the-same-model â€“ only by bring incrementally trained together do they get nudged to meaningful relative positions. </p>

<p>The most straightforward approach is, as @mujiga's answer suggests, to combine the two training corpuses which include all desired words, and train a new model on the combined texts. (And ideally, you'd shuffle the two corpuses together, rather than simply concatenate them, so that no words appear only in the beginning or end of the full set-of-texts.)</p>

<p>A more complicated approach is possible, when there are a lot of overlapping words. You can pick one of the two ""spaces"" as the coordinate-system you'd like to retain â€“ probably the one with more words, having been trained on more texts. Call that the 'reference' model. </p>

<p>You would take a large number of the words (maybe all) that are shared between the two models, and learn a 'translation' operation that projects those words' coordinates in the smaller model to roughly the right places for those same words in the reference model. (This is itself typically a mathematical optimization problem.) Finally, you'd use that translation operation on the non-shared words, to convert coordinates of the smaller model into the reference-model coordinate space â€“ and then construct a new data structure that includes all the original reference vectors, plus the translated vectors. </p>

<p>This is the technique used by <a href=""https://arxiv.org/abs/1309.4168"" rel=""nofollow noreferrer"">one of the original word2vec papers</a> for machine translation. It's also mentioned in <a href=""http://arxiv.org/abs/1506.06726"" rel=""nofollow noreferrer"">section 2.2 of the skip-thoughts paper</a> as a way to leverage words from another source when they don't appear in a local corpus. There's currently (August 2017) <a href=""https://github.com/RaRe-Technologies/gensim/pull/1434"" rel=""nofollow noreferrer"">some work-in-progress to add a facility for learning-the-translation</a> in gensim, but it's not yet fully-tested/documented or part of any formal release.</p>

<p>But really: the safe and straightforward thing is to train a new model on a common corpus. </p>
",2,2,103,2017-08-22 07:03:01,https://stackoverflow.com/questions/45810954/how-to-create-a-model-using-trained-models
How to modify word2vec code to build embedding for tab-delimited sequence of phrases?,"<p>Given text file with lines as follows:</p>

<pre><code>Phrase foo\tPhrase bla\tPhrase blabla\t...
Phrase bar\tPhrase blabla\tPhrase blablabla\t...
</code></pre>

<p>where each text line is a tab-delimited sequence of of phrases, which can contain <code>space</code> and other special characters. We are interested in embedding at phrase level, NOT word level.</p>

<p>The current word2vec.c support ""space"", ""tab"", ""new line"" as delimiters. How to disable ""space"" and enable only ""tab"" and ""new line"" as delimiters in word2vec.c in this case?</p>

<p>I got word2vec.c from Tomas Mikolov <a href=""https://github.com/tmikolov/word2vec"" rel=""nofollow noreferrer"">GitHub</a></p>
","c, text-parsing, word2vec","<p>The line <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L80"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L80</a> defines the delimiters in <code>word2vec.c</code>; if you're compiling that file, you could edit that line &amp; re-compile to make it behave differently. </p>

<p>But, it'd be easier and more robust (if in fact you're using some other word2vec implementation) if you simply pre-processed your text to transform it into the expected form. For example, you might change all spaces <code>' '</code> to underscores <code>'_'</code> (or some other plug character, if any original underscores are important to keep distinct). </p>

<p>When later interpreting the results, remember to apply the same space-to-underscore transform on lookups, or reverse it by replacing underscore-with-space to display results. </p>
",1,0,110,2017-08-23 06:28:08,https://stackoverflow.com/questions/45832134/how-to-modify-word2vec-code-to-build-embedding-for-tab-delimited-sequence-of-phr
Gensim word2vec WMD similarity dictionary,"<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>

<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        
2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          
</code></pre>

<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>

<p><strong>EDIT:</strong> training and scoring phases in my code:</p>

<p>Training and saving to disk:</p>

<pre><code>w2v_size = 300
word2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is usedÂ 
word2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
corpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)
corpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
</code></pre>

<p>Loading and scoring:</p>

<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
words = [t for t in proc_text if t in w2v.wv]
corpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
scores_w2v = np.array(corpus_w2v_wmd_index[words])  
</code></pre>
","python, nlp, cpu-word, gensim, word2vec","<p>The ""Word Mover's Distance"" calculation is relatively expensive â€“ for each pairwise document comparison, it searches for an optimal 'shifting' of semantic positions, and that shifting is itself dependent on the pairwise simple-distances between all words of each compared document. </p>

<p>That is, it involves far more calculation than a simple cosine-distance between two high-dimensional vectors, and it involves more calculation the longer the two documents are. </p>

<p>There isn't much that could be pre-calculated, from the <code>texts</code> corpus, until the query's words are known. (Each pairwise calculation depends on the query's words, and their simple-distances to each corpus document's words.)</p>

<p>That said, there are some optimizations the gensim <code>WmdSimilarity</code> class doesn't yet do. </p>

<p>The original WMD paper described a quicker calculation that could help eliminate corpus texts that couldn't possibly be in the top-N most-WMD-similar results. Theoretically, the gensim <code>WmdSimilarity</code> could also implement this optimization, and give quicker results, at least when initializing the <code>WmdSimilarity</code> with the <code>num_best</code> parameter. (Without it, every query returns all WMD-similarity-scores, so this optimization wouldn't help.)</p>

<p>Also, for now the <code>WmdSimilarity</code> class just calls <code>KeyedVectors.wmdistance(doc1, doc2)</code> for every query-to-corpus-document pair, as raw texts. Thus the pairwise simple-distances from all <code>doc1</code> words to <code>doc2</code> words will be recalculated each time, even if many pairs repeat across the corpus. (That is, if 'apple' is in the query and 'orange' is in every corpus doc, it will still calculate the 'apple'-to-'orange' distance repeatedly.) </p>

<p>So, some caching of interim values might help performance. For example, with a query of 1000 words, and a vocabulary of 100,000 words among all corpus documents, the <code>((1000 * 100,000) / 2)</code> 50 million pairwise word-distances could be precalculated once, using 200MB, then shared by all subsequent WMD-calculations. To add this optimization would require a cooperative refactoring of <code>WmdSimilarity.get_similarities()</code> and <code>KeyedVectors.wmdistance()</code>.</p>

<p>Finally, Word2Vec/Doc2Vec applications don't necessarily require or benefit much from stop-word removal or stemming. But because the expense of WMD calculation grows with document and vocabulary size, anything that shrinks effective document sizes could help performance. So various ways of discarding low-value words, or coalescing similar words, may be worth considering when using WMD on large document sets.  </p>
",8,4,4038,2017-08-25 07:51:09,https://stackoverflow.com/questions/45876711/gensim-word2vec-wmd-similarity-dictionary
TensorFlow word2vec tutorial input,"<p>While going through the TensorFlow <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">word2vec tutorial</a>, I had a hard time following the tutorial's explanation regarding the placeholders that store the inputs to the skip-gram model. The explanation states that </p>

<blockquote>
  <p>The skip-gram model takes two inputs. One is a batch full of integers 
  representing the source context words, the other is for the target words... Now what we need to do is look up the vector for each of the source words in the batch... Now that we have the embeddings for each word, we'd like to try to predict the target word.</p>
</blockquote>

<p>However, since we are using the skip-gram model (as opposed to CBOW), shouldn't we instead have to look up the word vector for each of the target words, and then predict the context words given the target words? </p>

<p>In addition, I'm assuming that the code below first declares a placeholder for the target words (inputs), and then one for the source context words (our labels).</p>

<pre><code>train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
</code></pre>

<p>Am I misunderstanding the tutorial? </p>
","tensorflow, word2vec","<p>The skip-gram tutorial assumes your dataset made like this one:</p>

<blockquote>
  <p>(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</p>
</blockquote>

<p>where it is intended as pair (input, output) = (center_word, context_word). </p>

<p>As a matter of fact, if you average over multiple (input, output) pairs, you will obtain a behavior similar to the one of predicting each of the context words at each example.</p>

<p>This choice is also justified by the use of NCE as loss function, and NCE tries to distinguish a single target word (one of the words of the context) among some noise words (randomly selected).</p>

<p>Your inputs and outputs placeholder should have the same dimension <code>(batch_size,1)</code> but the input are simply <code>(batch_size)</code> because the embedding layer automatically expands the dimension while the loss function (where you provide labels) needs a matrix as input.</p>

<p>So, the tutorial is not a precise implementation of the skip-gram model of Mikolov, but makes some approximations for the sake of code simplicity and readability.</p>
",0,1,407,2017-08-27 04:32:39,https://stackoverflow.com/questions/45901716/tensorflow-word2vec-tutorial-input
How to train word2vec with your own vocab,"<p>I am getting error while training word2vec with my own vocabulary. I am also not getting why its happening.</p>

<p>Code:</p>

<pre><code>from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

sentences = word2vec.LineSentence('test_data')

model = word2vec.Word2Vec(sentences, size=20)
model.build_vocab(sentences,update=True)
model.train(sentences)

print model.most_similar(['course'])
</code></pre>

<p>It throws an error</p>

<pre><code>2017-08-27 16:50:04,590 : INFO : precomputing L2-norms of word weight vectors
Traceback (most recent call last):
  File ""tryword2vec.py"", line 23, in &lt;module&gt;
    print model.most_similar(['course']) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 1285, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 97, in most_similar
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'course' not in vocabulary""
</code></pre>

<p>test_data contains:</p>

<blockquote>
  <p>Bachelor of Engg is a course. M.Tech is a course. ME is a course.
  B.Tech is a course. Bachelor of Arts is a course. Fashion Design is a
  course. Multimedia is a course. Mechanical engg is a course. Computer
  Science is a course. Electronics is a cource. Engineering is a course.
  MBA is a course. BBA is a course.</p>
</blockquote>

<p>Any help is appreciated?</p>
","nlp, stanford-nlp, word2vec, doc2vec","<p>The reason you are not getting the error is because the word <strong>course</strong> is not in the vocabulary. Instead the word present is <b>course.</b> </p>

<p>There is a period <strong>"".""</strong> at the end of course.</p>

<p>check your vocabulary 
<code>model.wv.vocab</code></p>

<pre><code>{u'a': &lt;gensim.models.keyedvectors.Vocab at 0x7fe086c461d0&gt;,
 u'course.': &lt;gensim.models.keyedvectors.Vocab at 0x7fe0b4704f90&gt;,
 u'is': &lt;gensim.models.keyedvectors.Vocab at 0x7fe086ba0d10&gt;}
</code></pre>

<p>And do hide your <strong>api</strong> keys.</p>
",1,2,1273,2017-08-27 11:32:49,https://stackoverflow.com/questions/45904499/how-to-train-word2vec-with-your-own-vocab
How is the Gensim doesnt_match function working?,"<p>Gensim implements a function called ""doesnt_match"" that return an outlier word from a list.</p>

<p>The function is called on a wordvector object.</p>

<p>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split())
'cereal'</p>

<p>The documentation is not specifying how this function really work (what is the computation background)</p>

<p>Anyone knows ?</p>
","python, gensim, word2vec","<p>You can review the <a href=""https://github.com/RaRe-Technologies/gensim/blob/1a73e4f110e0f199d39f9e051cbc081c4e2bab46/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer""><code>doesnt_match()</code> method's source code</a> â€“ it's only 11 lines â€“
for the full details.</p>

<p>But the gist of it is...</p>

<ul>
<li>take the mean of all the word-vectors â€“ a sort of 'center' for all candidates</li>
<li>calculate the cosine-distance from that center to each word â€“ this is the dot-product between unit-normalized versions of each relevant vector</li>
<li>return the single word with the highest cosine-distance from that mean</li>
</ul>
",4,2,2736,2017-08-29 21:22:30,https://stackoverflow.com/questions/45948533/how-is-the-gensim-doesnt-match-function-working
Optimizing Gensim word mover&#39;s distance function for speed (wmdistance),"<p>I am using <code>gensim</code> <code>wmdistance</code> for calculating similarity between a reference sentence and 1000 other sentences. </p>

<pre><code>    model = gensim.models.KeyedVectors.load_word2vec_format(
     'GoogleNews-vectors-negative300.bin', binary=True)
    model.init_sims(replace=True)  

    reference_sentence = ""it is a reference sentence""
    other_sentences = [1000 sentences]
    index = 0
    for sentence in other_sentences: 
      distance [index] = model.wmdistance(refrence_sentence, other_sentences)
      index = index + 1
</code></pre>

<p>According to <code>gensim</code> <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">source code</a>, <code>model.wmdistance</code> returns the following:</p>

<pre><code>emd(d1, d2, distance_matrix)
</code></pre>

<p>where </p>

<pre><code>d1 =  # Compute nBOW representation of reference_setence.
d2 =  # Compute nBOW representation of other_sentence (one by one).
distance_matrix = see the source code as its a bit too much to paste it here.
</code></pre>

<p>This code is inefficient in two ways for my use case. </p>

<p>1) For the reference sentence, it is repeatedly calculating d1 (1000 times) for the distance function <code>emd(d1, d2, distance_matrix)</code>.</p>

<p>2) This distance function is called by multiple users from different points which repeat this whole process of <code>model.wmdistance(doc1, doc2)</code> for the same other_sentences and it is computationally expensive. For this 1000 comparisons, it takes around 7-8 seconds. </p>

<p>Therefore, I would like to isolate the two tasks. The final calculation of distance: <code>emd(d1, d2, distance_matrix)</code> and the preparation of these inputs: d1, d2, and distance matrix. As distance matrix depends on both so at least its input preparation should be isolated from the final matrix calculation. </p>

<p>My initial plan is to create three customized functions: </p>

<pre><code>d1 = prepared1(reference_sentence)
d2 = prepared2(other_sentence)
distance_matrix inputs = prepare inputs 
</code></pre>

<p>Is it possible to do this with this <code>gensim</code> function or should I just go my own customized version? Any ideas and solutions to deal with this problem in a better way?</p>
","python, nlp, nltk, gensim, word2vec","<p>You are right to observe that this code could be refactored &amp; optimized to avoid doing repetitive operations, especially in the common case where one reference/query doc is evaluated against a larger set of documents. (Any such improvements would also be a welcome contribution back to gensim.)</p>

<p>Simply preparing single documents outside the calculation might not offer a big savings; in each case, all word-to-word distances between the two docs must be calculated. It might make sense to precalculate a larger <code>distance_matrix</code> (to the extent that the relevant vocabulary &amp; system memory allows) that includes all words needed for many pairwise WMD calculations. </p>

<p>(As tempting as it might be to precalculate all word-to-word distances, with a vocabulary of 3 million words like the <code>GoogleNews</code> vector-set, and mere 4-byte float distances, storing them all would take at least 18TB. So calculating distances for relevant words, on manageable batches of documents, may make more sense.)</p>

<p>A possible way to start would be to create a variant of <code>wmdistance()</code> that explicitly works on one document versus a set-of-documents, and can thus combine the creation of histograms/distance-matrixes for many comparisons at once. </p>

<p>For the common case of not needing <em>all</em> WMD values, but just wanting the top-N nearest results, there's an optimization described in the original WMD paper where another faster calculation (called there 'RWMD') can be used to deduce when there's no chance a document could be in the top-N results, and thus skip the full WMD calculation entirely for those docs.</p>
",2,3,2609,2017-08-30 12:38:55,https://stackoverflow.com/questions/45960671/optimizing-gensim-word-movers-distance-function-for-speed-wmdistance
Convert Python dictionary to Word2Vec object,"<p>I have obtained a dictionary mapping words to their vectors in python, and I am trying to scatter plot the n most similar words since TSNE on huge number of words is taking forever. The best option is to convert the dictionary to a w2v object to deal with it.</p>
","python, scikit-learn, data-visualization, word2vec","<p>I had the same issue and I finaly found the solution</p>

<p>So, I assume that your dictionary looks like mine</p>

<pre><code>d = {}
d['1'] = np.random.randn(300)
d['2'] = np.random.randn(300)
</code></pre>

<p>Basically, the keys are the users' ids and each of them has a vector with shape (300,).</p>

<p>So now, in order to use it as word2vec I need to firstly save it to binary file and then load it with gensim library</p>

<pre><code>from numpy import zeros, dtype, float32 as REAL, ascontiguousarray, fromstring
from gensim import utils

m = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=300)
m.vocab = d
m.vectors = np.array(list(d.values()))
my_save_word2vec_format(binary=True, fname='train.bin', total_vec=len(d), vocab=m.vocab, vectors=m.vectors)
</code></pre>

<p>Where my_save_word2vec_format function is:</p>

<pre><code>def my_save_word2vec_format(fname, vocab, vectors, binary=True, total_vec=2):
""""""Store the input-hidden weight matrix in the same format used by the original
C word2vec-tool, for compatibility.

Parameters
----------
fname : str
    The file path used to save the vectors in.
vocab : dict
    The vocabulary of words.
vectors : numpy.array
    The vectors to be stored.
binary : bool, optional
    If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.
total_vec : int, optional
    Explicitly specify total number of vectors
    (in case word vectors are appended with document vectors afterwards).

""""""
if not (vocab or vectors):
    raise RuntimeError(""no input"")
if total_vec is None:
    total_vec = len(vocab)
vector_size = vectors.shape[1]
assert (len(vocab), vector_size) == vectors.shape
with utils.smart_open(fname, 'wb') as fout:
    print(total_vec, vector_size)
    fout.write(utils.to_utf8(""%s %s\n"" % (total_vec, vector_size)))
    # store in sorted order: most frequent words at the top
    for word, row in vocab.items():
        if binary:
            row = row.astype(REAL)
            fout.write(utils.to_utf8(word) + b"" "" + row.tostring())
        else:
            fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
</code></pre>

<p>And then use</p>

<pre><code>m2 = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format('train.bin', binary=True)
</code></pre>

<p>To load the model as word2vec</p>
",6,4,3117,2017-08-31 12:19:22,https://stackoverflow.com/questions/45981305/convert-python-dictionary-to-word2vec-object
How to find most similar terms/words of a document in doc2vec?,"<p>I have applied Doc2vec to convert documents into vectors.After that, I used the vectors in clustering and figured out the 5 nearest/most similar document to the centroid of each cluster. Now I need to find the most dominant or important terms of these documents so that I can figure out the characteristics of each cluster. 
My question is is there any way to figure out the most dominat or simlar terms/word of a document in Doc2vec . I am using python's gensim package for the Doc2vec implementaton </p>
","python, cluster-analysis, gensim, word2vec, doc2vec","<p>To find out the most dominant words of your clusters, you can use any of these two classic approaches. I personally found the second one very efficient and effective for this purpose. </p>

<ul>
<li><p>Latent Drichlet Allocation (LDA): A topic modelling algorithm that will give you a set of topic given a collection of documents. You can treat the set of similar documents in the clusters as one document and apply LDA to generate the topics and see topic distributions across documents.</p></li>
<li><p>TF-IDF: TF-IDF calculate the importance of a word to a document given a collection of documents. Therefore, to find the most important keywords/ngrams, you can calculate TF-IDF for every word that appears in the documents. The words with the highest TF-IDF then are you keywords. So: </p>

<ul>
<li>calculate IDF for every single word that appears in the documents based on the number of documents that contain that keyword </li>
<li>concatenate the text of the similar documents (I 'd call it a super-document) and then calculate TF for each word that appears in this super-document</li>
<li>calculate TF*IDF for every word... and then TA DAAA... you have your keywords associated with each cluster. </li>
</ul>

<p>Take a look at Section 5.1 here for more details on the use of <a href=""https://pdfs.semanticscholar.org/9f3f/6f65344da1bd61f1311ab134c1a7bfcd0741.pdf"" rel=""nofollow noreferrer"">TF-IDF</a>.</p></li>
</ul>
",1,0,3461,2017-09-05 05:23:46,https://stackoverflow.com/questions/46047506/how-to-find-most-similar-terms-words-of-a-document-in-doc2vec
word2vec training procedure clarification,"<p>I'm trying to learn the skip-gram model within word2vec, however I'm confused by some of the basic concepts.  To start, here is my current understanding of the model motivated with an example.  I am using Python <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> as I go.</p>

<p>Here I have a corpus with three sentences.</p>

<pre><code>sentences = [
    ['i', 'like', 'cats', 'and', 'dogs'],
    ['i', 'like', 'dogs'],
    ['dogs', 'like', 'dogs']
]
</code></pre>

<p>From this, I can determine my vocabulary, <code>V = ['and', 'cats', 'dogs', 'i', 'like']</code>.</p>

<p>Following <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">this paper</a> by Tomas Mikolov (and others)</p>

<blockquote>
  <p>The basic Skip-gram formulation defines p(w_t+j |w_t) using the softmax
  function:</p>
</blockquote>

<p><a href=""https://i.sstatic.net/hza7Q.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hza7Q.gif"" alt=""skipgram""></a></p>

<blockquote>
  <p>where v_w and vâ€²_w are the â€œinputâ€ and â€œoutputâ€ vector representations
  of w, and W is the number of words in the vocabulary.</p>
</blockquote>

<p>To my understanding, the skip-gram model involves two matrices (I'll call them <em>I</em> and <em>O</em>) which are the vector representations of ""input/center"" words and the vector representation of ""output/context"" words.  Assuming d = 2 (vector dimension or 'size' as its called in genism), <em>I</em> should be a 2x5 matrix and <em>O</em> should be a 5x2 matrix.  At the start of the training procedure, these matrices are filled with random values (yes?).  So we might have</p>

<pre><code>import numpy as np
np.random.seed(2017)

I = np.random.rand(5,2).round(2)  # 5 rows by 2 cols
[[ 0.02  0.77] # and
 [ 0.45  0.12] # cats
 [ 0.93  0.65] # dogs
 [ 0.14  0.23] # i
 [ 0.23  0.26]] # like

O = np.random.rand(2,5).round(2)  # 2 rows by 5 cols
  #and  #cats #dogs  #i   #like 
[[ 0.11  0.63  0.39  0.32  0.63]
 [ 0.29  0.94  0.15  0.08  0.7 ]]
</code></pre>

<p>Now if I want to calculate the probability that the word ""dogs"" appears in the context of ""cats"" I should do</p>

<p>exp([0.39, 0.15] * [0.45  0.12])/(...) = (0.1125)/(...)</p>

<p>A few questions on this:</p>

<ol>
<li>Is my understanding of the algorithm correct thus far?</li>
<li>Using genism, I can train a model on this data using</li>
</ol>

<p>&nbsp;</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(sentences, sg = 1, size=2, window=1, min_count=1)
model.wv['dogs']  # array([ 0.06249372,  0.22618999], dtype=float32)
</code></pre>

<p>For the array given, is that the vector for ""dogs"" in the Input matrix or the Output matrix?  Is there a way to view both matrices in the final model?</p>

<ol start=""3"">
<li>Why does <code>model.wv.similarity('cats','cats')</code> = 1?  I thought this should be closer to 0, since the data would indicate that the word ""cats"" is unlikely to occur in the context of the word ""cats"".</li>
</ol>
","gensim, word2vec","<p>(1) Generally, yes, but:</p>

<p>The <em>O</em> output matrix â€“ more properly understood as the weights from the neural-network's hidden layer, to a number of output nodes â€“ is interpreted differently whether using 'negative sampling' ('NS') or 'hierarchical softmax' ('HS') training. </p>

<p>In practice in both <em>I</em> and <em>O</em> are <em>len(vocab)</em> rows and <em>vector-size</em> columns. (<em>I</em> is the <code>Word2Vec</code> <code>model</code> instance's <code>model.wv.syn0</code> array; <em>O</em> is its <code>model.syn1neg</code> array in NS or <code>model.syn1</code> in HS.)</p>

<p>I find NS a bit easier to think about: each predictable word corresponds to a single output node. For training data where (context)-indicates->(word), training tries to drive that word's node value toward 1.0, and the other randomly-chosen word node values toward 0.0. </p>

<p>In HS, each word is represented by a huffman-code of a small subset of the output nodes â€“ those 'points' are driven to 1.0 or 0.0 to make the network more indicative of a single word after a (context)-indicates->(word) example. </p>

<p>Only the <em>I</em> matrix, initial word values, are randomized to low-magnitude vectors at the beginning. (The hidden-to-output weights <em>O</em> are left zeros.)</p>

<p>(2) Yes, that'll train things - just note that tiny toy-sized examples won't necessarily generate the useful constellations-of-vector-coordinates that are valued from word2vec.</p>

<p>(3) Note, <code>model.similarity('cats', 'cats')</code> is actually checking the cosine-similarity between the (input) vectors for those two words. Those are the same word, thus they definitionally have the same vector, and the similarity between identical vectors is 1.0. </p>

<p>That is, <code>similarity()</code> is <em>not</em> asking the model for a prediction, it's retrieving learned words by key and comparing those vectors. (Recent versions of gensim do have a <code>predict_output_word()</code> function, but it only works in NS mode, and making predictions isn't really the point of word2vec, and many implementations don't offer any prediction API at all. Rather, the point is using those attempted predictions during training to induce word-vectors that turn out to be useful for various other tasks.)</p>

<p>But even if you were reading predictions, 'cats' might still be a reasonable-although-bad prediction from the model in the context of 'cats'. The essence of forcing large vocabularies into the smaller dimensionality of 'dense' embeddings is compression â€“ the model has no choice but to cluster related words together, because there's not enough internal complexity (learnable parameters) to simply memorize all details of the input. (And for the most part, that's a good thing, because it results in generalizable patterns, rather than just overfit idiosyncrasies of the training corpus.) </p>

<p>The word 'cats' will wind up close to 'dogs' and 'pets' â€“ because they all co-occur with similar words, or each other. And thus the model will be forced to make similar output-predictions for each, because their input-vectors don't vary that much. And a few predictions that are nonsensical in logical language use â€“ like a repeating word - may be made, but only because taking a larger error there still gives less error over the whole training set, compared to other weighting alternatives. </p>
",1,0,605,2017-09-06 00:00:34,https://stackoverflow.com/questions/46064892/word2vec-training-procedure-clarification
How does word embedding/ word vectors work/created?,"<p>How does word2vec create vectors for words? I trained two word2vec models using two different files (from commoncrawl website) but I am getting same word vectors for a given word from both models. </p>

<p>Actually, I have created multiple word2vec models using different text files from the commoncrawl website. Now I want to check which model is better among all. How can select the best model out of all these models and why I am getting same word vectors for different models?</p>

<p>Sorry, If the question is not clear. </p>
","neural-network, nlp, deep-learning, gensim, word2vec","<p>If you are getting identical word-vectors from models that you've prepared from different text corpuses, something is likely wrong in your process. You may not be performing any training at all, perhaps because of a problem in how the text iterable is provided to the <code>Word2Vec</code> class. (In that case, word-vectors would remain at their initial, randomly-initialized values.) </p>

<p>You should enable logging, and review the logs carefully to see that sensible counts of words, examples, progress, and incremental-progress are displayed during the process. You should also check that results for some superficial, ad-hoc checks look sensible after training. For example, does <code>model.most_similar('hot')</code> return other words/concepts somewhat like 'hot'? </p>

<p>Once you're sure models are being trained on varied corpuses â€“ in which case their word-vectors should be very different from each other â€“ deciding which model is 'best' depends on your specific goals with word-vectors.</p>

<p>You should devise a repeatable, quantitative way to evaluate a model against your intended end-uses. This might start crudely with a few of your own manual reviews of results, like looking over <code>most_similar()</code> results for important words for better/worse results â€“ but should become more extensive. rigorous, and automated as your project progresses. </p>

<p>An example of such an automated scoring is the <code>accuracy()</code> method on gensim's word-vectors object. See:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/6d6f5dcfa3af4bc61c47dfdf5cdbd8e1364d0c3a/gensim/models/keyedvectors.py#L652"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/6d6f5dcfa3af4bc61c47dfdf5cdbd8e1364d0c3a/gensim/models/keyedvectors.py#L652</a></p>

<p>If supplied with a specifically-formatted file of word-analogies, it will check how well the word-vectors solve those analogies. For example, the <a href=""https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"" rel=""nofollow noreferrer""><code>questions-words.txt</code></a> of Google's original <code>word2vec</code> code release includes the analogies they used to report vector quality. Note, though, that the word-vectors that are best for some purposes, like understanding text topics or sentiment, might not also be the best at solving this style of analogy, and vice-versa. If training your own word-vectors, it's best to choose your training corpus/parameters based on your own goal-specific criteria for what 'good' vectors will be. </p>
",0,2,337,2017-09-06 10:32:37,https://stackoverflow.com/questions/46072991/how-does-word-embedding-word-vectors-work-created
Word2Vec in Gensim using model.most_similar,"<p>I am new in 'Word2Vec' in Gensim. I want to build a Word2Vec model for the text (Extracted from Wikipedia: Machine Learning) and find <strong>most similar words</strong> to 'Machine Learning'.</p>

<p>My current code is as follows.</p>

<pre><code># import modules &amp; set up logging
from gensim.models import Word2Vec

sentences = ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] â€“ such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.""
# train word2vec on the sentences
model = Word2Vec(sentences, min_count=1)
vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, for vocab I get one character output.</p>

<pre><code>['M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'r']
</code></pre>

<p>Please help me to get the most_similar_words by using using <strong>model.most_similar</strong></p>
","python, gensim, word2vec","<p>The <code>Word2Vec</code> class expects its corpus of <code>sentences</code> to be an iterable source of individual items which are each a list-of-word-tokens. </p>

<p>You're providing a single string. If it iterates over that, it gets individual characters. If it then tries to interpret those individual characters as a list-of-tokens, it still just gets a single-character â€“ so the only 'words' it sees are single characters. </p>

<p>At the very least, you'd want your corpus to be constructed more like this:</p>

<pre><code>sentences = [
    ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] â€“ such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision."".split(),
]
</code></pre>

<p>That's still just one 'sentence', but it'll be split-on-whitespace into word-tokens.</p>

<p>Note also that useful word2vec results require large, varied text samples â€“ toy-sized examples won't usually show the kinds of word-similarities or word-relative-arrangements that word2vec is famous for creating. </p>
",1,0,981,2017-09-07 02:16:18,https://stackoverflow.com/questions/46086858/word2vec-in-gensim-using-model-most-similar
Get bigrams and trigrams in word2vec Gensim,"<p>I am currently using uni-grams in my word2vec model as follows.</p>

<pre><code>def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    #Returns a list of sentences, where each sentence is a list of words
    #
    #NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) &gt; 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences
</code></pre>

<p>However, then I will miss important bigrams and trigrams in my dataset.</p>

<pre><code>E.g.,
""team work"" -&gt; I am currently getting it as ""team"", ""work""
""New York"" -&gt; I am currently getting it as ""New"", ""York""
</code></pre>

<p>Hence, I want to capture the important bigrams, trigrams etc. in my dataset and input into my word2vec model.</p>

<p>I am new to wordvec and struggling how to do it. Please help me.</p>
","python, tokenize, word2vec, gensim, n-gram","<p>First of all you should use gensim's class <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">Phrases</a> in order to get bigrams, which works as pointed in the doc </p>

<pre><code>&gt;&gt;&gt; bigram = Phraser(phrases)
&gt;&gt;&gt; sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
&gt;&gt;&gt; print(bigram[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>To get trigrams and so on, you should use the bigram model that you already have and apply Phrases to it again, and so on. 
Example:</p>

<pre><code>trigram_model = Phrases(bigram_sentences)
</code></pre>

<p>Also there is a good notebook and video that explains how to use that .... <a href=""https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"" rel=""noreferrer"">the notebook</a>, <a href=""https://www.youtube.com/watch?v=6zm9NC9uRkk&amp;list=PLw5BhoADa9fUtHRGND_rrdeZlv1XFM8In&amp;index=14&amp;t=3339s"" rel=""noreferrer"">the video</a></p>

<p>The most important part of it is how to use it in real life sentences which is as follows:</p>

<pre><code>// to create the bigrams
bigram_model = Phrases(unigram_sentences)

// apply the trained model to a sentence
 for unigram_sentence in unigram_sentences:                
            bigram_sentence = u' '.join(bigram_model[unigram_sentence])

// get a trigram model out of the bigram
trigram_model = Phrases(bigram_sentences)
</code></pre>

<p>Hope this helps you, but next time give us more information on what you are using and etc.</p>

<p>P.S: Now that you edited it, you are not doing anything in order to get bigrams just splitting it, you have to use Phrases in order to get words like New York as bigrams.</p>
",24,17,40015,2017-09-09 09:49:15,https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim
Error in extracting phrases using Gensim,"<p>I am trying to get the bigrams in the sentences using Phrases in Gensim as follows.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
#print(sentence_stream)
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)

for sent in sentence_stream:
    tokens_ = bigram_phraser[sent]
    print(tokens_)
</code></pre>

<p>Even though it catches ""new"", ""york"" as ""new york"", it does not catch ""machine"", learning as ""machine learning""</p>

<p>However, in the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example shown in Gensim Website</a> they were able to catch the words ""machine"", ""learning"" as ""machine learning"".</p>

<p>Please let me know how to get ""machine learning"" as a bigram in the above example</p>
","python, data-mining, text-mining, word2vec, gensim","<p>The technique used by gensim <code>Phrases</code> is purely based on statistics of co-occurrences: how often words appear together, versus alone, in a formula also affected by <code>min_count</code> and compared against the <code>threshold</code> value. </p>

<p>It is only because your training set has 'new' and 'york' occur alongside each other twice, while other words (like 'machine' and 'learning') only occur alongside each other once, that 'new_york' becomes a bigram, and other pairings do not. What's more, even if you did find a combination of <code>min_count</code> and <code>threshold</code> that would promote 'machine_learning' to a bigram, it would <em>also</em> pair together every other bigram-that-appears-once â€“ which is probably not what you want.</p>

<p>Really, to get good results from these statistical techniques, you need lots of varied, realistic data. (Toy-sized examples may superficially succeed, or fail, for superficial toy-sized reasons.) </p>

<p>Even then, they will tend to miss combinations a person would consider reasonable, and make combinations a person wouldn't. Why? Because our minds have much more sophisticated ways (including grammar and real-world knowledge) for deciding when clumps of words represent a single concept. </p>

<p>So even with more better data, be prepared for nonsensical n-grams. Tune or judge the model on whether it is overall improving on your goal, not any single point or ad-hoc check of matching your own sensibility.</p>

<p>(Regarding the referenced gensim documentation comment, I'm pretty sure that if you try <code>Phrases</code> on just the two sentences listed there, it won't find any of the desired phrases â€“ not 'new_york' or 'machine_learning'. As a figurative example, the ellipses <code>...</code> imply the training set is larger, and the results indicate that the extra unshown texts are important. It's just because of the 3rd sentence you've added to your code that 'new_york' is detected. If you added similar examples to make 'machine_learning' look more like a statistically-outlying pairing, your code could promote 'machine_learning', too.)</p>
",6,3,1828,2017-09-10 05:27:14,https://stackoverflow.com/questions/46137572/error-in-extracting-phrases-using-gensim
Issues in getting trigrams using Gensim,"<p>I want to get bigrams and trigrams from the example sentences I have mentioned.</p>

<p>My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)</p>

<p><strong>Approach 1</strong> Mentioned below is my code using Phrases in Gensim.</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p><strong>Approach 2</strong> I even tried to use Phraser and Phrases both, but it didn't work.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>Please help me to fix this issue of getting trigrams.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">example documentation</a> of Gensim.</p>
","python, data-mining, text-mining, word2vec, gensim","<p>I was able to get bigrams and trigrams with a few modifications to your code:
</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>I removed the <code>threshold = 1</code> parameter from the bigram <code>Phrases</code> because otherwise it seems to form weird digrams that allow the construction of weird trigrams (notice that <code>bigram</code> is used to build the trigram <code>Phrases</code>); this parameter would probably come useful when you have more data. For trigrams, the <code>min_count</code> parameter also needs to be specified because it defaults to 5 if not provided.</p>

<p>In order to retrieve the bigrams and trigrams of each document, you can use this list comprehension trick to filter elements that aren't formed by two or three words, respectively. </p>

<hr>

<p><strong>Edit</strong> - a few details about the <code>threshold</code> parameter:</p>

<p>This parameter is used by the estimator to determine if two words <em>a</em> and <em>b</em> form a phrase, and that is only if:
</p>

<pre><code>(count(a followed by b) - min_count) * N/(count(a) * count(b)) &gt; threshold
</code></pre>

<p>where <em>N</em> is the total vocabulary size. By default the parameter value is 10 (see <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""noreferrer"">docs</a>). So, the higher the <code>threshold</code>, the harder the constraints for words to form phrases.</p>

<p>For example, in your first approach you were trying to use <code>threshold = 1</code>, so you would get <code>['human computer','interaction is']</code> as digrams of 3 out of your 5 sentences that begin with ""human computer interaction""; that weird second digram is a result of the more relaxed threshold. </p>

<p>Then, when you try to get trigrams with default <code>threshold = 10</code> you only get <code>['human computer interaction is']</code> for those 3 sentences, and nothing for the remaining two (filtered by threshold); and because that was a 4-gram instead of a trigram it would also be filtered by <code>if t.count(' ') == 2</code>. In case that, for example, you lower the trigram threshold to 1, you can get ['human computer interaction'] as trigram for the two remaining sentences. It doesn't seem easy to get a good combination of parameters, <a href=""https://stackoverflow.com/a/46143595/8507311"">here's</a> more about it. </p>

<p>I'm not an expert, so take this conclusion with a grain of salt: I think it's better to firstly get good digram results (not like 'interaction is') before moving on, as weird digrams can add confusion to further trigrams, 4-gram... </p>
",20,14,9082,2017-09-11 04:28:12,https://stackoverflow.com/questions/46148182/issues-in-getting-trigrams-using-gensim
Why Word2Vec&#39;s most_similar() function is giving senseless results on training?,"<p>I am running the gensim word2vec code on a corpus of resumes(stopwords removed) to identify similar context words in the corpus from a list of pre-defined keywords.</p>

<p>Despite several iterations with input parameters,stopword removal etc the similar context words are not at all making sense(in terms of distance or context)
Eg. correlation and matrix occurs in the same window several times yet matrix doesnt fall in the most_similar results for correlation </p>

<p>Following are the details of the system and codes
gensim 2.3.0 ,Running on Python 2.7 Anaconda
Training Resumes :55,418 sentences
Average words per sentence : 3-4 words(post stopwords removal)
Code :</p>

<pre><code>    wordvec_min_count=int()
    size = 50
    window=10
    min_count=5
    iter=50
    sample=0.001
    workers=multiprocessing.cpu_count()
    sg=1
    bigram = gensim.models.Phrases(sentences, min_count=10, threshold=5.0)
    trigram = gensim.models.Phrases(bigram[sentences], min_count=10, threshold=5.0)
    model=gensim.models.Word2Vec(sentences = trigram[sentences], size=size, alpha=0.005, window=window, min_count=min_count,max_vocab_size=None,sample=sample, seed=1, workers=workers, min_alpha=0.0001, sg=sg, hs=1, negative=0, cbow_mean=1,iter=iter)

model.wv.most_similar('correlation')
Out[20]: 
[(u'rankings', 0.5009744167327881),
 (u'salesmen', 0.4948525130748749),
 (u'hackathon', 0.47931140661239624),
 (u'sachin', 0.46358123421669006),
 (u'surveys', 0.4472047984600067),
 (u'anova', 0.44710394740104675),
 (u'bass', 0.4449636936187744),
 (u'goethe', 0.4413239061832428),
 (u'sold', 0.43735259771347046),
 (u'exceptional', 0.4313117265701294)]
</code></pre>

<p>I am lost as to why the results are so random ? Is there anyway to check the accuracy for word2vec ?</p>

<p>Also is there an alternative of word2vec for most_similar() function ? I read about gloVE but was not able to install the package.</p>

<p>Any information in this regard would be helpful</p>
","python-2.7, gensim, word2vec, word-embedding","<p>Enable INFO-level logging and make sure that it indicates real training is happening. (That is, you see incremental progress taking time over the expected number of texts, over the expected number of iterations.)</p>

<p>You may be hitting this <a href=""https://github.com/RaRe-Technologies/gensim/issues/1401"" rel=""nofollow noreferrer"">open bug issue in <code>Phrases</code></a>, where requesting the Phrase-promotion (as with <code>trigram[sentences]</code>) only offers a single-iteration, rather than the multiply-iterable collection object that <code>Word2Vec</code> needs. </p>

<p><code>Word2Vec</code> needs to pass over the corpus once for vocabulary-discovery, then <code>iter</code> times again for training. If <code>sentences</code> or the phrasing-wrappers only support single-iteration, only the vocabulary will be discovered â€“ training will end instantly, and the model will appear untrained.</p>

<p>As you'll see in that issue, a workaround is to perform the Phrases-transformation and save the results into an in-memory list (if it fits) or to a separate text corpus on disk (that's already been phrase-combined). Then, use a truly restartable iterable on that â€“ which will also save some redundant processing. </p>
",1,0,924,2017-09-11 14:19:26,https://stackoverflow.com/questions/46157937/why-word2vecs-most-similar-function-is-giving-senseless-results-on-training
Sentence matching with gensim word2vec: manually populated model doesn&#39;t work,"<p>I'm trying to solve a problem of sentence comparison using naive approach of summing up word vectors and comparing the results. My goal is to match people by interest, so the dataset consists of names and short sentences describing their hobbies. The batches are fairly small, few hundreds of people, so i wanted to give it a try before digging into doc2vec.</p>

<p>I prepare the data by cleaning it completely, removing stop words, tokenizing and lemmatizing. I use pre-trained model for word vectors which returns adequate results when finding similarities for some test words. Also tried summing up the sentence words to find similarities in the original model - the matches do make sense. The similarities would be around general sense of the phrase.</p>

<p>For sentence matching I'm trying the following: create an empty model</p>

<pre><code>b = gs.models.Word2Vec(min_count=1, size=300, sample=0, hs=0)
</code></pre>

<p>Build vocab out of names (or person id's), no training</p>

<pre><code>#first create vocab with an empty vector
test = [['test']]
b.build_vocab(test)
b.wv.syn0[b.wv.vocab['test'].index] = b.wv.syn0[b.wv.vocab['test'].index]*0

#populate vocab from an array
b.build_vocab([personIds], update=True)
</code></pre>

<p>Summ each sentence's word vectors and store the results into the model for a corresponding id</p>

<pre><code>#sentences are pulled from pandas dataset df. 'a' is a pre-trained model i use to get vectors for each word

def summ(phrase, start_model):
    '''
    vector addition function
    '''
    #starting with a vector of 0's
    sum_vec = start_model.word_vec(""cat_NOUN"")*0
    for word in phrase:
        sum_vec += start_model.word_vec(word)
    return sum_vec

for i, row in df.iterrows():
    try:
        personId = row[""ID""]
        summVec = summ(df.iloc[i,1],a)
        #updating syn0 for each name/id in vocabulary
        b.wv.syn0[b.wv.vocab[personId].index] = summVec
    except:
        pass
</code></pre>

<p>I understand that i shouldn't be expecting much accuracy here, but the t-SNE print doesn't show any clustering whatsoever. Finding similarities method also fails to find matches (&lt;0.2 similarity coefficient basically for everything). [<img src=""https://i.sstatic.net/w81nX.png"" alt=""]plot of the entire model[1]""></p>

<p>Wondering if anyone has an idea of where did i go wrong? Is my approach valid at all?</p>
","python, nlp, gensim, word2vec","<p>Your code, as shown, neither does any <code>train()</code> of word-vectors (using your local text), nor does it pre-load any vectors from elsewhere.  So any vectors which do exist â€“ created by the <code>build_vocab()</code> calls â€“ will still just be in their randomly-initialized starting locations, and be useless for any semantic purposes. </p>

<p>Suggestions:</p>

<ul>
<li>either (a) train your own vectors from your text, which makes sense if you have a good quantity of text; or (b) load vectors from elsewhere. But don't try to do both. (Or, in the case of the code above, neither.)</li>
<li>The <code>update=True</code> option for <code>build_vocab()</code> should be considered an expert, experimental option â€“ only worth tinkering with if you've already had things working in simpler modes, and you're sure you need it and understand all the implications.</li>
<li>Normal use won't ever explicitly re-assign new values into the <code>Word2Vec</code> model's <code>syn0</code> property - those are managed by the class's training routines, so you never need to zero them out or modify them. You should tally up your own text summary vectors, based on word-vectors, outside the model in your own data structures. </li>
</ul>
",0,0,997,2017-09-12 05:00:47,https://stackoverflow.com/questions/46168239/sentence-matching-with-gensim-word2vec-manually-populated-model-doesnt-work
How Word Mover&#39;s Distance (WMD) uses word2vec embedding space?,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X âˆˆ RdÃ—n for a finite size vocabulary of n words. The 
ith column, xi âˆˆ Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d âˆˆ Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
","nlp, nltk, gensim, word2vec, word-embedding","<p>For the purposes of WMD, a text is considered a bunch of 'piles' of meaning. Those piles are placed at the coordinates of the text's words â€“ and that's why WMD calculation is dependent on a set of word-vectors from another source. Those vectors position the text's piles.</p>

<p>The WMD is then the minimal amount of work needed to shift one text's piles to match another text's piles. And the measure of the work needed to shift from one pile to another is the euclidean distance between those pile's coordinates. </p>

<p>You could just try a naive shifting of the piles: look at the first word from text A, shift it to the first word from text B, and so forth. But that's unlikely to be the <em>cheapest</em> shifting â€“ which would likely try to match nearer words, to send the 'meaning' on the shortest possible paths. So actually calculating the WMD is an iterative optimization problem â€“ significantly more expensive than just a simple euclidean-distance or cosine-distance between two points. </p>

<p>That optimization is done inside the <code>emd()</code> call in the code you excerpt. But what that optimization requires is the pairwise distances between all words in text A, and all words in text B â€“ because those are all the candidate paths across which meaning-weight might be shifted. You can see those pairwise distances calculated in the code to populate the <code>distance_matrix</code>, using the word-vectors already loaded in the model and accessible via <code>self[t1]</code>, <code>self[t2]</code>, etc. </p>
",2,2,1652,2017-09-13 15:09:18,https://stackoverflow.com/questions/46201029/how-word-movers-distance-wmd-uses-word2vec-embedding-space
Word Mover&#39;s distance calculation between word pairs of two documents,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, travel cost or Euclidean distance between word pairs is calculated the way shown in the figure below. </p>

<p><a href=""https://i.sstatic.net/RZIyb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RZIyb.png"" alt=""enter image description here""></a></p>

<p>Is this distance calculated in pair wise in a specific order? Such that the first, second and so on from each document as shown in the figure Or Obama's distance is calculated from all four words in D0 and then the minimum of these four is shown in the figure only. </p>

<p>Can someone explain how this works?</p>

<p>Also, why is then all three words in D3 compared with President in D0?</p>
","machine-learning, nlp, word2vec, word-embedding, wmd","<p>The calculation of WMD requires finding the cheapest shifting of word-weight-configuration in a first text, into the word-weight-configuration of the second text. </p>

<p>The word-order is irrelevant. Any word's mass in one text could be shifted to the position of any word in the other text. The optimization process which finds the best shifts thus will consider many possible pairings. After it finds the best, the final single WMD number is the total travel distance in that best solution. </p>

<p>Because of differences in word count, words may not be shifted one-to-one, but as proportion of the full text's mass. So consider the bottom example in the graphic you included: the top text <em>D0</em> has 4 significant words, and the bottom text <em>D3</em> has just 3 significant words. So each of the top text's 4 words can be thought of as having 0.25 mass, and each of the bottom text's words can be thought of as having 0.33 mass. </p>

<p>'Obama' might thus map very closely to 'President' - but even moving 0.25 of 'Obama' mass to 'President' leaves 0.08 mass left over that must travel to another <em>D0</em> word. Similarly with 'Illinois' and 'Chicago' â€“ even if a 0.25 of 'Illinois' mass is moved to 'Chicago', 0.08 is left-over that must travel to another <em>D0</em> word. The exact mix of paths and proportions chosen will be the best possible, but will typically involve some words being fractionally shifted across multiple other words.</p>
",2,2,1099,2017-09-13 16:13:12,https://stackoverflow.com/questions/46202308/word-movers-distance-calculation-between-word-pairs-of-two-documents
How word2Vec or wod2Doc understand user sentiments,"<p>I have gone through numerous documents to read about doc2Vec and word2Vec. I do understand how powerful it is to represent the words as a vector and to perform simple operations like vector addition , subtraction to yield meaningful analogy between the words. 
Although one thing I am still not able to understand is how this technique can be used to understand user sentiments . </p>

<p>Can someone please elaborate as to how user sentiments are analysed using these techniques ?
Thanks
Samir</p>
","nlp, word2vec, doc2vec","<p>By representing a document or set of words with feature vectors, you can process text in other machine learning tasks. For example if you have a dataset which labeled each document <code>x</code> with its sentiment <code>y</code>, you can use the pretraind embedding as feature vectorisation to represent <code>x</code> as input to your machine learning method and test if these features help your task.</p>
",0,0,43,2017-09-19 11:36:23,https://stackoverflow.com/questions/46299420/how-word2vec-or-wod2doc-understand-user-sentiments
How is SpaCy&#39;s similarity computed?,"<p>Beginner NLP Question here:</p>
<h1>How does the .similiarity method work?</h1>
<p>Wow spaCy is great! Its tfidf model could be easier to preprocess, but w2v with only one line of code (token.vector)?! - Awesome!</p>
<p>In his  <a href=""https://github.com/cytora/pycon-nlp-in-10-lines/blob/master/00_spacy_intro.ipynb"" rel=""nofollow noreferrer"">10 line tutorial on spaCy</a> andrazhribernik show's us the .similarity method that can be run on tokens, sents, word chunks, and docs.</p>
<p>After <code>nlp = spacy.load('en')</code> and <code>doc = nlp(raw_text)</code>
we can do .similarity queries between tokens and chunks.
However, what is being calculated behind the scenes in this <code>.similarity</code> method?</p>
<p>SpaCy already has the incredibly simple <code>.vector</code>, which computes the w2v vector as trained from the GloVe model (how cool would a <code>.tfidf</code> or <code>.fasttext</code> method be?).</p>
<p>Is the model similarity model simply computing the cosine similarity between these two w2v-GloVe-vectors or doing something else? The specifics aren't clear in the <a href=""https://spacy.io/docs/usage/word-vectors-similarities"" rel=""nofollow noreferrer"">documentation</a>; any help appreciated!</p>
","python, machine-learning, nlp, word2vec, spacy","<p>Assuming that the method you are referring to is the token similarity one, you can find the function in the sourcecode <a href=""https://github.com/explosion/spaCy/blob/9003fd25e5e966bd8d1b67a18f3ebd6010d6f718/spacy/tokens/token.pyx#L106"" rel=""nofollow noreferrer"">here</a>. As you can see it computes the cosine similarity between the vectors. </p>

<p>As it says in the tutorial: </p>

<blockquote>
  <p>A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping.</p>
</blockquote>

<p>So the vector distance can be related to the word similarity.</p>
",3,3,3446,2017-09-21 15:48:03,https://stackoverflow.com/questions/46348209/how-is-spacys-similarity-computed
What is the initial value of Embedding layer?,"<p>I am studying embedding for word representations. In many dnn libraries, they support embedding layer. And this is really nice tutorial.</p>

<p><a href=""http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"" rel=""nofollow noreferrer"">Word Embeddings: Encoding Lexical Semantics</a></p>

<p>But I am not still sure how to calculate embed value. In below example, it outputs some value even before any trainings. Does it use some random weights? I realize a purpose of <code>Embedding(2, 5)</code>, but not sure its initial calculation. And I am no sure about how to learn weights of its Embedding too.</p>

<pre><code>word_to_ix = {""hello"": 0, ""world"": 1}
embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings
lookup_tensor = torch.LongTensor([word_to_ix[""hello""]])
hello_embed = embeds(autograd.Variable(lookup_tensor))
print(hello_embed)
--------
Variable containing:
-2.9718  1.7070 -0.4305 -2.2820  0.5237
[torch.FloatTensor of size 1x5]
</code></pre>

<p>I break down my thought to be sure. First of all, upper <code>Embedding(2, 5)</code> is a matrix of shape <code>(2, 5)</code>.</p>

<pre><code>Embedding(2, 5) = 
 [[0.1,-0.2,0.3,0.4,0.1],
 [-0.2,0.1,0.8,0.2,0.3]] # initiated by some function, like random normal distribution
</code></pre>

<p>Then, <code>hello</code> is <code>[1, 0]</code>. Then <code>hello</code> representation is calculated by <code>[1, 0].dot(Embedding(2, 5)) = [0.1,-0.2,0.3,0.4,0.1]</code>. This is actually first row of the Embedding. Am I understanding right?</p>

<hr>

<h1>Updates</h1>

<p>I found a code of embedding which is exactly use normal distribution for its value. Yes, but it is just a default value, and we can set arbitrary weights for embedding layers.
<a href=""https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58"" rel=""nofollow noreferrer"">https://github.com/chainer/chainer/blob/adba7b846d018b9dc7d19d52147ef53f5e555dc8/chainer/links/connection/embed_id.py#L58</a></p>
","machine-learning, nlp, deep-learning, word2vec, word-embedding","<p>Yes. You start off with random weights. I think it is more common to use a truncated normal distribution instead of the regular normal distribution. But, that probably doesn't make much of a difference.</p>
",1,3,1945,2017-09-22 03:09:46,https://stackoverflow.com/questions/46356227/what-is-the-initial-value-of-embedding-layer
How to implement synonyms for use in a search engine?,"<p>I am working on a pet search engine (SE). </p>

<p>What I have right now is boolean keyword SE, as a library that is split in two parts:</p>

<ul>
<li><p>index: this is a inverted index ie. it associate terms with the original document where it appears</p></li>
<li><p>query: which is supplied by the user and can be arbitrarily complex boolean expression that looks like <code>(mobile OR android OR iphone) AND game</code></p></li>
</ul>

<p>I'd like to improve the search engine, in a way that does automatically extend simple queries to boolean queries so that it includes search terms that do no appear in the original query ie. I'd like to support synonyms.</p>

<p>I need some help to build the synonyms graph.</p>

<p>How can I compute list of words that appears in similar context?</p>

<p>Here is example of list of synonyms I'd like to compute:</p>

<ul>
<li>psql, pgsql, postgres, postgresql</li>
<li>mobile, iphone, android</li>
</ul>

<p>and also synonyms that includes ngrams like:</p>

<ul>
<li>rdbms, relational database management systems, ...</li>
</ul>

<p>The algorithm doesn't have to be perfect, I can post-process by hand the result, but at least I need to have a clue about what terms are similar to what other terms. </p>
","nlp, full-text-search, artificial-intelligence, text-mining, word2vec","<p>In the standard Information Retrieval (IR) literature, this enrichment of a query with additional terms (that don't appear in the initial/original query) is known as <strong>query expansion</strong>.</p>

<p>There're a plenty of standard approaches which, generally speaking, are based on the idea of <em>scoring</em> terms based on some factors and then selecting a number of terms (say K, a parameter) that have the highest scores.</p>

<p>To compute the term selection score, it is assumed that the top (M) ranked documents retrieved after initial retrieval are relevant, this being called <strong>pseudo-relevance feedback</strong>.</p>

<p>The factors on which the term selection function generally depend are:</p>

<ol>
<li>The term frequency of a term in a top ranked document - higher the better.</li>
<li>The number of documents (out of top M) in which the term occurs in - higher the better.</li>
<li>How many times does an additional term <em>co-occur</em> with a query term - the higher the better.</li>
</ol>

<p>The co-occurrence factor is the most important and would be give you terms such as 'pgsql' if the original query contains 'psql'.</p>

<p>Note that if documents are too short, this method would not work well and you have to use other methods that are necessarily semantics based such as i) <a href=""http://dl.acm.org/citation.cfm?doid=2766462.2767780"" rel=""nofollow noreferrer"">word-vector based expansion</a> or ii) wordnet-based expansion.</p>
",4,1,1237,2017-09-23 18:42:18,https://stackoverflow.com/questions/46383076/how-to-implement-synonyms-for-use-in-a-search-engine
Text Processing - Word2Vec training after phrase detection (bigram model),"<p>I want to make a word2vec model with more n-grams that usual. As I found, Phrase class in gensim.models.phrase can find phrases that I want and it's possible to use phrases on corpus and use it's result model for word2vec train function.</p>

<p>So first of all I do something like below, exactly like sample codes in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim documentation</a>.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

sentences = MySentences('sentences_directory')

bigram = gensim.models.Phrases(sentences)

model = gensim.models.Word2Vec(bigram['sentences'], size=300, window=5, workers=8)
</code></pre>

<p>model has been created but without any good result in evaluation and a warning :</p>

<pre><code>WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable)
</code></pre>

<p>I searched for it and I found <a href=""https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0</a> and changed my code:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

class PhraseItertor(object):
    def __init__(self, my_phraser, data):
        self.my_phraser, self.data = my_phraser, data

    def __iter__(self):
        yield self.my_phraser[self.data]


sentences = MySentences('sentences_directory')

bigram_transformer = gensim.models.Phrases(sentences)

bigram = gensim.models.phrases.Phraser(bigram_transformer)

corpus = PhraseItertor(bigram, sentences)

model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
</code></pre>

<p>I get error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/fatemeh/Desktop/Thesis/bigramModeler.py"", line 36, in &lt;module&gt;
    model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 478, in init
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 553, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 575, in scan_vocab
    vocab[word] += 1
TypeError: unhashable type: 'list'
</code></pre>

<p>Now I want to know that what is wrong in my codes.</p>
","python, gensim, word2vec, text-processing","<p>I asked my question in <a href=""https://groups.google.com/forum/#!topic/gensim/hJ216c59TGM"" rel=""nofollow noreferrer"">Gensim GoogleGroup</a> and <a href=""https://www.linkedin.com/in/gojomo"" rel=""nofollow noreferrer"">Mr Gordon Mohr</a> answered me:</p>

<blockquote>
  <p>You typically wouldn't want an <code>__iter__()</code> method to do a single
  <code>yield</code>. It should return an iterator object (ready to return multiple
  objects via <code>next()</code> or a StopIteration exception). One way to effect
  a iterator is to use <code>yield</code> to have the method treated as a
  'generator' â€“ but that would typically require the <code>yield</code> to be
  inside a loop. </p>
  
  <p>But I now see that my example code in the thread you reference does
  the wrong thing with <em>its</em> <code>__iter__()</code> return line: it should not be
  returning the raw phrasifier, but one that has already been
  started-as-an-iterator, by use of the <code>iter()</code> built-in method. That
  is, the example there should have read:</p>

<pre><code>class PhrasingIterable(object):
    def __init__(self, phrasifier, texts):
        self. phrasifier, self.texts = phrasifier, texts
    def __iter__():
        return iter(phrasifier[texts])
</code></pre>
  
  <p>Making a similar change in your variation may resolve the <code>TypeError:
  iter() returned non-iterator of type 'TransformedCorpus'</code> error.</p>
</blockquote>
",0,1,774,2017-09-26 08:47:19,https://stackoverflow.com/questions/46421771/text-processing-word2vec-training-after-phrase-detection-bigram-model
User2Vec? representing a user based on the docs they consume,"<p>I'd like to form a representation of users based on the last N documents they have liked. </p>

<p>So i'm planning on using doc2vec to form this representation of each document but i'm just trying to figure out what would be a good way to essentially place users in the same space. </p>

<p>Something as simple as averaging the vectors of their last 5 documents they consumed springs to mind but am not sure if this might be a bit silly. Maybe some sort of knn approach in the space might be possible. </p>

<p>Then i'm wondering - the same way we just use a doc id in doc2vec, how crazy would it be to just add in a user id token and try that way to get a representation of a user in much the same way as a document. </p>

<p>I've not been able to find much on ways to use word2vec type embeddings to come up with both document vectors and user vectors that can then be used in a sort of vector space model approach. </p>

<p>Anyone any pointers or suggestions?</p>
","neural-network, word2vec, doc2vec","<p>It's reasonable to try Doc2Vec for analyzing such user-to-document relationships. </p>

<p>You could potentially represent a user by the average-of-the-last-N-docs-consumed, as you suggest. Or all docs they consumed. Or perhaps M centroids chosen to minimize the distances to the last N documents they consumed. But which might do well for the data/goals could only be found by exploratory experimentation. </p>

<p>You <em>could</em> try adding user-tags to whatever other doc-ID-tags (or doc-category-tags) provided during bulk Doc2Vec training. But, beware that adding more tags means a larger model, and in some rough sense ""dilutes"" the meaning that can be extracted from the corpus, or allows for overfitting based on idiosyncracies of seldom-occurring tags (rather than the desired generalization that's forced when a model is smaller). So if you have lots of user-tags, and perhaps lots of user-tags that are only applied to a small subset of documents, the overall quality of the doc-vectors may suffer. </p>

<p>One other interesting (but expensive-to-calculate) technique in the Word2Vec space is ""Word Mover's Distance"" (WMD), which compares texts based a cost to shift all one text's meaning, represented by a series of piles-of-meaning at vector positions for each word, to match another's piles. (Shifting words to word-vector nearby-words is cheap; to distant words is expensive. The calculation finds the optimal set of shifts, and reports its cost, with lower costs being more-similar texts.) </p>

<p>It strikes me that sets-of-doc-vectors could be treated the same way, and so the bag-of-doc-vectors associated with one user need not be reduced to any single average vector, but could instead be compared, via WMD, to another bag-of-doc-vectors, or even single doc-vectors. (There's support for WMD in the <code>wmdistance()</code> method of gensim's <code>KeyedVectors</code>, but not directly on <code>Doc2Vec</code> classes, so you'd need to do some manual object/array juggling or other code customization to adapt it.) </p>
",2,3,1635,2017-09-26 12:18:54,https://stackoverflow.com/questions/46426380/user2vec-representing-a-user-based-on-the-docs-they-consume
How to run MLlib&#39;s word2vec in CBOW mode?,"<p>My understanding is that word2vec can be ran in two modes:</p>

<ul>
<li>continuous bag-of-words (CBOW) (order of words does not matter)</li>
<li>continuous skip-gram (order of words matters)</li>
</ul>

<p>I would like to run the <strong>CBOW</strong> implementation from Spark's MLlib, but it is not clear to me from the documentation and their example how to do it. This is the example listed on their page. </p>

<p>From: <a href=""https://spark.apache.org/docs/2.1.0/mllib-feature-extraction.html#example"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/2.1.0/mllib-feature-extraction.html#example</a></p>

<pre><code>import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

val input = sc.textFile(""data/mllib/sample_lda_data.txt"").map(line =&gt; line.split("" "").toSeq)

val word2vec = new Word2Vec()

val model = word2vec.fit(input)

val synonyms = model.findSynonyms(""1"", 5)

for((synonym, cosineSimilarity) &lt;- synonyms) {
  println(s""$synonym $cosineSimilarity"")
}
</code></pre>

<p><strong>My questions:</strong></p>

<ul>
<li>Which of the two modes does this example use? </li>
<li>Do you know how I can run the model in the <strong>CBOW</strong> mode?</li>
</ul>

<p>Thanks in advance!</p>
","scala, apache-spark, apache-spark-mllib, word2vec","<p>Seems like MLlib's currently only implements skip-gram. </p>

<p>Here is the open ticket/pull request for the skip-gram model: <a href=""https://issues.apache.org/jira/browse/SPARK-20372"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/browse/SPARK-20372</a></p>
",1,2,774,2017-09-26 15:56:23,https://stackoverflow.com/questions/46430988/how-to-run-mllibs-word2vec-in-cbow-mode
Why do I get single letter vocabulary in Gensim word2vec?,"<p>I am building a word2vec model as follows.</p>

<pre><code>from gensim.models import word2vec, Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]

    print(bigrams_)
    print(trigrams_)


# Set values for various parameters
num_features = 10    # Word vector dimensionality                      
min_word_count = 1   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 5          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words


model = word2vec.Word2Vec(trigrams_, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, the output I get for the model's vocabulary is single character as follows. </p>

<pre><code>['h', 'u', 'm', 'a', 'n', ' ', 'c', 'o', 'p', 't']
</code></pre>

<p>I am getting the bigrams and trigrams correctly. Hence, I am just confused where I make the code wrong. Please let me know what is the problem?</p>
","python, gensim, word2vec","<p>This solved my issue. I should pass list of lists to the word2vec model as follows.</p>

<pre><code>trigram_sentences_project = []


bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')


for sent in sentence_stream:
    #bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    #trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)
</code></pre>
",4,6,2286,2017-09-27 07:27:33,https://stackoverflow.com/questions/46441876/why-do-i-get-single-letter-vocabulary-in-gensim-word2vec
Siamese Network with LSTM for sentence similarity in Keras gives periodically the same result,"<p>I'm a newbie in Keras and I'm trying to solve the task of sentence similairty using NN in Keras.
I use word2vec as word embedding, and then a Siamese Network to prediction how similar two sentences are. 
The base network for the Siamese Network is a LSTM, and to merge the two base network I use a Lambda layer with cosine similairty metric.
As dataset I'm using SICK dataset, that gives a score to each pair of sentences, from 1(different) to 5(very similar).</p>

<p>I created the network and it runs, but I have a lot of doubts : 
first of all I'm not sure if the way I feed the LSTM with sentences is fine. I take word2vec embedding for each word and I create only one array per sentence, padding it with zeros to seq_len in order to obtain same lenght arrays. And then I reshape it in this way :  <code>data_A = embedding_A.reshape((len(embedding_A), seq_len, feature_dim))</code></p>

<p>Besides I'm not sure if my Siamese Network is correct, beacuse a lot of predictionion for different pairs are equal and the loss doesn't change much (from 0.3300 to 0.2105 in 10 epochs, and it doesn't change much more in 100 epochs).</p>

<p>Someone can help me find and understand my mistakes? 
Thanks so much (and sorry for my bad english)</p>

<p>Interested part in my code</p>

<pre><code>def cosine_distance(vecs):
    #I'm not sure about this function too
    y_true, y_pred = vecs
    y_true = K.l2_normalize(y_true, axis=-1)
    y_pred = K.l2_normalize(y_pred, axis=-1)
    return K.mean(1 - K.sum((y_true * y_pred), axis=-1))

def cosine_dist_output_shape(shapes):
    shape1, shape2 = shapes
    print((shape1[0], 1))
    return (shape1[0], 1)

def contrastive_loss(y_true, y_pred):
    margin = 1
    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))

def create_base_network(feature_dim,seq_len):

    model = Sequential()  
    model.add(LSTM(100, batch_input_shape=(1,seq_len,feature_dim),return_sequences=True))
    model.add(Dense(50, activation='relu'))    
    model.add(Dense(10, activation='relu'))
    return model


def siamese(feature_dim,seq_len, epochs, tr_dataA, tr_dataB, tr_y, te_dataA, te_dataB, te_y):    

    base_network = create_base_network(feature_dim,seq_len)

    input_a = Input(shape=(seq_len,feature_dim,))
    input_b = Input(shape=(seq_len,feature_dim))

    processed_a = base_network(input_a)
    processed_b = base_network(input_b)

    distance = Lambda(cosine_distance, output_shape=cosine_dist_output_shape)([processed_a, processed_b])

    model = Model([input_a, input_b], distance)

    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    model.compile(optimizer=adam, loss=contrastive_loss)
    model.fit([tr_dataA, tr_dataB], tr_y,
              batch_size=128,
              epochs=epochs,
              validation_data=([te_dataA, te_dataB], te_y))


    pred = model.predict([tr_dataA, tr_dataB])
    tr_acc = compute_accuracy(pred, tr_y)
    for i in range(len(pred)):
        print (pred[i], tr_y[i])


    return model


def padding(max_len, embedding):
    for i in range(len(embedding)):
        padding = np.zeros(max_len-embedding[i].shape[0])
        embedding[i] = np.concatenate((embedding[i], padding))

    embedding = np.array(embedding)
    return embedding

def getAB(sentences_A,sentences_B, feature_dim, word2idx, idx2word, weights,max_len_def=0):
    #from_sentence_to_array : function that transforms natural language sentences 
    #into vectors of real numbers. Each word is replaced with the corrisponding word2vec 
    #embedding, and words that aren't in the embedding are replaced with zeros vector.  
    embedding_A, max_len_A = from_sentence_to_array(sentences_A,word2idx, idx2word, weights)
    embedding_B, max_len_B = from_sentence_to_array(sentences_B,word2idx, idx2word, weights)

    max_len = max(max_len_A, max_len_B,max_len_def*feature_dim)

    #padding to max_len
    embedding_A = padding(max_len, embedding_A)
    embedding_B = padding(max_len, embedding_B)

    seq_len = int(max_len/feature_dim)
    print(seq_len)

    #rashape
    data_A = embedding_A.reshape((len(embedding_A), seq_len, feature_dim))
    data_B = embedding_B.reshape((len(embedding_B), seq_len, feature_dim))

    print('A,B shape: ',data_A.shape, data_B.shape)

    return data_A, data_B, seq_len



FEATURE_DIMENSION = 100
MIN_COUNT = 10
WINDOW = 5

if __name__ == '__main__':

    data = pd.read_csv('data\\train.csv', sep='\t')
    sentences_A = data['sentence_A']
    sentences_B = data['sentence_B']
    tr_y = 1- data['relatedness_score']/5

    if not (os.path.exists(EMBEDDING_PATH)  and os.path.exists(VOCAB_PATH)):    
        create_embeddings(embeddings_path=EMBEDDING_PATH, vocab_path=VOCAB_PATH,  size=FEATURE_DIMENSION, min_count=MIN_COUNT, window=WINDOW, sg=1, iter=25)
    word2idx, idx2word, weights = load_vocab_and_weights(VOCAB_PATH,EMBEDDING_PATH)

    tr_dataA, tr_dataB, seq_len = getAB(sentences_A,sentences_B, FEATURE_DIMENSION,word2idx, idx2word, weights)

    test = pd.read_csv('data\\test.csv', sep='\t')
    test_sentences_A = test['sentence_A']
    test_sentences_B = test['sentence_B']
    te_y = 1- test['relatedness_score']/5

    te_dataA, te_dataB, seq_len = getAB(test_sentences_A,test_sentences_B, FEATURE_DIMENSION,word2idx, idx2word, weights, seq_len) 

    model = siamese(FEATURE_DIMENSION, seq_len, 10, tr_dataA, tr_dataB, tr_y, te_dataA, te_dataB, te_y)


    test_a = ['this is my dog']
    test_b = ['this dog is mine']
    a,b,seq_len = getAB(test_a,test_b, FEATURE_DIMENSION,word2idx, idx2word, weights, seq_len)
    prediction  = model.predict([a, b])
    print(prediction)
</code></pre>

<p>Some of the results : </p>

<pre><code>my prediction | true label 
0.849908 0.8
0.849908 0.8
0.849908 0.74
0.849908 0.76
0.849908 0.66
0.849908 0.72
0.849908 0.64
0.849908 0.8
0.849908 0.78
0.849908 0.8
0.849908 0.8
0.849908 0.8
0.849908 0.8
0.849908 0.74
0.849908 0.8
0.849908 0.8
0.849908 0.8
0.849908 0.66
0.849908 0.8
0.849908 0.66
0.849908 0.56
0.849908 0.8
0.849908 0.8
0.849908 0.76
0.847546 0.78
0.847546 0.8
0.847546 0.74
0.847546 0.76
0.847546 0.72
0.847546 0.8
0.847546 0.78
0.847546 0.8
0.847546 0.72
0.847546 0.8
0.847546 0.8
0.847546 0.78
0.847546 0.8
0.847546 0.78
0.847546 0.78
0.847546 0.46
0.847546 0.72
0.847546 0.8
0.847546 0.76
0.847546 0.8
0.847546 0.8
0.847546 0.8
0.847546 0.8
0.847546 0.74
0.847546 0.8
0.847546 0.72
0.847546 0.68
0.847546 0.56
0.847546 0.8
0.847546 0.78
0.847546 0.78
0.847546 0.8
0.852975 0.64
0.852975 0.78
0.852975 0.8
0.852975 0.8
0.852975 0.44
0.852975 0.72
0.852975 0.8
0.852975 0.8
0.852975 0.76
0.852975 0.8
0.852975 0.8
0.852975 0.8
0.852975 0.78
0.852975 0.8
0.852975 0.8
0.852975 0.78
0.852975 0.8
0.852975 0.8
0.852975 0.76
0.852975 0.8
</code></pre>
","python-3.x, keras, lstm, word2vec, sentence-similarity","<p>You're seeing consecutive equal values because the output shape of the function <code>cosine_distance</code> is wrong. When you take <code>K.mean(...)</code> without the <code>axis</code> argument, the result is a scalar. To fix it, just use <code>K.mean(..., axis=-1)</code> in <code>cosine_distance</code> to replace <code>K.mean(...)</code>.</p>

<h3>More Detailed Explanation:</h3>

<p>When <code>model.predict()</code> is called, the output array <code>pred</code> is first pre-allocated, and then filled with the batch predictions. From the source code <a href=""https://github.com/fchollet/keras/blob/master/keras/engine/training.py"" rel=""noreferrer"">training.py</a>:</p>

<pre><code>if batch_index == 0:
    # Pre-allocate the results arrays.
    for batch_out in batch_outs:
        shape = (num_samples,) + batch_out.shape[1:]
        outs.append(np.zeros(shape, dtype=batch_out.dtype))
for i, batch_out in enumerate(batch_outs):
    outs[i][batch_start:batch_end] = batch_out
</code></pre>

<p>In your case you only have single output, so <code>pred</code> is just <code>outs[0]</code> in the code above. When <code>batch_out</code> is a scalar (for example, 0.847546 as seen in your results), the code above is equivalent to <code>pred[batch_start:batch_end] = 0.847576</code>. As the default batch size is 32 for <code>model.predict()</code>, you can see 32 consecutive 0.847576 values appear in your posted result.</p>

<hr>

<p>Another possibly bigger problem is that the labels are wrong. You convert the relatedness score to labels by <code>tr_y = 1- data['relatedness_score']/5</code>. Now if two sentences are ""very similar"", the relatedness score is 5, so <code>tr_y</code> is 0 for these two sentences.</p>

<p>However, in the contrastive loss, when <code>y_true</code> is zero, the term <code>K.maximum(margin - y_pred, 0)</code> actually means that ""these two sentences should have a cosine distance <code>&gt;= margin</code>"". That's the opposite of what you want your model to learn (also I don't think you need <code>K.square</code> in the loss).</p>
",5,9,4257,2017-09-28 09:46:41,https://stackoverflow.com/questions/46466013/siamese-network-with-lstm-for-sentence-similarity-in-keras-gives-periodically-th
Word2Vec Tutorial: Tensorflow TypeError: Input &#39;y&#39; of &#39;Mul&#39; Op has type float32 that does not match type int32 of argument &#39;x&#39;,"<p>Version of Tensorflow: 1.2.1<br>
Version of Python: 3.5<br>
Operating System: Windows 10  </p>

<p>Another poster has asked about this same problem on StackOverflow <a href=""https://stackoverflow.com/questions/43786994/in-tensorflow-tf-nn-nce-loss-get-typeerror-input-y-of-mul-op-has-type-float"">here</a>, and he appears to be using code from the same Udacity Word2Vec tutorial. So, maybe I'm dense, but the code of this example is so busy and complex that I can't tell what fixed his problem. </p>

<p>The error occurs when I call <code>tf.reduce_means</code>:</p>

<pre><code>loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                               train_labels, num_sampled, vocabulary_size))
</code></pre>

<p>Right before the call to <code>tf.reduce_mean</code> the key variables have the following data types.</p>

<blockquote>
  <p>train_dataset.dtype<br>
  >> tf.int32<br>
  train_labels.dtype<br>
  >> tf.int32<br>
  valid_dataset.dtype<br>
  >> tf.int32<br>
  embeddings.dtype<br>
  >> tf.float32_ref<br>
  softmax_weights.dtype<br>
  >> tf.float32_ref<br>
  softmax_biases.dtype<br>
  >> tf.float32_ref<br>
  embed.dtype<br>
  >> tf.float32   </p>
</blockquote>

<p>I tried every permutation of data type in the definitions of the variables <code>train_dataset.dtype</code>, <code>train_labels.dtype</code> and <code>valid_dataset.dtype</code>: making them all <code>int64</code>, all <code>float32</code>, all <code>float64</code>, and combinations of integer and floating point. Nothing worked. I didn't try altering the data types of <code>softmax_weight</code> and <code>softmax_biases</code>, because I'm afraid that might foul up the optimization algorithm. Don't these need to be floats to support the calculus that is done during backpropagation? (Tensorflow is often a very opaque black box with documentation that verges on completely useless, so I can suspect things but never know for sure.)</p>

<p>Program Flow at Time of Error:   </p>

<p>After the call to <code>reduce_mean</code> program control transfers to <code>sampled_softmax_loss()</code> in file <code>nn_impl.py</code> which in turn calls <code>_compute_sampled_logits()</code>:</p>

<pre><code>  logits, labels = _compute_sampled_logits(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      num_sampled=num_sampled,
      num_classes=num_classes,
      num_true=num_true,
      sampled_values=sampled_values,
      subtract_log_q=True,
      remove_accidental_hits=remove_accidental_hits,
      partition_strategy=partition_strategy,
      name=name)
</code></pre>

<p>At this point I check the data types of the passed-in parameters and get the following:</p>

<blockquote>
  <p>weights.dtype<br>
  >> tf.float32_ref<br>
  biases.dtype<br>
  >> tf.float32_ref<br>
  labels.dtype<br>
  >> tf.float32<br>
  inputs.dtype<br>
  >> tf.int32   </p>
</blockquote>

<p>On the very next step an exception occurs, and I am thrown into the <code>StreamWrapper</code> class in file <code>ansitowin32.py</code>. Running to the end, I get the following Traceback:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    489                 as_ref=input_arg.is_ref,
--&gt; 490                 preferred_dtype=default_dtype)
    491           except TypeError as err:

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    740         if ret is None:
--&gt; 741           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    742 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    613         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--&gt; 614         % (dtype.name, t.dtype.name, str(t)))
    615   return t

ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Reshape_1:0"", shape=(?, 1, ?), dtype=float32, device=/device:CPU:0)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-66d378b94a16&gt; in &lt;module&gt;()
     34     loss = tf.reduce_mean(
     35       tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
---&gt; 36                                train_labels, num_sampled, vocabulary_size))
     37 
     38     # Optimizer.

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name)
   1266       remove_accidental_hits=remove_accidental_hits,
   1267       partition_strategy=partition_strategy,
-&gt; 1268       name=name)
   1269   sampled_losses = nn_ops.softmax_cross_entropy_with_logits(labels=labels,
   1270                                                             logits=logits)

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name)
   1005     row_wise_dots = math_ops.multiply(
   1006         array_ops.expand_dims(inputs, 1),
-&gt; 1007         array_ops.reshape(true_w, new_true_w_shape))
   1008     # We want the row-wise dot plus biases which yields a
   1009     # [batch_size, num_true] tensor of true_logits.

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\math_ops.py in multiply(x, y, name)
    284 
    285 def multiply(x, y, name=None):
--&gt; 286   return gen_math_ops._mul(x, y, name)
    287 
    288 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\gen_math_ops.py in _mul(x, y, name)
   1375     A `Tensor`. Has the same type as `x`.
   1376   """"""
-&gt; 1377   result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
   1378   return result
   1379 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    524                   ""%s type %s of argument '%s'."" %
    525                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--&gt; 526                    inferred_from[input_arg.type_attr]))
    527 
    528           types = [values.dtype]

TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.
</code></pre>

<p>Here's the complete program:</p>

<pre><code># These are all the modules we'll be using later. 
# Make sure you can import them before proceeding further.

# %matplotlib inline

from __future__ import print_function
import collections
import math
import numpy as np
import os
import random
import tensorflow as tf
import zipfile
from matplotlib import pylab
from six.moves import range
from six.moves.urllib.request import urlretrieve
from sklearn.manifold import TSNE

print(""Working directory = %s\n"" % os.getcwd())

def read_data(filename):
    """"""Extract the first file enclosed in a zip file as a list of words""""""
    with zipfile.ZipFile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

filename = 'text8.zip'

words = read_data(filename)
print('Data size %d' % len(words))

vocabulary_size = 50000

def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    dictionary = dict()
    # Loop through the keys of the count collection dictionary
    # (apparently, zeroing out counts)
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0  # count of unknown words
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary['UNK']
            unk_count = unk_count + 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary


data, count, dictionary, reverse_dictionary = build_dataset(words)
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10])
del words  # Hint to reduce memory.

data_index = 0

def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [ skip_window ]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

print('data:', [reverse_dictionary[di] for di in data[:8]])

for num_skips, skip_window in [(2, 1), (4, 2)]:
    data_index = 0
    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
    print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
    print('    batch:', [reverse_dictionary[bi] for bi in batch])
    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])

batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1  # How many words to consider left and right.
num_skips = 2  # How many times to reuse an input to generate a label.
# We pick a random validation set to sample nearest neighbors. here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent.
valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64  # Number of negative examples to sample.

graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Variables.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
        normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>
","tensorflow, typeerror, word2vec","<p>I had the same issue and it looks like that two parameters that are passed on to the loss function are swapped around. 
If you look at the tensorflow description for 'sample_softmax_loss' (<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a>):</p>

<pre><code>sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    partition_strategy='mod',
    name='sampled_softmax_loss'
)
</code></pre>

<p>The third expected parameter is 'labels' and the fourth 'inputs'. In the supplied code, these two parameters seem to have been switched around. I'm a bit puzzled how this is possible. Maybe this used to be different in an older version of TF. Anyway, swapping those two parameters around will solve the problem.</p>
",1,1,1317,2017-10-01 19:37:54,https://stackoverflow.com/questions/46516168/word2vec-tutorial-tensorflow-typeerror-input-y-of-mul-op-has-type-float32
Importing Glove to h2o with word2vec function throwing NullPointerException,"<p>I'm trying to import Glove to h2o cluster via R with word2vec function.
Regarding to this <a href=""https://stackoverflow.com/questions/42982176/does-or-will-h2o-provide-any-pretrained-vectors-for-use-with-h2o-word2vec"">Does or will H2O provide any pretrained vectors for use with h2o word2vec?</a>
I downloaded pretrained glove.840B.300d.txt file and tried to import it to h2o but there was problem with parsing.
Then I read Glove to R, removed one line recognized as a NA and saved it as csv. With the csv file parsing in h2o went well but I couldn't create word2vec model with it hence it threw <code>java.lang.NullPointerException</code></p>

<p>I have <code>h2o_3.15.0.99999</code> version.</p>

<p>My code:</p>

<pre><code>h2o.init()
glove&lt;-h2o.importFile(""glove.840B.300d.csv"",header = F)
model&lt;-h2o.word2vec(pre_trained = glove,vec_size = 300)
</code></pre>

<p>Full output:</p>

<pre><code>|==========================================================================| 100%

java.lang.NullPointerException
java.lang.NullPointerException
at water.AutoBuffer.tcpOpen(AutoBuffer.java:488)
at water.AutoBuffer.sendPartial(AutoBuffer.java:679)
at water.AutoBuffer.putA4f(AutoBuffer.java:1383)
at hex.word2vec.Word2VecModel$Word2VecOutput$Icer.write90(Word2VecModel$Word2VecOutput$Icer.java)
at hex.word2vec.Word2VecModel$Word2VecOutput$Icer.write(Word2VecModel$Word2VecOutput$Icer.java)
at water.Iced.write(Iced.java:61)
at water.AutoBuffer.put(AutoBuffer.java:771)
at hex.Model$Icer.write86(Model$Icer.java)
at hex.word2vec.Word2VecModel$Icer.write85(Word2VecModel$Icer.java)
at hex.word2vec.Word2VecModel$Icer.write(Word2VecModel$Icer.java)
at water.Iced.write(Iced.java:61)
at water.Iced.asBytes(Iced.java:42)
at water.Value.&lt;init&gt;(Value.java:348)
at water.TAtomic.atomic(TAtomic.java:22)
at water.Atomic.compute2(Atomic.java:56)
at water.Atomic.fork(Atomic.java:39)
at water.Atomic.invoke(Atomic.java:31)
at water.Lockable.unlock(Lockable.java:181)
at water.Lockable.unlock(Lockable.java:176)
at hex.word2vec.Word2Vec$Word2VecDriver.computeImpl(Word2Vec.java:72)
at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:205)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
</code></pre>
","r, word2vec, h2o","<p>Thanks for the report, the current implementation is restricted by JVM's maximum length of an array. This model seems to be too large and it exceeds the JVM's limits.</p>

<p>We will have to fix it in H2O.</p>
",1,1,272,2017-10-03 21:10:44,https://stackoverflow.com/questions/46553482/importing-glove-to-h2o-with-word2vec-function-throwing-nullpointerexception
How to do keyword mapping in pandas,"<p>I have keyword</p>

<pre><code>India
Japan
United States
Germany
China
</code></pre>

<p>Here's sample dataframe</p>

<pre><code>id    Address 
1     Chome-2-8 Shibakoen, Minato, Tokyo 105-0011, Japan
2     ArcisstraÃŸe 21, 80333 MÃ¼nchen, Germany
3     Liberty Street, Manhattan, New York, United States
4     30 Shuangqing Rd, Haidian Qu, Beijing Shi, China
5     Vaishnavi Summit,80feet Road,3rd Block,Bangalore, Karnataka, India
</code></pre>

<p>My Goal Is make </p>

<pre><code>id    Address                                                          India Japan United States  Germany China    
1     Chome-2-8 Shibakoen, Minato, Tokyo 105-0011, Japan              0     1     0              0       0                  
2     ArcisstraÃŸe 21, 80333 MÃ¼nchen, Germany                          0     0     0              1       0
3     Liberty Street, Manhattan, New York, USA                        0     0     1              0       0
4     30 Shuangqing Rd, Haidian Qu, Beijing Shi, China                0     0     0              0       1
5     Vaishnavi Summit,80feet Road,Bangalore, Karnataka, India        1     0     0              0       0
</code></pre>

<p>The basic idea is create keyword detector, I am thinking to use <code>str.contain</code> and <code>word2vec</code> but I can't get the logic</p>
","python, pandas, word2vec","<p>Make use of <code>pd.get_dummies()</code>:</p>

<pre><code>countries = df.Address.str.extract('(India|Japan|United States|Germany|China)', expand = False)
dummies = pd.get_dummies(countries)
pd.concat([df,dummies],axis = 1)
</code></pre>

<p>Also, the most straightforward way is to have the countries in a list and use a for loop, say</p>

<pre><code>countries = ['India','Japan','United States','Germany','China']
for c in countries:
    df[c] = df.Address.str.contains(c) * 1
</code></pre>

<p>but it can be slow if you have a lot of data and countries.</p>
",3,0,654,2017-10-06 08:24:13,https://stackoverflow.com/questions/46601437/how-to-do-keyword-mapping-in-pandas
Can we build word2vec model in a distributed way?,"<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
","nlp, deep-learning, distributed-computing, gensim, word2vec","<p>Training a model with a huge corpus will surely take a very long time because of a large number of weights involved.  Suppose your word vectors have 300 components and your vocabulary size is 10,000. The size of weight matrix would be 300*10000 = 3 million!</p>

<p>To build a model for huge datasets I would recommend you to first preprocess the dataset. Following preprocessing steps can be applied:</p>

<ul>
<li>Removing stop words.</li>
<li>Treating word pairs or phrases as single words, like new york as new_york, etc.</li>
<li>Subsampling frequent words to decrease the number of training examples.</li>
<li>Modifying the optimization objective with a technique they called â€œNegative Samplingâ€, which causes each training sample to update only a small percentage of the modelâ€™s weights. </li>
</ul>

<p>The above tasks were also done in official word2vec implementation released by Google. Gensim provides very beautiful high-level APIs to perform most of above tasks. Also, have a look at this <a href=""https://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""nofollow noreferrer"">blog</a> for further optimizing techniques.</p>

<p>One more thing that can be done is instead of training your own model use the already trained <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">word2vec mode</a>l released by Google Itâ€™s 1.5GB and includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. </p>
",0,3,1011,2017-10-06 19:45:38,https://stackoverflow.com/questions/46612949/can-we-build-word2vec-model-in-a-distributed-way
How to design the output layer of word-RNN model with use word2vec embedding,"<p>I try to build a Word-RNN equivalent of Char-RNN, the net should generate next word in a sentence.</p>

<p>As input I use pre-trained word2vec 100-dim vectors, hidden layer size is 200. My main problem is output layer, how it should be designed?</p>

<p>In char-rnn, output it is vocabulary size(number of unique chars) vector with char probabilities distribution (softmax). So generating next char is simply sampling form this distribution.
But using word2vec when my word vocabulary is over 300k this approach is not feasible.</p>

<p>Should my output generates 100-dim vector and then I should find nearest similar word with use of <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similar_by_vector"" rel=""nofollow noreferrer"">gensim similar_by_vector function</a></p>

<p>Could you provide some good and easy to understand python and tensorflow  implementation, some link to github or publication.</p>

<p>I have found a <a href=""https://datascience.stackexchange.com/questions/15149/output-a-word-instead-of-a-vector-after-word-embedding"">similar question</a>, but it doesn't answer my question:</p>
","python, tensorflow, neural-network, recurrent-neural-network, word2vec","<p>You can output an index of a word (per example), thus avoid one-hot word representation (which is indeed very big). Use <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/sequence_loss_by_example"" rel=""nofollow noreferrer""><code>tf.contrib.legacy_seq2seq.sequence_loss_by_example</code></a>:</p>

<blockquote>
  <p>Weighted cross-entropy loss for a sequence of logits (per example).</p>
  
  <ul>
  <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols]. </li>
  <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>
  <li>weights: List of 1D batch-sized
  float-Tensors of the same length as logits.</li>
  </ul>
</blockquote>

<p>Note that it doesn't reduce the size of your model, but it saves a lot of memory by computing the loss from sparsely encoded labels. A complete example of a word-rnn implementation can be found <a href=""https://github.com/hunkim/word-rnn-tensorflow"" rel=""nofollow noreferrer"">here</a>, and they use exactly this approach.</p>
",1,1,393,2017-10-08 10:41:59,https://stackoverflow.com/questions/46630171/how-to-design-the-output-layer-of-word-rnn-model-with-use-word2vec-embedding
How to manually change the vector dimensions of a word in Gensim Word2Vec,"<p>I have a Word2Vec model with a lot of word vectors. I can access a word vector as so. </p>

<pre><code>word_vectors = gensim.models.Word2Vec.load(wordspace_path)
print(word_vectors['boy'])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[ -5.48055351e-01   1.08748421e-01  -3.50534245e-02  -9.02988110e-03...]
</code></pre>

<p>Now I have a proper vector representation that I want to replace the word_vectors['boy'] with.</p>

<pre><code>word_vectors['boy'] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>But the following error is thrown</p>

<pre><code>TypeError: 'Word2Vec' object does not support item assignment
</code></pre>

<p>Is there any fashion or workaround to do this? That is manipulate word vectors manually once the model is trained? Is it possible in other platforms except Gensim? </p>
","python, vector, gensim, word2vec, vector-space","<p>Since word2vec vectors are typically only created by the iterative training process, then accessed, the gensim <code>Word2Vec</code> object does not support direct assignment of new values by its word indexes.</p>

<p>However, as it is in Python, all its internal structures are fully viewable/tamperable by you, and as it is open-source, you can view exactly how it does all of its existing functionality, and use that as a model for how to do new things. </p>

<p>Specifically, the raw word-vectors are (in recent versions of gensim) stored in a property of the <code>Word2Vec</code> object called <code>wv</code>, and this <code>wv</code> property is an instance of <code>KeyedVectors</code>. If you examine its source code, you can see accesses of word-vectors by string key (eg <code>'boy'</code>), including those by <code>[]</code>-indexing implemented by the <code>__getitem__()</code> method, go through its method <code>word_vec()</code>. You can view the source of that method either in your local installation, or at Github:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/c2201664d5ae03af8d90fb5ff514ffa48a6f305a/gensim/models/keyedvectors.py#L265"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/c2201664d5ae03af8d90fb5ff514ffa48a6f305a/gensim/models/keyedvectors.py#L265</a></p>

<p>There you'll see the word is actually converted to an integer-index (via <code>self.vocab[word].index</code>) then used to access an internal <code>syn0</code> or <code>syn0norm</code> array (depending on whether the user is accessing the raw or unit-normalized vector). If you look elsewhere where these are set up, or simply examine them in your own console/code (as if by <code>word_vectors.wv.syn0</code>), you'll see these are <code>numpy</code> arrays which <em>do</em> support direct assignment by index.</p>

<p>So, you <em>can</em> directly tamper with their values by integer index, as if by:</p>

<pre><code>word_vectors.wv.syn0[word_vectors.wv.vocab['boy'].index] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>And then, future accesses of <code>word_vectors.wv['boy']</code> will return your updated values. </p>

<p>Notes:</p>

<p>â€¢ If you want <code>syn0norm</code> to be updated, to have the proper unit-normed vectors (as are used in <code>most_similar()</code> and other operations), it'd likely be best to modify <code>syn0</code> first, then discard and recalculate <code>syn0norm</code>, via:</p>

<pre><code>word_vectors.wv.syn0norm = None
word_vectors.wv.init_sims()
</code></pre>

<p>â€¢ Adding new words would require more involved object-tampering, because it will require growing the <code>syn0</code> (replacing it with a larger array), and updating the <code>vocab</code> dict</p>
",11,5,3037,2017-10-09 13:41:42,https://stackoverflow.com/questions/46647945/how-to-manually-change-the-vector-dimensions-of-a-word-in-gensim-word2vec
How to create gensim word2vec model using pre trained word vectors?,"<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
","nlp, gensim, word2vec, text-analysis, word-embedding","<p>I am not sure if you created word2vec model using <code>gensim</code> or some other tools but if understand your question correctly you want to just load the word2vec model using gensim. This is done in the following way:</p>

<pre><code>import gensim
w2v_file = codecs.open(WORD2VEC_PATH, encoding='utf-8')
model = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)  # or binary=False if the model is not compressed
</code></pre>

<p>If, however, what you want to do is to train word2vec model from scratch (i.e. from raw text) using purely <code>gensim</code> here is a <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">tutorial on how to train word2vec model using gensim</a>.</p>
",4,3,3336,2017-10-12 03:59:07,https://stackoverflow.com/questions/46701173/how-to-create-gensim-word2vec-model-using-pre-trained-word-vectors
gensim with different context,"<p>I'm using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim library for word2vec</a>. I want to train the model on text examples that are unrelated, for example: ""The cat is brown. What time is it?""</p>

<p>I have created the following input to the model:</p>

<p><code>[[""The"", ""cat"", ""is"", ""brown""], [""What"", ""time"", ""is"", ""it""]]</code>, however I'm wondering whether the model assumes that ""brown"" and ""What"" are in the same context.</p>

<p>Tried to find the answer in the api, but could not find it.</p>
","machine-learning, nlp, word2vec","<p>The gensim API won't consider ""brown"" and ""What"" in the same context. Uneven windows are used near sentence boundaries. So, for your example, if the window size let's say is 1, the (context, target) pairs would look like as below:</p>

<p><code>([cat],The), ([The,is],cat), ([cat,brown],is), ([is],brown) ([time],What), ([What,is],time), ([time,it],is), ([is],it)</code></p>

<p>I hope this clears your doubt.</p>
",0,0,44,2017-10-12 08:20:21,https://stackoverflow.com/questions/46704738/gensim-with-different-context
Why are word embedding actually vectors?,"<p>I am sorry for my naivety, but I don't understand why word embeddings that are the result of NN training process (word2vec) are actually vectors.</p>

<p>Embedding is the process of dimension reduction, during the training process NN reduces the 1/0 arrays of words into smaller size arrays, the process does nothing that applies vector arithmetic.</p>

<p>So as result we got just arrays and not the vectors. Why should I think of these arrays as vectors?</p>

<p>Even though, we got vectors, why does everyone depict them as vectors coming from the origin (0,0)?</p>

<p>Again, I am sorry if my question looks stupid.</p>
","machine-learning, neural-network, nlp, word2vec, embedding","<p><strong>What are embeddings?</strong></p>
<blockquote>
<p>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the <strong>vocabulary are mapped to vectors of real numbers</strong>.</p>
<p>Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.</p>
</blockquote>
<p>(Source: <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Word_embedding</a>)</p>
<p><strong>What is Word2Vec?</strong></p>
<blockquote>
<p>Word2vec is a group of related models that are used to produce word embeddings. These <strong>models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words</strong>.</p>
<p>Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with <strong>each unique word in the corpus being assigned a corresponding vector in the space</strong>.</p>
<p>Word vectors are positioned in the vector space such that <strong>words that share common contexts in the corpus are located in close proximity to one another in the space</strong>.</p>
</blockquote>
<p>(Source: <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Word2vec</a>)</p>
<p><strong>What's an array?</strong></p>
<blockquote>
<p>In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key.</p>
<p>An array is stored so that the position of each element can be computed from its index tuple by a mathematical formula.</p>
<p><strong>The simplest type of data structure is a linear array, also called one-dimensional array.</strong></p>
</blockquote>
<p><strong>What's a vector / vector space?</strong></p>
<blockquote>
<p>A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (&quot;scaled&quot;) by numbers, called scalars.</p>
<p>Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field.</p>
<p>The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.</p>
</blockquote>
<p>(Source: <a href=""https://en.wikipedia.org/wiki/Vector_space"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Vector_space</a>)</p>
<p><strong>What's the difference between vectors and arrays?</strong></p>
<p>Firstly, the vector in word embeddings is not exactly the programming language data structure (so it's not <a href=""https://stackoverflow.com/questions/15079057/arrays-vs-vectors-introductory-similarities-and-differences"">Arrays vs Vectors: Introductory Similarities and Differences</a>).</p>
<p>Programmatically, a word embedding vector <strong>IS</strong> some sort of an array (data structure) of real numbers (i.e. scalars)</p>
<p>Mathematically, any element with one or more dimension populated with real numbers is a <a href=""https://en.wikipedia.org/wiki/Tensor"" rel=""nofollow noreferrer"">tensor</a>. And a vector is a single dimension of scalars.</p>
<hr />
<p>To answer the OP question:</p>
<p><strong>Why are word embedding actually vectors?</strong></p>
<blockquote>
<p>By definition, word embeddings are vectors (see above)</p>
</blockquote>
<p><strong>Why do we represent words as vectors of real numbers?</strong></p>
<blockquote>
<p>To learn the differences between words, we have to quantify the difference in some manner.</p>
</blockquote>
<p>Imagine, if we assign theses &quot;smart&quot; numbers to the words:</p>
<pre><code>&gt;&gt;&gt; semnum = semantic_numbers = {'car': 5, 'vehicle': 2, 'apple': 232, 'orange': 300, 'fruit': 211, 'samsung': 1080, 'iphone': 1200}
&gt;&gt;&gt; abs(semnum['fruit'] - semnum['apple'])
21
&gt;&gt;&gt; abs(semnum['samsung'] - semnum['apple'])
848
</code></pre>
<p>We see that the distance between <code>fruit</code> and <code>apple</code> is close but <code>samsung</code> and <code>apple</code> isn't. In this case, the single numerical &quot;feature&quot; of the word is capable of capturing some information about the word meanings but not fully.</p>
<p>Imagine the we have two real number values for each word (i.e. vector):</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; semnum = semantic_numbers = {'car': [5, -20], 'vehicle': [2, -18], 'apple': [232, 1010], 'orange': [300, 250], 'fruit': [211, 250], 'samsung': [1080, 1002], 'iphone': [1200, 1100]}
</code></pre>
<p>To compute the difference, we could have done:</p>
<pre><code>&gt;&gt;&gt; np.array(semnum['apple']) - np.array(semnum['orange'])
array([-68, 761])

&gt;&gt;&gt; np.array(semnum['apple']) - np.array(semnum['samsung'])
array([-848,    8])
</code></pre>
<p>That's not very informative, it returns a vector and we can't get a definitive measure of distance between the words, so we can try some vectorial tricks and compute the distance between the vectors, e.g. <a href=""https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy"">euclidean distance</a>:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; orange = np.array(semnum['orange'])
&gt;&gt;&gt; apple = np.array(semnum['apple'])
&gt;&gt;&gt; samsung = np.array(semnum['samsung'])

&gt;&gt;&gt; np.linalg.norm(apple-orange)
763.03604108849277

&gt;&gt;&gt; np.linalg.norm(apple-samsung)
848.03773500947466

&gt;&gt;&gt; np.linalg.norm(orange-samsung)
1083.4685043876448
</code></pre>
<p>Now, we can see more &quot;information&quot; that <code>apple</code> can be closer to <code>samsung</code> than <code>orange</code> to <code>samsung</code>. Possibly that's because <code>apple</code> co-occurs in the corpus more frequently with <code>samsung</code> than <code>orange</code>.</p>
<p>The big question comes, <strong>&quot;How do we get these real numbers to represent the vector of the words?&quot;</strong>. That's where the Word2Vec / embedding training algorithms (<a href=""https://link.springer.com/chapter/10.1007/3-540-33486-6_6"" rel=""nofollow noreferrer"">originally conceived by Bengio 2003</a>) comes in.</p>
<hr />
<h1>Taking a detour</h1>
<p>Since adding more real numbers to the vector representing the words is more informative then why don't we just add a lot more dimensions (i.e. numbers of columns in each word vector)?</p>
<p>Traditionally, we compute the differences between words by computing the word-by-word matrices in the field of <a href=""https://en.wikipedia.org/wiki/Distributional_semantics"" rel=""nofollow noreferrer"">distributional semantics/distributed lexical semantics</a>, but the matrices become really sparse with many zero values if the words don't co-occur with another.</p>
<p>Thus a lot of effort has been put into <a href=""https://en.wikipedia.org/wiki/Dimensionality_reduction"" rel=""nofollow noreferrer"">dimensionality reduction</a> after computing the <a href=""https://stackoverflow.com/questions/24073030/what-are-co-occurance-matrixes-and-how-are-they-used-in-nlp"">word co-occurrence matrix</a>. IMHO, it's like a top-down view of how global relations between words are and then compressing the matrix to get a smaller vector to represent each word.</p>
<p>So the &quot;deep learning&quot; word embedding creation comes from the another school of thought and starts with a randomly (sometimes not-so random) initialized a layer of vectors for each word and learning the parameters/weights for these vectors and optimizing these parameters/weights by minimizing some loss function based on some defined properties.</p>
<p>It sounds a little vague but concretely, if we look at the Word2Vec learning technique, it'll be clearer, see</p>
<ul>
<li><a href=""https://rare-technologies.com/making-sense-of-word2vec/"" rel=""nofollow noreferrer"">https://rare-technologies.com/making-sense-of-word2vec/</a></li>
<li><a href=""http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"" rel=""nofollow noreferrer"">http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</a></li>
<li><a href=""https://arxiv.org/pdf/1402.3722.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722.pdf</a> (more mathematical)</li>
</ul>
<p>Here's more resources to read-up on word embeddings: <a href=""https://github.com/keon/awesome-nlp#word-vectors"" rel=""nofollow noreferrer"">https://github.com/keon/awesome-nlp#word-vectors</a></p>
",9,8,4618,2017-10-13 07:27:55,https://stackoverflow.com/questions/46724680/why-are-word-embedding-actually-vectors
Tensorflow Model is not loading correctly,"<p>I currently try to train a word2vec model for my company. 
For this I made use of the code on <a href=""https://github.com/tensorflow/models"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models</a>, specifically <a href=""https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py</a>.</p>

<p>I downloaded the german wikipedia dump and extracted the text information out of it. The task is to train a model with this data.</p>

<p>I work on a virtual Machine that has Ubuntu 16.04 and access to a Tesla M60. For the weekend I trained the model and saved the checkpoints in a seperate folder. At the end of the weekend the model was able to answer <strong>36%</strong> of the evaluation questions I gave to him (german questions similar to the example ""questions-word.txt""). After training I want to load the model and run the evaluation task again.
For this I changed the code  in following lines (except for path changes): I added</p>

<pre><code>    with tf.Graph().as_default(), tf.Session() as session:
       saver = tf.train.import_meta_graph(opts.save_path + ""/model.ckpt-288720426.meta"")
       saver.restore(session, tf.train.latest_checkpoint('./results'))
       print(""Model restored."")
       with tf.device(""/cpu:0""):
          model = Word2Vec(opts, session)
          model.read_analogies()  # Read analogy questions
       for _ in xrange(opts.epochs_to_train):
          #model.train()  # Process one epoch
          model.eval()  # Eval analogies.
</code></pre>

<p>I added the two lines for loading the model (saver = ...) and commented out the training line. Looking at the meta and latest checkpoint files and tensorboard shows a trained model, but when I run the code, the evaluation results in <strong>0.1%</strong> correct answers, which seems to me like the model restarts with an untrained model. I expected the result to be again <strong>36%</strong>.</p>

<p>Can somebody tell me the error I made in the code, or maybe even in my thinking?</p>
","python, tensorflow, word2vec","<p>You are probably calling
<code>tf.global_variables_initializer().run()</code> in <code>build_graph</code> after you restored the model. So you basically load the weights and then overwrite them with the initialisation values and your network starts from scratch.</p>

<p>I implemented restoring the checkpoints with command line options for a small project working with Latin and you can have a look at the code here:
<a href=""https://github.com/CarstenIsert/LatinLearner/blob/master/word2vec.py"" rel=""nofollow noreferrer"">https://github.com/CarstenIsert/LatinLearner/blob/master/word2vec.py</a></p>
",2,3,1899,2017-10-17 08:24:44,https://stackoverflow.com/questions/46785690/tensorflow-model-is-not-loading-correctly
What are doc2vec training iterations?,"<p>I am new to doc2vec. I was initially trying to understand doc2vec and mentioned below is my code that uses Gensim. As I want I get a trained model and document vectors for the two documents.</p>

<p>However, I would like to know the benefits of retraining the model in several epoches and how to do it in Gensim? Can we do it using <code>iter</code> or <code>alpha</code> parameter or do we have to train it in a seperate <code>for loop</code>? Please let me know how I should change the following code to train the model for 20 epoches.</p>

<p>Also, I am interested in knowing is the multiple training iterations are needed for word2vec model as well.</p>

<pre><code># Import libraries
from gensim.models import doc2vec
from collections import namedtuple

# Load data
doc1 = [""This is a sentence"", ""This is another sentence""]

# Transform data
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(doc1):
    words = text.lower().split()
    tags = [i]
    docs.append(analyzedDocument(words, tags))

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)

# Get the vectors
model.docvecs[0]
model.docvecs[1]
</code></pre>
","python, deep-learning, word2vec, gensim, doc2vec","<p><code>Word2Vec</code> and related algorithms (like 'Paragraph Vectors' aka <code>Doc2Vec</code>) usually make multiple training passes over the text corpus. </p>

<p>Gensim's <code>Word2Vec</code>/<code>Doc2Vec</code> allows the number of passes to be specified by the <code>iter</code> parameter, if you're also supplying the corpus in the object initialization to trigger immediate training. (Your code above does this by supplying <code>docs</code> to the <code>Doc2Vec(docs, ...)</code> constructor call.)</p>

<p>If unspecified, the default <code>iter</code> value used by gensim is 5, to match the default used by Google's original word2vec.c release. So your code above is already using 5 training passes. </p>

<p>Published <code>Doc2Vec</code> work often uses 10-20 passes. If you wanted to do 20 passes instead, you could change your <code>Doc2Vec</code> initialization to:</p>

<pre><code>model = doc2vec.Doc2Vec(docs, iter=20, ...)
</code></pre>

<p>Because <code>Doc2Vec</code> often uses unique identifier tags for each document, more iterations can be more important, so that every doc-vector comes up for training multiple times over the course of the training, as the model gradually improves. On the other hand, because the words in a <code>Word2Vec</code> corpus might appear anywhere throughout the corpus, each words' associated vectors will get multiple adjustments, early and middle and late in the process as the model improves â€“ even with just a single pass. (So with a giant, varied <code>Word2Vec</code> corpus, it's thinkable to use fewer than the default-number of passes.)</p>

<p>You <em>don't</em> need to do your own loop, and most users shouldn't. If you do manage the separate <code>build_vocab()</code> and <code>train()</code> steps yourself, instead of the easier step of supplying the <code>docs</code> corpus in the initializer call to trigger immediate training, then you must supply an <code>epochs</code> argument to <code>train()</code> â€“ and it will perform that number of passes, so you still only need one call to <code>train()</code>. </p>
",7,3,4703,2017-10-18 09:33:27,https://stackoverflow.com/questions/46807010/what-are-doc2vec-training-iterations
How many epochs should Word2Vec be trained? What is a recommended training dataset?,"<p>I am learning about Word2Vec using the TensorFlow tutorial.  The code I am running for Word2Vec is also from the TensorFlow tutorial:  <a href=""https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py</a> .  When I ran the code for 15 epochs, the test accuracy was around 30%.  When I ran for 100 epochs, test accuracy got up to around 39%.  I am using the Text8 dataset for training and questions-words.txt for evaluation.</p>

<p>Do I need to run for more epochs?  Should I be using a different dataset?  How can I improve test accuracy?</p>
","tensorflow, word2vec","<p>Larger datasets are better; <code>text8</code> is very, very small â€“ sufficient for showing some of the analogy-solving power of word-vectors, but not good enough for other purposes. </p>

<p>More iterations may help squeeze slightly stronger vectors out of smaller datasets, but with diminishing returns. (No number of extra iterations over a weak dataset can extract the same rich interrelationships that a larger, more varied corpus can provide.) </p>

<p>There's a related <code>text9</code> from the same source that if I recall correctly, is 10x larger. You'll likely get better evaluation results from using it, than from doing 10x more iterations on <code>text8</code>.</p>

<p>I believe the 3 million pretrained vectors Google once released â€“ the <code>GoogleNews</code> set â€“ were trained on a corpus of 100 billion words' worth of news articles, but with just 3 passes.</p>

<p>Note that there's no single standard for word-vector quality: the <code>questions-words.txt</code> analogy solving is just one convenient evaluation, but it's possible the word-vectors best at that won't be best at your own domain-specific analyses. Similarly, word-vectors trained on one domain of text, like the <code>GoogleNews</code> set from news articles, might underperform compared to text that better matches your domain (like perhaps forum posts, scientific articles, etc. â€“ which all use different words in different ways). </p>

<p>So it's often best to use your own corpus, and your own goal-specific quantitative evaluation, to help adjust corpus/parameter choices. </p>
",7,7,8173,2017-10-20 20:25:31,https://stackoverflow.com/questions/46856838/how-many-epochs-should-word2vec-be-trained-what-is-a-recommended-training-datas
Pre-trained vector of skip-gram and skip-n-gram,"<p>I am doing a project where I need a pre-trained vector of the skip-gram model. I heard that there is also a variant named skip-n-gram model which gives better result.</p>

<p>I am wondering what do I need to train the models myself? Since I just need them to initialize the embedding layer for my model. </p>

<p>I have searched enough but didn't get good examples. I need suggestion from you. Where can I get such pre-trained model or there is no pre-trained model for this.</p>
","word2vec, pre-trained-model","<p>You can train our own word-vectors if you have enough data with you. This can be done using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""noreferrer"">gensim</a>. They provide very simple yet powerful APIs for topic modeling.  </p>

<p>But if you want to use already trained word2vec models, you can use the <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""noreferrer"">word2vec model</a>  released by Google. Itâ€™s 1.5GB and includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset.  </p>

<p>You can load this model with the gensim. Download the trained word2vec model and use following code to get started.   </p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim') 

from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport']

# load the model
model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)

# to extract word vector
print(model[words[0]])  # vector representing access
</code></pre>

<p>Result vector:</p>

<pre><code>[ -8.74023438e-02  -1.86523438e-01 .. ]
</code></pre>

<p>Please note that your system may freeze while loading of such huge model. </p>
",6,3,1581,2017-10-20 20:58:36,https://stackoverflow.com/questions/46857251/pre-trained-vector-of-skip-gram-and-skip-n-gram
Doc2vec and word2vec with negative sampling,"<p>My current doc2vec code is as follows.</p>

<pre><code># Train doc2vec model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>I also have a word2vec code as below.</p>

<pre><code> # Train word2vec model
model = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)
</code></pre>

<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>

<p>In Gensim I found the below mentioned sentence:
<strong>""Produce word vectors with deep learning via word2vecâ€™s â€œskip-gram and CBOW modelsâ€, using either hierarchical softmax or negative sampling""</strong></p>

<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>

<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>

<p>P.s. my application is a recommendation system :)</p>
","python, nlp, word2vec, gensim, doc2vec","<p>Skip-gram or CBOW are different ways to choose the <em>input contexts</em> for the neural-network. Skip-gram picks one nearby word, then supplies it as input to try to predict a target word; CBOW averages together a bunch of nearby words, then supplies that average as input to try to predict a target word.</p>

<p>DBOW is most similar to skip-gram, in that a single paragraph-vector for a whole text is used to predict individual target words, regardless of distance and without any averaging. It can mix well with simultaneous skip-gram training, where in addition to using the single paragraph-vector, individual nearby word-vectors are also used. The gensim option <code>dbow_words=1</code> will add skip-gram training to a DBOW <code>dm=0</code> training.</p>

<p>DM is most similar to CBOW: the paragraph-vector is averaged together with a number of surrounding words to try to predict a target word. </p>

<p>So in Word2Vec, you must choose between skip-gram (<code>sg=1</code>) and CBOW (<code>sg=0</code>) â€“ they can't be mixed. In Doc2Vec, you must choose between DBOW (<code>dm=0</code>) and DM (<code>dm=1</code>) - they can't be mixed. But you can, when doing Doc2Vec DBOW, also add skip-gram word-training (with <code>dbow_words=1</code>). </p>

<p>The choice between hierarchical-softmax and negative-sampling is separate and independent of the above choices. It determines how target-word predictions are read from the neural-network. </p>

<p>With negative-sampling, every possible prediction is assigned a single output-node of the network. In order to improve what prediction a particular input context creates, it checks the output-nodes for the 'correct' word (of the current training example excerpt of the corpus), and for N other 'wrong' words (that don't match the current training example). It then nudges the network's internal weights and the input-vectors to make the 'correct' word output node activation a little stronger, and the N 'wrong' word output node activations a little weaker. (This is called a 'sparse' approach, because it avoids having to calculate <em>every</em> output node, which is very expensive in large vocabularies, instead just calculation N+1 nodes and ignoring the rest.)</p>

<p>You could set negative-sampling with 2 negative-examples with the parameter <code>negative=2</code> (in Word2Vec or Doc2Vec, with any kind of input-context mode). The default mode, if no <code>negative</code> specified, is <code>negative=5</code>, following the default in the original Google word2vec.c code. </p>

<p>With hierarchical-softmax, instead of every preictable word having its own output node, some pattern of multiple output-node activations is interpreted to mean specific words. Which nodes should be closer to 1.0 or 0.0 in order to represent a word is matter of the word's encoding, which is calculated so that common words have short encodings (involving just a few nodes), while rare words will have longer encodings (involving more nodes). Again, this serves to save calculation time: to check if an input-context is driving just the right set of nodes to the right values to predict the 'correct' word (for the current training-example), just a few nodes need to be checked, and nudged, instead of the whole set. </p>

<p>You enable hierarchical-softmax in gensim with the argument <code>hs=1</code>. By default, it is not used. </p>

<p>You should generally disable negative-sampling, by supplying <code>negative=0</code>, if enabling hierarchical-softmax â€“ typically one or the other will perform better for a given amount of CPU-time/RAM. </p>

<p>(However, following the architecture of the original Google word2vec.c code, it is possible but not recommended to have them both active at once, for example <code>negative=5, hs=1</code>. This will result in a larger, slower model, which might appear to perform better since you're giving it more RAM/time to train, but it's likely that giving equivalent RAM/time to just one or the other would be better.)</p>

<p>Hierarchical-softmax tends to get slower with larger vocabularies (because the average number of nodes involved in each training-example grows); negative-sampling does not (because it's always N+1 nodes). Projects with larger corpuses tend to trend towards preferring negative-sampling. </p>
",23,8,5132,2017-10-21 04:58:22,https://stackoverflow.com/questions/46860197/doc2vec-and-word2vec-with-negative-sampling
"How to create a DataFrame with the word2ve vectors as data, and the terms as row labels?","<p>I tried to follow this documentation:
nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb
Where I have the following code snippet:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in food2vec.vocab.iteritems()]

ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)

ordered_terms, term_indices, term_counts = zip(*ordered_vocab)

word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],
                        index=ordered_terms
</code></pre>

<p>To get it to run i have change it to following:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in word2vecda.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab)
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectorsda = pd.DataFrame(word2vecda.wv.syn0norm[term_indices,],index=ordered_terms)
word_vectorsda [:20]
</code></pre>

<p>But the last line before I print the DataFrame give me an error I cannot get my head around. It keeps return that the noneType object cannot be in this line. To me, it looks like it is Term_indices there tracking it, but I do not get why? </p>

<pre><code> TypeError: 'NoneType' object is not subscriptable
</code></pre>

<p>Can any help me with this? Any inputs are most welcome
Best Niels</p>
","python-3.x, pandas, word2vec, gensim","<p>Use the following code:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab, key=lambda k: k[2])
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectors = pd.DataFrame(model.wv.syn0[term_indices, :], index=ordered_terms)
</code></pre>

<p>Replace <code>model</code> with <code>food2vec</code>.<br>
Working on <code>python 3.6.1</code>, <code>gensim '3.0.0'</code></p>
",3,0,3063,2017-10-23 09:00:26,https://stackoverflow.com/questions/46885454/how-to-create-a-dataframe-with-the-word2ve-vectors-as-data-and-the-terms-as-row
"word2vec - what is best? add, concatenate or average word vectors?","<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
","python, word2vec, gensim, word-embedding, language-model","<p>I have found an answer in the Stanford lecture ""Deep Learning for Natural Language Processing"" (Lecture 2, March 2016). It's available <a href=""https://www.youtube.com/watch?v=aRqn8t1hLxs"" rel=""noreferrer"">here</a>. In minute 46 Richard Socher states that the common way is to <strong>average</strong> the two word vectors.</p>
",8,21,20201,2017-10-23 12:44:40,https://stackoverflow.com/questions/46889727/word2vec-what-is-best-add-concatenate-or-average-word-vectors
Do word vectors mean anything on their own?,"<p>From my understanding, word vectors are only ever used in terms of relations to other word vectors. For example, the word vector for ""king"" minus the word vector for ""boy"" should give a vector close to ""queen"". </p>

<p>Given a vector of some unknown word, can assumptions about the word be made based solely on the values of that vector?</p>
","machine-learning, data-science, word2vec, text-analysis, word-embedding","<p>The individual coordinates â€“ such as dimension #7 of a 300-dimensional vector, etc â€“ don't have easily interpretable meanings. </p>

<p>It's primarily the relative <em>distances</em> to other words (neighborhoods), and relative <em>directions</em> with respect to other constellations of words (orientations without regard to the perpendicular coordinate axes, that may be vaguely interpretable, because they correlate with natural-language or natural-thinking semantics. </p>

<p>Further, the pre-training initialization of the model, and much of the training itself, uses randomization. So even on the exact same data, words can wind up in different coordinates on repeated training runs. </p>

<p>The resulting word-vectors should after each run be about as useful <em>with respect to each other</em>, in terms of distances and directions, but neighborhoods like ""words describing seasons"" or ""things that are 'hot'"" could be in very different places in subsequent runs. Only vectors that trained together are comparable.</p>

<p>(There are some constrained variants of word2vec that try to force certain dimensions or directions to be more useful for certain purposes, such as answering questions or detecting hypernym/hyponym relationships â€“ but that requires extra constraints or inputs to the training process. Plain vanilla word2vec won't be as cleanly interpretable.)</p>
",1,-2,228,2017-10-23 20:57:13,https://stackoverflow.com/questions/46898402/do-word-vectors-mean-anything-on-their-own
Gensim Word2Vec uses too much memory,"<p>I want to train a word2vec model on a tokenized file of size 400MB. I have been trying to run this python code :
</p>

<pre><code>import operator
import gensim, logging, os
from gensim.models import Word2Vec
from gensim.models import *

class Sentences(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for line in open(self.filename):
            yield line.split()

def runTraining(input_file,output_file):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    sentences = Sentences(input_file)
    model = gensim.models.Word2Vec(sentences, size=200)
    model.save(output_file)
</code></pre>

<p></p>

<p>When I call this function on my file, I get this :</p>

<pre><code>2017-10-23 17:57:00,211 : INFO : collecting all words and their counts
2017-10-23 17:57:04,071 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
2017-10-23 17:57:16,781 : INFO : Loading a fresh vocabulary
2017-10-23 17:57:18,873 : INFO : min_count=5 retains 290537 unique words (6% of original 4735816, drops 4445279)
2017-10-23 17:57:18,873 : INFO : min_count=5 leaves 42158450 word corpus (89% of original 47054017, drops 4895567)
2017-10-23 17:57:19,563 : INFO : deleting the raw counts dictionary of 4735816 items
2017-10-23 17:57:20,217 : INFO : sample=0.001 downsamples 34 most-common words
2017-10-23 17:57:20,217 : INFO : downsampling leaves estimated 35587188 word corpus (84.4% of prior 42158450)
2017-10-23 17:57:20,218 : INFO : estimated required memory for 290537 words and 200 dimensions: 610127700 bytes
2017-10-23 17:57:21,182 : INFO : resetting layer weights
2017-10-23 17:57:24,493 : INFO : training model with 3 workers on 290537 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-10-23 17:57:28,216 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:32,107 : INFO : PROGRESS: at 20.00% examples, 1314 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:36,071 : INFO : PROGRESS: at 40.00% examples, 1728 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:41,059 : INFO : PROGRESS: at 60.00% examples, 1811 words/s, in_qsize 0, out_qsize 0
Killed
</code></pre>

<p>I know that word2vec needs a lot of space, but I still think there is a problem here. As you see the estimated memory for this model is of 600MB, while my computer has 16GB of RAM. Yet monitoring the process while the code runs shows that it occupies all of my memory and then gets killed.</p>

<p>As other posts advise I have tried to increase min_count and decrease size. But even with ridiculous values (min_count=50, size=10) the process stops at 60%.</p>

<p>I also tried to make python an exception to OOM so that the process doesn't get killed. When I do that, I have a MemoryError instead of the killing.</p>

<p>What is going on ?</p>

<p>(I use a recent laptop with Ubuntu 17.04, 16GB RAM and a Nvidia GTX 960M. I run python 3.6 from Anaconda and gensim 3.0, but it does'nt do better with gensim 2.3)</p>
","python-3.x, memory, word2vec, gensim","<p>Your file is a single line, as indicated by the log output:</p>

<pre><code>2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
</code></pre>

<p>It is doubtful that this is what you want; in particular the optimized cython code in gensim's <code>Word2Vec</code> can only handle sentences of 10,000 words before truncating them (and discarding the rest). So most of your data isn't being considered during training (even if it were to finish). </p>

<p>But the bigger problem is that single 47-million-word line will come into memory as one gigantic string, then be <code>split()</code> into a 47-million-entry list-of-strings. So your attempt to use a memory-efficient iterator isn't helping any â€“ the full file is being brought into memory, twice over, for a single 'iteration'. </p>

<p>I still don't see that using a full 16GB RAM, but perhaps correcting that will resolve the issue, or make whatever remaining issues more evident. </p>

<p>If your tokenized data doesn't have natural line breaks around or below the 10,000-token sentence length, you can look how the example corpus class <code>LineSentence</code>, included in gensim to be able to work on the (also missing line breaks) <code>text8</code> or <code>text9</code> corpuses, limits each yielded sentence to 10,000 tokens:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/58b30d71358964f1fc887477c5dc1881b634094a/gensim/models/word2vec.py#L1620"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/58b30d71358964f1fc887477c5dc1881b634094a/gensim/models/word2vec.py#L1620</a></p>

<p>(It may not be a contributing factor but you may also want to use the <code>with</code> context-manager to ensure your <code>open()</code>ed file is promptly closed after the iterator is exhausted.)</p>
",3,4,4300,2017-10-23 21:48:12,https://stackoverflow.com/questions/46899062/gensim-word2vec-uses-too-much-memory
gensim - Word2vec online training - AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;model_trimmed_post_training,"<p>I am trying to use a pre-trained model and add additional vocabulary to it. I have a csv file with 1 column of sentences in it.</p>

<pre><code>import gensim

existing_model_fr = gensim.models.Word2Vec.load('./fr/fr.bin')

new_sentences = gensim.models.word2vec.LineSentence('./data/french.csv')
existing_model_fr.build_vocab(new_sentences, update=True)

existing_model_fr.train(new_sentences, total_examples=existing_model_fr.corpus_count, epochs=5)
existing_model_fr.save('new_model_fr')
</code></pre>

<p>I get following error on existing_model_fr.train() line. What am I missing?</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last) in ()</p>
  
  <p>/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py in
  train(self, sentences, total_examples, total_words, epochs,
  start_alpha, end_alpha, word_count, queue_factor, report_delay,
  compute_loss) 863 is only called once, the model's cached iter value
  should be supplied as epochs value. 864 """""" --> 865 if
  self.model_trimmed_post_training: 866 raise RuntimeError(""Parameters
  for training were discarded using model_trimmed_post_training method"")
  867 if FAST_VERSION &lt; 0:</p>
  
  <p>AttributeError: 'Word2Vec' object has no attribute
  'model_trimmed_post_training'</p>
</blockquote>
","nlp, word2vec, gensim","<p>It's likely you're loading a model from a earlier version of gensim where the property <code>model_trimmed_post_training</code> wasn't defined. You can likely work around the issue by setting the property yourself, after loading but before <code>train()</code>:</p>

<pre><code>existing_model_fr. model_trimmed_post_training = false
</code></pre>
",1,1,1932,2017-10-28 01:08:17,https://stackoverflow.com/questions/46985320/gensim-word2vec-online-training-attributeerror-word2vec-object-has-no-att
Using Custom Word2Vec to find semantic similarity between technical questions?,"<p>We can get the similarity of two sentences like ""The boy is playing football"" and ""A kid is playing football"" using Google news vectors by applying ""SIF Embeddings"".</p>

<p>I would like to get the similarity for two sentences which are technical like ""what is an abstract class?"" and ""what is a class?"".</p>

<p>I have used Google-news Vectors in getting the similarity but it didn't work well.</p>

<p>I would like to know how training data should be?</p>
","python, nlp, semantics, word2vec","<p>Word2Vec is an algorithm that generates vectors for words, that tend to be similar for similar words. It does not do sentences on its own.</p>

<p>You have more or less the following options:</p>

<ul>
<li>Create a sentence vector</li>
<li>Compare similarity of word vectors within two sentences</li>
</ul>

<h1>Create a sentence vector</h1>

<p>You could build sentence, paragraph or document vectors. There are different approaches to that. You could for example combine the word2vec of of the individual words. If you just want a solution you could go for gensim's doc2vec: <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>

<p>Other than that there are methods like concatenating all the word vectors (of a fixed length).</p>

<p>Similar questions:
<a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>

<h1>Compare similarity of word vectors within two sentences</h1>

<p>One such approach is Movers Distance: <a href=""https://stackoverflow.com/questions/44380199/pairwise-earth-mover-distance-across-all-documents-word2vec-representations"">Pairwise Earth Mover Distance across all documents (word2vec representations)</a></p>

<p>This seems like a good, but expensive approach.</p>

<p><strong>Update</strong>: You've updated your question since to mention that you are using ""SIF Embeddings"" (instead of word2vec): <a href=""https://openreview.net/forum?id=SyK00v5xx"" rel=""nofollow noreferrer"">https://openreview.net/forum?id=SyK00v5xx</a></p>
",1,0,2645,2017-10-31 07:02:04,https://stackoverflow.com/questions/47029595/using-custom-word2vec-to-find-semantic-similarity-between-technical-questions
How to train word2vec model to work better on producing synonym of adjective words?,"<p>I need to train word2vec model to produce synonyms of any user input adjetive words using <strong>unsupervised learning</strong> from some corpus, and ideally, the produced synonyms are also adjective words.</p>

<p>I have removed all punctuations, space, treated all numbers and proper noun as the same term respectively, and done lemmatization when pre-processing the corpus. </p>

<p>I use Skip Gram model(not sure if this is the best soluion for this problem) and generate training batch using <em>generate_batch()</em> function taken from TensorFlow, which generates (center_word, context_word) pairs:</p>

<pre><code>def generate_batch(batch_size, num_skips, skip_window):
  global data_index
  assert batch_size % num_skips == 0
  assert num_skips &lt;= 2 * skip_window
  batch = np.ndarray(shape=(batch_size), dtype=np.int32)
  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
  span = 2 * skip_window + 1  # [ skip_window target skip_window ]
  buffer = collections.deque(maxlen=span)
  if data_index + span &gt; len(data):
    data_index = 0
  buffer.extend(data[data_index:data_index + span])
  data_index += span
  for i in range(batch_size // num_skips):
    target = skip_window  # target label at the center of the buffer
    targets_to_avoid = [skip_window]
    for j in range(num_skips):
      while target in targets_to_avoid:
        target = random.randint(0, span - 1)
      targets_to_avoid.append(target)
      batch[i * num_skips + j] = buffer[skip_window]
      labels[i * num_skips + j, 0] = buffer[target]
    if data_index == len(data):
      buffer[:] = data[:span]
      data_index = span
    else:
      buffer.append(data[data_index])
      data_index += 1
  # Backtrack a little bit to avoid skipping words in the end of a batch
  data_index = (data_index + len(data) - span) % len(data)
  return batch, labels
</code></pre>

<p>Basically, the code for training the model is also almost the same as TensorFlow tutorial sample code. I have run it few times with different batch-size, skip-windwo-size, learning rate etc, but the result is far from being acceptable, most produced synonyms are not even adjective. So my questions are:</p>

<ol>
<li>If I only generate training batch when the center word is adjective, and simply slide the window when it is not, is this approach considered as <strong>unsupervised</strong>?</li>
<li>Is there anything that needs to be redesigned in <em>generate_batch()</em> function? I was told that it is better to redesign this function to work better for this specific case, but I have no idea what can be improved except question 1 approach.</li>
<li>How to produce adjective synonyms? I used to think that a skip-window of size 4-7 would tend to capture semantic meanings(if I did not understand what I have learned wrong), and distinguish adjective from other type of words, but this is not what I am getting.</li>
<li>Regarding the parameters: skip-window-size, batch-size, learning-rate, is there any commonly used values to experiment? </li>
</ol>

<p>Any advice on how to improve would be appreciated!</p>
","python, machine-learning, tensorflow, nlp, word2vec","<p>Our linguistic concept of a 'synonym' is narrower than the similarity reflected in the word-positions found by word2vec-like algorithms. </p>

<p>In particular, what we consider 'antonyms` generally appear as very similar in word-vectors, because the words <em>are</em> very similar in most aspects and the contexts in which they appear â€“ only contrasting in some specific, topic-related way.</p>

<p>As a result, most-similar (nearest-neighbor) word lists tend to include synonyms, but also other related words. </p>

<p>Possible directions for better results would include:</p>

<ul>
<li><p>labeling words with part-of-speech info before training, and then filtering neighbor-lists to only include adjectives</p></li>
<li><p>testing different context-window sizes - often small windows emphasize functional similarities (""can this word be used in the same places?"") and larger windows topical similarities (""are these words used in the same discussions?"")</p></li>
</ul>

<p>(Unverified thought: the best adjectival synonyms might appear near the top of most-similar lists based on small-context-windows, <em>and</em> large-context-windows.)</p>
",1,0,958,2017-10-31 07:50:08,https://stackoverflow.com/questions/47030286/how-to-train-word2vec-model-to-work-better-on-producing-synonym-of-adjective-wor
Optimizing gensim(C compilier and BLAS) in Window 7,"<p>I wants to optimize gensim to run doc2vec in Window7</p>
<p>[1] C compiler</p>
<p>I installed gensim by following this instruction: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/install.html</a></p>
<pre><code>pip install --upgrade gensim
</code></pre>
<p>However, in this page(<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a>), it is saying that C compiler is needed before installing gensim.</p>
<blockquote>
<p>Make sure you have a C compiler before installing gensim, to use optimized (compiled) doc2vec training (70x speedup [blog]).</p>
</blockquote>
<ol>
<li>Should I do something before using pip?</li>
</ol>
<p>[2] BLAS</p>
<p>In the tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a> it is saying that</p>
<blockquote>
<p>Time to Train</p>
<p>If the BLAS library is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time.</p>
</blockquote>
<p>So it seems like I have to install BLAS for optimization,
but I have no idea what BLAS is and there are little and complex BLAS installation guides for window.</p>
<ol start=""2"">
<li>Which BLAS library should I install for running gensim in Window?</li>
<li>If I install BLAS library, will it be automatically linked to python code when I am running gensim doc2vec? or should I do something to link it to doc2vec code?</li>
</ol>
","python-2.7, word2vec, gensim, blas, doc2vec","<p>It's not just BLAS that gensim's optimized code needs, but native-compiled libraries based on Cython code. </p>

<p>If at all possible, this kind of work should be done on UNIX-like systems (Linux/MacOS), because that's where most of the open-source libraries are most developed, tested, and used. So you'll be closer to the system configurations of the primary developers, and larger user community â€“ meaning default installation instructions are more likely to ""just work"", and any problems you run into are more likely to have existing answers in findable places. </p>

<p>But if you're trapped using Windows, the 'conda' distribution of Python generally does a good job of installing optimized versions of the key libraries on Windows, so it can be a good choice. I especially like to start with the '<a href=""https://conda.io/miniconda.html"" rel=""nofollow noreferrer"">miniconda</a>' variant, so that only the exact packages I explicitly need are installed into an environment. </p>

<p>The <a href=""https://conda.io/docs/user-guide/install/index.html"" rel=""nofollow noreferrer"">Miniconda installation instructions</a> and <a href=""https://conda.io/docs/user-guide/getting-started.html"" rel=""nofollow noreferrer"">getting-started-guide</a> are both quite good. Generally once you are in a <code>conda</code> environment you can <code>conda install PACKAGENAME</code> for major foundational packages like <code>numpy</code> or <code>scipy</code>, and still choose to <code>pip install PACKAGENAME</code> for anything that's not in the conda repositories, or not as up-to-date in the conda repositories. (Sometimes it makes sense to get <code>gensim</code> from <code>pip</code> even if otherwise using a <code>conda</code>-based environment.)</p>
",1,0,1436,2017-10-31 14:01:09,https://stackoverflow.com/questions/47037276/optimizing-gensimc-compilier-and-blas-in-window-7
TensorFlow Word2Vec model running on GPU,"<p>In <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">this TensorFlow example</a> a training of skip-gram Word2Vec model described. It contains the following code fragment, which explicitly requires CPU device for computations, i.e. <code>tf.device('/cpu:0')</code>:</p>

<pre><code>batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1  # How many words to consider left and right.
num_skips = 2  # How many times to reuse an input to generate a label.

# We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent. 
valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64  # Number of negative examples to sample.

graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Variables.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)

    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
        tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                   biases=softmax_biases, inputs=embed,
                                   labels=train_labels, num_sampled=num_sampled,
                                   num_classes=vocabulary_size))

    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>

<p>When trying switch to GPU, the following exception is raised:</p>

<blockquote>
  <p><strong>InvalidArgumentError</strong> (see above for traceback): Cannot assign a device for operation 'Variable_2/Adagrad': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.</p>
</blockquote>

<p>I wonder what is the reason why the provided graph cannot be computed on GPU? Does it happen due to <code>tf.int32</code> type? Or should I switch to another optimizer? In other words, is there any way to make possible processing Word2Vec model on GPU? (Without types casting).</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Following Akshay Agrawal recommendation, here is an updated fragment of the original code that achieves required result:</p>

<pre><code>with graph.as_default(), tf.device('/gpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))    
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)

    with tf.device('/cpu:0'):
        loss = tf.reduce_mean(
            tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                       biases=softmax_biases,
                                       inputs=embed,
                                       labels=train_labels,
                                       num_sampled=num_sampled,
                                       num_classes=vocabulary_size))

    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)

    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>
","python, word2vec, tensorflow, word-embedding","<p>The error is raised because <code>AdagradOptimizer</code> does not have a GPU kernel for its sparse apply operation; a sparse apply is triggered because differentiating through the embedding lookup results in a sparse gradient. </p>

<p><code>GradientDescentOptimizer</code> and <code>AdamOptimizer</code> do support sparse apply operations. If you were to switch to one of these optimizers, you would unfortunately see another error: tf.nn.sampled_softmax_loss appears to create an op that does not have a GPU kernel. To get around that, you could wrap the <code>loss = tf.reduce_mean(...</code> line with a <code>with tf.device('/cpu:0'):</code> context, though doing so would introduce cpu-gpu communication overhead.</p>
",2,0,2854,2017-11-03 09:21:23,https://stackoverflow.com/questions/47092185/tensorflow-word2vec-model-running-on-gpu
Difference between Fasttext .vec and .bin file,"<p>I recently downloaded fasttext pretrained model for english. I got two files:</p>

<ol>
<li>wiki.en.vec</li>
<li>wiki.en.bin</li>
</ol>

<p>I am not sure what is the difference between the two files?</p>
","python, nlp, deep-learning, word2vec, fasttext","<p>The <code>.vec</code> files contain only the aggregated word vectors, in plain-text. The <code>.bin</code> files <em>in addition</em> contain the model parameters, and crucially, the vectors for all the n-grams. </p>

<p>So if you want to encode words you did not train with using those n-grams (FastText's famous ""subword information""), you <em>need</em> to find an API that can handle FastText <code>.bin</code> files (most only support the <code>.vec</code> files, however...).</p>
",26,25,14414,2017-11-05 06:03:18,https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file
NLP - Embeddings selection of `start` and `end` of sentence tokens,"<p>Suppose we're training a neural network model to learn the mapping from the following input to output, where the output is <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Name Entity</a> (NE).</p>

<p><strong>Input</strong>: EU rejects German call to boycott British lamb .</p>

<p><strong>Output</strong>: ORG O MISC O O O MISC O O</p>

<p>A sliding window is created to capture the context information and its outcomes are fed into the training model as model_input. The sliding window generates results as following:</p>

<pre><code> [['&lt;s&gt;', '&lt;s&gt;', 'EU', 'rejects', 'German'],\
 ['&lt;s&gt;', 'EU', 'rejects', 'German', 'call'],\
 ['EU', 'rejects', 'German', 'call', 'to'],\
 ['rejects', 'German', 'call', 'to', 'boycott'],\
 ['German', 'call', 'to', 'boycott', 'British'],\
 ['call', 'to', 'boycott', 'British', 'lamb'],\
 ['to', 'boycott', 'British', 'lamb', '.'],\
 ['boycott', 'British', 'lamb', '.', '&lt;/s&gt;'],\
 ['British', 'lamb', '.', '&lt;/s&gt;', '&lt;/s&gt;']]
</code></pre>

<p><code>&lt;s&gt;</code> represents start of sentence token and <code>&lt;/s&gt;</code> represents end of sentence token, and every sliding window corresponds to one NE in output. </p>

<p>To process these tokens, a pre-trained embedding model is used converting words to vectors (e.g., Glove), but those pre-trained models do not include tokens such as <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>. I think random initialization for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> won't be a good idea here, because the scale of such random results might not be consistent with other Glove embeddings.</p>

<p><strong>Question</strong>:
What suggestions of setting up embeddings for <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> and why?</p>
","machine-learning, nlp, deep-learning, word2vec, word-embedding","<p>In general, the answer depends on how you intend to use the embeddings in your task.</p>

<p>I suspect that the use of <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> tokens is dictated by LSTM or other recurrent neural network, that goes after embedding layer. If you were to train word embeddings themselves, I'd suggest you to simply get rid of these tokens, because they don't add any value. Start and stop tokens do matter in LSTM (though not always), but their word embeddings can be arbitrary, small random numbers will do fine, because this vector would be equally far from all ""normal"" vectors.</p>

<p>If you don't want to mess with pre-trained GloVe vectors, I would suggest you to <em>freeze the embedding layer</em>. For example, in tensorflow this can be achieved by <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""noreferrer""><code>tf.stop_gradient</code></a> op right after the embedding lookup. This way the network won't learn any relation between <code>&lt;s&gt;</code> and other words, but it's totally fine, and any existing relations won't change.</p>
",6,6,6659,2017-11-07 00:51:13,https://stackoverflow.com/questions/47148247/nlp-embeddings-selection-of-start-and-end-of-sentence-tokens
Cluster word2vec vectors using Affinity Propagation in python (sklearn),"<p>I want to cluster my word2vec clusters using Affinity Propagation and get the cluster center words.</p>

<p>My current code is as follows.</p>

<pre><code>model = word2vec.Word2Vec.load(""word2vec"")
word_vectors = model.wv.syn0
affprop = AffinityPropagation(affinity=""precomputed"", damping=0.5)
af= affprop.fit(word_vectors)
</code></pre>

<p>However, this raise the following error:
<code>ValueError: S must be a square array (shape=(77, 300))</code></p>

<p>As I understand 300 means word2vec hidden layer dimension and 77 is my vocabulary size.</p>

<p>I am just wondering how to use Affinity Propagation for word2vec vectors that are not square arrays.</p>

<p>Please help me!</p>
","python, scikit-learn, cluster-analysis, text-mining, word2vec","<p>It's because you stated that the affinity between samples is already computed, which is always a square matrix. So you can either use Euclidean distance which is implemented, or if you want to use a different metric you have to precompute it, see the example code below:</p>

<pre><code># some dummy data
word_vectors = np.random.random((77, 300))

# using eucliden distance
affprop = AffinityPropagation(affinity='euclidean', damping=0.5)
af = affprop.fit(word_vectors)

# using cosine
from sklearn.metrics.pairwise import cosine_distances
word_cosine = cosine_distances(word_vectors)
affprop = AffinityPropagation(affinity='precomputed', damping=0.5)
af = affprop.fit(word_cosine)
</code></pre>
",5,4,2256,2017-11-07 08:31:19,https://stackoverflow.com/questions/47153203/cluster-word2vec-vectors-using-affinity-propagation-in-python-sklearn
Given a word vector get the word of it in word2vec,"<p>I obtain word vectors from my code. e.g., </p>

<pre><code>array([ -3.09521449e-04,   2.73033947e-06,   2.15601496e-04, ...,
         5.12349070e-04,   5.04256517e-04,   8.16784304e-05], dtype=float32)
</code></pre>

<p>Now, I want to identify what is the word that represents this word vector in wor2vec genism.</p>

<p>I tried it using the below code. However it did not work.</p>

<pre><code>print(model.wv.index2word(kmeans_clustering.cluster_centers_))
</code></pre>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding, doc2vec","<p>The gensim <code>most_similar()</code> method will take a vector as an argument, as well, but you have to explicitly supply it as one item inside a list of <code>positive</code> examples â€“ so that it's not misunderstood as a something else. </p>

<p>For example:</p>

<pre><code>wv = model.wv.['book']
similars = model.wv.most_similar(positive=[wv,])
</code></pre>

<p>Naturally, 'book' will be at the top of this list of words most-similar to its own vector. </p>
",0,1,2171,2017-11-07 10:21:23,https://stackoverflow.com/questions/47155414/given-a-word-vector-get-the-word-of-it-in-word2vec
Why are multiple model files created in gensim word2vec?,"<p>When I try to create a word2vec model (skipgram with negative sampling) I received 3 files as output as follows.</p>

<pre><code>word2vec (File)
word2vec.syn1nef.npy (NPY file)
word2vec.wv.syn0.npy (NPY file)
</code></pre>

<p>I am just worried why this happens as for my previous test examples in word2vec I only received one model(no npy files).</p>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding","<p>Models with larger internal vector-arrays can't be saved via Python 'pickle' to a single file, so beyond a certain threshold, the gensim <code>save()</code> method will store subsidiary arrays in separate files, using the more-efficient raw format of numpy arrays (<code>.npy</code> format). </p>

<p>You still <code>load()</code> the model by just specifying the root model filename; when the subsidiary arrays are needed, the loading code will find the side files â€“ as long as they're kept beside the root file. So when moving a model elsewhere, be sure to keep all files with the same root filename together.  </p>
",33,19,5267,2017-11-08 07:07:15,https://stackoverflow.com/questions/47173538/why-are-multiple-model-files-created-in-gensim-word2vec
Word2vec loss function explodes,"<p>I'm using the <a href=""https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">basic word2vec</a> script from the tensorflow tutorial. </p>

<p>I am running it on a text file of about 100M and it starts of fine, but the loss function explodes after a while. </p>

<p>I would like to better understand how this could happen while I am using negative sampling. Do you think my dataset is too small or could some internal properties be off?</p>
","tensorflow, word2vec","<p>In my experience this happens when the learning rate is too large. I experimented with different values and if the learning rate is > 1.0 this happens to me with skip-gram and > 0.5 with cbow.</p>
",0,0,182,2017-11-10 05:10:02,https://stackoverflow.com/questions/47216023/word2vec-loss-function-explodes
Word2Vec + LSTM on API Sequence,"<p>I am trying to apply word2Vec and LSTM on a dataset that contains files' API trace log including API function calls and their parameters for a binary classification.  </p>

<p>The data looks like:</p>

<pre><code>File_ID,    Label,   API Trace log
 1,           M,      kernel32 LoadLibraryA kernel32.dll
                      kernel32 GetProcAddress MZ\x90 ExitProcess
                      ...

 2,           V,     kernel32 GetModuleHandleA RPCRT4.dll
                     kernel32 GetCurrentThreadId d\x8B\x0D0 POINTER POINTER
                     ...
</code></pre>

<p>The API trace including: module name, API function name, parameters (that separated by blank space)</p>

<p>Take first API trace of file 1 as example, kernel32 is the module name, LoadLibraryA is function name, kernel32.dll is parameter. Each API trace is separated by \n so that each line represents a API sequence information sequentially.</p>

<p>Firstly I trained a word2vec model based on the line sentence of all API trace log. There are about 5k API function calls, e.g. LoadLibraryA, GetProcAddress. However, because parameter value could be vary, the model becomes quite big (with 300,000 vocabulary) after including those parameters. </p>

<p>After that, I trained a LSTM by applying word2vec's embedding_wrights, the model structure looks like:</p>

<pre><code>model = Sequential() 
model.add(Embedding(output_dim=vocab_dim, input_dim=n_symbols, \
                mask_zero=False, weights=[embedding_weights], \
                trainable=False))
model.add(LSTM(dense_dim,kernel_initializer='he_normal', dropout=0.15, 
recurrent_dropout=0.15, implementation=2))
model.add(Dropout(0.3))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, parallel_check_cb])
</code></pre>

<p>The way I get embedding_weights is to create a matrix, for each vocabulary in word2vec model, map the index of the word in the model, to it's vector</p>

<pre><code>def create_embedding_weights(model, max_index=0):
    # dimensionality of your word vectors
    num_features = len(model[model.vocab.keys()[0]])
    n_symbols = len(model.vocab) + 1  # adding 1 to account for 0th index (for masking)
    # Only word2vec feature set
    embedding_weights = np.zeros((max(n_symbols + 1, max_index + 1), num_features))
    for word, value in model.vocab.items():
        embedding_weights[value.index, :] = model[word]

    return embedding_weights
</code></pre>

<p>For training data, what I did is that for each word in API call, convert the actual word to the index of word2vec model so that it's consistent to the index in embedding_weights above. <code>e.g. kernel32 -&gt; 0,  LoadLibraryA -&gt; 1, kernel32.dll -&gt; 2. GetProcAddress -&gt; 4,  MZ\x90 -&gt; 5, ExitProcess -&gt;6</code> </p>

<p>So the train data for file 1 looks like <code>[0, 1, 2, 3, 4, 5, 6]</code>.  Noted, I didn't do line split for each API trace. As a result, the model may not know where is the start and end of API trace?  And the training accuracy of the model is pretty bad - accuracy is 50% :( </p>

<p><strong>My question is that, when prepare the training and validation dataset, should I also split the line when mapping the actual words to their index</strong>? then The above training data would be changed to following, each API trace is separated by a line, and maybe padd the missing value to -1 which doesn't exist in word2vec's indexes. </p>

<pre><code>[[0, 1, 2, -1]
 [3, 4, 5, 6]]
</code></pre>

<p>Meanwhile I am using a very simple structure for training, while word2vec model is quite big, any suggestion on structure would also be appreciated. </p>
","tensorflow, deep-learning, keras, lstm, word2vec","<p>I would at least split the trace lines in three:</p>

<ul>
<li>Module (make a dictionary and an embedding)  </li>
<li>Function (make a dictionary and an embedding)    </li>
<li>Parameters (make a dictionary and an embedding - see details later)   </li>
</ul>

<p>Since this is a very specific application, I believe it would be best to keep the embeddings trainable (the whole point of the embeddings is to create meaningful vectors, and the meanings depend a lot on the model that is going to use them. Question: how did you create the word2vec model? From what data does it learn?).    </p>

<p>This model would have more inputs. All of them as integers from zero to max dictionary index. Consider using <code>mask_zero=True</code> and padding all files to <code>maxFileLines</code>. </p>

<pre><code>moduleInput = Input(maxFileLines,) 
functionInput = Input(maxFileLines,)    
</code></pre>

<p>For the parameters, I'd probably make a subsequence as if the list of parameters were a sentence. (Again, mask_zero=True, and pad up to <code>maxNumberOfParameters</code>)   </p>

<pre><code>parametersInput = Input(maxFileLines, maxNumberOfParameters)
</code></pre>

<p>Function and module embeddings:</p>

<pre><code>moduleEmb = Embedding(.....mask_zero=True,)(moduleInput)    
functionEmb = Embedding(.....mask_zero=True)(functionInput)
</code></pre>

<p>Now, for the parameters, I though of creating a sequence of sequences (maybe this is too much). For that, I first transfer the lines dimension to the batch dimension and work with only length = maxNumberOfParameters:</p>

<pre><code>paramEmb = Lambda(lambda x: K.reshape(x,(-1,maxNumberOfParameters)))(parametersInput)
paramEmb = Embedding(....,mask_zero=True)(paramEmb)
paramEmb = Lambda(lambda x: K.reshape(x,(-1,maxFileLines,embeddingSize)))(paramEmb)
</code></pre>

<p>Now we concatenate all of them in the last dimension and we're ready to get into the LSTMs:</p>

<pre><code>joinedEmbeddings = Concatenate()([moduleEmb,functoinEmb,paramEmb])
out = LSTM(...)(joinedEmbeddings)
out = ......

model = Model([moduleInput,functionInput,parametersInput], out)
</code></pre>

<h2>How to prepare the inputs</h2>

<p>With this model, you need three separate inputs. One for the module, one for the function and one for the parameters. </p>

<p>These inputs will contain only indices (no vectors). And they don't need a previous word2vec model. Embeddings are word2vec transformers. </p>

<p>So, get the file lines and split. First we split by commas, then we split the API calls by spaces:</p>

<pre><code>import numpy as np

#read the file
loadedFile = open(fileName,'r')
allLines = [l.strip() for l in loadedFile.readlines()] 
loadedFile.close()

#split by commas
splitLines = []
for l in allLines[1:]: #use 1 here only if you have headers in the file
    splitLines.append (l.split(','))
splitLines = np.array(splitLines)

#get the split values and separate ids, targets and calls
ids = splitLines[:,0]
targets = splitLines[:,1]
calls = splitLines[:,2]

#split the calls by space, adding dummy parameters (spaces) to the max length
splitCalls = []
for c in calls:
    splitC = c.strip().split(' ')

    #pad the parameters (space for dummy params)
    for i in range(len(splitC),maxParams+2):
        splitC.append(' ') 

    splitCalls.append(splitC)

splitCalls = np.array(splitCalls)

modules = splitCalls[:,0]
functions = splitCalls[:,1]
parameters = splitCalls[:,2:] #notice the parameters have an extra dimension
</code></pre>

<p>Now lets make the indices:</p>

<pre><code>modIndices, modCounts = np.unique(modules,return_counts=True)
funcIndices, funcCounts = np.unique(functions,return_counts=True)

#for de parameters, let's flatten the array first (because we have 2 dimensions)
flatParams = parameters.reshape((parameters.shape[0]*parameters.shape[1],))
paramIndices, paramCounts = np.unique(flatParams,return_counts=True)
</code></pre>

<p>These will create a list of unique words and get their counts. Here you can customize which words you're going to group in ""another word"" class. (Maybe based on the counts, if the count is too little, make it an ""another word"").   </p>

<p>Let's then make the dictionaries:</p>

<pre><code>def createDic(uniqueWords):
    dic = {}
    for i,word in enumerate(uniqueWords):
         dic[word] = i + 1 # +1 because we want to reserve the zeros for padding     
    return dic
</code></pre>

<p>Just take care with the parameters, because we used a dummy space there:</p>

<pre><code>moduleDic = createDic(modIndices)
funcDic = createDic(funcIndices)
paramDic = createDic(paramIndices[1:]) #make sure the space got the first position here    
paramDic[' '] = 0
</code></pre>

<p>Well, now we just replace the original values:</p>

<pre><code>moduleData = [moduleDic[word] for word in modules]
funcData = [funcDic[word] for word in functions]
paramData = [[paramDic[word] for word in paramLine] for paramLine in parameters]
</code></pre>

<p>Pad them:</p>

<pre><code>for i in range(len(moduleData),maxFileLines):
    moduleData.append(0)
    funcData.append(0)
    paramData.append([0] * maxParams)
</code></pre>

<p>Do this for every file, and store in a list of files:</p>

<pre><code>moduleTrainData = []  
functionTrainData = []
paramTrainData = []
for each file do the above and:
    moduleTrainData.append(moduleData)
    functionTrainData.append(funcData)
    paramTrainData.append(paramData)

moduleTrainData = np.asarray(moduleTrainData)
functionTrainData = np.asarray(functionTrainData)
paramTrainData = np.asarray(paramTrainData)
</code></pre>

<p>That's all for the inputs. </p>

<pre><code>model.fit([moduleTrainData,functionTrainData,paramTrainData],outputLabels,...)
</code></pre>
",2,3,1048,2017-11-12 06:23:39,https://stackoverflow.com/questions/47246104/word2vec-lstm-on-api-sequence
Understanding input and labels in word2vec (TensorFlow),"<p>I am trying to properly understand the <code>batch_input</code> and <code>batch_labels</code> from the tensorflow <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">""Vector Representations of Words""</a> tutorial. </p>

<p>For instance, my data </p>

<pre><code> 1 1 1 1 1 1 1 1 5 251 371 371 1685 ...
</code></pre>

<p>... starts with</p>

<pre><code>skip_window = 2 # How many words to consider left and right.
num_skips = 1 # How many times to reuse an input to generate a label.
</code></pre>

<p>Then the generated input array is:</p>

<pre><code>bach_input = 1 1 1 1 1 1 5 251 371 ....  
</code></pre>

<p>This makes sense, starts from after 2 (= window size) and then continuous. The labels:</p>

<pre><code>batch_labels = 1 1 1 1 1 1 251 1 1685 371 589 ...
</code></pre>

<p>I don't understand these labels very well. There are supposed to be 4 labels for each input right (window size 2, on each side). But the <code>batch_label</code> variable is the same length. </p>

<p>From the tensorflow tutorial: </p>

<blockquote>
  <p>The skip-gram model takes two inputs. One is a batch full of integers
  representing the source context words, the other is for the target
  words.</p>
</blockquote>

<p>As per the tutorial, I have declared the two variables as: </p>

<pre><code>  batch = np.ndarray(shape=(batch_size), dtype=np.int32)
  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
</code></pre>

<p>How should I interpret the <code>batch_labels</code>?</p>
","arrays, machine-learning, tensorflow, nlp, word2vec","<blockquote>
  <p>There are supposed to be 4 labels for each input right (window size 2, on each side). But the batch_label variable is the same length.</p>
</blockquote>

<p>The key setting is <strong><code>num_skips = 1</code></strong>. This value defines the number of <code>(input, label)</code> tuples each word generates. See the examples with different <code>num_skips</code> below (my <code>data</code> sequence seems to be different from yours, sorry about that).</p>



<h2>Example #1 - <code>num_skips=4</code></h2>

<pre class=""lang-py prettyprint-override""><code>batch, labels = generate_batch(batch_size=8, num_skips=4, skip_window=2)
</code></pre>

<p>It generates 4 labels for each word, i.e. uses the whole context; since <code>batch_size=8</code> only 2 words are processed in this batch (<em>12</em> and <em>6</em>), the rest will go into the next batch:</p>

<pre class=""lang-py prettyprint-override""><code>data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]
batch = [12 12 12 12  6  6  6  6]
labels = [[6 3084 5239 195 195 3084 12 2]]
</code></pre>

<h2>Example #2 - <code>num_skips=2</code></h2>

<pre class=""lang-py prettyprint-override""><code>batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)
</code></pre>

<p>Here you would expect each word appear twice in the <code>batch</code> sequence; the 2 labels are randomly sampled from 4 possible words:</p>

<pre class=""lang-py prettyprint-override""><code>data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]
batch = [ 12  12   6   6 195 195   2   2]
labels = [[ 195 3084   12  195 3137   12   46  195]]
</code></pre>

<h2>Example #3 - <code>num_skips=1</code></h2>

<pre class=""lang-py prettyprint-override""><code>batch, labels = generate_batch(batch_size=8, num_skips=1, skip_window=2)
</code></pre>

<p>Finally, this setting, same as yours, produces exactly one label per each word; each label is drawn randomly from the 4-word context:</p>

<pre class=""lang-py prettyprint-override""><code>data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]
batch = [  12    6  195    2 3137   46   59  156]
labels = [[  6  12  12 195  59 156  46  46]]
</code></pre>

<blockquote>
  <p>How should I interpret the batch_labels?</p>
</blockquote>

<p>Each label is the center word to be predicted from the context. But the generated data may take <em>not all</em> <code>(context, center)</code> tuples, depending on the settings of the generator.</p>

<p>Also note that the <code>train_labels</code> tensor is 1-dimensional. Skip-Gram trains the model to predict <em>any</em> context word from the given center word, not all 4 context words <em>at once</em>. This explains why all training pairs <code>(12, 6)</code>, <code>(12, 3084)</code>, <code>(12, 5239)</code> and <code>(12, 195)</code> are valid.</p>
",3,3,1000,2017-11-15 08:48:23,https://stackoverflow.com/questions/47302947/understanding-input-and-labels-in-word2vec-tensorflow
What is considered a good accuracy for trained Word2Vec on an analogy test?,"<p>After training Word2Vec, how high should the accuracy be during testing on analogies? What level of accuracy should be expected if it is trained well?</p>
",word2vec,"<p>The analogy test is just a interesting automated way to evaluate models, or compare algorithms. </p>

<p>It might not be the best indicator of how well word-vectors will work for your own project-specific goals. (That is, a model which does <em>better</em> on word-analogies might be worse for whatever other info-retrieval, or classification, or other goal you're really pursuing.) So if at all possible, create an automated evaluation that's tuned to your own needs. </p>

<p>Note that the absolute analogy scores can also be quite sensitive to how you trim the vocabulary before training, or how you treat analogy-questions with out-of-vocabulary words, or whether you trim results at the end to just higher-frequency words. Certain choices for each of these may boost the supposed ""correctness"" of the simple analogy questions, but not improve the overall model for more realistic applications. </p>

<p>So there's no absolute accuracy rate on these simplistic questions that should be the target. Only relative rates are somewhat indicative - helping to show when more data, or tweaked training parameters, seem to improve the vectors. But even vectors with small apparent accuracies on generic analogies might be useful elsewhere. </p>

<p>All that said, you can review a demo notebook like the gensim ""Comparison of FastText and Word2Vec"" to see what sorts of accuracies on the Google word2vec.c `questions-words.txt' analogy set (40-60%) are achieved under some simple defaults and relatively small training sets (100MB-1GB). </p>
",0,1,514,2017-11-23 00:47:40,https://stackoverflow.com/questions/47445982/what-is-considered-a-good-accuracy-for-trained-word2vec-on-an-analogy-test
Creating a wordvector model combining words from other models,"<p>I have two different word vector models created using word2vec algorithm . Now issue i am facing is few words from first model is not there in second model . I want to create a third model from two different word vectors models where i can use word vectors from both models without loosing meaning and the context of word vectors. </p>

<p>Can I do this, and if so, how?</p>
","machine-learning, nlp, word2vec, gensim","<p>You could potentially translate the vectors for the words only in one model to the other model's coordinate space, using other shared words to learn a translation-function. </p>

<p>There's a facility to do this in recent gensim versions â€“ see the <a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">TranslationMatrix</a> tool. There's a demo Jupyter notebook included in the <code>docs/notebooks</code> directory, viewable online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb</a></p>

<p>You'd presumably take the <em>larger</em> model (or whichever one is thought to be <em>better</em>, perhaps because it was trained on more data), and translate the smaller number of words its missing into its space. You'd use as many common-reference 'anchor' words as is practical.</p>
",0,2,99,2017-11-27 09:01:16,https://stackoverflow.com/questions/47507091/creating-a-wordvector-model-combining-words-from-other-models
Gensim word2vec / doc2vec multi-threading parallel queries,"<p>I would like to call <code>model.wv.most_similar_cosmul</code>, on the same copy of <code>model</code> object, using <code>multiple cores</code>, on <code>batches of input pairs</code>.</p>

<p>The <code>multiprocessing</code> module requires multiple copies of <code>model</code>, which will require too much RAM because my <code>model</code> is 30+ GB in RAM.</p>

<p>I have tried to evaluate my query pairs. It took me ~12 hours for the first round. There may be more rounds coming. That's why I am looking for a threading solution. I understand Python has <code>Global Interpreter Lock</code> issue.</p>

<p>Any suggestions?</p>
","python, multithreading, word2vec, gensim, doc2vec","<p>Forking off processes using <code>multiprocessing</code> <em>after</em> your text-vector model is in memory and unchanging <em>might</em> work to let many processes share the same object-in-memory. </p>

<p>In particular, you'd want to be sure that the automatic generation of unit-normed vectors (into a <code>syn0norm</code> or <code>doctag_syn0norm</code>) has already happened. It'll be automatically triggered the first time it's needed by a <code>most_similar()</code> call, or you can force it with the <code>init_sims()</code> method on the relevant object. If you'll <em>only</em> be doing most-similar queries between unit-normed vectors, never needing the original raw vectors, use <code>init_sims(replace=True)</code> to clobber the raw mixed-magnitude <code>syn0</code> vectors in-place and thus save a lot of addressable memory.</p>

<p>Gensim also has options to use memory-mapped files as the sources of model giant arrays, and when multiple processes use the same read-only memory-mapped file, the OS will be smart enough to only map that file into physical memory once, providing both processes pointers to the shared array. </p>

<p>For more discussion of the tricky parts of using this technique in a similar-but-not-identical use case, see my answer at:</p>

<p><a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",1,1,3447,2017-11-28 14:18:29,https://stackoverflow.com/questions/47533772/gensim-word2vec-doc2vec-multi-threading-parallel-queries
Does word2vec realization from gensim go beyond sentence level when examining context?,"<p>I found <a href=""https://stackoverflow.com/questions/36790867"">this question</a> which provides evidence that sentence order probably matters (but effect can be also a result of different random initialization).</p>

<p>I want to process <a href=""https://files.pushshift.io/reddit/comments/"" rel=""nofollow noreferrer"">Reddit comment dumps</a> for my project, but the strings extracted from json would be unsorted and belong to very different subreddits and topics, so I don't want to mess up contexts:</p>

<pre><code>{""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}
</code></pre>

<p>So does the neighbor sentences matter for gensim word2vec and should I recover whole comment tree structure, or I can simply extract ""bag of sentences"" and train the model on it?</p>
","word2vec, gensim, word-embedding","<p>The corpus expected by gensim Word2Vec is an <em>iterable</em> of <em>lists-of-tokens</em>. (For example, a list of lists-of-tokens would work, but for larger corpuses you'd usually want to provide a restartable iterable that streams text examples from persistent storage, to avoid holding the whole corpus in memory.) </p>

<p>The word-vector training only considrs context <em>within individual text examples</em>. That is, within one list-of-tokens. So if two consecutive examples are...</p>

<pre><code>['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']
['Everybody', 'needs', 'a', 'thneed']
</code></pre>

<p>...there's no influence, in these examples between 'ham' and 'Everybody'. (The contexts are only within each example.)</p>

<p>Still, there could be subtle effects on quality if the ordering of examples clumps together all words or topics of a certain type. For example, you'd not want all examples of word X to happen in the beginning of the corpus, and all examples of word Y to happen late â€“ that prevents the sort of interleaved variety-of-examples that achieves the best results.</p>

<p>So if your corpus comes in any kind of sorted order, clumped by topic or author or size or language, it is often beneficial to perform an initial shuffle to remove such clumping. (Re-shuffling any more, such as between training passes, usually has negligible additional benefit.)</p>
",1,0,175,2017-12-01 17:19:25,https://stackoverflow.com/questions/47598369/does-word2vec-realization-from-gensim-go-beyond-sentence-level-when-examining-co
Using word2vec to classify words in categories,"<p><strong>BACKGROUND</strong></p>

<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>

<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'
['yellow', 'red','green'] -&gt; 'Colors'
['tokyo','bejing','washington','mumbai'] -&gt; 'Places'
</code></pre>

<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is ""purple"" then I should be able to predict 'Colors' as the correct category. If the new input is ""Calgary"" it should predict 'Places' as the correct category.</p>

<p><strong>APPROACH</strong></p>

<p>I did some research and came across <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Word2vec</a>. This library has a ""similarity"" and ""mostsimilarity"" function which i can use. So one brute force approach I thought of is the following:</p>

<ol>
<li>Take new input.</li>
<li>Calculate it's similarity with each word in each vector and take an average.</li>
</ol>

<p>So for instance for input ""pink"" I can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>

<p><strong>ISSUE</strong></p>

<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>
","python, machine-learning, nlp, word2vec, gensim","<p>If you're looking for the simplest / fastest solution then I'd suggest you take the pre-trained word embeddings (Word2Vec or GloVe) and just build a simple query system on top of it. The vectors have been trained on a huge corpus and are likely to contain good enough approximation to your domain data.</p>

<p>Here's my solution below:</p>

<pre><code>import numpy as np

# Category -&gt; words
data = {
  'Names': ['john','jay','dan','nathan','bob'],
  'Colors': ['yellow', 'red','green'],
  'Places': ['tokyo','bejing','washington','mumbai'],
}
# Words -&gt; category
categories = {word: key for key, words in data.items() for word in words}

# Load the whole embedding matrix
embeddings_index = {}
with open('glove.6B.100d.txt') as f:
  for line in f:
    values = line.split()
    word = values[0]
    embed = np.array(values[1:], dtype=np.float32)
    embeddings_index[word] = embed
print('Loaded %s word vectors.' % len(embeddings_index))
# Embeddings for available words
data_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}

# Processing the query
def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return scores

# Testing
print(process('pink'))
print(process('frank'))
print(process('moscow'))
</code></pre>

<p>In order to run it, you'll have to download and unpack the pre-trained GloVe data from <a href=""http://nlp.stanford.edu/data/glove.6B.zip"" rel=""noreferrer"">here</a> (careful, 800Mb!). Upon running, it should produce something like this:</p>

<pre><code>{'Colors': 24.655489603678387, 'Names': 5.058711671829224, 'Places': 0.90213905274868011}
{'Colors': 6.8597321510314941, 'Names': 15.570847320556641, 'Places': 3.5302454829216003}
{'Colors': 8.2919375101725254, 'Names': 4.58830726146698, 'Places': 14.7840416431427}
</code></pre>

<p>... which looks pretty reasonable. And that's it! If you don't need such a big model, you can filter the words in <code>glove</code> according to their <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer"">tf-idf</a> score. Remember that the model size only depends on the data you have and words you might want to be able to query.</p>
",32,19,14447,2017-12-06 04:16:35,https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories
FastText using pre-trained word vector for text classification,"<p>I am working on a text classification problem, that is, given some text, I need to assign to it certain given labels.</p>

<p>I have tried using fast-text library by Facebook, which has two utilities of interest to me:</p>

<p>A) Word Vectors with pre-trained models</p>

<p>B) Text Classification utilities</p>

<p>However, it seems that these are completely independent tools as I have been unable to find any tutorials that merge these two utilities.</p>

<p>What I want is to be able to classify some text, by taking advantage of the pre-trained models of the Word-Vectors. Is there any way to do this?</p>
","nlp, word2vec, text-classification, fasttext","<p>FastText's native classification mode depends on you training the word-vectors yourself, using texts with known classes. The word-vectors thus become optimized to be useful for the specific classifications observed during training. So that mode typically <em>wouldn't</em> be used with pre-trained vectors. </p>

<p>If using pre-trained word-vectors, you'd then somehow compose those into a text-vector yourself (for example, by averaging all the words of a text together), then training a separate classifier (such as one of the many options from scikit-learn) using those features. </p>
",4,13,14094,2017-12-07 10:28:32,https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification
What does a weighted word embedding mean?,"<p>In the <a href=""http://www.aclweb.org/anthology/S17-2100"" rel=""noreferrer"">paper</a> that I am trying to implement, it says,</p>

<blockquote>
  <p>In this work, tweets were modeled using three types of text
  representation. The first one is a bag-of-words model weighted by
  tf-idf (term frequency
  - inverse document frequency) (Section
  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a
  sentence by averaging the weighted word embeddings of all words, the
  weight of a word is given by tf-idf (Section
  2.1.2).</p>
</blockquote>

<p>I am not sure about the <em>third representation</em> which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. </p>
","machine-learning, nlp, word2vec, tf-idf, word-embedding","<p>Averaging (possibly weighted) of word embeddings makes sense, though depending on the main algorithm and the training data this sentence representation may not be the best. The intuition is the following: </p>

<ul>
<li>You might want to handle sentences of different length, hence the averaging (better than plain sum).</li>
<li>Some words in a sentence are usually much more valuable than others. TF-IDF is the simplest measure of the word value. Note that the scale of the result doesn't change.</li>
</ul>

<p>See also <a href=""http://aclweb.org/anthology/P/P16/P16-1089.pdf"" rel=""noreferrer"">this paper by Kenter et al</a>. There is a <a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""noreferrer"">nice post</a> that performs the comparison of these two approaches in different algorithms, and concludes that none is significantly better than the other: some algorithms favor simple averaging, some algorithms perform better with TF-IDF weighting.</p>
",22,18,15211,2017-12-09 09:16:02,https://stackoverflow.com/questions/47727078/what-does-a-weighted-word-embedding-mean
How&#39;s it even possible to use softmax for word2vec?,"<p>How is it possible to use softmax for word2vec? I mean softmax outputs probabilities of all classes which sum up to <code>1</code>, e.g. <code>[0, 0.1, 0.8, 0.1]</code>. But if my label is, for example <code>[0, 1, 0, 1, 0]</code> (multiple correct classes), then it is impossible for softmax to output the correct value?</p>

<p>Should I use softmax instead? Or am I missing something?</p>
","neural-network, nlp, classification, word2vec, softmax","<p>I suppose you're talking about Skip-Gram model (i.e., predict the context word by the center), because CBOW model predicts the single center word, so it assumes exactly one correct class.</p>

<p>Strictly speaking, if you were to train word2vec using SG model and ordinary softmax loss, the correct label would be <code>[0, 0.5, 0, 0.5, 0]</code>. Or, alternatively, you can feed several examples per center word, with labels <code>[0, 1, 0, 0, 0]</code> and <code>[0, 0, 0, 1, 0]</code>. It's hard to say, which one performs better, but the label must be a valid probability distribution per input example.</p>

<p>In practice, however, ordinary softmax is rarely used, because there are too many classes and strict distribution is too expensive and simply not needed (almost all probabilities are nearly zero all the time). Instead, the researchers use <em>sampled</em> loss functions for training, which approximate softmax loss, but are much more efficient. The following loss functions are particularly popular:</p>

<ul>
<li><a href=""http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"" rel=""nofollow noreferrer"">Negative Sampling</a></li>
<li><a href=""https://datascience.stackexchange.com/q/13216/18375"">Noise-Contrastive Estimation</a></li>
</ul>

<p>These losses are more complicated than softmax, but if you're using tensorflow, <a href=""https://stackoverflow.com/q/47034888/712995"">all of them are implemented</a> and can be used just as easily.</p>
",3,2,1812,2017-12-14 01:08:25,https://stackoverflow.com/questions/47804462/hows-it-even-possible-to-use-softmax-for-word2vec
word2Vec vector representation for text classification algorithm,"<p>I am trying to use word2vec in text classification algorithm.
I want t create vectorizer using word2vec, I have used below script. But I am not able to get one row for each document instead I am getting matrix of different dimension for every document. 
For example for 1st document matrix of 31X100,  2nd 163X100 and 3rd 73X100 and so on.
Actually I need dimension of every document as 1X100 , so that i can use these as input feature for training model</p>

<p>Can anyone help me here.</p>

<pre><code>import os
import pandas as pd       
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # Import the stop word list
import gensim
import numpy as np

train = pd.read_csv(""Data.csv"",encoding='cp1252')
wordnet_lemmatizer = WordNetLemmatizer()

def Description_to_words(raw_Description):
    Description_text = BeautifulSoup(raw_Description).get_text() 
    letters_only = re.sub(""[^a-zA-Z]"", "" "", Description_text)
    words = word_tokenize(letters_only.lower())    
    stops = set(stopwords.words(""english"")) 
    meaningful_words = [w for w in words if not w in stops]
    return( "" "".join(wordnet_lemmatizer.lemmatize(w) for w in meaningful_words))

num_Descriptions = train[""Summary""].size
clean_train_Descriptions = []
print(""Cleaning and parsing the training set ticket Descriptions...\n"")
clean_train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    clean_train_Descriptions.append(Description_to_words( train[""Summary""][i] ))

model = gensim.models.Word2Vec(clean_train_Descriptions, size=100)
w2v = dict(zip(model.wv.index2word, model.wv.syn0))

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        #self.dim = len(word2vec.itervalues().next())
        self.dim = 100

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

a=MeanEmbeddingVectorizer(w2v)
clean_train_Descriptions[1]
a.transform(clean_train_Descriptions[1])

train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    train_Descriptions.append(a.transform("" "".join(clean_train_Descriptions[i])))
</code></pre>
","python, word2vec, word-embedding","<p>You have 2 issues in your code causing problems, both easily solved.</p>

<p>First, Word2Vec requires sentences to be actually a list of words, rather than an actual sentence as a single string. So from your <code>Description_to_words</code> just return the list, don't join.</p>

<p><code>return [wordnet_lemmatizer.lemmatize(w) for w in meaningful_words]</code></p>

<p>Since word2vec iterates over each sentence to get the words, previously it was iterating over a string, and you were actually getting a character level embedding from <code>wv</code>.</p>

<p>Secondly, a similar issue with the way you are calling transform - <code>X</code> is expected to be a list of documents, not an individual document. So when you are doing <code>for words in X</code>, you are actually creating a list of characters, and then iterating over that to create embedding. So your output was actually the individual character embeddings for each character in your sentences. Simply changed, just convert all documents at once!</p>

<p><code>train_Descriptions = a.transform(clean_train_Descriptions)</code></p>

<p>(to do one at a time, wrap in a list (<code>[clean_train_Descriptions[1]]</code>), or select 1 using the range selector( <code>clean_train_Descriptions[1:2]</code>).</p>

<p>With those two changes you should get 1 row back per input sentence.</p>
",2,2,2755,2017-12-22 06:02:29,https://stackoverflow.com/questions/47936578/word2vec-vector-representation-for-text-classification-algorithm
Sentiment Analysis Code (word2vec) not properly working in my python version (vocabulary not built),"<p>I have taken a code online to do sentiment analysis on twitter database. I tried running it and it gave me at the beginning error for printing, which I figured out that the newer version of python has changed its way to do print. I am getting error that shows my data is not filled in the array, if anyone has worked with python and has eagle eye to see where I am going wrong please help. </p>

<pre><code>    import numpy as np 
    from copy import deepcopy
    from string import punctuation
    from random import shuffle
    import chardet
    from sklearn.manifold import TSNE
    from sklearn.preprocessing import scale


    import bokeh.plotting as bp
    from bokeh.models import HoverTool, BoxSelectTool
    from bokeh.plotting import figure, show, output_notebook

    import gensim
    from gensim.models.word2vec import Word2Vec 
    LabeledSentence = gensim.models.doc2vec.LabeledSentence 

    import pandas as pd 
    pd.options.mode.chained_assignment = None

    from tqdm import tqdm
    tqdm.pandas(desc=""progress-bar"")

    from nltk.tokenize import TweetTokenizer 
    tokenizer = TweetTokenizer()

    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer

    def ingest(filename):
        with open(filename, 'rb') as f:
            result = chardet.detect(f.read())
        data = pd.read_csv(filename, encoding=result['encoding'])
        data.drop(['ItemID', 'Date', 'Blank', 'SentimentSource'], axis=1, inplace=True)
        data = data[data.Sentiment.isnull() == False]
        data['Sentiment'] = data['Sentiment'].map({4:1, 0:0})
        data = data[data['SentimentText'].isnull() == False]
        data.reset_index(inplace=True)
        data.drop('index', axis=1, inplace=True)
        print('dataset loaded with shape {}', format(data.shape)) 

        return data

    def tokenize(tweet):
        try:
            tweet = unicode(tweet.decode('utf-8').lower())
            tokens = tokenizer.tokenize(tweet)
            tokens = filter(lambda t: not t.startswith('@'), tokens)
            tokens = filter(lambda t: not t.startswith('#'), tokens)
            tokens = filter(lambda t: not t.startswith('http'), tokens)
            return tokens
        except:
            return 'NC'

    def postprocess(data, n=100):
        data = data.head(n)
        data['tokens'] = data['SentimentText'].progress_map(tokenize)  
        data = data[data.tokens != 'NC']
        data.reset_index(inplace=True)
        data.drop('index', inplace=True, axis=1)
        return data


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in  enumerate(tweets):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
            print("":::::::::::::::::::::::::"")
        return labelized


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in tqdm(enumerate(tweets)):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
        return labelized


    def buildWordVector(tokens, size):
        vec = np.zeros(size).reshape((1, size))
        count = 0.
        for word in tokens:
            try:
                vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]
                count += 1.
            except KeyError: 

                continue
        if count != 0:
            vec /= count
        return vec



    if __name__ == '__main__':

        filename = './training.csv'

        #n = 1000000
        n = 100
        n_dim = 200

        data = ingest(filename)
        #data = data.head(5)
        data = postprocess(data, n)

        x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens), np.array(data.head(n).Sentiment), test_size=0.2)


        print(""training length X"", len(x_train))

        print(""training length Y"", len(y_train))


        x_train = labelizeTweets(x_train, 'TRAIN')
        x_test = labelizeTweets(x_test, 'TEST')

        print(""jljkjkjlkjlj"", len(x_train))

        tweet_w2v = Word2Vec(size=n_dim, min_count=10)
        #tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])
        tweet_w2v.build_vocab([x.words for x in x_train])

        #tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
        tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)




        print(tweet_w2v.most_similar('good'))

        if True:
            print('building tf-idf matrix ...')
            vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
            matrix = vectorizer.fit_transform([x.words for x in x_train])
            tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
            print('vocab size :', len(tfidf))

            train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])
            train_vecs_w2v = scale(train_vecs_w2v)

            test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])
            test_vecs_w2v = scale(test_vecs_w2v)

            model = Sequential()
            model.add(Dense(32, activation='relu', input_dim=200))
            model.add(Dense(1, activation='sigmoid'))
            model.compile(optimizer='rmsprop',
                                        loss='binary_crossentropy',
                                        metrics=['accuracy'])

            model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=32, verbose=2)

            score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)
            print (score[1])

    output_notebook()
    plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=""A map of 10000 word vectors"",
        tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",
        x_axis_type=None, y_axis_type=None, min_border=1)

    word_vectors = [tweet_w2v[w] for w in tweet_w2v.wv.vocab.keys()[:5000]]

    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
    tsne_w2v = tsne_model.fit_transform(word_vectors)

    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])
    tsne_df['words'] = tweet_w2v.wv.vocab.keys()[:5000]

    plot_tfidf.scatter(x='x', y='y', source=tsne_df)
    hover = plot_tfidf.select(dict(type=HoverTool))
    hover.tooltips={""word"": ""@words""}
    show(plot_tfidf)
</code></pre>

<p>This is the error I am getting </p>

<pre><code>    C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
dataset loaded with shape {} (505, 2)
progress-bar: 100%|##########################################################################| 505/505 [00:00&lt;?, ?it/s]
training length X 0
training length Y 0
0it [00:00, ?it/s]
0it [00:00, ?it/s]
jljkjkjlkjlj 0
Traceback (most recent call last):
  File ""Sentiment_Analysis.py"", line 127, in &lt;module&gt;
    tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\models\word2vec.py"", line 951, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>
","python, twitter, nltk, sentiment-analysis, word2vec","<p>I had the same issue with the same code. There is absolutely no problem with the code on the website, but it returns an empty vocabulary no matter how you order it. </p>

<p>My workaround was that it runs smoothly when you run the same exact code in Python 2.7 instead of 3.x. However, if you do manage to port it successfully to Python 3.x, you have faster data/ memory access rates which is quite desirable.</p>

<p>Edit: Found the problem, now it works with Python 3 too. Edit the corresponding code segment to this and vocabulary should build without any issue.</p>

<pre><code>def tokenize(tweet):
        try:
            tweet = unicode(tweet.decode('utf-8').lower())
            tokens = tokenizer.tokenize(tweet)
            tokens = list(filter(lambda t: not t.startswith('@'), tokens))
            tokens = list(filter(lambda t: not t.startswith('#'), tokens))
            tokens = list(filter(lambda t: not t.startswith('http'), tokens))
            return tokens
        except:
            return 'NC'
</code></pre>
",1,0,621,2017-12-26 09:02:50,https://stackoverflow.com/questions/47976170/sentiment-analysis-code-word2vec-not-properly-working-in-my-python-version-vo
Word embedding for OOV words,"<p>I have generated word vectors from a corpus, but I am facing out of vocabulary issues for many words. How can I generate word vectors for OOV words on the fly using existing word embedding?</p>
","machine-learning, nlp, word2vec, gensim","<p>A very late answer (not even the answer you are looking for) but, with <code>skip-gram</code> models what you ask is almost impossible because each word is a distinct entity in and of itself.</p>

<p>The feature you ask can be done with <a href=""https://github.com/facebookresearch/fastText/"" rel=""nofollow noreferrer"">FastText</a> out of the box. It generates OOV word vectors using it's <code>n-gram</code>s.</p>

<p>Gensim has a high-level <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">API</a> to use FastText.</p>
",2,4,5351,2017-12-28 14:47:23,https://stackoverflow.com/questions/48009532/word-embedding-for-oov-words
How to convert gensim Word2Vec model to FastText model?,"<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
","nlp, word2vec, gensim, word-embedding, fasttext","<p>FastText is able to create vectors for subword fragments by including those fragments in the initial training, from the original corpus. Then, when encountering an out-of-vocabulary ('OOV') word, it constructs a vector for those words using fragments it recognizes. For languages with recurring word-root/prefix/suffix patterns, this results in vectors that are better than random guesses for OOV words. </p>

<p>However, the FastText process does <em>not</em> extract these subword vectors from final full-word vectors. Thus there's no simple way to turn full-word vectors into a FastText model that also includes subword vectors.</p>

<p>There might be workable way to approximate the same effect, for example by taking all known-words with the same subword fragment, and extracting some common average/vector-component to be assigned to the subword. Or modeling OOV words as some average of in-vocabulary words that are a short edit-distance from the OOV word. But these techniques wouldn't quite be FastText, just vaguely analogous to it, and how well they work, or could be made to work with tweaking, would be an experimental question. So, it's not a matter of grabbing an off-the-shelf library.</p>

<p>There are a couple of research papers with other OOV-bootstrapping ideas, mentioned in <a href=""http://ruder.io/word-embeddings-2017/index.html#oovhandling"" rel=""nofollow noreferrer"">this blog post by Sebastien Ruder</a>.</p>

<p>If you need the FastText OOV functionality, the best-grounded approach would be to train FastText vectors from scratch on the same corpus as was used for your traditional full-word-vectors.</p>
",3,3,3003,2017-12-29 04:18:46,https://stackoverflow.com/questions/48017343/how-to-convert-gensim-word2vec-model-to-fasttext-model
PCA on word2vec embeddings,"<p>I am trying to reproduce the results of this paper: <a href=""https://arxiv.org/pdf/1607.06520.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.06520.pdf</a></p>

<p>Specifically this part:</p>

<blockquote>
  <p>To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/EOJJK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/EOJJK.png"" alt=""enter image description here""></a></p>

<p>I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec. </p>

<p>The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:</p>

<p><a href=""https://i.sstatic.net/7b6Dj.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7b6Dj.png"" alt=""enter image description here""></a></p>

<p>I've computed the differences between each normalized vector in the following way:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-
negative300.bin', binary = True)
model.init_sims()

pairs = [('she', 'he'),
('her', 'his'),
('woman', 'man'),
('Mary', 'John'),
('herself', 'himself'),
('daughter', 'son'),
('mother', 'father'),
('gal', 'guy'),
('girl', 'boy'),
('female', 'male')]

difference_matrix = np.array([model.word_vec(a[0], use_norm=True) - model.word_vec(a[1], use_norm=True) for a in pairs])
</code></pre>

<p>I then perform PCA on the resulting matrix, with 10 components, as per the paper:</p>

<pre><code>from sklearn.decomposition import PCA
pca = PCA(n_components=10)
pca.fit(difference_matrix)
</code></pre>

<p>However I get very different results when I look at <code>pca.explained_variance_ratio_</code> :</p>

<pre><code>array([  2.83391436e-01,   2.48616155e-01,   1.90642492e-01,
         9.98411858e-02,   5.61260498e-02,   5.29706681e-02,
         2.75670634e-02,   2.21957722e-02,   1.86491774e-02,
         1.99108478e-32])
</code></pre>

<p>or with a chart:</p>

<p><a href=""https://i.sstatic.net/RuNEi.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/RuNEi.png"" alt=""enter image description here""></a></p>

<p>The first component accounts for less than 30% of the variance when it should be above 60%! </p>

<p>The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what.</p>

<p>Note: I've tried without normalizing the vectors, but I get the same results.</p>
","python, scikit-learn, nlp, pca, word2vec","<p>They released the code for the paper on github: <a href=""https://github.com/tolga-b/debiaswe"" rel=""noreferrer"">https://github.com/tolga-b/debiaswe</a></p>

<p>Specifically, you can see their code for creating the PCA plot in <a href=""https://github.com/tolga-b/debiaswe/blob/master/debiaswe/we.py"" rel=""noreferrer"">this</a> file. </p>

<p>Here is the relevant snippet of code from that file:</p>

<pre><code>def doPCA(pairs, embedding, num_components = 10):
    matrix = []
    for a, b in pairs:
        center = (embedding.v(a) + embedding.v(b))/2
        matrix.append(embedding.v(a) - center)
        matrix.append(embedding.v(b) - center)
    matrix = np.array(matrix)
    pca = PCA(n_components = num_components)
    pca.fit(matrix)
    # bar(range(num_components), pca.explained_variance_ratio_)
    return pca
</code></pre>

<p>Based on the code, looks like they are taking the difference between each word in a pair and the average vector of the pair. To me, it's not clear this is what they meant in the paper. However, I ran this code with their pairs and was able to recreate the graph from the paper:</p>

<p><a href=""https://i.sstatic.net/Cy2wi.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Cy2wi.png"" alt=""enter image description here""></a></p>
",14,27,16131,2017-12-29 08:45:29,https://stackoverflow.com/questions/48019843/pca-on-word2vec-embeddings
doc2vec/gensim - issue with shuffling sentences in the epochs,"<p>I am trying to get started with <code>word2vec</code> and <code>doc2vec</code> using the excellent tutorials, <a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104"" rel=""nofollow noreferrer"">here</a> and trying to use the code samples. I only added in a <code>line_clean()</code> method to remove punctuation, stopwords etc. </p>

<p>But I am having trouble with the <code>line_clean()</code> method called in the training iterations.  I understand the call to the global method is messing it up, but I am not sure how to get past this problem.</p>

<pre><code>Iteration 1
Traceback (most recent call last):
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 96, in &lt;module&gt;
    train()
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 91, in train
    model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 61, in sentences_perm
    shuffled = list(self.sentences)
AttributeError: 'TaggedLineSentence' object has no attribute 'sentences'
</code></pre>

<p>My code is below:</p>

<pre><code>import gensim
from gensim import utils
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import os
import random
import numpy
from sklearn.linear_model import LogisticRegression
import logging
import sys
from nltk import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stopword_set = set(stopwords.words('english'))


def clean_line(line):
    new_str = unicode(line, errors='replace').lower() #encoding issues
    dlist = tokenizer.tokenize(new_str)
    dlist = list(set(dlist).difference(stopword_set))
    new_line = ' '.join(dlist)
    return new_line


class TaggedLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no]))
        return(self.sentences)

    def sentences_perm(self):
        shuffled = list(self.sentences)
        random.shuffle(shuffled)
        return(shuffled)


def train():
    #create a list data that stores the content of all text files in order of their names in docLabels
    doc_files = [f for f in os.listdir('./data/') if f.endswith('.csv')]

    sources = {}
    for doc in doc_files:
        doc2 = os.path.join('./data',doc)
        sources[doc2] = doc.replace('.csv','')

    sentences = TaggedLineSentence(sources)


    # #iterator returned over all documents
    model = gensim.models.Doc2Vec(size=300, min_count=2, alpha=0.025, min_alpha=0.025)
    model.build_vocab(sentences)

    #training of model
    for epoch in range(10):
        #random.shuffle(sentences)
        print 'iteration '+str(epoch+1)
        #model.train(it)
        model.alpha -= 0.002
        model.min_alpha = model.alpha
        model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
    #saving the created model
    model.save('reddit.doc2vec')
    print ""model saved"" 

train()
</code></pre>
","python, word2vec, gensim, doc2vec","<p>Those aren't great tutorials for the latest versions of <code>gensim</code>. In particular, it's a bad idea to be calling <code>train()</code> multiple times in a loop with your own manual management of <code>alpha</code>/<code>min_alpha</code>. It's easy to mess up â€“ the wrong things will happen in your code, for example â€“ and offers no benefit for most users. Don't change <code>min_alpha</code> from the default, and call <code>train()</code> exactly once â€“ it'll then do exactly <code>epochs</code> iterations, decaying the learning-rate <code>alpha</code> from its max to min values properly. </p>

<p>Your specific error is because your <code>TaggedLineSentence</code> class doesn't have a <code>sentences</code> property â€“ at least not until after <code>to_array()</code> is called â€“ and yet the code is trying to access that non-existent property. </p>

<p>The whole <code>to_array()</code>/<code>sentences_perm()</code> approach is a bit broken. The reason for using such an iterable class is typically to keep a large dataset out of main-memory, streaming it from disk. But <code>to_array()</code> then just loads everything, caching it <em>inside</em> the class - eliminating the iterable benefit. If you can afford that, because the full dataset easily fits in memory, you can just do...</p>

<pre><code>sentences = list(TaggedLineSentence(sources)
</code></pre>

<p>...to iterate-from-disk once, then keep the corpus in an in-memory list. </p>

<p>And shuffling repeatedly during training isn't usually needed. Only if the training data has some existing clumping â€“ like all the examples with certain words/topics are stuck together at the top or bottom of the ordering â€“ is the native ordering likely to cause training problems. And in that case, a single shuffle, before any training, should be enough to remove the clumping. So again assuming your data fits in memory, you can just do...</p>

<pre><code>sentences = random.shuffle(list(TaggedLineSentence(sources)
</code></pre>

<p>...once, then you've got a <code>sentences</code> that's fine to pass to <code>Doc2Vec</code> in both <code>build_vocab()</code> and <code>train()</code> (once) below. </p>
",7,2,1004,2017-12-31 17:55:45,https://stackoverflow.com/questions/48044670/doc2vec-gensim-issue-with-shuffling-sentences-in-the-epochs
Why does the train set not transform from words to Word2Vec Numpy vectors?,"<p>While searching online, I was working on creating a Keras model for analysing tweets. However, in my efforts to utilize a LSTM for a sentence level sentiment analysis, I've encountered trouble.</p>

<p>My goal is to use the input matrix which is a numpy array of word token arrays  to index into the vocabulary dictionary to give me the numpy vectors and replace the words with the corresponding word  vectors.</p>

<p>There are numerous tweets which I wish to replace accordingly. Following is the code segment for a CSV file and a pre-prepared Word2Vec vocab with a million tweets extracted and previous model run succesfully.</p>

<pre><code>x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),np.array(data.head(n).Sentiment), test_size=0.2)

tweet_w2v = Word2Vec.load(""vocaber.txt"")
vocabul = tweet_w2v.wv.vocab
x_train=np.asarray(x_train)

vectorz=copy(x_train)
for k, v in vocabul.items():
        vectorz[x_train==k] = v
print(vectorz[1])
</code></pre>

<p>The output appears unchanged when printed. </p>

<blockquote>
  <p>[02:05&lt;00:00, 7942.71it/s] [u'it', u'happens', u'.', u""it's"", u'just',
  u'his', u'phone', u'got', u'cut', u'off', u'so', u'ya']</p>
</blockquote>

<p>Edit:</p>

<p>The data element stores one million such rows with the sentiment text tokenized.</p>

<p>Sentiment   ItemID  Date    NO_QUERY    SentimentSource SentimentText
0   1467810672  Mon Apr 06 22:19:49 PDT 2009    NO_QUERY    scotthamilton   is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!
0   1467810917  Mon Apr 06 22:19:53 PDT 2009    NO_QUERY    mattycus    @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</p>
","python, numpy, machine-learning, word2vec","<p>Since you'll likely want to perform the same word-to-vector replacement on your test set, it'd be better to do it before the train/test split, on `data' itself. </p>

<p>And when you do it, rather than iterating over the vocabulary dictionary, you'll want to iterate over each example in your data, then replace each token within those examples according to a lookup on the Word2Vec model â€“ <em>not</em> just its <code>vocab</code> dictionary, which doesn't have vectors as its values. </p>

<p>That is, something more like a Python list-comprehension:</p>

<pre><code>data_vectors = [[tweet_w2v.wv(token) for token in each_example]
                for each_example in data.head(n).tokens]
# ...*then* do your split...
x_train, x_test, y_train, y_test = train_test_split(np.array(data_vectors),np.array(data.head(n).Sentiment), test_size=0.2)
</code></pre>

<p>Note that depending on the different number of tokens in each example, <code>data_vectors</code> here will also have a different number of vectors for example after the transformation. For typical later machine-learning steps, you'll likely need another step that converts all examples to same-sized vectors - perhaps by averaging all the token-vectors of each example together. But you haven't yet asked about that.</p>
",0,0,559,2018-01-02 14:22:51,https://stackoverflow.com/questions/48062662/why-does-the-train-set-not-transform-from-words-to-word2vec-numpy-vectors
How does Pyspark Calculate Doc2Vec from word2vec word embeddings?,"<p>I have a pyspark dataframe with a corpus of ~300k unique rows each with a ""doc"" that contains a few sentences of text in each.</p>

<p>After processing, I have a 200 dimension vectorized representation of each row/doc. My NLP Process: </p>

<ol>
<li>Remove Punctuation with regex udf </li>
<li>Word Stemming with nltk snowball udf)</li>
<li>Pyspark Tokenizer</li>
<li>Word2Vec (ml.feature.Word2Vec, vectorSize=200, windowSize=5)</li>
</ol>

<p>I understand how this implementation uses the skipgram model to create embeddings for each word based on the full corpus used. My question is: <strong>How does this implementation go from a vector for each word in the corpus to a vector for each document/row?</strong></p>

<p>Is it the same processes as in the gensim doc2vec implementation where it simply concatenates the word vectors in each doc together?: <a href=""https://stackoverflow.com/questions/40413866/how-does-gensim-calculate-doc2vec-paragraph-vectors"">How does gensim calculate doc2vec paragraph vectors</a>. If so, how does it cut the vector down to the specified size of 200 (Does it use just the first 200 words? Average?)? </p>

<p>I was unable to find the information from the sourcecode: <a href=""https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec"" rel=""noreferrer"">https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec</a> </p>

<p>Any help or reference material to look at is super appreciated!</p>
","apache-spark, nlp, pyspark, word2vec, doc2vec","<p>One simple way to go from word-vectors, to a single vector for a range-of-text, is to average the vectors together. And, that often works well-enough for some tasks. </p>

<p>However, that's not how the <code>Doc2Vec</code> class in <code>gensim</code> does it. That class implements the <a href=""https://arxiv.org/abs/1405.4053"" rel=""noreferrer"">'Paragraph Vectors' technique</a>, where separate document-vectors are trained in a manner analogous to word-vectors. </p>

<p>The doc-vectors participate in training a bit like a floating synthetic word, involved in every sliding window/target-word-prediction. They're <em>not</em> composed-up or concatenated-from preexisting word-vectors, though in some modes they may be simultaneously trained alongside word-vectors. (However, the fast and often top-performing PV-DBOW mode, enabled in gensim with the parameter <code>dm=0</code>, doesn't train or use input-word-vectors at all. It just trains doc-vectors that are good for predicting the words in each text-example.)</p>

<p>Since you've mentioned multiple libraries (both Spark MLib and gensim), but you've not shown your code, it's not certain exactly what <em>your</em> 
existing process is doing.</p>
",5,8,6185,2018-01-02 16:20:45,https://stackoverflow.com/questions/48064378/how-does-pyspark-calculate-doc2vec-from-word2vec-word-embeddings
How to create dataframe of top 5 close words to a particular word lists from a dictionary in pandas,"<p>I have a word2vec dictionary which gives a top similar words  to given word.</p>

<p>I want to pass the list of words for which similarity  needs to calculated  from  a file or list</p>

<p><strong>Input</strong> </p>

<pre><code>word_list =['wan,'floor','street']
</code></pre>

<p>Similarity of these words should be checked against the word2vec dictionary and  similar words to the input word_list must found and written to a dataframe in the below shown format.</p>

<pre><code>model.most_similar(""wan"")

[('wan.', 0.7509685754776001),
 ('want', 0.7326164245605469),
 ('aupuiwan', 0.7161564230918884),
 ('puiwan', 0.7119397521018982),
 ('wanstreet', 0.7096157073974609),
 ('woshing', 0.7046518921852112),
 ('futan', 0.6979573369026184),
 ('won', 0.696295440196991),
 ('fota', 0.6961145401000977),
 ('pul', 0.6921802759170532)]
</code></pre>

<p>I want create a dataframe with  two columns Word and Similar words. </p>

<p><strong>Output Dataframe</strong></p>

<pre><code>Word    Similar Words
wan     ('wan.', 'want','aupuiwan','puiwan','wanstreet')
floor   ('fl','flooor','flor','flr','gf')
street  ('st','rosestreet','stret','strt','str')
</code></pre>

<p>Any help is appreciated.</p>
","python, string, pandas, word2vec, gensim","<p>Try this:</p>

<pre class=""lang-py prettyprint-override""><code>words = ['wan', 'floor', 'street']
similar = [[item[0] for item in model.most_similar(word)[:5]] for word in words]
df = pd.DataFrame({'Word': words, 'Similar Words': similar})
</code></pre>
",0,2,226,2018-01-03 17:11:12,https://stackoverflow.com/questions/48082018/how-to-create-dataframe-of-top-5-close-words-to-a-particular-word-lists-from-a-d
Spark MLib Word2Vec Error: The vocabulary size should be &gt; 0,"<p>I am trying to implement word vectorization using Spark's MLLib. I am following the example given <a href=""http://spark.apache.org/docs/latest/mllib-feature-extraction.html#example"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I have bunch of sentences which I want to give as input to train the model. But am not sure if this model takes sentences or just takes all the words as a sequence of string. </p>

<p>My input is as below:</p>

<pre><code>scala&gt; v.take(5)
res31: Array[Seq[String]] = Array(List([WrappedArray(0_42)]), List([WrappedArray(big, baller, shoe, ?)]), List([WrappedArray(since, eliud, win, ,, quick, fact, from, runner, from, country, kalenjins, !, write, ., happy, quick, fact, kalenjins, location, :, kenya, (, kenya's, western, highland, rift, valley, ), population, :, 4, ., 9, million, ;, compose, 11, subtribes, language, :, kalenjin, ;, swahili, ;, english, church, :, christianity, ~, africa, inland, church, [, aic, ],, church, province, kenya, [, cpk, ],, roman, catholic, church, ;, islam, translation, :, kalenjin, translate, "", tell, "", formation, :, wwii, ,, gikuyu, tribal, member, wish, separate, create, identity, ., later, ,, student, attend, alliance, high, school, (, first, british, public, school, kenya, ), form, ...
</code></pre>

<p>But when I try to train my word2vec model on this input it does not work.</p>

<pre><code>scala&gt; val word2vec = new Word2Vec()
word2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@51567040

scala&gt; val model = word2vec.fit(v)
java.lang.IllegalArgumentException: requirement failed: The vocabulary size should be &gt; 0. You may need to check the setting of minCount, which could be large enough to remove all your words in sentences.
</code></pre>

<p>Does <code>Word2Vec</code> not take sentences as input?</p>
","scala, apache-spark, machine-learning, apache-spark-mllib, word2vec","<p>Your input is correct. However, <code>Word2Vec</code> will automatically remove words that do not occur a minimum number of times in the vocabulary (all sentences combined). By default this value is 5. In your case, it is highly likely that no word occurs 5 or more times in the data you use. </p>

<p>To change the minimum required word occurrences use <code>setMinCount()</code>, for example a min count of 2:</p>

<pre><code>val word2vec = new Word2Vec().setMinCount(2)
</code></pre>
",4,2,3098,2018-01-03 22:41:51,https://stackoverflow.com/questions/48086226/spark-mlib-word2vec-error-the-vocabulary-size-should-be-0
How to write words having similarity above .6 to a specific word from a dictionary to a dataframe in pandas,"<p>I have a word2vec dictionary which has a list of similar words to given word.</p>

<p><strong>Example</strong></p>

<pre><code>model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p>I want to create  dataframe containing root and similar_words(having similarity above .6)</p>

<p>Currently I am able to write all the  similar words corresponding to root word</p>

<pre><code>words = y
similar = [[item[0] for item in model.most_similar(word)[:6]] for word in words]
similarity_matrix = pd.DataFrame({'Root_Word': words, 'Similar_Words': similar})
</code></pre>

<p><strong>Current Output</strong></p>

<pre><code>Root_Word    Similar_word
[st]         [st., sreet, rd;, yop, tseun, tsven] 
[limited]    [ltd, lt, wt, serial, (h.k., dk] 
[centre]     [cent, ct, cte, entre, ctr., ce]
</code></pre>

<p><strong>Expected output is have only Similar words which having similarity above .6.</strong></p>

<p>How can this be done</p>
","python, string, pandas, word2vec, gensim","<p>Based on your current method:</p>

<pre><code> similar = [[item[0] for item in model.most_similar(word) if item[1] &gt; 0.6] for word in words]
</code></pre>
",0,0,64,2018-01-04 05:24:03,https://stackoverflow.com/questions/48089141/how-to-write-words-having-similarity-above-6-to-a-specific-word-from-a-dictiona
Spark MLin Word2vec,"<p>I am trying to run Spark MLlibs word2vec implementation.I am using scala for this.My input for the model is Array of Sequence of strings.It looks as shown below</p>

<pre><code>scala&gt; f.take(5)
res11: Array[org.apache.spark.sql.Row] = Array([WrappedArray(0_42)], [WrappedArray(big, baller, shoe, ?)], [WrappedArray(since, eliud, win, ,, quick, fact, from, runner, from, country, kalenjins, !, write, ., happy, quick, fact, kalenjins, location, :, kenya, (, kenya's, western, highland, rift, valley, ), population, :, 4, ., 9, million, ;, compose, 11, subtribes, language, :, kalenjin, ;, swahili, ;, english, church, :, christianity, ~, africa, inland, church, [, aic, ],, church, province, kenya, [, cpk, ],, roman, catholic, church, ;, islam, translation, :, kalenjin, translate, "", tell, "", formation, :, wwii, ,, gikuyu, tribal, member, wish, separate, create, identity, ., later, ,, student, attend, alliance, high, school, (, first, british, public, school, kenya, ), form, tribe, become, future, kal...

val v=f.map(l=&gt;Seq(l.toString))
scala&gt; v.take(5)
res31: Array[Seq[String]] = Array(List([WrappedArray(0_42)]), List  ([WrappedArray(big, baller, shoe, ?)]), List([WrappedArray(since, eliud, win, ,, quick, fact, from, runner, from, country, kalenjins, !, write, ., happy, quick, fact, kalenjins, location, :, kenya, (, kenya's, western, highland, rift, valley, ), population, :, 4, ., 9, million, ;, compose, 11, subtribes, language, :, kalenjin, ;, swahili, ;, english, church, :, christianity, ~, africa, inland, church, [, aic, ],, church, province, kenya, [, cpk, ],, roman, catholic, church, ;, islam, translation, :, kalenjin, translate, "", tell, "", formation, :, wwii, ,, gikuyu, tribal, member, wish, separate, create, identity, ., later, ,, student, attend, alliance, high, school, (, first, british, public, school, kenya, ), form, ....
</code></pre>

<p>Each sentence is in a separate list as shown above.I run the model by giving v as the input</p>

<pre><code>scala&gt; val model = word2vec.fit(v)
</code></pre>

<p>But the output of this model does not look to be proper. When I save the model and try to read its parquet file(a) I get the below results.</p>

<pre><code>   model.save(sc, ""myModelPath"")
   val a=sqlContext.read.parquet(""myModelPath"")
   a.show(20,false)
+--------------------------------------------------------------------+
|word                                                                |
+--------------------------------------------------------------------+
|[WrappedArray(coffee, machine)]                                     |
|[WrappedArray(good, experience)]                                    |
|[WrappedArray(love, room, !)]                                       |
|[WrappedArray(parking, .)]                                          |
|[WrappedArray(breakfast, great, !)]                                 |
|[WrappedArray(bed, comfortable, room, spacious, .)]                 |
</code></pre>

<p>This word2vec model instead of creating the vectors for each word is creating vectors for array of words.
I am not sure what is the correct way of feeding input to this model and how does it break sentences or words.</p>
","scala, apache-spark, apache-spark-mllib, word2vec","<p>I'll bet that if you look at <code>v.first</code> you'll see <code>List([WrappedArray(0_42)])</code> and if you look at <code>v.first.head</code> you'll see <code>[WrappedArray(0_42)]</code>.  But <code>v.first.head</code> is a String, and what you're actually seeing is <code>""[WrappedArray(0_42)]""</code>.  There is no WrappedArray, just a string.  Perhaps you accidentally called <code>toString</code> on a <code>WrappedArray</code> (or fell victim to an implicit conversion to String).  Word2Vec is actually seeing strings like <code>""[WrappedArray(coffee, machine)]""</code> in its input, and generating a model based on those strings.</p>

<p>UPDATE</p>

<p>If I have your types right, f is a <code>DataFrame</code> where each <code>Row</code> contains a single field holding a <code>Seq[String]</code> (which is actually a <code>WrappedArray</code>).</p>

<p>So, instead of</p>

<pre><code>val v=f.map(l=&gt;Seq(l.toString))
</code></pre>

<p>what you should be doing to extract that field is</p>

<pre><code>val v = f.map(r =&gt; r.getSeq[String](0))
</code></pre>

<p>This produces a <code>Dataset[Seq[String]]</code> that should be suitable for input to <code>Word2Vec</code>.</p>
",1,0,208,2018-01-08 16:06:43,https://stackoverflow.com/questions/48153957/spark-mlin-word2vec
Gensim word embedding training with initial values,"<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
","machine-learning, nlp, word2vec, gensim, word-embedding","<p>I think the easiest solution is to <em>save</em> the embeddings after training on the first data set, then <em>load</em> the trained model and continue training for the second data set. This way you shouldn't expect the embeddings to drift away from the saved state much (unless your data sets are very different).</p>

<p>It would also make sense to create a single vocabulary from all documents: vocabulary words that aren't present in a particular document will get some random representation, but still it will be a working word2vec model.</p>

<p>Example from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model.save(fname)
&gt;&gt;&gt; model = Word2Vec.load(fname)  # continue training with the loaded model
</code></pre>
",0,1,181,2018-01-09 09:25:03,https://stackoverflow.com/questions/48164954/gensim-word-embedding-training-with-initial-values
Gensim Doc2Vec.infer_vector() equivalent in KeyedVector,"<p>I have a working app using <code>doc2vec</code> from <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">gensim</a>. I know the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVector</code></a> is now the recommended approach, and trying to port over however I am not sure what is the equivalent method for the <code>infer_vector</code> method in <code>Doc2Vec</code>?</p>

<p>Or better put, how do I obtain a document vector for an entire document using the <code>KeyedVector</code> model to write to my Annoy model?</p>
","machine-learning, nlp, word2vec, gensim, doc2vec","<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVectors</code></a> doesn't replace <code>Doc2Vec</code>, it's a storage and index system for word vectors:</p>

<blockquote>
  <p>Word vector storage and similarity look-ups. Common code independent
  of the way the vectors are trained(Word2Vec, FastText, WordRank,
  VarEmbed etc)</p>
  
  <p>The word vectors are considered read-only in this class.</p>
</blockquote>

<p>This class doesn't know anything about tagged documents and it can't implement <code>infer_vector</code> or an equivalent because this procedure requires training and the idea of <code>KeyedVectors</code> is to abstract from the training method.</p>
",0,1,733,2018-01-12 22:00:38,https://stackoverflow.com/questions/48234595/gensim-doc2vec-infer-vector-equivalent-in-keyedvector
How can I get a vector after each training iter in word2vec?,"<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
","python-3.x, nlp, word2vec, gensim, word-embedding","<p>You can call <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.train"" rel=""nofollow noreferrer""><code>train()</code></a> method iteratively 6 times, each with <code>epochs=50</code>:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec(size=300, window=4, workers=6, sg=1, 
                                        min_count=10)
model.build_vocab(sentences)
for i in range(6):
  model.train(sentences, total_examples=model.corpus_count, epochs=50)
  print(model.wv.word_vec('the'))  # get the intermediate vector(s)
</code></pre>
",2,2,285,2018-01-15 12:32:11,https://stackoverflow.com/questions/48263122/how-can-i-get-a-vector-after-each-training-iter-in-word2vec
Why word2vec doesn&#39;t use regularization?,"<p>ML models with huge number of parameters will tend to overfit (since they have a large variance). In my opinion, <code>word2vec</code> is one such models. One of the ways to reduce the model variance is to apply a regularization technique, which is very common thing for the other embedding models, such as matrix factorization. However, the basic version of <code>word2vec</code> doesn't have any regularization part. Is there a reason for this?</p>
","machine-learning, nlp, word2vec, embedding, regularized","<p>That's an interesting question.</p>

<p>I'd say that <em>overfitting in Word2Vec</em> doesn't make a lot of sense, because the goal of word embeddings to match the word occurrence distribution as exactly as possible. Word2Vec is not designed to learn anything outside of the training vocabulary, i.e., <em>generalize</em>, but to approximate the one distribution defined by the text corpus. In this sense, Word2Vec is actually trying to fit exactly, so it can't <em>over</em>-fit. </p>

<p>If you had a small vocabulary, it'd be possible to compute the co-occurrence matrix and find the exact global minimum for the embeddings (of a given size), i.e., get the perfect fit and that would define the best contextual word model for this <em>fixed</em> language.</p>
",6,10,2303,2018-01-15 15:33:50,https://stackoverflow.com/questions/48266070/why-word2vec-doesnt-use-regularization
doc2vec: any way to fetch closest matching terms for a given vector?,"<p>The use-case I have is to have a collection of ""upvoted"" documents and ""downvoted"" documents and using those to re-order a set of results in a search.</p>

<p>I am using gensim <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec</a> and am able to run the <code>most_similar</code> queries for word(s) and fetch matching words. But how would I be able to fetch the matching keywords given a vector fetched by a vector sum of the above doc vectors? </p>
","word2vec, gensim, doc2vec","<p>Ohh silly me, I found the answer staring right in my face, posting here in case anyone else has the issue:</p>

<pre><code>similar_by_vector(vector, topn=10, restrict_vocab=None)
</code></pre>

<p>This is however found not in the Doc2Vec class, but in the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVector</a> class.</p>
",0,0,579,2018-01-15 17:10:08,https://stackoverflow.com/questions/48267720/doc2vec-any-way-to-fetch-closest-matching-terms-for-a-given-vector
Python node2vec (Gensim Word2Vec) &quot;Process finished with exit code 134 (interrupted by signal 6: SIGABRT)&quot;,"<p>I am working on node2vec in Python, which uses Gensim's <code>Word2Vec</code> internally.</p>
<p>When I am using a small dataset, the code works well. But as soon as I try to run the same code on a large dataset, the code crashes:</p>
<blockquote>
<p>Error:  Process finished with exit code 134 (interrupted by signal 6: SIGABRT).</p>
</blockquote>
<p>The line which is giving the error is</p>
<pre><code>model = Word2Vec(walks, size=args.dimensions,
                 window=args.window_size, min_count=0, sg=1,
                 workers=args.workers, iter=args.iter)
</code></pre>
<p>I am using <a href=""https://en.wikipedia.org/wiki/PyCharm"" rel=""nofollow noreferrer"">PyCharm</a> and PythonÂ 3.5.</p>
<p>What is happening? I could not find any post which could solve my problem.</p>
","python, pycharm, word2vec, gensim","<p>You are almost certainly running out of memory â€“ which causes the OS to abort your memory-using process with the <code>SIGABRT</code>.</p>
<p>In general, solving this means looking at how your code is using memory, leading up to and at the moment of failure. (The actual 'leak' of excessive bulk memory usage might, however, be arbitrarily earlier - with only the last small/proper increment triggering the error.)</p>
<p>Specifically with the usage of Python, and the <code>node2vec</code> tool which makes use of the Gensim <code>Word2Vec</code> class, some things to try include:</p>
<p>Watch a readout of the Python process size during your attempts.</p>
<p>Enable Python logging to at least the <code>INFO</code> level to see more about what's happening leading-up to the crash.</p>
<p>Further, be sure to:</p>
<ol>
<li>Optimize your <code>walks</code> iterable to <em>not</em> compose a large in-memory list. (Gensim's <code>Word2Vec</code> can work on a corpus of <em>any</em> length, iuncluding those far larger than RAM, as long as (a) the corpus is streamed from disk via a re-iterable Python sequence; and (b) the model's number of <em>unique</em> word/node tokens can be modeled within RAM.)</li>
<li>Ensure the number of unique words (tokens/nodes) in your model doesn't require a model larger than RAM allows. Logging output, once enabled, will show the raw sizes involved just before the main model-allocation (which is likely failing) happens. (If it fails, either: (a) use a system with more RAM to accomdate your full set of nodes; or (b) or use a higher <code>min_count</code> value to discard more less-important nodes.)</li>
</ol>
<p>If your <code>Process finished with exit code 134 (interrupted by signal 6: SIGABRT)</code> error does not involve Python, Gensim, &amp; <code>Word2Vec</code>, you should instead:</p>
<ol>
<li>Search for occurrences of that error combined with more specific details of <em>your</em> triggering situations - the tools/libraries and lines-of-code that create your error.</li>
<li>Look into general <em>memory-profiling</em> tools for your situation, to identify where (even long before the final error) your code might be consuming almost-all of the available RAM.</li>
</ol>
",5,14,55222,2018-01-16 21:51:28,https://stackoverflow.com/questions/48290403/python-node2vec-gensim-word2vec-process-finished-with-exit-code-134-interrup
Are the document vectors used in doc2vec one-hot?,"<p>I understand conceptually how word2vec and doc2vec work, but am struggling with the nuts and bolts of how the numbers in the vectors get processed algorithmically.</p>

<p>If the vectors for three context words are: [1000], [0100], [0010]</p>

<p>and the vector for the target word is [0001], does the algorithm perform one backward pass for each input/target output pair, like this:</p>

<pre><code>[1000]--&gt;[0001]
[0100]--&gt;[0001]
[0010]--&gt;[0001]
</code></pre>

<p>or are the input (context) vectors added together, like this:</p>

<pre><code>[1110]--&gt;[0001]
</code></pre>

<p>or is some other process used?</p>

<p>Additionally, do the document vectors used in doc2vec take the one-hot form of the word vectors, or are documents tagged with individual numbers on a continuous scale, like 1, 2, 3, etc.?</p>

<p>I get that the document tags are included as input nodes during the training process, but how are they used in the test phase? When entering the context word vectors to try to predict the target word (or vice versa) during testing, shouldn't an input for some document ID be required as well?</p>
","python, nlp, word2vec, doc2vec","<p>No, the vectors created by Word2Vec or the 'Paragraph Vectors' form of Doc2Vec are 'dense embeddings' â€“ scattered continuous real-valued coordinates throughout a smaller number of dimensions, rather than 0/1 coordinates in a very-high number of dimensions.</p>

<p>It's possible to think of parts of the training as having a 'one-hot' encoding of the presence or absence of a word, or of a particular document-ID â€“ with these raw 'one-hot' layers then activating a 'projection' layer that maps/averages the one-hots into a dense space. But the implementations I'm familiar with, such as the original Google word2vec.c or Python gensim, don't ever realize giant vocabulary-sized one-hot vectors. </p>

<p>Rather, they use words/document-tags as lookup keys to select the right dense vectors for later operations. These looked-up dense vectors start at random low-magnitude coordinates, but then get continually adjusted by training until they reach the useful distance/direction arrangements for which people use Word2Vec/PV-Doc2Vec. </p>

<p>So in skip-gram, the word 'apple' will pull up a vector, initially random, and that context vector is forward-propagated to see how well it predicts a specific in-window target word. Then, nudges to all values (including to the 'apple' vector's individual dimensions) are applied to make the prediction slightly better. </p>

<p>In PV-Doc2Vec PV-DBOW, the document ID 'doc#123' (or perhaps just the int slot 123) will pull up a candidate vector for that document, initially random, and evaluated/nudged for how well it predicts the words in that document.</p>

<p>Word2Vec CBOW and Doc2Vec PV-DM involve some extra averaging of multiple candidate vectors together before forward-propagation, and then fractional distribution of the nudges back across all vectors that combined to make the context, but it's still the same general approach â€“ and involves working with dense continuous vectors (often of 100-1000 dimensions) rather than one-hot vectors (of dimensionality as large as the whole vocabulary, or whole document-set-size).</p>
",3,0,856,2018-01-17 21:08:17,https://stackoverflow.com/questions/48310002/are-the-document-vectors-used-in-doc2vec-one-hot
Why does skipgram model take more time than CBOW,"<p>Why does skipgram model take more time than CBOW model. I train the model with same parameters (Vector size and window size).</p>
","word2vec, word-embedding","<p>The skip-gram approach involves more calculations.</p>

<p>Specifically, consider a single 'target word' with a context-window of 4 words on either side. </p>

<p>In CBOW, the vectors for all 8 nearby words are averaged together, then used as the input for the algorithm's prediction neural-network. The network is run forward, and its success at predicting the target word is checked. Then back-propagation occurs: all neural-network connection values â€“ including the 8 contributing word-vectors â€“ are nudged to make the prediction slightly better. </p>

<p>Note, though, that the 8-word-window and one-target-word only require one forward-propagation, and one-backward-propagation â€“ and the initial averaging-of-8-values and final distribution-of-error-correction-over-8-vectors are each relatively quick/simple operations. </p>

<p>Now consider instead skip-gram. Each of the 8 context-window words is in turn individually provided as input to the neural-network, forward-checked for how well the target word is predicted, then backward-corrected. Though the averaging/splitting is not done, there's 8 times as much of the neural-network operations. Hence, much more net computation and more run-time. </p>

<p>Note the extra effort/time may pay itself back by improving vector quality on your final evaluations. Whether and to what extent depends on your specific goals and corpus. </p>
",5,2,1411,2018-01-18 23:30:17,https://stackoverflow.com/questions/48331975/why-does-skipgram-model-take-more-time-than-cbow
string vector to list python,"<p>I'm working in Python and I  have a column in data frame that is a string and looks like that : </p>

<pre><code>df['set'] 

0  [911,3040]
1  [130055, 99832, 62131]
2  [19397, 3987, 5330, 14781]
3  [76514, 70178, 70301, 76545]
4  [79185, 38367, 131155, 79433]
</code></pre>

<p>I would like it to be: </p>

<pre><code>['911','3040'],['130055','99832','62131'],['19397','3987','5330','14781'],['76514',70178','70301','76545'],['79185','38367','131155','79433']
</code></pre>

<p>in order to be able to run Word2Vec:</p>

<pre><code>model = gensim.models.Word2Vec(df['set'] , size=100)
</code></pre>

<p>Thanks ! </p>
","python, pandas, dataframe, word2vec","<p>If you have a column of strings, I'd recommend looking <a href=""https://stackoverflow.com/a/48008192/4909087"">here</a> at different ways of parsing it.</p>

<p>Here's how I'd do it, using <code>ast.literal_eval</code>.</p>

<pre><code>&gt;&gt;&gt; import ast
&gt;&gt;&gt; [list(map(str, x)) for x in df['set'].apply(ast.literal_eval)]
</code></pre>

<p>Or, using <code>pd.eval</code> -</p>

<pre><code>&gt;&gt;&gt; [list(map(str, x)) for x in df['set'].apply(pd.eval)]  # 100 rows or less
</code></pre>

<p>Or, using <code>yaml.load</code> -</p>

<pre><code>&gt;&gt;&gt; import yaml
&gt;&gt;&gt; [list(map(str, x)) for x in df['set'].apply(yaml.load)]
</code></pre>

<p></p>

<pre><code>[
     ['911', '3040'], 
     ['130055', '99832', '62131'], 
     ['19397', '3987', '5330', '14781'], 
     ['76514', '70178', '70301', '76545'],
     ['79185', '38367', '131155', '79433']
 ]
</code></pre>
",1,1,975,2018-01-22 13:26:24,https://stackoverflow.com/questions/48382663/string-vector-to-list-python
Word2Vec Doesn&#39;t Contain Embedding for Number 23,"<p>Hi I am on the course of developing Encoder-Decoder model with Attention which predicts WTO Panel Report for the given Factual Relation given as Text_Inputs. </p>

<p>Sample_sentence for factual relation is as follow:</p>

<p>sample_sentence = ""On 23 January 1995, the United States received a request from Venezuela to hold consultations under Article XXII:1 of the General Agreement on Tariffs and Trade 1994 (\""General Agreement\""), Article 14.1 of the Agreement on Technical Barriers to Trade (\""TBT Agreement\"") and Article 4 of the Understanding on Rules and Procedures Governing the Settlement of Disputes (\""DSU\""), on the rule issued by the Environmental Protection Agency on 15 December 1993, entitled \""Regulation of Fuels and Fuel Additives - Standards for Reformulated and Conventional Gasoline\"" (WT/DS2/1). The consultations between Venezuela and the United States took place on 24 February 1995. As they did not result in a satisfactory solution of the matter, Venezuela, in a communication dated 25 March 1995, requested the Dispute Settlement Body (\""DSB\"") to establish a panel to examine the matter under Article XXIII:2 of the General Agreement and Article 6 of the DSU (WT/DS2/2). On 10 April 1995, the DSB established a panel in accordance with the request made by Venezuela. On 28 April 1995, the parties to the dispute agreed that the Panel should have standard terms of reference (DSU, Art. 7) and agreed on the composition of the Panel as follows""</p>

<p>I am trying to using Word2Vec from google and encode each word into 300dim Word Vectors however, like number 23 appears as not included in the Word2Vec VocaSets.</p>

<p><a href=""https://i.sstatic.net/BGrN2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BGrN2.png"" alt=""enter image description here""></a></p>

<p>Which would be the solution for this problem?</p>

<p>1) Use another Word Embedding for example Glovec?</p>

<p>2) Or Another any other advice? </p>

<p>Thx in advance for your help</p>

<ul>
<li>edit)</li>
</ul>

<p>I think to succefully fulfill this task, I think first I have to understand how current NMT application deals with Named Entity Recognition problem in advance before they actually train it.</p>

<p>Any suggestive literatures? </p>
","word2vec, encoder, attention-model","<p>Word2Vec only learns words it has seen a lot.</p>

<p>Maybe try replacing the numbers in your source with text ie (""On the twenty third of ..."")?</p>
",0,0,199,2018-01-24 06:20:12,https://stackoverflow.com/questions/48415986/word2vec-doesnt-contain-embedding-for-number-23
Can Word2Vec be used for information extraction?,"<p>I am using Gensim to train Word2Vec. I know word similarities are deteremined by if the words can replace each other and make sense in a sentence. But can word similarities be used to extract relationships between entities?</p>

<p>Example:
I have a bunch of interview documents and in each interview, the interviewee always says the name of their manager. If I wanted to extract the name of the manager from these interview transcripts could I just get a list of all human name's in the document (using nlp), and the name that is the most similar to the word ""manager"" using Word2Vec, is most likely the manager.</p>

<p>Does this thought process make any sense with Word2Vec? If it doesn't, would the ML solution to this problem then be to input my word embeddings into a sequence to sequence model?</p>
","machine-learning, word2vec, gensim, recurrent-neural-network, information-extraction","<p>Yes, word-vector similarities &amp; relative-arrangements can indicate relationships. </p>

<p>In the original Word2Vec paper, this was demonstrated by using word-vectors to solve word-analogies. The most famous example involves the analogy ""'man' is to 'king' as 'woman' is to ?"".  </p>

<p>By starting with the word-vector for 'king', then subtracting the vector for 'man', and adding the vector for 'woman', you arrive at a new point in the coordinate system. And then, if you look for other words close to that new point, often the closest word will be <code>queen</code>. Essentially, the directions &amp; distances have helped find a word that's related in a particular way â€“ a gender-reversed equivalent. </p>

<p>And, in large news-based corpuses, famous names like 'Obama' or 'Bush' do wind up with vectors closer to their well-known job titles like 'president'. (There will be many contexts in such corpuses where the words appear immediately together â€“ ""President Obama today signedâ€¦"" â€“ or simply in similar roles â€“ ""The President appointedâ€¦"" or ""Obama appointedâ€¦"", etc.)</p>

<p>However, I suspect that's less-likely to work with your 'manager' interview-transcripts example. Achieving meaningful word-to-word arrangements depends on lots of varied examples of the words in shared usage contexts. Strong vectors require large corpuses of millions to billions of words. So the transcripts with a single manager wouldn't likely be enough to get a good model â€“ you'd need transcripts across many managers. </p>

<p>And in such a corpus each manager's name might not be strongly associated with just <code>manager</code>-like contexts. The same name(s) will be repeated when also mentioning other roles, and transcripts may not especially refer to managerial-action in helpful third-person ways that make specific name-vectors well-positioned. (That is, there won't be clean expository statements like, ""John_Smith called a staff meeting"", or ""John_Smith cancelled the project, alongside others like ""â€¦manager John_Smithâ€¦"" or ""The manager cancelled the project"".)</p>
",1,4,1659,2018-01-26 04:12:40,https://stackoverflow.com/questions/48455703/can-word2vec-be-used-for-information-extraction
How to use saved Keras Model for sentiment classification?,"<p>I trained my model and got the accuracy of 79% for sentiment classification. For embedding layer, I used word2vec. 
Now that I have my model trained and saved, how do I use it in such a way that I can put some input as a sentence and it shows me how much positive/negative by probability the sentence is?</p>

<p>Given below is some relevant code.</p>

<pre><code>word_model = gensim.models.Word2Vec(train_x+test_x, size=100, min_count=1, window=5, iter=100)

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, 
                weights=[pretrained_weights]))

model.add(Dropout(0.2))
model.add(Conv1D(filters,
             kernel_size,
             padding='valid',
             activation='relu',
             strides=1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims))
model.add(Dropout(0.2))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.summary()
model.fit(train_x, train_y,
      batch_size=batch_size,
      epochs=epochs,
      validation_data=(test_x, test_y))
</code></pre>
","keras, word2vec","<p>You can't pass strings (or 'sentences' in this case) directly to predict. <a href=""https://keras.io/models/sequential/"" rel=""nofollow noreferrer""><code>Predict</code></a> accepts the input data as the numpy arrays.
So, you need to process your new input sentence the same way you processed your train data.</p>
<p>I can't see what you did with your train data but the process is usually the same :</p>
<ul>
<li>Clean</li>
<li>Tokenize</li>
<li>Vectorize</li>
<li>Match the word index</li>
<li>Pad</li>
<li>Flatten</li>
</ul>
<p>Then you can pass the resulting numpy array to predict to get the desired result.</p>
",1,1,206,2018-01-29 15:42:02,https://stackoverflow.com/questions/48504933/how-to-use-saved-keras-model-for-sentiment-classification
Word2Vec on Spark Scala,"<p>I'm trying to use Word2Vec from mllib, in order to apply a kmeans subsequently. I'm using scala 2.10.5 and spark 1.6.3. This is my code (after a Tokenization):</p>

<pre><code>val word2Vec = new Word2Vec()
  .setMinCount(2)
  .setInputCol(""FilteredFeauturesEntities"")
  .setOutputCol(""Word2VecFeatures"")
  .setVectorSize(1000)

val model = word2Vec.fit(CleanedTokenizedDataFrame)
val word2VecDataFrame = model.transform(CleanedTokenizedDataFrame)

word2VecDataFrame.show()
</code></pre>

<p>I'm not getting a special error but my job don't reach the finishing lines.
This is the log output :</p>

<pre><code>18/02/05 15:39:32 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 23) in 3143 ms on dhadlx122.haas.xxxxxx (2/9)
18/02/05 15:39:32 INFO TaskSetManager: Starting task 5.1 in stage 4.0 (TID 28, dhadlx121.haas.xxxxxx, partition 5,NODE_LOCAL, 2329 bytes)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 20) in 3217 ms on dhadlx121.haas.xxxxxx (3/9)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 22) in 3309 ms on dhadlx123.haas.xxxxxx (4/9)
18/02/05 15:39:32 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 21) in 3677 ms on dhadlx121.haas.xxxxxx (5/9)
18/02/05 15:39:33 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 25) in 3901 ms on dhadlx126.haas.xxxxxx (6/9)
18/02/05 15:39:33 INFO YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (dhadlx127.haas.xxxxxx:48384) with ID 6
18/02/05 15:39:33 INFO BlockManagerMasterEndpoint: Registering block manager dhadlx127.haas.xxxxxx:37909 with 5.3 GB RAM, BlockManagerId(6, dhadlx127.haas.xxxxxx, 37909)
18/02/05 15:39:33 INFO TaskSetManager: Lost task 5.1 in stage 4.0 (TID 28) on executor dhadlx121.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 1]
18/02/05 15:39:33 INFO TaskSetManager: Starting task 5.2 in stage 4.0 (TID 29, dhadlx128.haas.xxxxxx, partition 5,RACK_LOCAL, 2329 bytes)
18/02/05 15:39:33 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 27) in 2948 ms on dhadlx125.haas.xxxxxx (7/9)
18/02/05 15:39:34 INFO TaskSetManager: Lost task 5.2 in stage 4.0 (TID 29) on executor dhadlx128.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 2]
18/02/05 15:39:34 INFO TaskSetManager: Starting task 5.3 in stage 4.0 (TID 30, dhadlx127.haas.xxxxxx, partition 5,RACK_LOCAL, 2329 bytes)
18/02/05 15:39:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on dhadlx127.haas.xxxxxx:37909 (size: 26.4 KB, free: 5.3 GB)
18/02/05 15:39:35 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 6321 ms on dhadlx120.haas.xxxxxx (8/9)
18/02/05 15:39:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on dhadlx127.haas.xxxxxx:37909 (size: 58.9 KB, free: 5.3 GB)
18/02/05 15:39:40 INFO TaskSetManager: Lost task 5.3 in stage 4.0 (TID 30) on executor dhadlx127.haas.xxxxxx: java.lang.NullPointerException (null) [duplicate 3]
18/02/05 15:39:40 ERROR TaskSetManager: Task 5 in stage 4.0 failed 4 times; aborting job
18/02/05 15:39:40 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool
18/02/05 15:39:40 INFO YarnScheduler: Cancelling stage 4
18/02/05 15:39:40 INFO DAGScheduler: ShuffleMapStage 4 (map at Word2Vec.scala:161) failed in 11.037 s
18/02/05 15:39:40 INFO DAGScheduler: Job 3 failed: collect at Word2Vec.scala:170, took 11.058049 s
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 30, dhadlx127.haas.xxxxxx): java.lang.NullPointerException
    at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
    at java.util.regex.Matcher.reset(Matcher.java:309)
    at java.util.regex.Matcher.&lt;init&gt;(Matcher.java:229)
    at java.util.regex.Pattern.matcher(Pattern.java:1093)
    at scala.util.matching.Regex.replaceAllIn(Regex.scala:385)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:51)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:49)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:247)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:801)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1590)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:622)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1831)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1844)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1857)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:934)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:323)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:933)
    at org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:170)
    at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:284)
    at org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:149)
    at SemanticAnalysis.App$.main(App.scala:126)
    at SemanticAnalysis.App.main(App.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:750)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NullPointerException
    at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
    at java.util.regex.Matcher.reset(Matcher.java:309)
    at java.util.regex.Matcher.&lt;init&gt;(Matcher.java:229)
    at java.util.regex.Pattern.matcher(Pattern.java:1093)
    at scala.util.matching.Regex.replaceAllIn(Regex.scala:385)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at SemanticAnalysis.App$$anonfun$extractPattern$1$1.apply(App.scala:63)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:51)
    at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$1.apply(basicOperators.scala:49)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:247)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)
18/02/05 15:39:40 INFO SparkContext: Invoking stop() from shutdown hook
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/02/05 15:39:40 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/02/05 15:39:40 INFO SparkUI: Stopped Spark web UI at http://xxx.xx.xx.xxx:xxxx
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Shutting down all executors
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Asking each executor to shut down
18/02/05 15:39:40 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
services=List(),
started=false)
18/02/05 15:39:40 INFO YarnClientSchedulerBackend: Stopped
18/02/05 15:39:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/05 15:39:40 INFO MemoryStore: MemoryStore cleared
18/02/05 15:39:40 INFO BlockManager: BlockManager stopped
18/02/05 15:39:40 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/05 15:39:40 INFO SparkContext: Successfully stopped SparkContext
18/02/05 15:39:40 INFO ShutdownHookManager: Shutdown hook called
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e769e7c5-4336-45bd-97cd-e0731803f45f
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f427cf4c-4236-4e57-a304-6be2a52932f3
18/02/05 15:39:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/05 15:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f427cf4c-4236-4e57-a304-6be2a52932f3/httpd-0ab9e5ee-930e-4a48-be77-f5a6d2b01250
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
18/02/05 15:39:40 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
</code></pre>

<p>Moreover, the same code works for a small example, in the same working environment : </p>

<pre><code>package BIGDATA

/**
* @author ${user.name}
*/

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}
import org.apache.spark.ml.feature.StopWordsRemover
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import org.apache.spark.ml.feature.Word2Vec
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.mllib.linalg.{VectorUDT, Vectors}


object App {

  def main(args : Array[String]) {

val conf = new SparkConf()
  .setAppName(""SEMANTIC ANALYSIS - TEST"")

val sc = new SparkContext(conf)
val hiveContext = new HiveContext(sc)
import hiveContext.implicits._

println(""===================================================="")
println(""READING DATA"")
println(""===================================================="")


val pattern: scala.util.matching.Regex = ""(([\\w\\.-]+@[\\w\\.-]+)|((X|A|x|a)\\d{6})|(MA\\d{7}\\w|MA\\d{7}|FR\\d{8}\\w)|(w+\\..*(\\.com|fr))|([|\\[\\]!\\(\\)?,;:@&amp;*#_=\\/]*))"".r

def extractPattern(pattern: scala.util.matching.Regex) = udf(
  (title: String) =&gt; pattern.replaceAllIn(title, """")
)

val df = Seq(
  (8, ""Hi I heard about Spark x163021. Now, letâ€™s use trained model by loading it. We need to import KMeansModel in order to use it for loading the model from file.""),
  (64, ""I wish Java could use case classes. Above is a very naive example in which we use training dataset as input data too. In real world we will train a model, save it and later use it for predicting clusters of input data.""),
  (-27, ""Logistic regression models are neat. Here is how you can save a trained model and later load it for prediction."")
).toDF(""number"", ""word"").select($""number"", $""word"",
  extractPattern(pattern)($""word"").alias(""NewWord""))


println(""===================================================="")
println(""FEATURE TRANSFORMERS"")
println(""===================================================="")

val tokenizer = new Tokenizer()
  .setInputCol(""NewWord"")
  .setOutputCol(""FeauturesEntities"")

val TokenizedDataFrame = tokenizer.transform(df)

val remover = new StopWordsRemover()
  .setInputCol(""FeauturesEntities"")
  .setOutputCol(""FilteredFeauturesEntities"")

val CleanedTokenizedDataFrame = remover.transform(TokenizedDataFrame)

CleanedTokenizedDataFrame.show()


println(""===================================================="")
println(""WORD2VEC : LEARN A MAPPING FROM WORDS TO VECTORS"")
println(""===================================================="")


// Learn a mapping from words to Vectors.
val word2Vec = new Word2Vec()
  .setMinCount(2)
  .setInputCol(""FilteredFeauturesEntities"")
  .setOutputCol(""Word2VecFeatures"")
  .setVectorSize(1000)

val model = word2Vec.fit(CleanedTokenizedDataFrame)
val word2VecDataFrame = model.transform(CleanedTokenizedDataFrame)

word2VecDataFrame.show()

  }

}
</code></pre>

<p>What's wrong with the first example ? thx !</p>
","scala, apache-spark, nlp, apache-spark-mllib, word2vec","<p>You code never reaches <code>Word2Vec</code>. It fails on <code>udf</code> call because <code>word</code> column contains <code>nulls</code>. For example </p>

<pre><code>val df = Seq((1, null), (2, ""foo bar"")).toDF(""id"", ""word"")
df.select(extractPattern(pattern)($""word"").alias(""NewWord"")).show
</code></pre>

<p>will fail with the same way:</p>

<pre><code>java.lang.NullPointerException
    at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
    at java.util.regex.Matcher.reset(Matcher.java:309)
    at java.util.regex.Matcher.&lt;init&gt;(Matcher.java:229)
    at java.util.regex.Pattern.matcher(Pattern.java:1093)
</code></pre>

<p>Clean your data using <code>na.drop</code> before you proceed, and in general use <code>regexp_replace</code>, not <code>udf</code>.</p>
",3,0,1442,2018-02-05 15:31:37,https://stackoverflow.com/questions/48625984/word2vec-on-spark-scala
How do I create a Keras Embedding layer from a pre-trained word embedding dataset?,"<p>How do I load a pre-trained word-embedding into a Keras <code>Embedding</code> layer? </p>

<p>I downloaded the <code>glove.6B.50d.txt</code> (glove.6B.zip file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">https://nlp.stanford.edu/projects/glove/</a>) and I'm not sure how to add it to a Keras Embedding layer. See: <a href=""https://keras.io/layers/embeddings/"" rel=""noreferrer"">https://keras.io/layers/embeddings/</a></p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You will need to pass an embeddingMatrix to the <code>Embedding</code> layer as follows:</p>

<p><code>Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable)</code></p>

<ul>
<li><code>vocabLen</code>: number of tokens in your vocabulary</li>
<li><code>embDim</code>: embedding vectors dimension (50 in your example)</li>
<li><code>embeddingMatrix</code>: embedding matrix built from glove.6B.50d.txt</li>
<li><code>isTrainable</code>: whether you want the embeddings to be trainable or froze the layer</li>
</ul>

<p>The <code>glove.6B.50d.txt</code> is a list of whitespace-separated values: word token + (50) embedding values. e.g. <code>the 0.418 0.24968 -0.41242 ...</code></p>

<p>To create a <code>pretrainedEmbeddingLayer</code> from a Glove file:</p>

<pre><code># Prepare Glove File
def readGloveFile(gloveFile):
    with open(gloveFile, 'r') as f:
        wordToGlove = {}  # map from a token (word) to a Glove embedding vector
        wordToIndex = {}  # map from a token to an index
        indexToWord = {}  # map from an index to a token 

        for line in f:
            record = line.strip().split()
            token = record[0] # take the token (word) from the text line
            wordToGlove[token] = np.array(record[1:], dtype=np.float64) # associate the Glove embedding vector to a that token (word)

        tokens = sorted(wordToGlove.keys())
        for idx, tok in enumerate(tokens):
            kerasIdx = idx + 1  # 0 is reserved for masking in Keras (see above)
            wordToIndex[tok] = kerasIdx # associate an index to a token (word)
            indexToWord[kerasIdx] = tok # associate a word to a token (word). Note: inverse of dictionary above

    return wordToIndex, indexToWord, wordToGlove

# Create Pretrained Keras Embedding Layer
def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):
    vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking
    embDim = next(iter(wordToGlove.values())).shape[0]  # works with any glove dimensions (e.g. 50)

    embeddingMatrix = np.zeros((vocabLen, embDim))  # initialize with zeros
    for word, index in wordToIndex.items():
        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding

    embeddingLayer = Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable)
    return embeddingLayer

# usage
wordToIndex, indexToWord, wordToGlove = readGloveFile(""/path/to/glove.6B.50d.txt"")
pretrainedEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)
model = Sequential()
model.add(pretrainedEmbeddingLayer)
...
</code></pre>
",11,7,8848,2018-02-08 03:30:38,https://stackoverflow.com/questions/48677077/how-do-i-create-a-keras-embedding-layer-from-a-pre-trained-word-embedding-datase
Implementing numba for word2vec gradient descent but getting LoweringError,"<p>I am running gradient descent for word2vec and would like to implement numba to speed up the training. </p>

<p>EDIT: It seems the real error is this </p>

<blockquote>
  <p>NotImplementedError: unsupported nested memory-managed object</p>
</blockquote>

<p>This is a subsequent error: </p>

<pre><code>raise NotImplementedError(""%s: %s"" % (root_type, str(e)))

numba.errors.LoweringError: Failed at nopython (nopython mode backend)
reflected list(reflected list(int64)): unsupported nested memory-managed object
File ""test.py"", line 36
[1] During: lowering ""negative_indices = arg(6, name=negative_indices)"" at test.py (36)
</code></pre>

<p>I've searched through the <a href=""http://numba.pydata.org/numba-doc/dev/user/troubleshoot.html#numba-troubleshooting"" rel=""nofollow noreferrer"">numba documentation</a> and googled this error with no luck.</p>

<p>Here is a replicable code snippet:</p>

<pre><code>import numpy as np
import random
from numba import jit

random.seed(10)
np.random.seed(10)

@jit(nopython=True)
def sigmoid(x):
    return float(1)/(1+np.exp(-x))

@jit(nopython=True)
def listsum(list1):
    ret=0
    for i in list1:
        ret += i
    return ret


num_samples = 2
learning_rate = 0.05
center_token = 50
hidden_size = 100
sequence_chars = [2000, 1500, 400, 600]
W1 = np.random.uniform(-.5, .5, size=(11000, hidden_size))
W2 = np.random.uniform(-.5, .5, size=(11000, hidden_size))
negative_indices = [[800,1000], [777,950], [650,300], [10000,9999]]

@jit(nopython=True)
def performDescent(num_samples, learning_rate, center_token, sequence_chars,W1,W2,negative_indices):
    nll_new = 0
    neg_idx = 0
    for k in range(0, len(sequence_chars)):
        w_c = sequence_chars[k]
        W_neg = negative_indices[k]
        w_j = [w_c] + W_neg
        t_j = [1] + [0]*len(W_neg)
        h = W1[center_token]

        update_i = np.zeros((hidden_size,len(w_j)))
        for i in range(0,len(w_j)):
            v_j = W2[w_j[i]]
            update_i[:,i] = (sigmoid(np.dot(v_j.T,h))-t_j[i])*v_j
            W2[w_j[i]] = v_j - learning_rate*(sigmoid(np.dot(v_j.T,h))-t_j[i])*h #creates v_j_new
        W1[center_token] = h - learning_rate*np.sum(update_i, axis=1)

        update_nll = []
        for i in range(1,len(w_j)):
            update_nll.append(np.log(sigmoid(-np.dot(W2[w_j[i]].T,h))))  #h is updated in memory
        nll = -np.log(sigmoid(np.dot(W2[w_j[0]].T,h))) - listsum(update_nll)
        print(""nll:"",nll)
        nll_new += nll
    return [nll_new]

performDescent(num_samples, learning_rate, center_token, sequence_chars,W1,W2,negative_indices)
</code></pre>

<p>I don't understand why negative_indices is giving an issue.</p>
","python, word2vec, gradient-descent, numba","<p>As the error message hints at, lists in numba have only partial support. They can't contain ""memory-managed"" objects, which means they can only hold scalar, primitive types - for example:</p>

<pre><code>@njit
def list_first(l):
    return l[0]

list_first([1, 2, 3])
# Out[65]: 1

list_first([[1], [2]])
# LoweringError: Failed at nopython (nopython mode backend)
# reflected list(reflected list(int64)): unsupported nested memory-managed object
</code></pre>

<p>Assuming your example if representative, it seems like in the places you are using a list, it isn't necessary, and would be hurting performance even if it was supported, because you know the allocation sizes in advance.</p>

<p>Here's a potential refactoring that numba can handle.</p>

<pre><code>sequence_chars = np.array([2000, 1500, 400, 600], dtype=np.int64)
negative_indices = np.array([[800,1000], [777,950], [650,300], [10000,9999]], dtype=np.int64)

@jit(nopython=True)
def performDescent2(num_samples, learning_rate, center_token, sequence_chars, W1, W2 ,negative_indices):
    nll_new = 0
    neg_idx = 0

    neg_ind_length = len(negative_indices[0])
    w_j = np.empty(neg_ind_length + 1, dtype=np.int64)
    t_j = np.zeros(neg_ind_length + 1, dtype=np.int64)
    t_j[0] = 1

    for k in range(0, len(sequence_chars)):
        w_j[0] = sequence_chars[k]
        w_j[1:] = negative_indices[k]

        h = W1[center_token]

        update_i = np.zeros((hidden_size,len(w_j)))
        for i in range(0,len(w_j)):
            v_j = W2[w_j[i]]
            update_i[:,i] = (sigmoid(np.dot(v_j.T, h)) - t_j[i]) * v_j
            W2[w_j[i]] = v_j - learning_rate * (sigmoid(np.dot(v_j.T, h)) - t_j[i]) * h #creates v_j_new
        W1[center_token] = h - learning_rate * np.sum(update_i, axis=1)

        update_nll = np.zeros(len(w_j))

        for i in range(1, len(w_j)):
            update_nll[i-1] = np.log(sigmoid(-np.dot(W2[w_j[i]].T, h)))  #h is updated in memory
        nll = -np.log(sigmoid(np.dot(W2[w_j[0]].T,h))) - update_nll.sum()
        print(""nll:"",nll)
        nll_new += nll
    return nll_new
</code></pre>
",0,0,276,2018-02-09 04:52:51,https://stackoverflow.com/questions/48698936/implementing-numba-for-word2vec-gradient-descent-but-getting-loweringerror
how to use build_vocab in gensim?,"<ol>
<li>Build_vocab extend my old vocabulary? </li>
</ol>

<p>For example, my idea is when I use doc2vec(s) to train a model, it just builds the vocabulary from the datasets.  If I want to extend it, I need to use build_vocab()</p>

<ol start=""2"">
<li>Where should I use it?  Should I put it after ""gensim.doc2vec()""?  </li>
</ol>

<p>For example:</p>

<pre><code>sentences = gensim.models.doc2vec.TaggedLineDocument(f_path)
dm_model = gensim.models.doc2vec.Doc2Vec(sentences, dm=1, size=300, window=8, min_count=5, workers=4)
dm_model.build_vocab()
</code></pre>
","nlp, word2vec, gensim, doc2vec","<p>You should follow working examples in gensim documentation/tutorials/notebooks or online tutorials to understand which steps are necessary and in what order. </p>

<p>In particular, <em>if</em> you provide your <code>sentences</code> corpus iterable on the <code>Doc2Vec()</code> initialization, it will <em>automatically</em> do both the vocabulary-discovery pass and all training â€“ so you <em>donâ€™t</em> then need to call either <code>build_vocab()</code> or <code>train()</code> yourself. And further, you would <em>never</em> call <code>build_vocab()</code> with no arguments. (No working example in docs or online will do what your code does â€“ so donâ€™t improvise new things until youâ€™ve followed the examples and know why they do what they do.)</p>

<p>There is an optional <code>update</code> argument to <code>build_vocab()</code>, which purports to allow the expansion of a vocabulary from an earlier training session (in preparation for further training with the newer words). HOWEVER, itâ€™s only been developed/tested with regard to <code>Word2Vec</code> models â€“ there are reports it causes crashes when used with <code>Doc2Vec</code>. And even in <code>Word2Vec</code>, its overall effects and best-ways-to-use arenâ€™t clear, across all training modes. So I donâ€™t recommend its use except for experts who can read &amp; interpret the source code, and many involved tradeoffs, on their own. If you receive a chunk of new texts, with new words, the best-grounded course of action, and easiest to evaluate/reason-about, is to re-train from scratch, using a combined corpus of all text examples. </p>
",9,3,12168,2018-02-09 09:53:11,https://stackoverflow.com/questions/48703067/how-to-use-build-vocab-in-gensim
Does Mikolov 2014 Paragraph2Vec models assume sentence ordering?,"<p>In Mikolov 2014 paper regarding paragraph2Vectors, <a href=""https://arxiv.org/pdf/1405.4053v2.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1405.4053v2.pdf</a>, do the authors assume in both PV-DM and PV-DBOW, the ordering of sentences need to make sense?</p>

<p>Imagine I am handling a stream of tweets, and each tweet is a paragraph. The paragraphs/tweets do not necessarily have ordering relations. After training, does the vector embedding for paragraphs still make sense?</p>
","word2vec, doc2vec, sentence-similarity","<p>Each document/paragraph is treated as a single unit for training â€“ and thereâ€™s no explicit way that the neighboring documents directly affect a documentâ€™s vector. So the ordering of documents doesnâ€™t have to be natural. </p>

<p>In fact, you generally <em>donâ€™t</em> want all similar text-examples to be clumped together â€“ for example, all those on a certain topic, or using a certain vocabulary, in the front or back of all training examples. Thatâ€™d mean those examples are all trained with a similar <code>alpha</code> learning rate, and affect all related words <em>without</em> interleaved offsetting examples with other words. Either of those could make a model slightly less balanced/general, across all possible documents. For this reason, it can be good to perform at least one initial shuffle of the text examples before training a gensim <code>Doc2Vec</code> (or <code>Word2Vec</code>) model, if your natural ordering might not spread all topics/vocabulary words evenly through the training corpus.</p>

<p>The PV-DM modes (default <code>dm=1</code> mode in gensim) do involve sliding context-windows of nearby words, so word proximity within each example matters. (Donâ€™t shuffle the words inside each text!)</p>
",1,0,190,2018-02-09 19:40:16,https://stackoverflow.com/questions/48712989/does-mikolov-2014-paragraph2vec-models-assume-sentence-ordering
Gensim: Word2Vec Recommender accuracy Improvement,"<p>I am trying to implement something similar in <a href=""https://arxiv.org/pdf/1603.04259.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.04259.pdf</a> using awesome gensim library however I am having trouble improving quality of results when I compare to Collaborative Filtering.</p>

<p>I have two models one built on Apache Spark and other one using gensim Word2Vec on grouplens 20 million ratings dataset. My apache spark model is hosted on AWS <a href=""http://sparkmovierecommender.us-east-1.elasticbeanstalk.com"" rel=""nofollow noreferrer"">http://sparkmovierecommender.us-east-1.elasticbeanstalk.com</a>
and I am running gensim model on my local. However when I compare the results I see superior results with CF model 9 out of 10 times(like below example more similar to searched movie - affinity towards Marvel movies) </p>

<p>e.g.:- If I search for Thor movie I get below results </p>

<p><strong><em>Gensim</em></strong></p>

<ul>
<li>Captain America: The First Avenger (2011) </li>
<li>X-Men: First Class (2011)</li>
<li>Rise of the Planet of the Apes (2011) </li>
<li>Iron Man 2 (2010) </li>
<li>X-Men Origins: Wolverine (2009) </li>
<li>Green Lantern (2011) </li>
<li>Super 8 (2011) </li>
<li>Tron:Legacy (2010) </li>
<li>Transformers: Dark of the Moon (2011)</li>
</ul>

<p><strong><em>CF</em></strong></p>

<ul>
<li>Captain America: The First Avenger</li>
<li>Iron Man 2</li>
<li>Thor: The Dark World</li>
<li>Iron Man</li>
<li>The Avengers</li>
<li>X-Men: First Class</li>
<li>Iron Man 3</li>
<li>Star Trek</li>
<li>Captain America: The Winter Soldier</li>
</ul>

<p>Below is my model configuration, so far I have tried playing with window, min_count and size parameter but not much improvement.</p>

<pre><code>word2vec_model = gensim.models.Word2Vec(
    seed=1,
    size=100, 
    min_count=50, 
    window=30)

word2vec_model.train(movie_list, total_examples=len(movie_list), epochs=10)
</code></pre>

<p>Any help in this regard is appreciated.</p>
","word2vec, gensim, recommendation-engine","<p>You don't mention what Collaborative Filtering algorithm you're trying, but maybe it's just better than <code>Word2Vec</code> for this purpose. (<code>Word2Vec</code> is not doing awful; why do you expect it to be better?) </p>

<p>Alternate meta-parameters might do better.</p>

<p>For example, the <code>window</code> is the max-distance between tokens that might affect each other, but the effective windows used in each target-token training randomly chosen from 1 to <code>window</code>, as a way to give nearby tokens more weight. Thus when some training-texts are much larger than the <code>window</code> (as in your example row), some of the correlations will be ignored (or underweighted). Unless ordering is very significant, a giant <code>window</code> (MAX_INT?) might do better, or even a related method where ordering is irrelevant (such as <code>Doc2Vec</code> in pure PV-DBOW <code>dm=0</code> mode, with every token used as a doc-tag).</p>

<p>Depending on how much data you have, your <code>size</code> might be too large or small. Different <code>min_count</code>, <code>negative</code> count, greater 'iter'/'epochs', or <code>sample</code> level might work much better. (And perhaps even things you've already tinkered with would only help after other changes are in place.)</p>
",3,1,1014,2018-02-10 06:32:46,https://stackoverflow.com/questions/48717970/gensim-word2vec-recommender-accuracy-improvement
Getting embedding matrix of all zeros after performing word embedding on any input data,"<p>I am trying to do word embeddings in Keras. I am using 'glove.6B.50d.txt' for the purpose. I am able to get correct output till the preparation of embedding index from the ""glove.6B.50d.txt"" file.</p>

<p>But I'm always getting embedding matrix full of zeros whenever I map the word from the input provided by me to that in the embedding index.</p>



<p>Here is the code:</p>

<pre class=""lang-python prettyprint-override""><code>#here is the example sentence given as input

line=""The quick brown fox jumped over the lazy dog""
line=line.split("" "")

#this is my embedding file
EMBEDDING_FILE='glove.6B.50d.txt'

embed_size = 10 # how big is each word vector
max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 10 # max number of words in a comment to use


tokenizer = Tokenizer(num_words=max_features,split="" "",char_level=False)
tokenizer.fit_on_texts(list(line))
list_tokenized_train = tokenizer.texts_to_sequences(line)
sequences = tokenizer.texts_to_sequences(line)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)

print(sequences)
print(word_index)
print('Shape of data tensor:', X_t.shape)

#got correct output here as 

 # Found 8 unique tokens.
    #[[1], [2], [3], [4], [5], [6], [1], [7], [8]]
    #{'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumped': 5, 'over': 6, 'lazy': 7, 'dog': 8}
   # Shape of data tensor: (9, 10)


#loading the embedding file to prepare embedding index matrix
embeddings_index = {}
for i in open(EMBEDDING_FILE, ""rb""):
    values = i.split()
    word = values[0]
    #print(word)
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

print('Found %s word vectors.' % len(embeddings_index))

#Found 400000 word vectors.

#making the embedding matrix

embedding_matrix = np.zeros((len(word_index) + 1, embed_size))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>Here when I print the embedding matrix ,I get all zeros in it (i.e not a single word in input is recognized).</p>

<pre class=""lang-python prettyprint-override""><code>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>

<p>Also if I print the <code>embeddings_index.get(word)</code> for each iteration, it is unable to fetch the word and returns NONE.</p>

<p>Where am I going wrong in the code?</p>
","keras, tokenize, word2vec, word-embedding","<p>Got the problem solved today.
Seems like <code>embeddings_index.get(word)</code> was unable to get the word because of some encoding issues.</p>

<p>I changed <code>for i in open(EMBEDDING_FILE, ""rb""):</code> present in the preparation of embedding matrix to <code>for i in open(EMBEDDING_FILE, 'r', encoding='utf-8'):</code>
and this solved the problem.</p>
",0,1,1685,2018-02-14 10:50:24,https://stackoverflow.com/questions/48785325/getting-embedding-matrix-of-all-zeros-after-performing-word-embedding-on-any-inp
How to search Word2Vec or GloVe Embedding to find words by semantic relationship,"<p>Common examples of showing Word Embedding's strength is to show semantic relationship between some words such <code>king:queen = male:female</code>. How can this type of relationship be discovered? Is that through some kind of visualization based on geometric clustering? Any pointer will be appreciated.</p>
","machine-learning, nlp, keras, word2vec, word-embedding","<p>If by ""discovered"" you mean <strong>supervised</strong> learning, there are <a href=""https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip"" rel=""nofollow noreferrer"">datasets</a> that contain lots of already extracted relationships, such as ""city-in-state"", ""capital-world"", ""superlative"", etc.</p>

<p><img src=""https://i.sstatic.net/e6KhY.png"" width=""360"" >
<img src=""https://i.sstatic.net/V4Eex.png"" width=""245"" ></p>

<p>This dataset is a popular choice for intrinsic evaluation of word vectors 
in completing word vector analogies. See also <a href=""https://stackoverflow.com/q/29591485/712995"">this question</a>.</p>

<p>Efficient <strong>unsupervised</strong> extraction of these relationships can be tricky. A naive algorithm requires O(n<sup>2</sup>) time and memory, where n is the number of words in a vocabulary, which is huge. In general, this problem boils down to efficient index construction.</p>

<p>But if you want just to train it yourself and play around with word embeddings, you can simply use <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim</a>:</p>



<pre class=""lang-py prettyprint-override""><code>model = gensim.models.word2vec.Word2Vec(sentences=sentences, size=100, window=4,
                                        workers=5, sg=1, min_count=20, iter=10)
word_vectors = model.wv
similar = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
# [(u'queen', 0.7188869714736938), (u'empress', 0.6739267110824585), ...
</code></pre>

<p>Note that you'll need a big corpus for that, such as <a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">text8</a>.</p>
",1,1,1088,2018-02-20 03:10:13,https://stackoverflow.com/questions/48877277/how-to-search-word2vec-or-glove-embedding-to-find-words-by-semantic-relationship
Can i build vocaburay in twice with gensim word2vec or doc2vec?,"<p>I have two different corpus and what i want is to train the model with both and to do it it I thought that it could be something like this:</p>

<pre><code>model.build_vocab(sentencesCorpus1)
model.build_vocab(sentencesCorpus2)
</code></pre>

<p>Would it be right?</p>
","python, word2vec, gensim, doc2vec","<p>No: each time you call <code>build_vocab(corpus)</code>, like that, it creates a fresh vocabulary from scratch â€“ discarding any prior vocabulary. </p>

<p>You can provide an optional argument to <code>build_vocab()</code>, <code>update=True</code>, which tries to add to the existing vocabulary. However:</p>

<ul>
<li><p>it wasn't designed/tested with <code>Doc2Vec</code> in mind, and as of right now (February 2018), using it with <code>Doc2Vec</code> is unlikely to work and often causes memory-fault crashes. (See <a href=""https://github.com/RaRe-Technologies/gensim/issues/1019"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1019</a>.)</p></li>
<li><p>it's still best to <code>train()</code> with all available data together - any sort of multiple-calls to <code>train()</code>, with differing data subsets each time, introduces other murky tradeoffs in model quality/correctness that are easy to get wrong. (And, when calling <code>train()</code>, be sure to provide correct values for its required parameters â€“ the practices shown in most online examples are typically only correct for the case where <code>build_vocab()</code> was called once, with exactly the same texts as later calling <code>train()</code>.)</p></li>
</ul>
",0,0,123,2018-02-22 17:53:38,https://stackoverflow.com/questions/48934154/can-i-build-vocaburay-in-twice-with-gensim-word2vec-or-doc2vec
How to find semantic similarity using gensim and word2vec in python,"<p>I have a list of words in my python programme. Now I need to iterate through this list and find out the semantically similar words and put them into another list. I have been trying to do this using gensim with word2vec but could find a proper solution.This is what I have implemeted up to now. I need a help on how to iterate through the list of words in the variable sentences and find the semantically similar words and save it in another list.</p>

<pre><code>import gensim, logging

import textPreprocessing, frequentWords , summarizer
from gensim.models import Word2Vec, word2vec

import numpy as np
from scipy import spatial

sentences = summarizer.sorteddict

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
</code></pre>
","python, machine-learning, nlp, word2vec, gensim","

<p>If you don't care about <em>proper</em> clusters, you can use this code:</p>

<pre class=""lang-py prettyprint-override""><code>similar = [[item[0] for item in model.most_similar(word)[:5]] for word in words]
</code></pre>

<hr>

<p>If you really want to clusterize the words, here are few notes:</p>

<ul>
<li>There can be several such clusters.</li>
<li>The number of clusters depends on a <em>hyperparameter</em>, some threshold. When the threshold is big, all of the words are similar and belong to the same cluster, when it's too small, none of them are.</li>
<li>Words can be naturally included <em>transitively</em> into a cluster, i.e. <code>A</code> is similar to <code>B</code> and <code>B</code> is similar to <code>C</code>, so all three should be in the same cluster. This means you'll have to implement some sort of graph traversal algorithm.</li>
<li>The performance greatly depends on the training corpus: only if it's large enough, gensim word2vec will be able to capture proper similarity. Gemnsim hyperparameters and text pre-processing thus also matter.</li>
</ul>

<p>Here's a naive and probably not very efficient algorithm and identifies clusters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
vocab = model.wv.vocab.keys()

threshold = 0.9
clusters = {}
for word in vocab:
  for similar_word, distance in model.most_similar(word)[:5]:
    if distance &gt; threshold:
      cluster1 = clusters.get(word, set())
      cluster2 = clusters.get(similar_word, set())
      joined = set.union(cluster1, cluster2, {word, similar_word})
      clusters[word] = joined
      clusters[similar_word] = joined
</code></pre>
",2,3,738,2018-02-26 14:36:55,https://stackoverflow.com/questions/48990935/how-to-find-semantic-similarity-using-gensim-and-word2vec-in-python
Sentence2vec and Word2vec involving stop words and Named Entities,"<p>I'm working on a NLP project, involving sentence2vec. I'm presuming I would be using pre-trained word embeddings for converting tokens into vectors and then proceeding to sentence embedding. </p>

<p>Since my sentence involves : 
stop words like <strong>can't, won't, aren't</strong> etc. which <em>NLTK</em> would reduce to <strong>{ca, wo, are} + not</strong>. <br> So I can't reduce them, and I don't want to remove them as stop words since sentences like mentioned below, should have different embedding. </p>

<p><strong>My name is Priyank <br>
My name is not Priyank</strong></p>

<p>Another Important doubt is that how to incorporate Named entities such as the name of a person like <strong>Mark K. Hogg</strong> in my sentence vector.  </p>
","python, nlp, word2vec, sentence-similarity","<p>you can remove the ones you do not want to be as stop words from this <a href=""https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK&#39;s%2520list%2520of%2520english%2520stopwords"" rel=""nofollow noreferrer"">list</a></p>

<pre><code># Open a file and read it into memory
file = open('words.txt')
text = file.read()

# Apply the stoplist to the text
clean = [word for word in text.split() if word not in stoplist]
</code></pre>
",1,0,1404,2018-02-27 14:16:27,https://stackoverflow.com/questions/49010792/sentence2vec-and-word2vec-involving-stop-words-and-named-entities
Skip-Gram implementation in tensorflow/models - Subsampling of Frequent Words,"<p>I have some experiments in mind related to skipgram model. So I have started to study and modify the optimized implementation in <strong>tensorflow/models</strong> repository in <code>tutorials/embedding/word2vec_kernels.cc</code>. Suddenly I came above the part where corpus subsampling is done.
According to TomÃ¡Å¡ Mikolov paper (<a href=""https://arxiv.org/abs/1310.4546"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1310.4546</a>, eq.5), the word should be <strong>kept</strong> with probability</p>

<p><a href=""https://i.sstatic.net/fIgsE.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fIgsE.gif"" alt=""eq1""></a></p>

<p>where <code>t</code> denotes threshold parameter (according to paper chosen as <code>10^-5</code>), and <code>f(w)</code> frequency of the word <code>w</code>,
but the code in <code>word2vec_kernels.cc</code> is following:</p>

<pre><code>float keep_prob = (std::sqrt(word_freq / (subsample_ * corpus_size_)) + 1) *
                  (subsample_ * corpus_size_) / word_freq;
</code></pre>

<p>which can be transformed into previously presented notation as
<a href=""https://i.sstatic.net/j5S6g.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j5S6g.gif"" alt=""eq2""></a></p>

<p>What is the motivation behind this change? Is it just to model 'some kind of relation' to corpus size into this formula? Or is it some transformation of the original formula? Was it chosen empirically?</p>

<p><em>Edit</em>: link to the mentioned file on github
<a href=""https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_kernels.cc"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_kernels.cc</a></p>
","c++, tensorflow, word2vec","<p>Okay so I guess that without <code>corpus_size</code>, the graph looks somewhat the same as the original formula. Corpus size adds a <strong>relation to the corpus size</strong> to the formula and also ""it works with the large numbers"", so we can compute discard/keep probability without normalizing word frequency to proper distribution.</p>
",0,1,383,2018-02-27 15:16:07,https://stackoverflow.com/questions/49012064/skip-gram-implementation-in-tensorflow-models-subsampling-of-frequent-words
Doc2vec: Only 10 docvecs in gensim doc2vec model?,"<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>

<p>The example of training data (length>10)</p>

<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]
</code></pre>

<p>with some pre-treatment</p>

<pre><code>doc_=[d.strip().split("" "") for d in doc]
doc_tagged = []
for i in range(len(doc_)):
  tagd = TaggedDocument(doc_[i],str(i))
  doc_tagged.append(tagd)
</code></pre>

<p>tagged docs</p>

<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],
  dtype='&lt;U32'), tags='117')
</code></pre>

<p>fit a doc2vec model</p>

<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(doc_tagged)
model.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)
</code></pre>

<p>then i get the final model</p>

<pre><code>len(model.docvecs)
</code></pre>

<p>the result is 10...</p>

<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.
So, my question is:
How to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)
Is <code>model.docvecs</code> designed to provide all training docvecs?</p>
","machine-learning, nlp, word2vec, gensim, doc2vec","<p>The bug is in this line:</p>



<pre class=""lang-py prettyprint-override""><code>tagd = TaggedDocument(doc[i],str(i))
</code></pre>

<p>Gensim's <code>TaggedDocument</code> accepts a <strong>sequence of tags</strong> as a second argument. When you pass a string <code>'123'</code>, it's turned into <code>['1', '2', '3']</code>, because it's treated as a <em>sequence</em>. As a result, all of the documents are tagged with just 10 tags <code>['0', ..., '9']</code>, in various combinations.</p>

<p>Another issue: you're defining <code>doc_</code> and never actually using it, so your documents will be split incorrectly as well.</p>

<p>Here's the proper solution:</p>

<pre class=""lang-py prettyprint-override""><code>docs = [doc.strip().split(' ') for doc in docs]
tagged_docs = [doc2vec.TaggedDocument(doc, [str(i)]) for i, doc in enumerate(docs)]
</code></pre>
",11,6,975,2018-02-28 03:14:02,https://stackoverflow.com/questions/49021389/doc2vec-only-10-docvecs-in-gensim-doc2vec-model
Word2vec with Conv1D for text classification confusion,"<p>I am doing text classification and plan to use word2vec word embeddings and pass it to Conv1D layers for text classification. I have a <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataframe</a> which contains the texts and corresponding labels(sentiments). I have used the gensim module and used word2vec algorithm to generate the word-embedding model. The code I used:</p>

<pre><code>import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']
df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)
model = Word2Vec(df_tokenized, min_count=1)
</code></pre>

<p>I plan to use CNN and use this word-embedding model. But how should I use this word-embedding model for my cnn? What should be my input?</p>

<p>I plan to use something like(obviously not with the same hyper-parameters):</p>

<pre><code>model = Sequential()
model.add(layers.Embedding(max_features, 128, input_length=max_len))
model.add(layers.Conv1D(32, 7, activation='relu'))
model.add(layers.MaxPooling1D(5))
model.add(layers.Conv1D(32, 7, activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(1))
</code></pre>

<p>Can somebody help me out and point me in the right direction? Thanks in advance.</p>
","python, keras, conv-neural-network, word2vec, multiclass-classification","<p>Sorry for the late response, I hope it is still useful for you.
Depending on your application you may need to download a specific wordembedding file, for example here yoou have the <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">Glove files</a> </p>

<pre><code>EMBEDDING_FILE='glove.6B.50d.txt'

embed_size = 50 # how big is each word vector
max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 100 # max number of words in a comment to use

word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i &gt;= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

#this is how you load the weights in the embedding layer
inp = Input(shape=(maxlen,))
x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
</code></pre>

<p>I took this code from <a href=""https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"" rel=""nofollow noreferrer"">Jeremy Howard</a>, I think this is all you need, if you want to load other file the process is pretty similar, usually you just have to change the loading file</p>
",2,1,1204,2018-03-01 11:29:00,https://stackoverflow.com/questions/49048758/word2vec-with-conv1d-for-text-classification-confusion
Error while doing text-classification in Keras,"<p>I am doing text classification in Keras. First, I am creating an embedding matrix with Word2Vec and passing it to Keras <code>Embedding</code> layer. Then I am running <code>Conv1D</code> on top of it. This is the <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataset</a> I am using. Here is my code below:</p>

<pre><code>from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D,GlobalMaxPooling1D
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import pandas as pd
from nltk.tokenize import word_tokenize


# def dataframe_to_list_of_words(df_name, col):
#   df = pd.read_csv(df_name)
#   lst = df[col].drop_duplicates().values.tolist()
#   tokenized_sents = [word_tokenize(i) for i in lst]
#   tokenized_sents_mod = [word for sublist in tokenized_sents for word in sublist]
#   return tokenized_sents_mod

# def convert_data_to_index(string_data, wv):
#     index_data = []
#     for word in string_data:
#         if word in wv:
#             index_data.append(wv.vocab[word].index)
#     return index_data

df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']

df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)


model = Word2Vec(df_tokenized, min_count=1,size=300)
##############
embedding_matrix = np.zeros((len(model.wv.vocab), 300))
for i in range(len(model.wv.vocab)):
#     print(model.wv.index2word[i])
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
################
labels=df['sentiment']
encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
labels_encoded= np_utils.to_categorical(encoded_Y)
#########################

maxlen=30
tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
data = pad_sequences(sequences, maxlen=37)
############################
embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                      weights=[embedding_matrix],trainable=False)
model=Sequential()
model.add(embeddings)
model.add(Conv1D(32,7,activation='relu'))
model.add(MaxPooling1D(5))
model.add(Conv1D(32,7,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(labels_encoded.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(data, labels, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>

<p>I am getting the following error when I am running my code:</p>

<pre><code>Error when checking target: expected dense_1 to have shape (None, 8) but got array with shape (19283, 1)
</code></pre>

<p>Can someone please help me?</p>
","python, nlp, keras, conv-neural-network, word2vec","<p>You have encoded the labels as categorical, but didn't actually used the result of this. Change:</p>

<pre><code>model.fit(data, labels, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>

<p>to ...</p>

<pre><code>model.fit(data, labels_encoded, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>
",1,1,116,2018-03-05 11:03:48,https://stackoverflow.com/questions/49109014/error-while-doing-text-classification-in-keras
Word2vec: distangling semantic from syntactic,"<p>I want to use pre-train word vectors (e.g., fasttest on Wikipedia) to find clusters of a set of words. However, in the list of words I have words like 'kindness', 'kind', 'kindly' and they fall in different clusters. That is sometimes words with similar part of speech are clusters together. I want to know how can I have word vectors that only captures meaning?</p>
","nlp, word2vec","<p>You can lemmatize or stem the words before using word2vec.</p>

<p><a href=""https://pypi.python.org/pypi/stemming/1.0"" rel=""nofollow noreferrer"">stemming</a> library has several such algorithm implemented.</p>
",0,2,181,2018-03-10 13:04:38,https://stackoverflow.com/questions/49209388/word2vec-distangling-semantic-from-syntactic
Word2Vec Python similarity,"<p>I made a word embedding with this code:</p>

<pre><code>with open(""text.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)
    model = Word2Vec(sentences,workers=4, window=5)
</code></pre>

<p>I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between <code>model[""word""]</code>,<code>model.wv.most_similar()</code>, <code>model.similar_by_vector()</code> and <code>model.similarity()</code>?
Which one should I use?</p>
","python, similarity, word2vec, gensim, word-embedding","<p>Edit: Maybe we should tag <code>gensim</code> here, because it is the library we are using</p>

<p>If you want to find the neighbours of both you can use
<code>model.wv.most_similar()</code> this will give you a dict (top n) for each word and its similarities for a given string (word). This method will calculate the cosine similarity between the word-vectors.</p>

<p>Note that the other methods you mentioned are deprecated in <code>3.4.0</code>, use <code>model.wv.similarity()</code> and <code>model.wv.similar_by_vector()</code> instead.</p>

<p>You can also use <code>model.wv.similar_by_vector()</code> to do the exact same thing but by passing a vector. Eg. <code>model[""woman""]</code> would give you such a vector. Actually if you look at the implementation, all the method does is call <code>most_similar()</code></p>

<pre><code>def similar_by_vector(self, vector, topn=10, restrict_vocab=None):
   return self.most_similar(positive=[vector], topn=topn, restrict_vocab=restrict_vocab)
</code></pre>

<p>Same goes for the <code>similar_by_word()</code> method. I actually don't know why these methods exist in the first place.</p>

<p>To find a similarity measure between exactly two words you can either use
<code>model.wv.similarity()</code> to find the cosine similarity or <code>model.wv.distance()</code> to find the cosine distance between the two.</p>

<p>To answer your actual question, I would simply compute the similarity between the two instead of comparing the results of <code>most_similar()</code>.</p>

<p>I hope this helps. Look at the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">docs</a> or the source files to get even more information, the code documentation is pretty good I think.</p>
",3,4,3373,2018-03-20 09:11:53,https://stackoverflow.com/questions/49380138/word2vec-python-similarity
Applying word2vec to find all words above a similarity threshold,"<p>The command model.most_similar(positive=['france'], topn=100) gives the top 100 most similar words to ""france"". However, I would like to know if there is a method which will output the most similar words above a similarity threshold to a given word. Is there a method like the following?:
model.most_similar(positive=['france'], threshold=0.9)</p>
","word2vec, gensim","<p>No, you'd have to request a large number (or all, with <code>topn=0</code>) then apply the cutoff yourself. </p>

<p>What you request could theoretically be added as an option. </p>

<p>However, the cosine-similarity absolute magnitudes don't necessarily have a stable meaning, like ""90% similar"" across different model runs. Their distribution can vary based on model training parameters, such as the vector <code>size</code>, and they are most-often interpreted only in ranked-comparison to other pairwise values from the same model. </p>

<p>For example, the composition of the top-100 most-similar words for 'cold' may be very similar in models with different training parameters, but the range of absolute similarity values for the #1 to #100 words can be quite different. So if you were picking an absolute threshold, you'd likely want to vary the cutoff based on observing the model, or along with other model training metaparameters.</p>
",3,7,2146,2018-03-20 18:22:22,https://stackoverflow.com/questions/49391597/applying-word2vec-to-find-all-words-above-a-similarity-threshold
Gensim Word2Vec most similar different result python,"<p>I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_1</code>; in the second, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_2</code>. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:</p>

<pre><code>import os
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

with open(""HarryPotter1.txt"", 'r') as original, \
        open(""HarryPotter1_1.txt"", 'w') as mod1, \
        open(""HarryPotter1_2.txt"", 'w') as mod2:

    data=original.read()
    data_1 = data.replace(""Hermione"", 'Hermione_1')
    data_2 = data.replace(""Hermione"", 'Hermione_2')
    mod1.write(data_1 + r""\n"")
    mod2.write(data_2 + r""\n"")

with open(""longText.txt"",'w') as longFile:
    with open(""HarryPotter1_1.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)
    with open(""HarryPotter1_2.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)


model = """"
word_vectors = """"
modelName = ""ModelTest""
vectorName = ""WordVectorsTestst""

answer2 = raw_input(""Overwrite  embeddig? (yes or n)"")
if(answer2 == 'yes'):
    with open(""longText.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)

    model = Word2Vec(sentences,workers=4, window=5,min_count=5)

    model.save(modelName)
    model.wv.save_word2vec_format(vectorName+"".bin"",binary=True)
    model.wv.save_word2vec_format(vectorName+"".txt"", binary=False)
    model.wv.save(vectorName)

    word_vectors = model.wv

else:
    model = Word2Vec.load(modelName)
    word_vectors = KeyedVectors.load_word2vec_format(vectorName + "".bin"", binary=True)

    print(model.wv.similarity(""Hermione_1"",""Hermione_2""))
    print(model.wv.distance(""Hermione_1"",""Hermione_2""))
    print(model.wv.most_similar(""Hermione_1""))
    print(model.wv.most_similar(""Hermione_2""))
</code></pre>

<p>How is possible that <code>model.wv.most_similar(""Hermione_1"")</code> and <code>model.wv.most_similar(""Hermione_2"")</code> give me different output? 
Their neighbour are completely different. This is the output of the four print:</p>

<pre><code>0.00799602753634
0.992003972464
[('moments,', 0.3204237222671509), ('rose;', 0.3189219534397125), ('Peering', 0.3185565173625946), ('Express,', 0.31800806522369385), ('no...', 0.31678506731987), ('pushing', 0.3131707012653351), ('triumph,', 0.3116190731525421), ('no', 0.29974159598350525), ('them?""', 0.2927379012107849), ('first.', 0.29270970821380615)]
[('go?', 0.45812922716140747), ('magical', 0.35565727949142456), ('Spells.""', 0.3554503619670868), ('Scabbets', 0.34701400995254517), ('cupboard.""', 0.33982667326927185), ('dreadlocks', 0.3325180113315582), ('sickening', 0.32789379358291626), ('First,', 0.3245708644390106), ('met', 0.3223033547401428), ('built', 0.3218075931072235)]
</code></pre>
","python, string, word2vec, gensim, word-embedding","<p>Training word2Vec models is random to an extent. That is why you may get different results. Also, <code>Hermione_2</code> starts appearing in the second half of the text data. In my understanding over the course of processing the data when the <code>Hermione_1</code> context is already established and so is the vector for this word you introduce a second word in exactly the same context and the algorithm tries to find what differentiates the two.
Secondly, you use a very short vector which may under-represent the complexity of the conceptual space. Due to the simplifications you get two vectors without any overlap.</p>
",1,0,966,2018-03-21 09:08:39,https://stackoverflow.com/questions/49402113/gensim-word2vec-most-similar-different-result-python
How can I load Word2vec with Gensim without getting an AttributeError?,"<p>I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: <em>xxxx.model.wv</em> and a bigger one <em>xxxx.model.wv.syn0.npy</em>.</p>

<p>When I call the following line:</p>

<pre><code>gensim.models.Word2Vec.load('xxxx.model.wv')
</code></pre>

<p>I get the following error:</p>

<pre><code>AttributeError: 'EuclideanKeyedVectors' object has no attribute 'negative'
</code></pre>

<p>How can I solve this error?</p>
","python, word2vec, gensim, word-embedding","<p>Are you sure your <code>xxxx.model.wv</code> file was a saved full <code>Word2Vec</code> model object? </p>

<p>That error suggests it was instead a <code>EuclideanKeyedVectors</code> â€“ just the vectors, and not a full model with all properties like <code>negative</code> â€“ so you might need to load it as that instead. </p>
",1,2,1204,2018-03-22 13:29:27,https://stackoverflow.com/questions/49429971/how-can-i-load-word2vec-with-gensim-without-getting-an-attributeerror
How to combine two pre-trained Word2Vec models?,"<p>I successfully followed deeplearning4j.org tutorial on Word2Vec, so I am able to load already trained model or train a new one based on some raw text (more specifically, I am using <code>GoogleNews-vectors-negative300</code> and <code>Emoji2Vec</code> pre-trained model). </p>

<p>However, I would like to combine these two above models for the following reason: Having a sentence (for example, a comment from Instagram or Twitter, which consists of emoji), I want to identify the emoji in the sentence and then map it to the word it is related to. In order to do that, I was planning to iterate over all the words in the sentence and calculate the closeness (how near the emoji and the word are located in the vector space).</p>

<p>I <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecUptrainingExample.java"" rel=""nofollow noreferrer"">found the code</a> how to uptrain the already existing model. However, it is mentioned that new words are not added in this case and only weights for the existing words will be updated based on a new text corpus. </p>

<p>I would appreciate any help or ideas on the problem I have. Thanks in advance!</p>
","java, nlp, emoji, word2vec, deeplearning4j","<p>Combining two models trained from different corpuses is not a simple, supported operation in the word2vec libraries with which I'm most familiar. </p>

<p>In particular, even if the same word appears in both corpuses, and even in similar contexts, the randomization that's used by this algorithm during initialization and training, and extra randomization injected by multithreaded training, mean that word may appear in wildly different places. It's only the <em>relative</em> distances/orientation with respect to other words that should be roughly similar â€“ not the specific coordinates/rotations. </p>

<p>So to merge two models requires translating one's coordinates to the other. That in itself will typically involve learning-a-projection from one space to the other, then moving unique words from a source space to the surviving space. I don't know if DL4J has a built-in routine for this; the Python <code>gensim</code> library has a <code>TranslationMatrix</code> example class in recent versions which can do this, as motivated by the use of word-vectors for language-to-language translations. </p>
",1,0,1940,2018-03-23 13:49:15,https://stackoverflow.com/questions/49451160/how-to-combine-two-pre-trained-word2vec-models
What does representation matrix of context word mean in SkipGram?,"<p>I am learning Stanford NLP Course and I have issue understanding a concept in Skipgram from picture below.</p>

<p><a href=""https://i.sstatic.net/0XwOX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0XwOX.jpg"" alt=""enter image description here""></a>
From left to right, the first column vector is one-hot encoder, the second is the word embedding matrix from 1-layer neural network, the third is word representation vector. However, when it comes to the fourth one, which is a matrix with 'v by d' dimension. Not sure if I listen it correctly, but the speaker said this is a representation of context word and these three matrix are identical? </p>

<p>My questions are:
1. Why these three matrix are identical but the three multiplication results are different?
2. How do we get this matrix (v by d dimension)?</p>

<p>The link to the lecture is: </p>

<p><a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=1481s</a></p>
","machine-learning, nlp, deep-learning, stanford-nlp, word2vec","<p>Before answering your question I have to add a bit of background for the sake of argument from previous slides. First, the optimization is on the probability of one word co-occurring with another word: the center word and a context word. The vector representations could be shared between these two but practically we have two collections of matrixes (list of word vectors) 1. center word vectors (first red matrix on the left) 2. context word vectors (three red matrices in the middle).</p>

<p>The picture in this question shows how we estimate the probabilities with the multiplication of two kinds of vectors and the softmax normalization. Now the questions:</p>

<blockquote>
  <ol start=""2"">
  <li>How do we get this matrix (v by d dimension)?</li>
  </ol>
</blockquote>

<p>As it was mentioned before, this can be the same matrix as word vectors but transposed. Or, you can imagine that we learning two vectors for each word: 1. center 2. context </p>

<p>The context word-vectors in calculations are used in its transposed form:</p>

<pre><code>(center words-vectors, v)  W : (d,V)
(outside words-vectors, uT) W': (V,d)
</code></pre>

<p><code>V</code> being the size of vocabulary and <code>d</code> the dimension size of the vectors. (these are the parameters which we want to learn from data)</p>

<p>Notice how dimensions change in each matrix multiplication:</p>

<pre><code>      W: (d,V)
      x: (V,1)
v = W.x: (d,1) 
     W': (V,d)
   W'.v: (V,1)
</code></pre>

<p><code>x</code> is the one-hot encoding of the center word, <code>W</code> is the list of all word vectors. <code>W.x</code> multiplication basically select the right word vector out of this list. The final result is a list of all possible dot-product of context word vector and the center word vector. The one-hot vector of the true observed context word selects the intended results. Then, based on the loss, updates will be backpropagated through the computation flow updating <code>W</code> and <code>W'</code>.</p>

<blockquote>
  <ol>
  <li>Why these three matrix are identical but the three multiplication results are different?</li>
  </ol>
</blockquote>

<p>The square and two rhombi in the middle are representing one matrix. The three multiplications are happening in three different observations. Although they represent the same matrix, on each observation parameters (<code>W</code> and <code>W'</code>) change using backpropagations. That is why the results are different on three multiplications. </p>

<p><strong>UPDATE FROM CHAT</strong>
However, your expectations are valid, the presentation could show exactly the same results in these multiplications. Because the objective function is the sum of all co-occurrence probabilities in one window. </p>
",2,3,1828,2018-03-25 06:02:28,https://stackoverflow.com/questions/49472999/what-does-representation-matrix-of-context-word-mean-in-skipgram
How to convert variable length string to vector?,"<p>I am working on classification algorithm and I get different string codes which have some pattern.</p>

<pre><code>|:-----------|------------:|:------------:|
| Column 1   | Column 2    | Column 3     |
|:-----------|------------:|:------------:|
| MN009      | JIK9PO      | LEFTu        |
| MN010      | JIK9POS     | LEFTu        |
| MN011      | JIK9POKI    | LEFTu        |
| MN012      | KIJU        | LEFTu        |
| MN013      | RANDOM      | LEFTu        |
| MN014      | FT          | LEFTu        |
|:-----------|------------:|:------------:|
</code></pre>

<p>For column 1 and 3 the feature set can be a vector length 5. </p>

<p>But I do not know how to create feature set which can accommodate column 2 as well.</p>

<p><strong>Considerations:</strong></p>

<ol>
<li>Create a feature vector of size equal to size of longest string
value and for smaller strings add some filler.</li>
<li>Truncate strings to a fixed length like 5 here and ignore extra
characters.</li>
</ol>

<p>Hope I am clear with the question. Thanks :)</p>
","machine-learning, classification, word2vec","<p>There are two solutions:</p>

<ol>
<li><p>The one you mentioned; predefine a length, zero-padding sequences
that fall short of it. This length can either be set to:</p>

<ul>
<li>the longest currently present sample (larger feature space â‡’ time / memory complexity consequences),</li>
<li><p>or to a shorter length (information loss â‡’ predictive power penalty). Information loss stems from either ignoring sequences
above that length or truncating them and using their cut-down
versions.</p>

<p>In both cases you should probably quantify the impact of your choice
(i.e. how much information have I discarded from my data by discarding/truncating, or how much larger is my problem space compared to if I used a smaller length).</p></li>
</ul></li>
<li>Dynamic graphs, essentially variable shape networks, can handle sequences of different sizes. Such capacities are offered by <a href=""http://pytorch.org/about/"" rel=""nofollow noreferrer"">PyTorch</a> and are (relatively) straightforward to implement (<a href=""https://stackoverflow.com/questions/48244053/pytorch-relation-between-dynamic-computational-graphs-padding-dataloader"">related</a> SO question)</li>
</ol>
",2,1,1031,2018-03-29 10:34:22,https://stackoverflow.com/questions/49553880/how-to-convert-variable-length-string-to-vector
How to use Keras Embedding layer when there are more than 1 text features,"<p>I understand how to use the Keras Embedding layer in case there is a single text feature like in IMDB review classification. However, I am confused how to use the Embedding Layers when I have a Classification problem, where there are more than a single text feature. For example, I have a dataset with 2 text features Diagnosis Text, and Requested Procedure and the label is binary class (1 for approved, 0 for not approved). In the example below, x_train has 2 columns Diagnosis and Procedure, unlike the IMDB dataset. Do I need to create 2 Embedding layers, one for Diagnosis, and Procedure? If so, what code changes would be required? </p>



<pre class=""lang-python prettyprint-override""><code>x_train = preprocessing.sequences.pad_sequences(x_train, maxlen=20)
x_test = preprocessing.sequences.pad_sequences(x_test, maxlen=20)
model = Sequential()
model.add(Embedding(10000,8,input_length=20)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid')
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
</code></pre>
","keras, word2vec, word-embedding","<p>You have some choices, you could concatenate the the two features into one
and create a single embedding for both of them. Here is the logic</p>

<pre class=""lang-python prettyprint-override""><code>all_features = np.hstack(X['diag'] + X['proc'])
X = pad_sequence(all_features, max_len)
# build model as usual, as you can see on a single embedding layer is
# needed.
</code></pre>
<p>or you can use the <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">Functional api</a> and build multiple input model</p>
<pre class=""lang-python prettyprint-override""><code>diag_inp = Input()
diag_emb = Embedding(512)(diag_input)
proc_inp = Input()
proc_emb = Embedding(512)(proc_input)

# concatenate them to makes a single vector per sample
merged = Concatenate()[diag_emb, proc_emb]
out = Dense(2,  activation='sigmoid')(merged)
model = Model(inputs=[diag_inp, proc_inp], outputs=[out])
</code></pre>
<p>That is you can learn an embedding for the concatenation or you can learn
multiple embeddings and concatenate them while training.</p>
",6,9,4312,2018-04-02 05:28:48,https://stackoverflow.com/questions/49605800/how-to-use-keras-embedding-layer-when-there-are-more-than-1-text-features
converting a sentence to an embedding representation,"<p>If I have a sentence, ex: â€œget out of hereâ€
And I want to use word2vec Embed. to represent it .. I found three different ways to do that:</p>

<p>1- for each word, we compute the AVG of its embedding vector, so each word replaced by a single value.</p>

<p>2- as in 1, but with using the standard deviation of the embedding vector values.</p>

<p>3- or by adding the Embed. vector as it is. So if I use 300 length embedding vector .. for the above example, I will have in the final a vector of (300 * 4 words) 1200 length as a final vector to represent the sentence.</p>

<p>Which one of them is most suitable .. ? specifically, for the sentence similarity applications ..  </p>
","word2vec, sentence-similarity","<p>The way you describe option (1) makes it sound like each word becomes a single number. That wouldn't work. </p>

<p>The simple approach that's often used is to average all word-vectors for words in the sentence together - so with 300-dimensional word-vectors, you still wind up with a 300-dimensional sentence-average vector. Perhaps that's what you mean by your option (1). </p>

<p>(Sometimes, all vectors are normalized to unit-length before this operation, but sometimes not - because the non-normalized vector lengths can sometimes indicate the strength of a word's meaning. Sometimes, word-vectors are weighted by some other frequency-based indicator of their relative importance, such as TF/IDF.)</p>

<p>I've never seen your option (2) used and don't quite understand what you mean or how it could possibly work. </p>

<p>Your option (3) would be better described as ""concatenating the word-vectors"". It gives different-sized vectors depending on the number of words in the sentence. Slight differences in word placement, such as comparing ""get out of here"" and ""of here get out"", would result in very different vectors, that usual methods of comparing vectors (like cosine-similarity) would not detect as being 'close' at all. So it doesn't make sense, and I've not seen it used. </p>

<p>So, only your option (1), as properly implemented to (weighted-)average word-vectors, is a good baseline for sentence-similarities. </p>

<p>But, it's still fairly basic and there are many other ways to compare sentences using text-vectors. Here are just a few:</p>

<p>One algorithm closely related to word2vec itself is called 'Paragraph Vectors', and is often called <code>Doc2Vec</code>. It uses a very word2vec-like process to train vectors for full ranges of text (whether they're phrases, sentences, paragraphs, or documents) that work kind of like 'floating document-ID words' over the full text. It sometimes offers a benefit over just averaging word-vectors, and in some modes can produce both doc-vectors and word-vectors that are also comparable to each other. </p>

<p>If your interest isn't just pairwise sentence similarities, but some sort of downstream classification task, then Facebook's 'FastText' refinement of word2vec has a classification mode, where the word-vectors are trained not just to predict neighboring words, but to be good at predicting known text classes, when simply added/averaged together. (Text-vectors constructed from such classification vectors might be good at similarities too, depending on how well the training-classes capture salient contrasts between texts.)</p>

<p>Another way to compute pairwise similarities, using just word-vectors, is ""Word Mover's Distance"". Rather than averaging all the word-vectors for a text together into a single text-vector, it considers each word-vector as a sort of ""pile of meaning"". Compared to another sentence, it calculates the minimum routing work (distance along lots of potential word-to-word paths) to move all the ""piles"" from one sentence into the configuration of another sentence. It can be expensive to calculate, but usually represents sentence-contrasts better than the simple single-vector-summary that naive word-vector averaging achieves. </p>
",3,3,2318,2018-04-04 17:04:55,https://stackoverflow.com/questions/49656630/converting-a-sentence-to-an-embedding-representation
"UnpicklingError: invalid load key, &#39;3&#39;","<p>I am creating a chatbot. So, i need word2vec file in binary format.
When i am loading bin file then i am getting this type of error.</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 975, in load
return super(Word2Vec, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 629, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 278, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 395, in load
obj = unpickle(fname)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 1302, in unpickle
return _pickle.load(f, encoding='latin1')_pickle.

UnpicklingError: invalid load key, '3'.
</code></pre>
","python-3.x, word2vec, gensim","<p>If it is a binary file you need to mention it like this:</p>

<pre><code>import gensim.models.keyedvectors as word2vec
model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
",16,9,9661,2018-04-05 15:21:59,https://stackoverflow.com/questions/49676060/unpicklingerror-invalid-load-key-3
How to predict word using trained CBOW,"<p>I have a question about CBOW prediction. Suppose my job is to use 3 surrounding words w(t-3), w(t-2), w(t-1)as input to predict one target word w(t). Once the model is trained and I want to predict a missing word after a sentence. Does this model only work for a sentence with four words which the first three are known and the last is unknown? If I have a sentence in 10 words. The first nine words are known, can I use 9 words as input to predict the last missing word in that sentence?</p>
","neural-network, nlp, deep-learning, word2vec, language-model","<p>Word2vec CBOW mode typically uses symmetric windows around a target word. But it simply averages the (current in-training) word-vectors for all words in the window to find the 'inputs' for the prediction neural-network. Thus, it is tolerant of asymmetric windows â€“ if there are fewer words are available on either side, fewer words on that side are used (and perhaps even zero on that side, for words at the front/end of a text).</p>

<p>Additionally, during each training example, it doesn't always use the maximum-window specified, but some random-sized window up-to the specified size. So for <code>window=5</code>, it will sometimes use just 1 on either side, and other times 2, 3, 4, or 5. This is done to effectively overweight closer words.</p>

<p>Finally and most importantly for your question, word2vec doesn't really do a full-prediction during training of ""what exact word does the model say should be heat this target location?"" In either the 'hierarchical softmax' or 'negative-sampling' variants, such an exact prediction can be expensive, requiring calculations of neural-network output-node activation levels proportionate to the size of the full corpus vocabulary. </p>

<p>Instead, it does the much-smaller number-of-calculations required to see how strongly the neural-network is predicting the actual target word observed in the training data, perhaps in contrast to a few other words. In hierarchical-softmax, this involves calculating output nodes for a short encoding of the one target word â€“ ignoring all other output nodes encoding other words. In negative-sampling, this involves calculating the one distinct output node for the target word, plus a few output nodes for other randomly-chosen words (the 'negative' examples). </p>

<p>In neither case does training know if this target word is being predicted in preference over all other words â€“ because it's not taking the time to evaluate all others words. It just looks at the current strength-of-outputs for a real example's target word, and nudges them (via back-propagation) to be slightly stronger. </p>

<p>The end result of this process is the word-vectors that are usefully-arranged for other purposes, where similar words are close to each other, and even certain relative directions and magnitudes also seem to match human judgements of words' relationships. </p>

<p>But the final word-vectors, and model-state, might still be just mediocre at predicting missing words from texts â€“ because it was only ever nudged to be better on individual examples. You could theoretically compare a model's predictions for every possible target word, and thus force-create a sort of ranked-list of predicted-words â€“ but that's more expensive than anything needed for training, and prediction of words like that isn't the usual downstream application of sets of word-vectors. So indeed most word2vec libraries don't even include any interface methods for doing full target-word prediction. (For example, the original word2vec.c from Google doesn't.)</p>

<p>A few versions ago, the Python <code>gensim</code> library added an experimental method for prediction, <code>[predict_output_word()][1]</code>. It only works for negative-sampling mode, and it doesn't quite handle window-word-weighting the same way as is done in training. You could give it a try, but don't be surprised if the results aren't impressive. As noted above, making actual predictions of words isn't the usual real goal of word2vec-training. (Other more stateful text-analysis, even just large co-occurrence tables, might do better at that. But they might not force word-vectors into interesting constellations like word2vec.)</p>
",4,2,1529,2018-04-09 17:18:34,https://stackoverflow.com/questions/49738313/how-to-predict-word-using-trained-cbow
Search for the nearest array in a huge array of arrays,"<p>I need to find the closest possible sentence.
I have an array of sentences and a user sentence, and I need to find the closest to the user's sentence element of the array.</p>

<p>I presented each sentence in the form of a vector using word2vec:</p>

<pre><code>def get_avg_vector(word_list, model_w2v, size=500):
    sum_vec = np.zeros(shape = (1, size))
    count = 0

    for w in word_list:
        if w in model_w2v and w != '':
            sum_vec += model_w2v[w]
            count +=1
    if count == 0:
        return sum_vec
    else:
        return sum_vec / count + 1
</code></pre>

<p>As a result, the array element looks like this:</p>

<pre><code>array([[ 0.93162371,  0.95618944,  0.98519795,  0.98580566,  0.96563747,
         0.97070891,  0.99079191,  1.01572807,  1.00631016,  1.07349398,
         1.02079309,  1.0064849 ,  0.99179418,  1.02865136,  1.02610303,
         1.02909719,  0.99350413,  0.97481178,  0.97980362,  0.98068508,
         1.05657591,  0.97224562,  0.99778703,  0.97888296,  1.01650529,
         1.0421448 ,  0.98731804,  0.98349052,  0.93752996,  0.98205837,
         1.05691232,  0.99914532,  1.02040555,  0.99427229,  1.01193818,
         0.94922226,  0.9818139 ,  1.03955   ,  1.01252615,  1.01402485,
         ...
         0.98990598,  0.99576604,  1.0903802 ,  1.02493086,  0.97395976,
         0.95563786,  1.00538653,  1.0036294 ,  0.97220088,  1.04822631,
         1.02806122,  0.95402776,  1.0048053 ,  0.97677222,  0.97830801]])
</code></pre>

<p>I represent the sentence of the user also as a vector, and I compute the closest element to it is like this:</p>

<pre><code>%%cython
from scipy.spatial.distance import euclidean

def compute_dist(v, list_sentences):
    dist_dict = {}

    for key, val in list_sentences.items():
        dist_dict[key] = euclidean(v, val)

    return sorted(dist_dict.items(), key=lambda x: x[1])[0][0]
</code></pre>

<p><code>list_sentences</code> in the method above is a dictionary in which keys are a text representation of sentences, and values are vector.</p>

<p>It takes a very long time, because I have more than 60 million sentences.
How can I speed up, optimize this process?</p>

<p>I'll be grateful for any advice.</p>
","python, arrays, performance, numpy, word2vec","<p>The initial calculation of the 60 million sentences' vectors is essentially a fixed cost you'll pay once. I'm assuming you mainly care about the time for each subsequent lookup, for a single user-supplied query sentence.</p>

<p>Using numpy native array operations can speed up the distance calculations over doing your own individual calculations in a Python loop. (It's able to do things in bulk using its optimized code.)</p>

<p>But first you'd want to replace <code>list_sentences</code> with a true numpy array, accessed only by array-index. (If you have other keys/texts you need to associate with each slot, you'd do that elsewhere, with some dict or list.) </p>

<p>Let's assume you've done that, in whatever way is natural for your data, and now have <code>array_sentences</code>, a 60-million by 500-dimension numpy array, with one sentence average vector per row.</p>

<p>Then a 1-liner way to get an array full of the distances is as the vector-length (""norm"") of the difference between each of the 60 million candidates and the 1 query (which gives a 60-million entry answer with each of the differences):</p>

<pre><code>dists = np.linalg.norm(array_sentences - v)  
</code></pre>

<p>Another 1-liner way is to use the numpy utility function <code>cdist()</code> for comuting distance between each pair of two collections of inputs. Here, your first collection is just the one query vector <code>v</code> (but if you had batches to do at once, supplying more than one query at a time could offer an additional slight speedup): </p>

<pre><code>dists = np.linalg.cdists(array[v], array_sentences)
</code></pre>

<p>(Note that such vector comparisons often use cosine-distance/cosine-similarity rather than euclidean-distance. If you switch to that, you might be doing other norming/dot-products instead of the first option above, or use the <code>metric='cosine'</code> option to <code>cdist()</code>.)</p>

<p>Once you have all the distances in a numpy array, using a numpy-native sort option is likely to be faster than using Python <code>sorted()</code>. For example, numpy's indirect sort <code>argsort()</code>, which just returns the sorted indexes (and thus avoids moving all the vector coordinates-around), since you just want to know <em>which</em> items are the best match(es). For example:</p>

<pre><code>sorted_indexes = argsort(dists)
best_index = sorted_indexes[0]
</code></pre>

<p>If you need to turn that int index back into your other key/text, you'd use your own dict/list that remembered the slot-to-key relationships.</p>

<p>All these still give an exactly right result, by comparing against all candidates, which (even when done optimally well) is still time-consuming. </p>

<p>There are ways to get faster results, based on pre-building indexes to the full set of candidates â€“ but such indexes become very tricky in high-dimensional spaces (like your 500-dimensional space). They often trade off perfectly accurate results for faster results. (That is, what they return for 'closest 1' or 'closest N' will have some errors, but usually not be off by much.) For examples of such libraries, see <a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">Spotify's ANNOY</a> or <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">Facebook's FAISS</a>. </p>
",2,3,918,2018-04-12 13:49:15,https://stackoverflow.com/questions/49798313/search-for-the-nearest-array-in-a-huge-array-of-arrays
Fasttext algorithm use only word and subword? or sentences too?,"<p>I read the paper and googled as well if there is any good example of the learning method(or more likely learning procedure)</p>

<p>For word2vec, suppose there is corpus sentence</p>

<blockquote>
  <p>I go to school with lunch box that my mother wrapped every morning</p>
</blockquote>

<p>Then with window size 2, it will try to obtain the vector for 'school' by using surrounding words </p>

<blockquote>
  <p>['go', 'to', 'with', 'lunch']</p>
</blockquote>

<p>Now, FastText says that it uses the subword to obtain the vector, so it is definitely use n gram subword, for example with n=3,</p>

<blockquote>
  <p>['sc', 'sch', 'cho', 'hoo', 'ool', 'school']</p>
</blockquote>

<p>Up to here, I understood.
But it is not clear that if the other words are being used for learning for 'school'. I can only guess that other surrounding words are used as well like the word2vec, since the paper mentions</p>

<p>=> the terms <em>Wc</em> and <em>Wt</em> are both used in functions</p>

<p>where Wc is context word and Wt is word at sequence t.</p>

<p>However, it is not clear that how FastText learns the vectors for word.</p>

<p>.</p>

<p>.</p>

<p>Please clearly explain how FastText learning process goes in procedure?</p>

<p>.</p>

<p>.</p>

<p>More precisely I want to know that if FastText also follows the same procedure as Word2Vec while it learns the n-gram characterized subword <strong><em>in addition</em></strong>. Or only n-gram characterized subword with word being used?</p>

<p>How does it vectorize the subword at initial? etc</p>
","nlp, vectorization, word2vec, word-embedding, fasttext","<p>Any context word has its candidate input vector assembled from the combination of both its full-word token and all its character-n-grams. So if the context word is 'school', and you're using 3-4 character n-grams, the in-training input vector is a combination of the full-word vector for <code>school</code>, <em>and</em> all the n-gram vectors for <code>['sch', 'cho', 'hoo', 'ool', 'scho', 'choo', 'hool']</code>.)</p>

<p>When that candidate vector is adjusted by training, <em>all</em> the constituent vectors are adjusted. (This is a little like how in word2vec CBOW, mode, all the <em>words</em> of the single average context input vector get adjusted together, when their ability to predict a single target output word is evaluated and improved.) </p>

<p>As a result, those n-grams that happen to be meaningful hints across many similar words â€“ for example, common word-roots or prefixes/suffixes â€“ get positioned where they confer that meaning. (Other n-grams may remain mostly low-magnitude noise, because there's little meaningful pattern to where they appear.)</p>

<p>After training, reported vectors for individual in-vocabulary words are also constructed by combining the full-word vector and all n-grams. </p>

<p>Then, when you also encounter an out-of-vocabulary word, to the extent it shares some or many n-grams with morphologically-similar in-training words, it will get a similar calculated vector â€“ and thus be better than nothing, in guessing what that word's vector should be. (And in the case of small typos or slight variants of known words, the synthesized vector may be pretty good.)</p>
",10,8,4426,2018-04-13 07:22:50,https://stackoverflow.com/questions/49811479/fasttext-algorithm-use-only-word-and-subword-or-sentences-too
gensim model return ids not related with input doc2vec,"<p>I created a model from mongodb db news and I tagged the documents by mongo collection id</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
i=0
docs=[]
for artical in lstcontent:
    doct = TaggedDocument(clean_str(artical), [lstids[i]])
    docs.append(doct)
    i+=1
</code></pre>

<p>after that I created the model by</p>

<pre><code>pretrained_emb='tweet_cbow_300/tweets_cbow_300'
saved_path = ""documentmodel/doc2vec_model.bin""
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)
model.save(saved_path)
</code></pre>

<p>when I using the model by the code :</p>

<pre><code>import gensim.models as g
import codecs
model=""documentmodel/doc2vec_model.bin""
start_alpha=0.01
infer_epoch=1000
m = g.Doc2Vec.load(model)
sims = m.docvecs.most_similar(['5aa94578094b4051695eeb10'])
sims
</code></pre>

<p>the output is</p>

<pre><code>[('5aa944c1094b4051695eeaef', 0.9255372881889343),
('5aa945c1094b4051695eeb1d', 0.9222575426101685),
('5aa94584094b4051695eeb12', 0.9210859537124634),
('5aa945d2094b4051695eeb20', 0.9083569049835205),
('5aa945c7094b4051695eeb1e', 0.905883252620697),
('5aa9458f094b4051695eeb14', 0.9054019451141357),
('5aa944c7094b4051695eeaf0', 0.9019848108291626),
('5aa94589094b4051695eeb13', 0.9012798070907593),
('5aa945b1094b4051695eeb1a', 0.9000773429870605),
('5aa945bc094b4051695eeb1c', 0.8999895453453064)]
</code></pre>

<p>the ids not related with 5aa94578094b4051695eeb10
where is my proplem !?</p>
","word2vec, gensim, cosine-similarity, doc2vec","<p>It looks like you might be providing a string as the <code>words</code> of your <code>TaggedDocument</code> texts. It should be a list-of-words. (If you supply a string, it will see it as a list-of-single-character-words, and try to run the algorithm as character-to-character predictions â€“ which won't lead to very good vectors.)</p>

<p>If you enable INFO level logging, and watch the output, you may see hints that this is the problem, in the form of a very small count of vocabulary words, dozens rather than tens-of-thousands. Or if that's not the problem, you may see other discrepancies that hint at what's going wrong. </p>

<p>Separate observations &amp; tips:</p>

<ul>
<li><p>you're using a 'pretrained_emb' argument that's not part of standard gensim. If you're using an unofficial variant, based on an older gensim, you might have other issues. Pretrained word embeddings are <em>not</em> necessary for <code>Doc2Vec</code> to work, and may not offer much benefit. (I would always try without any such extra complications, first, then only after you have a simple approach working as a baseline, try such added tweaks and always evaluate if they're really helping or not.)</p></li>
<li><p>it's unclear how many <code>iter</code> passes you're using, but 10-20 are typical values, perhaps more if your corpus is small and/or typical texts are short</p></li>
<li><p><code>dm=1, dm_concat=1</code> (PV-DM with a concatenative input layer) results in larger, slower models that may require much more data to become well-trained. It's not clear this <code>dm_concat=1</code> mode is ever worth the trouble. At best, it should be considered experimental. So I would get things working without it, before perhaps trying it as an advanced experiment. </p></li>
</ul>
",1,2,253,2018-04-14 09:10:00,https://stackoverflow.com/questions/49829787/gensim-model-return-ids-not-related-with-input-doc2vec
CNN on word vectors throws input dimension error,"<p>I have a dataframe with approximately 14560 word vectors of dimension 400. I have reshaped each vector in 20*20 and used 1 channel for applying a CNN so the dimension has become <code>(14560,20,20,1)</code>. When I try to fit the CNN model it throws an error.</p>

<p>Code:</p>

<pre><code>from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers import BatchNormalization
from keras.utils import np_utils
from keras import backend as K

model_cnn=Sequential()
model_cnn.add(Convolution2D(filters = 16, kernel_size = (3, 3), 
activation='relu',input_shape = (20, 20,1)))

model_cnn.compile(loss='categorical_crossentropy', optimizer = 'adadelta', 
metrics=[""accuracy""])

model_cnn.fit(x_tr_,y_tr_,validation_data=(x_te_,y_te))
</code></pre>

<p>Error:</p>

<blockquote>
  <p>Error when checking target: expected conv2d_6 to have 4 dimensions,
  but got array with shape (14560, 1). When I reshape train data to
  (14560,1,20,20) still it gives error as model receives input
  =(1,20,20) and required is (20,20,1).</p>
</blockquote>

<p>How do I fix it ? </p>
","machine-learning, neural-network, keras, word2vec, conv-neural-network","<h2>Problem</h2>

<p>The problem is not only with <code>x_tr</code> shape, which should be <code>(-1,20,20,1)</code> as correctly pointed out in another answer. It's also the network architecture itself. If you do <code>model_cnn.summary()</code>, you'll see the following:</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 18, 18, 16)        160       
=================================================================
Total params: 160
Trainable params: 160
Non-trainable params: 0
</code></pre>

<p>The output of the model is rank 4: <code>(batch_size, 18, 18, 16)</code>. It can't compute the loss when the labels are <code>(batch_size, 1)</code>.</p>

<h2>Solution</h2>

<p>The correct architecture must reshape the convolutional output tensor <code>(batch_size, 18, 18, 16)</code> to <code>(batch_size, 1)</code>. There can be many ways to do it, here's one:</p>



<pre class=""lang-py prettyprint-override""><code>model_cnn = Sequential()
model_cnn.add(Convolution2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(20, 20, 1)))
model_cnn.add(MaxPool2D(pool_size=18))
model_cnn.add(Flatten())
model_cnn.add(Dense(units=1))
model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""])
</code></pre>

<p>The summary:</p>

<pre class=""lang-py prettyprint-override""><code>Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 18, 18, 16)        160       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 1, 16)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 177
Trainable params: 177
Non-trainable params: 0
</code></pre>

<p>Note that I added max-pooling to reduce <code>18x18</code> feature maps to <code>1x1</code>, then flatten layer to squeeze the tensor to <code>(None, 16)</code> and finally the dense layer to output a single value. Also pay attention to the loss function: it's <code>sparse_categorical_crossentropy</code>. If you wish to do <code>categorical_crossentropy</code>, you have to do one-hot encoding and output not a single number, but the probability distribution over classes: <code>(None, classes)</code>.</p>

<p>By the way, also check that your validation arrays have valid shape.</p>
",2,3,183,2018-04-14 19:57:28,https://stackoverflow.com/questions/49835610/cnn-on-word-vectors-throws-input-dimension-error
Gensim build_vocab taking too long,"<p>I'm trying to train a doc2vec model using the gensim library on 50 million sentences of variable length.</p>

<p>Some tutorials (eg. <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a>) have a <code>model.build_vocab</code> step before the actual training process. This part has been running for 3 hours now without any updates.</p>

<p>Is this step necessary for the training process? Why could this step be taking so long since it's just a linear pass over the data?</p>

<p>Using gensim version 3.4.0 with python 3.6.0</p>
","python-3.x, nlp, word2vec, gensim","<p>The <code>build_vocab()</code> step to discover all words, then set-up the known-vocabulary structures, is required. (Though, if you supply your corpus as an argument to <code>Doc2Vec</code>, both the <code>build_vocab()</code> and <code>train()</code> will be done automatically.)</p>

<p>You should enable Python logging at the INFO level to see logged information about the progress of this, and other long-running gensim steps. This will help you see if progress is truly being made, or has stopped or slowed at some point. </p>

<p>If the vocabulary-discovery starts fast but then slows, perhaps your system has too little memory and has begun using very-slow virtual memory (swapping). If it seems to stop, perhaps there's a silent error in your method of reading the corpus. If it's just slow the whole way, perhaps there's something wrong with your method of reading the corpus. </p>
",5,3,2024,2018-04-22 05:18:53,https://stackoverflow.com/questions/49962749/gensim-build-vocab-taking-too-long
How to use vectors from Doc2Vec in Tensorflow,"<p>I am trying to use <code>Doc2Vec</code> to convert sentences to vectors, then use those vectors to train a tensorflow classifier.</p>

<p>I am a little confused at what tags are used for, and how to extract all of the document vectors from <code>Doc2Vec</code> after it has finished training.</p>

<p>My code so far is as follows:</p>

<pre><code>fake_data = pd.read_csv('./sentences/fake.txt', sep='\n')
real_data = pd.read_csv('./sentences/real.txt', sep='\n')
sentences = []

for i, row in fake_data.iterrows():
    sentences.append(TaggedDocument(row['title'].lower().split(), ['fake', len(sentences)]))

for i, row in real_data.iterrows():
    sentences.append(TaggedDocument(row['title'].lower().split(), ['real', len(sentences)]))

model = gensim.models.Doc2Vec(sentences)
</code></pre>

<p>I get vectors when I do <code>print(model.docvecs[1])</code> etc, but they are different every time I remake the model.</p>

<p>First of all: have I used <code>Doc2Vec</code> correctly?
Second: Is there a way I can grab all documents tagged 'real' or 'fake', then turn them into a numpy array and pass it into tensorflow?</p>
","python, tensorflow, nlp, word2vec, doc2vec","<p>I believe the <code>tag</code> that you use for each <code>TaggedDocument</code> is not what you expect. Doc2Vec algorithm is learning vector representations of the specified <em>tags</em> (some of which can be shared between the documents). So if your goal is simply to convert sentences to vectors, the recommended choice of a tag is some kind of unique sentence identifier, such as sentence index.</p>

<p>The learned model is then stored in <code>model.docvecs</code>. E.g., if you use sentence index as a tag, you can then get the 1st document vector by accessing <code>model.docvecs</code> for the tag <code>""0""</code>, the second document - for the tag <code>""1""</code>, and so on.</p>

<p>Example code:</p>



<pre class=""lang-py prettyprint-override""><code>documents = [doc2vec.TaggedDocument(sentence, ['real-%d' % i])
             for i, sentence in enumerate(sentences)]
model = doc2vec.Doc2Vec(documents, vector_size=10)  # 10 is just for illustration

# Raw vectors are stored in `model.docvecs.vectors_docs`.
# It's easier to access each one by the tag, which are stored in `model.docvecs.doctags`.
for tag in model.docvecs.doctags.keys():
  print(tag, model.docvecs[tag])  # Prints the learned numpy array for this tag
</code></pre>

<p>By the way, to control the model randomness, use <code>seed</code> parameter of <code>Doc2Vec</code> class.</p>
",0,2,844,2018-04-22 16:00:55,https://stackoverflow.com/questions/49967931/how-to-use-vectors-from-doc2vec-in-tensorflow
"Why does Tensorflow&#39;s sampled_softmax_loss force you to use a bias, when experts recommend no bias be used for Word2Vec?","<p>All the tensorflow implementations of Word2Vec that I have seen has a bias in the negative sampling softmax function, including on the official tensorflow website</p>

<p><a href=""https://www.tensorflow.org/tutorials/word2vec#vector-representations-of-words"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec#vector-representations-of-words</a></p>

<pre><code>loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))
</code></pre>

<p>This is from Google's free Deep Learning course <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</a></p>

<pre><code> loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))
</code></pre>

<p>However, from both Andrew Ng and Richard Socher's lectures, they do not include a bias in their negative sampling softmaxes. </p>

<p>Even where this idea originated, Mikolov states that:</p>

<blockquote>
  <p>biases are not used in the neural network, as no significant
  improvement of performance was observed - following the Occam's razor,
  the solution is as simple as it needs to be.</p>
</blockquote>

<p>Mikolov, T.: Statistical Language Models Based on Neural Networks, p. 29
<a href=""http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf"" rel=""nofollow noreferrer"">http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf</a></p>

<p>So why do the official tensorflow implementations have a bias, and why does there not seem to be an option to not include a bias in the sampled_softmax_loss function ?</p>
","tensorflow, deep-learning, word2vec, word-embedding","<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">exercise you link</a> defines <code>softmax_biases</code> to be zeros:</p>

<pre><code>softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
</code></pre>

<p>That is: they're not using any actual bias in their word2vec example.</p>

<p>The <code>sampled_softmax_loss()</code> function is generic and used for many neural-networks; its decision to require a <code>biases</code> argument is unrelated to what's best for one particular neural-network application (word2vec), and accommodates the word2vec case by allowing (as here) all zeros.</p>
",1,0,478,2018-04-24 20:36:26,https://stackoverflow.com/questions/50010376/why-does-tensorflows-sampled-softmax-loss-force-you-to-use-a-bias-when-experts
How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit,"<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
","python, nlp, word2vec, gensim, word-sense-disambiguation","<p>There's a method <code>[most_similar()][1]</code> that will report the words of the closest vectors, by cosine-similarity in the model's coordinates, to a given word. For example:</p>

<pre><code>similars = loaded_w2v_model.most_similar('bright')
</code></pre>

<p>However, Word2vec won't find strictly synonyms â€“ just words that were contextually-related in its training-corpus. These are often synonym-like, but also can be similar in other ways â€“ such as used in the same topical domains, or able to replace each other functionally. (In that last respect, sometimes the highly-similar word-vectors are for <em>antonyms</em>, because words like 'hot' and 'cold' appear in the same places, referring the the same aspect of something.)</p>

<p>Plain word2vec also doesn't deal with polysemy (that a token like 'bright' is both a word for 'well-lit' and a word for 'smart') well. So the list of most-similar words for 'bright' will include a mix from its alternate senses.</p>
",6,2,3500,2018-05-05 15:44:52,https://stackoverflow.com/questions/50191231/how-do-i-find-a-synonym-of-a-word-or-multi-word-paraphrase-using-the-gensim-tool
Understanding model.similarity in word2vec,"<p>Hello I am fairly new to word2vec, I wrote a small program to teach myself</p>

<pre><code>import gensim
from gensim.models import Word2Vec

sentence=[['Yellow','Banana'],['Red','Apple'],['Green','Tea']]
model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)
print(model.similarity('Yellow', 'Banana'))
</code></pre>

<p>The similarity came out to be:
-0.048776340629810115</p>

<p>My question is why not is the similarity between banana and yellow closer to 1 like .70 or something. What am I missing? Kindly guide me.</p>
",word2vec,"<p>Word2Vec doesn't work well on toy-sized examples â€“ it's the subtle push-and-pull of many varied examples of the same words that moves word-vectors to useful relative positions. </p>

<p>But also, especially, in your tiny tiny example, you've given the model 300-dimensional vectors to work with, and only a 6-word vocabulary. With so many parameters, and so little to learn, it can essentially 'memorize' the training task, quickly becoming nearly-perfect in its internal prediction goal â€“ and further, it can do that in many, many alternate ways, that may not involve much change from the word-vectors random initialization. So it is never forced to move the vectors to a useful position that provides generalized info about the words. </p>

<p>You can sometimes get somewhat meaningful results from small datasets by shrinking the vectors, and thus the model's free parameters, and giving the model more training iterations. So you could try <code>size=2, iter=20</code>. But you'd still want more examples than just a few, and more than a single occurrence of each word. (Even in larger datasets, the vectors for words with just a small number of examples tend to be poor - hence the default <code>min_count=5</code>, which should be increased even higher in larger datasets.)</p>

<p>To really see word2vec in action, aim for a training corpus of millions of words.</p>
",1,0,952,2018-05-07 04:37:30,https://stackoverflow.com/questions/50207170/understanding-model-similarity-in-word2vec
Document classification using word vectors,"<p>While I was classifying and clustering the documents written in natural language, I came up with a question ...</p>

<p>As word2vec and glove, and or etc, vectorize the word in distributed spaces, I wonder if there are any method recommended or commonly used for document vectorization <strong>USING word vectors.</strong></p>

<p>For example,</p>

<blockquote>
  <p>Document1: ""If you chase two rabbits, you will lose them both.""</p>
</blockquote>

<p>can be vectorized as,</p>

<blockquote>
  <p>[0.1425, 0.2718, 0.8187, .... , 0.1011]</p>
</blockquote>

<p>I know about the one also known as doc2vec, that this document has n dimensions just like word2vec. But this is 1 x n dimensions and I have been testing around to find out the limits of using doc2vec. </p>

<p>So, I want to know how other people apply the word vectors for applications with steady size. </p>

<p>Just stacking vectors with m words will be formed m x n dimensional vectors. In this case, the vector dimension <strong>will not be uniformed since dimension m will depends on the number of words in document</strong>. </p>

<blockquote>
  <p>If: [0.1018, ... , 0.8717]</p>
  
  <p>you: [0.5182, ... , 0.8981]</p>
  
  <p>..: [...]</p>
  
  <p>m th word: [...]</p>
</blockquote>

<p>And this form is not favorable form to run some machine learning algorithms such as CNN. What are the suggested methods to produce the document vectors in steady form using word vectors?</p>

<p>It would be great if it is provided with papers as well.</p>

<p>Thanks!</p>
","machine-learning, nlp, vectorization, word2vec, doc2vec","<p>The most simple approach to get a fixed-size vector from a text, when all you have is word-vectors, to average all the word-vectors together. (The vectors could be weighted, but if they haven't been unit-length-normalized, their raw magnitudes from training are <em>somewhat</em> of an indicator of their strength-of-single-meaning â€“ polysemous/ambiguous words tend to have vectors with smaller magnitudes.) It works OK for many purposes. </p>

<p>Word vectors can be specifically trained to be better at composing like this, if the training texts are already associated with known classes. Facebook's FastText in its 'classification' mode does this; the word-vectors are optimized as much or more for predicting output classes of the texts they appear in, as they are for predicting their context-window neighbors (classic word2vec). </p>

<p>The 'Paragraph Vector' technique, often called 'doc2vec', gives every training text a sort-of floating pseudoword, that contributes to every prediction, and thus winds up with a word-vector-like position that may represent that full text, rather than the individual words/contexts. </p>

<p>There are many further variants, including some based on deeper predictive networks (eg 'Skip-thought Vectors'), or slightly different prediction targets (eg neighboring sentences in 'fastSent'), or other genericizations that can even include a mixture of symbolic and numeric inputs/targets during training (an option in Facebook's StarSpace, which explores other entity-vectorization possibilities related to word-vectors and FastText-like classification needs). </p>

<p>If you don't need to collapse a text to fixed-size vectors, but just compare texts, there are also techniques like ""Word Mover's Distance"" which take the ""bag of word-vectors"" for one text, and another, and give a similarity score.  </p>
",4,2,1142,2018-05-08 03:21:58,https://stackoverflow.com/questions/50225323/document-classification-using-word-vectors
gensim: &#39;Doc2Vec&#39; object has no attribute &#39;intersect_word2vec_format&#39; when I load the Google pre-trained word2vec model,"<p>I get this error when I load the google pre-trained word2vec to train doc2vec model with my own data. Here is part of my code:</p>

<pre><code>model_dm=doc2vec.Doc2Vec(dm=1,dbow_words=1,vector_size=400,window=8,workers=4)
model_dm.build_vocab(document)
model_dm.intersect_word2vec_format('home/xxw/Downloads/GoogleNews-vectors-negative300.bin',binary=True)
model_dm.train(document)
</code></pre>

<p>But I got this error:</p>

<blockquote>
  <p>'Doc2Vec' object has no attribute 'intersect_word2vec_format'</p>
</blockquote>

<p>Can you help me with the error? I get the google model from <a href=""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</a>, and my gensim is the latest version I think.</p>
","word2vec, gensim, doc2vec","<p>A recent refactor made <code>Doc2Vec</code> no longer share a superclass with this method. You might be able to call the method on your <code>model_dm.wv</code> object instead, but I'm not sure. Otherwise you could look at the source and mimic the code to achieve the same effect, if you really need that step.</p>

<p>But note that <code>Doc2Vec</code> doesn't need word-vectors as input: it can learn everything it needs from your own training data. Whether word-vectors from elsewhere will help will depend on a lot of factors â€“ and the larger your own data is, or the more unique, the less preloaded vectors from elsewhere are likely to help, or even have any residual effect when your own training is done. </p>

<p>Other notes on your apparent setup:</p>

<ul>
<li><p><code>dbow_words=1</code> will have no effect in <code>dm=1</code> mode - that mode already inherently trains word-vectors. (It only has effect in <code>dm=0</code> DBOW mode, where it adds extra interleaved word-training, if you need word-vectors. Often plain DBOW, without word-vector training, is a fast and effective option.)</p></li>
<li><p>Recent versions of gensim require more arguments to train, and note that typical published work with this algorithm use 10-20 (or sometimes more) passes over the data (as can be specified to <code>train()</code> via the <code>epochs</code> argument), rather than the default (in some versions of gensim) of 5. </p></li>
</ul>
",2,2,7703,2018-05-08 15:28:24,https://stackoverflow.com/questions/50237247/gensim-doc2vec-object-has-no-attribute-intersect-word2vec-format-when-i-loa
Difference between most_similar and similar_by_vector in gensim word2vec?,"<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>

<p>Running them with one word gives identical results, for example:
model.most_similar(['obama']) and similar_by_vector(model['obama'])</p>

<p>but if I give it an equation:</p>

<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>gives:</p>

<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]
</code></pre>

<p>while with:</p>

<pre><code>q = model['king'] - model['man'] + model['woman']
model.similar_by_vector(q)
</code></pre>

<p>gives:</p>

<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]
</code></pre>

<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>

<p>Thanks!</p>
","nlp, word2vec, gensim","<p>The <code>most_similar</code> similar function retrieves the vectors corresponding to <code>""king""</code>, <code>""woman""</code> and <code>""man""</code>, and <em>normalizes</em> them before computing <code>king - man + woman</code> (<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L426"" rel=""noreferrer"">source code</a>: <code>use_norm=True</code>).</p>

<p>The function call <code>model.similar_by_vector(v)</code> just calls <code>model.most_similar(positive=[v])</code>. So the difference is due to <code>most_similar</code> having a behaviour depending on the type of input (string or vector).</p>

<p>Finally, when <code>most_similar</code> has string inputs, it removes the words from the output (that is why ""king"" does not appear in the results).</p>

<p>A piece of code to see the differences:</p>

<pre><code>&gt;&gt;&gt; un = False
&gt;&gt;&gt; v = model.word_vec(""king"", use_norm=un) + model.word_vec(""woman"", use_norm=un) - model.word_vec(""man"", use_norm=un)
&gt;&gt;&gt; un = True
&gt;&gt;&gt; v2 = model.word_vec(""king"", use_norm=un) + model.word_vec(""woman"", use_norm=un) - model.word_vec(""man"", use_norm=un)
&gt;&gt;&gt; model.most_similar(positive=[v], topn=6)
[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.6454660892486572), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722)]
&gt;&gt;&gt; model.most_similar(positive=[v2], topn=6)
[('king', 0.7992597222328186), ('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133)]
&gt;&gt;&gt; model.most_similar(positive=[""king"", ""woman""], negative=[""man""], topn=6)
[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044)]
</code></pre>
",11,9,10927,2018-05-10 14:44:57,https://stackoverflow.com/questions/50275623/difference-between-most-similar-and-similar-by-vector-in-gensim-word2vec
How do you compute the distance between text documents for k-means with word2vec?,"<p>I have recently been introduced to word2vec and I'm having some trouble trying to figure out how exactly it is used for k-means clustering. </p>

<p>I do understand how k-means works with tf-idf vectors. For each text document you have a vector of tf-idf values and after choosing some documents as initial cluster centers, you can use the euclidian distance to minimise the the distances between the vectors of the documents. Here's an <a href=""https://www.experfy.com/blog/k-means-clustering-in-text-data"" rel=""nofollow noreferrer"">example</a>.</p>

<p>However, when using word2vec, each word is represented as a vector. Does this mean that each document corresponds to a matrix? And if so, how do you compute the minimum distance w.r.t. other text documents? </p>

<p><strong>Question:</strong> How do you compute the distance between text documents for k-means with word2vec? </p>

<p><strong>Edit:</strong> To explain my confusion in a bit more detail, please consider the following code:</p>

<pre><code>vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(sentences_tfidf)
print(tfidf_matrix.toarray())

model = Word2Vec(sentences_word2vec, min_count=1)

word2vec_matrix = model[model.wv.vocab]
print(len(word2vec_matrix))
for i in range(0,len(word2vec_matrix)):
    print(X[i])
</code></pre>

<p>It returns the following code:</p>

<pre><code>[[ 0.          0.55459491  0.          0.          0.35399075  0.          0.
   0.          0.          0.          0.          0.          0.          0.437249
   0.35399075  0.35399075  0.35399075  0.        ]
 [ 0.          0.          0.          0.44302215  0.2827753   0.          0.
   0.          0.34928375  0.          0.          0.          0.34928375
   0.          0.2827753   0.5655506   0.2827753   0.        ]
 [ 0.          0.          0.35101741  0.          0.          0.27674616
   0.35101741  0.          0.          0.35101741  0.          0.35101741
   0.27674616  0.27674616  0.44809973  0.          0.          0.27674616]
 [ 0.40531999  0.          0.          0.          0.2587105   0.31955894
   0.          0.40531999  0.31955894  0.          0.40531999  0.          0.
   0.          0.          0.2587105   0.2587105   0.31955894]]
20
[  4.08335682e-03  -4.44161100e-03   3.92342824e-03   3.96498619e-03
   6.99949533e-06  -2.14108804e-04   1.20419310e-03  -1.29191438e-03
   1.64671184e-03   3.41688609e-03  -4.94929403e-03   2.90348311e-03
   4.23802016e-03  -3.01274913e-03  -7.36164337e-04   3.47558968e-03
  -7.02908786e-04   4.73567843e-03  -1.42914290e-03   3.17237526e-03
   9.36070050e-04  -2.23833631e-04  -4.03443904e-04   4.97530040e-04
  -4.82502300e-03   2.42140982e-03  -3.61089432e-03   3.37070058e-04
  -2.09900597e-03  -1.82093668e-03  -4.74618562e-03   2.41499138e-03
  -2.15628324e-03   3.43719614e-03   7.50159554e-04  -2.05973233e-03
   1.92534993e-03   1.96503079e-03  -2.02400610e-03   3.99564439e-03
   4.95056808e-03   1.47033704e-03  -2.80071306e-03   3.59585625e-04
  -2.77896033e-04  -3.21732066e-03   4.36303904e-03  -2.16396619e-03
   2.24438333e-03  -4.50925855e-03  -4.70488053e-03   6.30825118e-04
   3.81869613e-03   3.75767215e-03   5.01064525e-04   1.70175335e-03
  -1.26033701e-04  -7.43318116e-04  -6.74833194e-04  -4.76678275e-03
   1.53754558e-03   2.32421421e-03  -3.23472451e-03  -8.32759659e-04
   4.67014220e-03   5.15853462e-04  -1.15449808e-03  -1.63017167e-03
  -2.73897988e-03  -3.95627553e-03   4.04657237e-03  -1.79282576e-03
  -3.26930732e-03   2.85121426e-03  -2.33304151e-03  -2.01760884e-03
  -3.33597139e-03  -1.19233003e-03  -2.12347694e-03   4.36858647e-03
   2.00414215e-03  -4.23572073e-03   4.98410035e-03   1.79121632e-03
   4.81655030e-03   3.33247939e-03  -3.95260006e-03   1.19335402e-03
   4.61675343e-04   6.09758368e-04  -4.74696746e-03   4.91552567e-03
   1.74517138e-03   2.36604619e-03  -3.06009664e-04   3.62954312e-03
   3.56943789e-03   2.92139384e-03  -4.27138479e-03  -3.51175456e-03]
[ -4.14272398e-03   3.45513038e-03  -1.47538856e-04  -2.02292087e-03
  -2.96578306e-04   1.88684417e-03  -2.63865804e-03   2.69249966e-03
   4.57606697e-03   2.19206396e-03   2.01336667e-03   1.47434452e-03
   1.88332598e-03  -1.14452699e-03  -1.35678309e-03  -2.02636060e-04
  -3.26160830e-03  -3.95368552e-03   1.40415027e-03   2.30542314e-03
  -3.18884710e-03  -4.46776347e-03   3.96415358e-03  -2.07852037e-03
   4.98413946e-03  -6.43568579e-04  -2.53325375e-03   1.30117545e-03
   1.26555841e-03  -8.84680718e-04  -8.34991166e-04  -4.15050285e-03
   4.66807076e-04   1.71844949e-04   1.08140183e-03   4.37910948e-03
  -3.28412466e-03   2.09890743e-04   2.29888223e-03   4.70223464e-03
  -2.31004297e-03  -5.10134443e-04   2.57104915e-03  -2.55978899e-03
  -7.55646848e-04  -1.98197929e-04   1.20443532e-04   4.63618943e-03
   1.13036349e-05   8.16594984e-04  -1.65917678e-03   3.29331891e-03
  -4.97825304e-03  -2.03667139e-03   3.60272871e-03   7.44500838e-04
  -4.40325850e-04   6.38399797e-04  -4.23364760e-03  -4.56386572e-03
   4.77551389e-03   4.74880403e-03   7.06148741e-04  -1.24937459e-03
  -9.50689311e-04  -3.88551364e-03  -4.45985980e-03  -1.15060725e-03
   3.27067473e-03   4.54987818e-03   2.62327422e-03  -2.40981602e-03
   4.55576897e-04   3.19155119e-03  -3.84227419e-03  -1.17610034e-03
  -1.45622855e-03  -4.32460709e-03  -4.12792247e-03  -1.74557802e-03
   4.66075348e-04   3.39668151e-03  -4.00651991e-03   1.41077011e-03
  -7.89384532e-04  -6.56061340e-04   1.14822399e-03   4.12205653e-03
   3.60721885e-03  -3.11746349e-04   1.44255662e-03   3.11965472e-03
  -4.93455213e-03   4.80490318e-03   2.79991422e-03   4.93505970e-03
   3.69034940e-03   4.76422161e-03  -1.25827035e-03  -1.94680784e-03]
                                  ...

[ -3.92252317e-04  -3.66805331e-03   1.52376946e-03  -3.81564132e-05
  -2.57118000e-03  -4.46725264e-03   2.36480637e-03  -4.70252614e-03
  -4.18651942e-03   4.54758806e-03   4.38804098e-04   1.28351408e-03
   3.40470579e-03   1.00038981e-03  -1.06557179e-03   4.67202952e-03
   4.50591929e-03  -2.67829909e-03   2.57702312e-03  -3.65824508e-03
  -4.54068230e-03   2.20785337e-03  -1.00554363e-03   5.14690124e-04
   4.64830594e-03   1.91410910e-03  -4.83837258e-03   6.73376708e-05
  -2.37796479e-03  -4.45193471e-03  -2.60163331e-03   1.51159777e-03
   4.06868104e-03   2.55690538e-04  -2.54662265e-03   2.64597777e-03
  -2.62586889e-03  -2.71554058e-03   5.49281889e-04  -1.38776843e-03
  -2.94354092e-03  -1.13887887e-03   4.59292997e-03  -1.02300232e-03
   2.27600057e-03  -4.88117011e-03   1.95790920e-03   4.64376673e-04
   2.56658648e-03   8.90390365e-04  -1.40368659e-03  -6.40658545e-04
  -3.53228673e-03  -1.30717538e-03  -1.80223631e-03   2.94505036e-03
  -4.82233381e-03  -2.16079340e-03   2.58940039e-03   1.60595961e-03
  -1.22245611e-03  -6.72614493e-04   4.47060820e-03  -4.95934719e-03
   2.70283176e-03   2.93257344e-03   2.13279200e-04   2.59435410e-03
   2.98801321e-03  -2.79974379e-03  -1.49789048e-04  -2.53924704e-03
  -7.83207070e-04   1.18357304e-03  -1.27669750e-03  -4.16665291e-03
   1.40916929e-03   1.63017987e-07   1.36708119e-03  -1.26687710e-05
   1.24729215e-03  -2.50442210e-03  -3.20308795e-03  -1.41550787e-03
  -1.05747324e-03  -3.97984264e-03   2.25877413e-03  -1.28316227e-03
   3.60359484e-03  -1.97929185e-04   3.21712159e-03  -4.96298913e-03
  -1.83640339e-03  -9.90608009e-04  -2.03964626e-03  -4.87274351e-03
   7.24950165e-04   3.85614252e-03  -4.18979349e-03   2.73840013e-03]
</code></pre>

<p>Using tfidf, k-means would be implemented by the lines</p>

<pre><code>kmeans = KMeans(n_clusters = 5)
kmeans.fit(tfidf_matrix)
</code></pre>

<p>Using word2vec, k-means would be implemented by the lines</p>

<pre><code>kmeans = KMeans(n_clusters = 5)
kmeans.fit(word2vec_matrix)
</code></pre>

<p>(Here's an <a href=""http://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/"" rel=""nofollow noreferrer"">example</a> of k-means with word2vec). So in the first case, k-means gets a matrix with the tf-idf values of each word per document, while in the second case k-means gets a vector for each word. How can k-means cluster the documents in the second case if it just has the word2vec representations?</p>
","nlp, k-means, word2vec","<p>Since you are interested in clustering documents, probably the best you can do is to use the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec package</a>, which can prepare a vector for each one of your documents. Then you can apply any clustering algorithm to the set of your document vectors for further processing. If, for any reason, you want to use word vectors instead, there are a few things you can do. Here is a very simple method:</p>

<ol>
<li>For each document, collect all words with the highest TF-IDF values w.r.t. that document.</li>
<li>Average the Word2Vec vectors of those words to create a vector for the whole document</li>
<li>Apply your clustering on the averaged vectors.</li>
</ol>

<p>Don't try to average <em>all</em> the words in a document, it won't work. </p>
",1,3,628,2018-05-11 19:23:56,https://stackoverflow.com/questions/50298979/how-do-you-compute-the-distance-between-text-documents-for-k-means-with-word2vec
"Gensim: &quot;C extension not loaded, training will be slow.&quot;","<p>I am running gensim on Linux Suse. I can start my python program but on startup I get: </p>

<blockquote>
  <p>C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>GCC is installed. Does anyone know what I have to do?</p>
","pip, word2vec, gensim, opensuse, suse","<p>Try the following:</p>

<p><strong>Python 3.x</strong></p>

<pre><code>$ pip3 uninstall gensim    
$ apt-get install python3-dev build-essential      
$ pip3 install --upgrade gensim
</code></pre>

<p><strong>Python 2.x</strong></p>

<pre><code>$ pip uninstall gensim    
$ apt-get install python-dev build-essential      
$ pip install --upgrade gensim
</code></pre>
",4,3,7439,2018-05-12 13:22:37,https://stackoverflow.com/questions/50306710/gensim-c-extension-not-loaded-training-will-be-slow
Extract main feature of paragraphs using word2vec,"<p>I just got a hold of Google's word2vec model and am quite new to the concept. i am trying to extract the main feature of a paragraph using the following method.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('../../usr/myProject/word2vec/GoogleNews-vectors-negative300.bin', binary=True)

...

for para in paragraph_array:
    para_name = ""para_""+ file_name + '{0}'
    sentence_array = d[para_name.format(number_of_paragraphs)] = []

    # Split Paragraph on basis of '.' or ? or !.
    for l in re.split(r""\.|\?|\!"", para):
        # Split line into list using space.
        sentence_array.append(l)
        #sentence_array.append(l.split("" ""))

     print (model.wv.most_similar(positive=para, topn = 1))
</code></pre>

<p>But am getting the following error where it says that the paragraph checked is not a word in the vocabulary.</p>

<blockquote>
  <p>KeyError: 'word \'The Republic of Ghana is a country in West Africa. It borders CÃ´te d\'Ivoire (also known as Ivory Coast) to the west, Burkina Faso to the north, Togo to the east, and the Gulf of Guinea to the south. The word ""Ghana"" means ""Warrior King"", Jackson, John G. Introduction to African Civilizations, 2001. Page 201.  and was the source of the name ""Guinea"" (via French Guinoye) used to refer to the West African coast (as in Gulf of Guinea).\' not in vocabulary'</p>
</blockquote>

<p>Now I am aware that the <code>most_similar()</code> function expects a single array. But I would like to know how this can be translated to extract one main feature or word that displays the  main concept of the paragraph using the word2vec model.</p>

<p><strong>Modified</strong></p>

<p>I modified the above code to pass the word_array into the <code>most_similar()</code> method and I' getting the following error.</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/new_approach.py"", line 108, in 
      print(model.wv.most_similar(positive=word_array, topn=1))
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 361, in most_similar
      for word, weight in positive + negative:
  ValueError: too many values to unpack (expected 2)</p>
</blockquote>

<p><strong>Modified Implementation</strong></p>

<pre><code>for sentence in sentence_array:
    if sentence:
        for w in re.split(r""\.|\?|\!|\@|\#|\$|\%|\^|\&amp;|\*|\(|\)|\-"",   sentence):
            split_word = w.split("" "")
            if split_word:
                word_array.append(split_word)
print(model.wv.most_similar(positive=word_array, topn=1))
</code></pre>

<p>Any suggestions in this regard are much appreciated.</p>
","python, word2vec, feature-extraction","<p>I reworked the entire code adding checkpoints to avoid the storage of empty strings to objects at each level starting from paragraphs, sentences down to words.</p>

<p><strong>Working Version</strong></p>

<pre><code>for file_name in files:
    file_identifier = file_name
    file_array = file_dictionary[file_identifier] =[]
    #file_array = file_dictionary[file_name.format((file_count))] = []
    file_path = directory_path+'/'+file_name

    with open(file_path) as f:
        #Level 2 Intents : Each file's main intent (One for each file)
        first_line = f.readline()
        print ()
        print(""Level 2 Intent for "", c, "" : "", first_line)

        #Level 3 Intents : Each paragraph's main intent (one for each para)

        paragraph_count = 0

        data = f.read()
        splat = data.split(""\n"")
        paragraph_array = []

        for number, paragraph in enumerate(splat, 1):
            paragraph_identifier = file_name + ""_paragraph_"" + str(paragraph_count)
            #print(paragraph_identifier)
            paragraph_array = paragraph_dictionary[paragraph_identifier.format(paragraph_count)] = []
            if paragraph :
                paragraph_array.append(paragraph)
            paragraph_count += 1
            if len(paragraph_array) &gt;0 :
                file_array.append(paragraph_array)

            # Level 4 Intents : Each sentence's main intent (one for each sentence)

            sentence_count = 0
            sentence_array = []

            for sentence in paragraph_array:
                for line in re.split(r""\.|\?|\!"", sentence):
                    sentence_identifier = paragraph_identifier + ""_sentence_"" + str(sentence_count)
                    sentence_array = sentence_dictionary[sentence_identifier.format(sentence_count)] = []
                    if line :
                        sentence_array.append(line)
                        sentence_count += 1

                    # Level 5 Intents : Each word with a certain level of prominance (one for each prominant word)

                    word_count = 0
                    word_array = []

                    for words in sentence_array:
                        for word in re.split(r"" "", words):
                            word_identifier = sentence_identifier + ""_word_"" + str(word_count)
                            word_array = word_dictionary[word_identifier.format(word_count)] = []

                            if word :
                                word_array.append(word)
                                word_count += 1
</code></pre>

<p><strong>Code to access dictionary items</strong></p>

<pre><code>#Accessing any paragraph array can be done as follows
print (paragraph_dictionary['S08_set4_a5.txt.clean_paragraph_4'])

#Accessing any sentence corresponding to a paragraph
print (sentence_dictionary['S08_set4_a5.txt.clean_paragraph_4_sentence_1'])

#Accessing any word corresponding to a sentence
print (word_dictionary['S08_set4_a5.txt.clean_paragraph_4_sentence_1_word_3'])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>['Celsius was born in Uppsala in Sweden. He was professor of astronomy at Uppsala University from 1730 to 1744, but traveled from 1732 to 1735 visiting notable observatories in Germany, Italy and France.']
[' He was professor of astronomy at Uppsala University from 1730 to 1744, but traveled from 1732 to 1735 visiting notable observatories in Germany, Italy and France']
['of']
</code></pre>
",1,0,566,2018-05-16 11:53:32,https://stackoverflow.com/questions/50370240/extract-main-feature-of-paragraphs-using-word2vec
How to generate Word2vec Vectors in Python?,"<p>I am trying to generate Word2vec vectors.</p>

<p>I have pandas data frame.</p>

<p>I transformed it into tokens.</p>

<p><code>df[""token""]</code></p>

<p>Used Word2vec from gensim.models</p>

<pre><code>model = w2v.Word2Vec(
sentences=df[""token""],
seed=seed,
workers=num_workers,
size=num_features,
min_count=min_word_count,
window=context_size,
sample=downsampling
)
</code></pre>

<p>How do I transform my dataframe df now?</p>

<p>That is what is the equivalent of doing </p>

<pre><code>model.transform(df)
</code></pre>
","python, neural-network, nlp, text-mining, word2vec","<p>If your dataframe is composed only of words, you could just make</p>

<pre><code>df['new_column'] = model[df['words']]
</code></pre>

<p><code>model['word']</code> or <code>model[list()]</code> both give you the vector representation of your <code>word</code> or of your <code>list</code></p>
",1,2,3467,2018-05-16 14:11:43,https://stackoverflow.com/questions/50373248/how-to-generate-word2vec-vectors-in-python
"In spacy, how to use your own word2vec model created in gensim?","<p>I have trained my own word2vec model in gensim and I am trying to load that model in spacy. First, I need to save it in my disk and then try to load an init-model in spacy but unable to figure out exactly how.</p>

<pre><code>gensimmodel
Out[252]:
&lt;gensim.models.word2vec.Word2Vec at 0x110b24b70&gt;

import spacy
spacy.load(gensimmodel)

OSError: [E050] Can't find model 'Word2Vec(vocab=250, size=1000, alpha=0.025)'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
","model, word2vec, gensim, spacy","<p>Train and save your model in plain-text format:</p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

path = get_tmpfile(""./data/word2vec.model"")

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.wv.save_word2vec_format(""./data/word2vec.txt"")
</code></pre>

<p>Gzip the text file:</p>

<pre><code>gzip word2vec.txt
</code></pre>

<p>Which produces a <code>word2vec.txt.gz</code> file.</p>

<p>Run the following command:</p>

<pre><code>python -m spacy init-model en ./data/spacy.word2vec.model --vectors-loc word2vec.txt.gz
</code></pre>

<p>Load the vectors using:</p>

<pre><code>nlp = spacy.load('./data/spacy.word2vec.model/')
</code></pre>
",26,19,12324,2018-05-22 11:32:20,https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim
Use a pre trained model with text2vec?,"<p>I would like to use a pre trained model with text2vec. My understanding was that the benefit here is that these models have been trained on a huge volume of data already, e.g. <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google News Model</a>.</p>

<p>Reading the text2vec <a href=""https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html"" rel=""nofollow noreferrer"">documentation</a> it looks like the getting started code reads in text data then trains a model with it:</p>

<pre><code>library(text2vec)
text8_file = ""~/text8""
if (!file.exists(text8_file)) {
  download.file(""http://mattmahoney.net/dc/text8.zip"", ""~/text8.zip"")
  unzip (""~/text8.zip"", files = ""text8"", exdir = ""~/"")
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
</code></pre>

<p>The documentation then proceeds to show one how to create tokens and a vocab:</p>

<pre><code># Create iterator over tokens
tokens &lt;- space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab &lt;- create_vocabulary(it)
vocab &lt;- prune_vocabulary(vocab, term_count_min = 5L)
# Use our filtered vocabulary
vectorizer &lt;- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L)
</code></pre>

<p>Then, this looks like the step to fit the model:</p>

<pre><code>glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
</code></pre>

<p>My question is, is the well know Google pre trained word2vec model usable here without the need to rely on my own vocab or my own local device to train the model? If yes, how could I read it in and use it in r?</p>

<p>I think I'm misunderstanding or missing something here? Can I use text2vec for this task?</p>
","r, nlp, word2vec, text2vec","<p>At the moment <code>text2vec</code> doesn't provide any functionality for downloading/manipulating pre-trained word embeddings. 
I have a drafts to add such utilities to the next release.</p>

<p>But on other side you can easily do it manually with just standard R tools. For example here is how to read <a href=""https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md#models"" rel=""nofollow noreferrer"">fasttext</a> vectors:</p>

<pre><code>con = url(""https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.af.300.vec.gz"", ""r"")
con = gzcon(con)
wv = readLines(con, n = 10)
</code></pre>

<p>Then you need just to parse it - <code>strsplit</code> and <code>rbind</code> are your friends.</p>
",2,2,2145,2018-05-28 15:18:43,https://stackoverflow.com/questions/50569420/use-a-pre-trained-model-with-text2vec
UnicodeDecodeError error when loading word2vec,"<p><strong>Full Description</strong></p>

<p>I am starting to work with word embedding and found a great amount of information about it. I understand, this far, that I can train my own word vectors or use previously trained ones, such as Google's or Wikipedia's, which are available for the English language and aren't useful to me, since I am working with texts in <em>Brazilian Portuguese</em>. Therefore, I went on a hunt for pre-trained word vectors in Portuguese and I ended up finding <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan's List of Pretrained Word Embeddings</a> which led me to Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> from which I learned about Rami Al-Rfou's <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>. After downloading both, I unsuccessfully have been trying to simply load the word vectors.</p>

<p><strong>Short Description</strong></p>

<p>I can't load pre-trained word vectors; I am trying <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> and <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>.</p>

<p><strong>Downloads</strong></p>

<ul>
<li><a href=""https://drive.google.com/open?id=0B0ZXk88koS2KRDcwcV9IVWFTeUE"" rel=""nofollow noreferrer"">Kyubyong's pre-trained word2vector format word vectors for Portuguese</a>;</li>
<li><a href=""https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c1ch6rdnp89glqmi8g81ev2somslu7cs/1527537600000/10341224892851088318/*/0B5lWReQPSvmGNEh0VTdmSHlHZ1k?e=download"" rel=""nofollow noreferrer"">Polyglot's pre-trained word vectors for Portuguese</a>;</li>
</ul>

<p><strong>Loading attempts</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
First attempt: using Gensim as suggested by <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan</a>;</p>

<pre><code>from gensim.models import KeyedVectors
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
word_vectors = KeyedVectors.load_word2vec_format(kyu_path, binary=True)
</code></pre>

<p>And the error returned:</p>

<pre><code>[...]
File ""/Users/luisflavio/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The zip downloaded also contains other files but all of them return similar errors.</p>

<p><em><a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a></em>
First attempt: following <a href=""http://nbviewer.jupyter.org/gist/aboSamoor/6046170"" rel=""nofollow noreferrer"">Al-Rfous's instructions</a>;</p>

<pre><code>import pickle
import numpy
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
words, embeddings = pickle.load(open(pol_path, 'rb'))
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/Desktop/Python/w2v_loading_tries.py"", line 14, in &lt;module&gt;
    words, embeddings = pickle.load(open(polyglot_path, ""rb""))

UnicodeDecodeError: 'ascii' codec can't decode byte 0xd4 in position 1: ordinal not in range(128)
</code></pre>

<p>Second attempt: using <a href=""https://polyglot.readthedocs.io/en/latest/Embeddings.html"" rel=""nofollow noreferrer"">Polyglot's word embedding load function</a>;</p>

<p>First, we have to install polyglot via pip:</p>

<pre><code>pip install polyglot
</code></pre>

<p>Now we can import it:</p>

<pre><code>from polyglot.mapping import Embedding
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
embeddings = Embedding.load(polyglot_path)
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/anaconda3/lib/python3.6/codecs.py"", line 321, in decode
(result, consumed) = self._buffer_decode(data, self.errors, final)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p><strong>Extra Information</strong></p>

<p>I am using python 3 on MacOS High Sierra.</p>

<p><strong>Solutions</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
As pointed out by <a href=""https://stackoverflow.com/a/50579950?noredirect=1"">Aneesh Joshi</a>, the correct way to load Kyubyong's model is by calling the native load function of Word2Vec.</p>

<pre><code>from gensim.models import Word2Vec
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
model = Word2Vec.load(kyu_path)
</code></pre>

<p>Even though I am more than grateful for Aneesh Joshi solution, polyglot seems to be a better model for working with Portuguese. Any ideas about that one?</p>
","python, word2vec, gensim, python-unicode, polyglot","<p>For Kyubyong's pre-trained word2vector .bin file:
it may have been saved using gensim's save function.</p>

<p>""load the model with <code>load()</code>. Not <code>load_word2vec_format</code> (that's for the C-tool compatibility).""</p>

<p>i.e., <code>model = Word2Vec.load(fname)</code></p>

<p>Let me know if that works.</p>

<p>Reference : <a href=""https://groups.google.com/forum/#!msg/gensim/gZ0BRnETMnw/m4WnXSyZFdcJ"" rel=""nofollow noreferrer"">Gensim mailing list</a></p>
",4,4,3791,2018-05-28 20:25:36,https://stackoverflow.com/questions/50573054/unicodedecodeerror-error-when-loading-word2vec
How do I limit word length in FastText?,"<p>I am using FastText to compute skipgrams on a corpus containing a long sequence of characters with no spaces. After an hour or so, FastText produces a model containing vectors (of length 100) corresponding to ""words"" of length 50 characters from the corpus.</p>

<p>I tried setting <b>-minn</b> and <b>-maxn</b> parameters, but that does not help (I kind of knew it won't, but tried anyway), and <b>-wordNgrams</b> parameter only applies if there are spaces, I guess (?!). This is just a long stream of characters representing state, without spaces.</p>

<p>The documentation doesn't seem to have any information on this <i>(or perhaps I'm missing something?)</i></p>
","nlp, word2vec, fasttext","<p>The tool just takes whatever space-delimited tokens you feed it. </p>

<p>If you want to truncate, or discard, tokens that are longer than 50 characters (or any other threshold), you'd need to preprocess the data yourself. </p>

<p>(If your question is actually something else, add more details to the question showing example lines from your corpus, how you're invoking fasttext on it, how you are reviewing unsatisfactory results, and how you would expect satisfactory results to look instead.</p>
",1,0,706,2018-05-31 14:56:52,https://stackoverflow.com/questions/50627237/how-do-i-limit-word-length-in-fasttext
Why word2bits RAM usage like word2vec?,"<p>I am using word2bits for large datasets. Actually, what I observed was word2bits occupying RAM like word2vec. So, I thought that what will happen for a small dataset.</p>

<p>Because of this purpose, I ran the word2bits example which mentioned in Readme (<a href=""https://github.com/agnusmaximus/Word2Bits).I"" rel=""nofollow noreferrer"">https://github.com/agnusmaximus/Word2Bits).I</a> observed the RAM usage in htop.
<a href=""https://i.sstatic.net/xaCxQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xaCxQ.png"" alt="" ""></a></p>

<p>When I ran this text8 on word2vec. It is also occupying the same RAM.</p>
",word2vec,"<p>The word2bits use RAM like word2vec. This issue was solved in the GitHub issues (github.com/agnusmaximus/Word2Bits/issues/7)</p>
",0,0,84,2018-06-01 04:52:20,https://stackoverflow.com/questions/50636287/why-word2bits-ram-usage-like-word2vec
Using gensim word2vec in scikit-learn pipeline,"<p>I am trying to use <code>word2vec</code> in a scikit-learn pipeline.</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np

class ItemSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
pipeline_word2vec = Pipeline([
                ('selector', ItemSelector(key='X')),
                ('w2v', W2VTransformer()),
            ])

pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))
</code></pre>

<p>this gives me </p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-9e2dd309d07c&gt; in &lt;module&gt;()
     23                 ('w2v', W2VTransformer()),
     24             ])
---&gt; 25 pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))

/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py in fit(self, X, y)
     62             sg=self.sg, hs=self.hs, negative=self.negative, cbow_mean=self.cbow_mean,
     63             hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word, trim_rule=self.trim_rule,
---&gt; 64             sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
     65         )
     66         return self

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)
    525             batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window, seed=seed,
    526             hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 527             fast_version=FAST_VERSION)
    528 
    529     def _do_train_job(self, sentences, alpha, inits):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in __init__(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    336             self.train(
    337                 sentences, total_examples=self.corpus_count, epochs=self.epochs, start_alpha=self.alpha,
--&gt; 338                 end_alpha=self.min_alpha, compute_loss=compute_loss)
    339         else:
    340             if trim_rule is not None:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    609             sentences, total_examples=total_examples, total_words=total_words,
    610             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 611             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    612 
    613     def score(self, sentences, total_sentences=int(1e6), chunksize=100, queue_factor=2, report_delay=1):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    567             sentences, total_examples=total_examples, total_words=total_words,
    568             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 569             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    570 
    571     def _get_job_params(self, cur_epoch):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)
    239             epochs=epochs,
    240             total_examples=total_examples,
--&gt; 241             total_words=total_words, **kwargs)
    242 
    243         for callback in self.callbacks:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in _check_training_sanity(self, epochs, total_examples, total_words, **kwargs)
    599 
    600         if not self.wv.vocab:  # should be set by `build_vocab`
--&gt; 601             raise RuntimeError(""you must first build vocabulary before training the model"")
    602         if not len(self.wv.vectors):
    603             raise RuntimeError(""you must initialize vectors before training the model"")

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>in a jupyter notebook. Instead I seek a trained model. How can I fix this?</p>
","python, scikit-learn, word2vec, gensim","<p>The <code>W2VTransformer</code> has a parameter <code>min_count</code> and it is by default equal to 5. So the error is simply a result of the fact that you only feed 2 documents but require for each word in the vocabulary to appear at least in 5 documents.</p>

<p>Possible solutions:</p>

<ul>
<li><p>Decrease <code>min_count</code></p></li>
<li><p>Give the model more documents</p></li>
</ul>
",6,4,4672,2018-06-01 22:50:23,https://stackoverflow.com/questions/50651861/using-gensim-word2vec-in-scikit-learn-pipeline
Small model from Google news Word2Vec model,"<p>I am using <code>GoogleNews-vectors-negative300.bin</code> model and <a href=""https://github.com/ian-beaver/pycontractions"" rel=""nofollow noreferrer"">pycontractions</a> library to determinate with machine learning the best option to expand contractions when there are ambiguous meanings like <code>I'd</code> with can be <code>I would</code> and <code>I had</code>. The size of this model is very large, around to 3.5Gb.</p>

<p>I think that 3.5Gb is a very large model to use for my purpose. Probably I'll never use all words representations in this model. Is there a way to reduce the size extracting only a subset of words representations that are useful to my purposes?</p>
","machine-learning, models, word2vec, gensim","<p>Truncating the set to the 1st N words is easy with an optional argument to <code>gensim</code>'s <code>load_word2vec_format()</code> method, <code>limit</code>. If present, only the given number of words will be loaded. For example, <code>limit=500000</code> reads only the 1st 500,000 words from the supplied file. </p>

<p>Since such files are usually sorted to put the most-frequent words first, you often don't lose much by discarding the 'long tail' of later words. (They'll appear less frequently in your texts, and their word-vectors were trained on fewer examples and thus of lower-quality, anyway.)</p>

<p>You could then re-<code>save_word2vec_format()</code> the truncated set, if you wanted a smaller file on disk. </p>

<p>You could also tamper with the file on disk to make it only include some <em>other</em> subset of words to retain. It might be easier to do so in the text (<code>binary=False</code>) format. Looking at the <code>gensim</code> source code for <code>load_word2vec_format()</code>/<code>save_word2vec_format()</code> could help you to understand what the file must look like to read back in. </p>
",4,0,1996,2018-06-05 21:41:04,https://stackoverflow.com/questions/50709355/small-model-from-google-news-word2vec-model
Custom word2vec Transformer on pandas dataframe and using it in FeatureUnion,"<p>For the below pandas DataFrame <code>df</code>, I want to transform the <code>type</code> column to OneHotEncoding, and transform the <code>word</code> column to its vector representation using the dictionary <code>word2vec</code>. Then I want to concatenate the two transformed vectors with the <code>count</code> column to form the final feature for classification.</p>

<pre><code>&gt;&gt;&gt; df
       word type  count
0     apple    A      4
1       cat    B      3
2  mountain    C      1 

&gt;&gt;&gt; df.dtypes
word       object
type     category
count       int64

&gt;&gt;&gt; word2vec
{'apple': [0.1, -0.2, 0.3], 'cat': [0.2, 0.2, 0.3], 'mountain': [0.4, -0.2, 0.3]}
</code></pre>

<p>I defined customized <code>Transformer</code>, and use <code>FeatureUnion</code> to concatenate the features. </p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder

class w2vTransformer(TransformerMixin):

    def __init__(self,word2vec):
        self.word2vec = word2vec

    def fit(self,x, y=None):
        return self

    def wv(self, w):
        return self.word2vec[w] if w in self.word2vec else [0, 0, 0]

    def transform(self, X, y=None):
         return df['word'].apply(self.wv)

pipeline = Pipeline([
    ('features', FeatureUnion(transformer_list=[
        # Part 1: get integer column
        ('numericals', Pipeline([
            ('selector', TypeSelector(np.number)),
        ])),

        # Part 2: get category column and its onehotencoding
        ('categoricals', Pipeline([
            ('selector', TypeSelector('category')),
            ('labeler', StringIndexer()),
            ('encoder', OneHotEncoder(handle_unknown='ignore')),
        ])), 

        # Part 3: transform word to its embedding
        ('word2vec', Pipeline([
            ('w2v', w2vTransformer(word2vec)),
        ]))
    ])),
])
</code></pre>

<p>When I run <code>pipeline.fit_transform(df)</code>, I got the error: <code>blocks[0,:] has incompatible row dimensions. Got blocks[0,2].shape[0] == 1, expected 3.</code></p>

<p>However, if I <strong>removed</strong> the word2vec Transformer (Part 3) from the pipeline, the pipeline (Part1 1 + Part 2) works fine. </p>

<pre><code>&gt;&gt;&gt; pipeline_no_word2vec.fit_transform(df).todense()
matrix([[4., 1., 0., 0.],
        [3., 0., 1., 0.],
        [1., 0., 0., 1.]])
</code></pre>

<p>And if I <strong>keep only</strong> the w2v transformer in the pipeline, it also works.</p>

<pre><code>&gt;&gt;&gt; pipeline_only_word2vec.fit_transform(df)
array([list([0.1, -0.2, 0.3]), list([0.2, 0.2, 0.3]),
       list([0.4, -0.2, 0.3])], dtype=object)
</code></pre>

<p>My guess is that there is something wrong in my <code>w2vTransformer</code> class but don't know how to fix it. Please help.</p>
","pandas, scikit-learn, pipeline, word2vec","<p>This error is due to the fact that the FeatureUnion expects a 2-d array from each of its parts.</p>

<p>Now the first two parts of your FeatureUnion:- <code>'numericals'</code> and <code>'categoricals'</code> are correctly sending 2-d data of shape (n_samples, n_features). </p>

<p><code>n_samples</code> = 3 in your example data. <code>n_features</code> will depend on individual parts (like OneHotEncoder will change them in 2nd part, but will be 1 in first part).</p>

<p>But the third part <code>'word2vec'</code> returns a pandas.Series object which have the 1-d shape <code>(3,)</code>. FeatureUnion takes this a shape (1, 3) by default and hence the complains that it does not match other blocks.</p>

<p>So you need to correct that shape.</p>

<p>Now even if you simply do a <code>reshape()</code> at the end and change it to shape (3,1), your code will not run, because the internal contents of that array are lists from your word2vec dict, which are not transformed correctly to a 2-d array. Instead it will become a array of lists.</p>

<p>Change the w2vTransformer to correct the error:</p>

<pre><code>class w2vTransformer(TransformerMixin):
    ...
    ...
    def transform(self, X, y=None):
        return np.array([np.array(vv) for vv in X['word'].apply(self.wv)])
</code></pre>

<p>And after that the pipeline will work.</p>
",1,0,1381,2018-06-06 02:24:58,https://stackoverflow.com/questions/50711419/custom-word2vec-transformer-on-pandas-dataframe-and-using-it-in-featureunion
How is Word2Vec min_count applied,"<p>Say that I'm training a (Gensim) Word2Vec model with min_count=5. The documentation learns us what min_count does:</p>

<blockquote>
  <p>Ignores all words with total frequency lower than this.</p>
</blockquote>

<p>What is the effect of min_count on the context? Lets say that I have a sentence of frequent words (min_count > 5) and infrequent words (min_count &lt; 5), annotated with f and i:</p>

<blockquote>
  <p>This (f) is (f) a (f) test (i) sentence (i) which (f) is (f) shown (i) here (i)</p>
</blockquote>

<p>I just made up which word is frequently used and which word is not for demonstration purposes.</p>

<p>If I remove all infrequent words, we get a completely different context from which word2vec is trained. In this example, your sentence would be ""This is a which is"", which would then be a training sentence for Word2Vec. Moreover, if you have a lot of infrequent words, words that were originally very far away from each other are now placed within the same context.</p>

<p>Is this the correct interpretation of Word2Vec? Are we just assuming that you shouldn't have too many infrequent words in your dataset (or set a lower min_count threshold)?</p>
","python, word2vec, gensim","<p>Words below the <code>min_count</code> frequency are dropped before training occurs. So, the relevant context <code>window</code> is the word-distance among surviving words. </p>

<p>This de facto shrinking of contexts is usually a good thing: the infrequent words don't have enough varied examples to obtain good vectors for themselves. Further, while individually each infrequent word is rare, in total there are lots of them, so these doomed-to-poor-vector rare-words intrude on most other words' training, serving as a sort of noise that makes those word-vectors worse too.</p>

<p>(Similarly, when using the <code>sample</code> parameter to down-sample frequent words, the frequent words are randomly dropped â€“ which also serves to essentially ""shrink"" the distances between surviving words, and often improves overall vector quality.)</p>
",10,4,10116,2018-06-06 14:46:50,https://stackoverflow.com/questions/50723303/how-is-word2vec-min-count-applied
Find the closest word to set of words,"<p>I would need to find something like the opposite of <code>model.most_similar()</code><br>
While <code>most_similar()</code> returns an array of words most similar to the one given as input, I need to find a sort of ""center"" of a list of words.</p>

<p>Is there a function in gensim or any other tool that could help me?</p>

<p>Example:<br>
Given <code>{'chimichanga', 'taco', 'burrito'}</code> the center would be maybe <code>mexico</code> or <code>food</code>, depending on the corpus that the model was trained on</p>
","python, nlp, word2vec, gensim","<p>If you supply a list of words as the <code>positive</code> argument to <code>most_similar()</code>, it will report words closest to their mean (which would seem to be one reasonable interpretation of the words' 'center'). </p>

<p>For example:</p>

<pre><code>sims = model.most_similar(positive=['chimichanga', 'taco', 'burrito'])
</code></pre>

<p>(I somewhat doubt the top result <code>sims[0]</code> here will be either 'mexico' or 'food'; it's most likely to be another mexican-food word. There isn't necessarily a ""more generic""/hypernym relation to be found either between word2vec words, or in certain directions... but some other embedding techniques, such as <a href=""https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/"" rel=""nofollow noreferrer"">hyperbolic embeddings</a>, might provide that.)</p>
",3,2,1607,2018-06-06 15:11:38,https://stackoverflow.com/questions/50723841/find-the-closest-word-to-set-of-words
Fine-tuning Glove Embeddings,"<p>Has anyone tried to fine-tune <strong>Glove embeddings</strong> on a domain-specific corpus?<br>
<strong>Fine-tuning word2vec</strong> embeddings has proven very efficient for me in a various NLP tasks, but I am wondering whether generating a cooccurrence matrix on my domain-specific corpus, and training glove embeddings (initialized with pre-trained embeddings) on that corpus would generate similar improvements.</p>
","machine-learning, nlp, word2vec, word-embedding","<p>I myself am trying to do the exact same thing. You can try <a href=""https://github.com/ashutoshsingh0223/mittens"" rel=""nofollow noreferrer"">mittens</a>.</p>

<p>They have succesfully built a framework for it. Christopher D. Manning(co-author of GloVe) is associated with it.</p>
",4,3,4128,2018-06-18 12:38:34,https://stackoverflow.com/questions/50909726/fine-tuning-glove-embeddings
Gensim Word2Vec select minor set of word vectors from pretrained model,"<p>I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. </p>

<p>The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.</p>

<p>Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?</p>
","python, keras, word2vec, gensim, word-embedding","<p>Thanks to <a href=""https://stackoverflow.com/a/54258997/7339624"">this answer</a> (I've changed the code a little bit to make it better). you can use this code for solving your problem.</p>

<p>we have all our minor set of words in <code>restricted_word_set</code>(it can be either list or set) and <code>w2v</code> is our model, so here is the function:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def restrict_w2v(w2v, restricted_word_set):
    new_vectors = []
    new_vocab = {}
    new_index2entity = []
    new_vectors_norm = []

    for i in range(len(w2v.vocab)):
        word = w2v.index2entity[i]
        vec = w2v.vectors[i]
        vocab = w2v.vocab[word]
        vec_norm = w2v.vectors_norm[i]
        if word in restricted_word_set:
            vocab.index = len(new_index2entity)
            new_index2entity.append(word)
            new_vocab[word] = vocab
            new_vectors.append(vec)
            new_vectors_norm.append(vec_norm)

    w2v.vocab = new_vocab
    w2v.vectors = np.array(new_vectors)
    w2v.index2entity = np.array(new_index2entity)
    w2v.index2word = np.array(new_index2entity)
    w2v.vectors_norm = np.array(new_vectors_norm)
</code></pre>

<blockquote>
  <p><strong>WARNING:</strong> when you first create the model the <code>vectors_norm == None</code> so
  you will get an error if you use this function there. <code>vectors_norm</code>
  will get a value of the type <code>numpy.ndarray</code> after the first use. so
  before using the function try something like <code>most_similar(""cat"")</code> so
  that <code>vectors_norm</code> not be equal to <code>None</code>.</p>
</blockquote>

<p>It rewrites all of the variables which are related to the words based on the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">Word2VecKeyedVectors</a>.</p>

<p>Usage:</p>

<pre><code>w2v = KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('beers', 0.8409687876701355),<br>
   ('lager', 0.7733745574951172),<br>
   ('Beer', 0.71753990650177),<br>
   ('drinks', 0.668931245803833),<br>
   ('lagers', 0.6570086479187012),<br>
   ('Yuengling_Lager', 0.655455470085144),<br>
   ('microbrew', 0.6534324884414673),<br>
   ('Brooklyn_Lager', 0.6501551866531372),<br>
   ('suds', 0.6497018337249756),<br>
   ('brewed_beer', 0.6490240097045898)]</p>
</blockquote>

<pre><code>restricted_word_set = {""beer"", ""wine"", ""computer"", ""python"", ""bash"", ""lagers""}
restrict_w2v(w2v, restricted_word_set)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('lagers', 0.6570085287094116),<br>
   ('wine', 0.6217695474624634),<br>
   ('bash', 0.20583480596542358),<br>
   ('computer', 0.06677375733852386),<br>
   ('python', 0.005948573350906372)]</p>
</blockquote>

<p>it can be used for removing some words either.</p>
",14,8,2822,2018-06-18 17:32:32,https://stackoverflow.com/questions/50914729/gensim-word2vec-select-minor-set-of-word-vectors-from-pretrained-model
How to get all the weight updates from Word2Vec,"<p>I am not only interested in the final W0 and W1 (also, to some known as W and W'), but all the variations of these two matrices during the learning.</p>

<p>For now, I am using the gensim implementation, but compared to sklearn, gensim's API is not very well organized in my mind. Hence, I am open to moving to tf if need be, given that getting access to these values would be possible/easier.</p>

<p>I know I can hack the main code; my question is whether there already is a function/variable for it.</p>
","tensorflow, gensim, word2vec","<p>There's no specific API for seeing individual training example updates, or interim weights mid-training. </p>

<p>But as you've intuited, instead of calling <code>train()</code> once, letting it run all epochs and all learning-rate-updates (as is recommended), you could call it one epoch at a time, providing it the right incremental <code>start_alpha</code> and <code>end_alpha</code> yourself each call, and between the calls look at the word-vectors (aka ""projection weights"") and hidden-to-output weights (<code>syn1neg</code> for default negative-sampling, or <code>syn1</code> for hierarchical-softmax). </p>

<p>If you needed more fine-grained reporting, you'd need to modify the source code to add the extra logging/callouts/etc you need.</p>
",0,0,490,2018-06-19 22:39:53,https://stackoverflow.com/questions/50937881/how-to-get-all-the-weight-updates-from-word2vec
Hierarchical training for doc2vec: how would assigning same labels to sentences of the same document work?,"<p>What is the effect of assigning the same label to a bunch of sentences in doc2vec? I have a collection of documents that I want to learn vectors using gensim for a ""file"" classification task where file refers to a collection of documents for a given ID. I have several ways of labeling in mind and I want to know what would be the difference between them and which is the best - </p>

<ul>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags and train. Repeat for others</p></li>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags. Then tokenize document into sentences and assign label <code>doc1</code> to its tags and then train with both full document and individual sentences. Repeat for others</p></li>
</ul>

<p>For example (ignore that the sentence isn't tokenized) -</p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1'])
TaggedDocument(words=[""It is small.""], tags=['doc1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1'])
</code></pre>

<ul>
<li>Similar to above, but also assign a unique label for each sentence along with <code>doc1</code>. The full document has the all the sentence tags along with <code>doc1</code>.</li>
</ul>

<p>Example - </p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1', 'doc1_sentence1', 'doc1_sentence2'])
TaggedDocument(words=[""It is small.""], tags=['doc1', 'doc1_sentence1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1', 'doc1_sentence2'])
</code></pre>

<p>I also have some additional categorical tags that I'd be assigning. So what would be the best approach?</p>
","python, nlp, word2vec, gensim, doc2vec","<p>You can do all this! Assigning the same tag to multiple texts has <em>almost</em> the same effect as would combining those texts into one larger text, and assigning it that tag. The slight differences would be for <code>Doc2Vec</code> modes where there's a context-window â€“ PV-DM (<code>dm=1</code>). With separate texts, there'd never be contexts stretching across the end/beginning of sentences.</p>

<p>In fact, as <code>gensim</code>'s optimized code paths have a 10,000-token limit to text sizes, splitting larger documents into subdocuments, but repeating their tags is sometimes necessary as a workaround.</p>

<p>What you've specifically proposed, training both the full-doc, and the doc-fragments, would work, but also have the effect of doubling the amount of text (and thus training-attention/individual-prediction-examples) for the <code>'doc1'</code> tags, compared to the narrower per-sentence tags. You might want that, or not - it could affect the relative quality of each. </p>

<p>What's best is unclear - it depends on your corpus, and end goals, so should be determined through experimentation, with a clear end-evaluation so that you can automate/systematize a rigorous search for what's best. </p>

<p>A few relevant notes, though:</p>

<ul>
<li><code>Doc2Vec</code> tends to works better with docs of at least a dozen or more words per document.</li>
<li>The <code>'words'</code> need to be tokenized - a list-of-strings, not a string.</li>
<li>It benefits from a lot of varied data, and in particular if you're training a larger model â€“ more unique tags (including overlapping ones), and many-dimension vectors â€“ you'll need more data to avoid overfitting.</li>
</ul>
",2,0,806,2018-06-24 22:25:09,https://stackoverflow.com/questions/51014463/hierarchical-training-for-doc2vec-how-would-assigning-same-labels-to-sentences
Keras Word2Vec implementation,"<p>I'm using the implementation found in <a href=""http://adventuresinmachinelearning.com/word2vec-keras-tutorial/"" rel=""noreferrer"">http://adventuresinmachinelearning.com/word2vec-keras-tutorial/</a> to learn something about word2Vec. What I am not understanding is why isn't the loss function decreasing?</p>

<pre><code>Iteration 119200, loss=0.7305528521537781
Iteration 119300, loss=0.6254740953445435
Iteration 119400, loss=0.8255964517593384
Iteration 119500, loss=0.7267132997512817
Iteration 119600, loss=0.7213149666786194
Iteration 119700, loss=0.6156617999076843
Iteration 119800, loss=0.11473365128040314
Iteration 119900, loss=0.6617216467857361
</code></pre>

<p>The net, from my understanding, is a standard one used in this task:</p>

<pre><code>input_target = Input((1,))
input_context = Input((1,))

embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')

target = embedding(input_target)
target = Reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = Reshape((vector_dim, 1))(context)

dot_product = Dot(axes=1)([target, context])
dot_product = Reshape((1,))(dot_product)
output = Dense(1, activation='sigmoid')(dot_product)

model = Model(inputs=[input_target, input_context], outputs=output)
model.compile(loss='binary_crossentropy', optimizer='rmsprop') #adam??
</code></pre>

<p>Words come from a vocabulary of size 10000 from <a href=""http://mattmahoney.net/dc/text8.zip"" rel=""noreferrer"">http://mattmahoney.net/dc/text8.zip</a> (english text)</p>

<p>What I notice is that some words are somewhat learned in time like the context for numbers and articles is easily guessed, yet the loss is quite stuck around 0.7 from the beginning, and as iterations goes it only fluctuates randomly.</p>

<p>The training part is made like this (which I sense strange since the absence of the standard fit method)</p>

<pre><code>arr_1 = np.zeros((1,))
arr_2 = np.zeros((1,))
arr_3 = np.zeros((1,))
for cnt in range(epochs):
    idx = np.random.randint(0, len(labels)-1)
    arr_1[0,] = word_target[idx]
    arr_2[0,] = word_context[idx]
    arr_3[0,] = labels[idx]
    loss = model.train_on_batch([arr_1, arr_2], arr_3)
    if cnt % 100 == 0:
        print(""Iteration {}, loss={}"".format(cnt, loss))
</code></pre>

<p>Am i missing something important about these type of net? What is not written is implemented exactly like the link above</p>
","python, keras, word2vec","<p>I followed the same tutorial and the loss drops after the algorithm went through a sample again. Note that the loss function is calculated only for the current target and context word pair. In the code example from the tutorial one epoch is only one sample, therefore you would need more than the number of target and context words to come to a point where the loss drops. </p>

<p>I implemented the training part with the following line</p>

<pre><code>model.fit([word_target, word_context], labels, epochs=5)
</code></pre>

<p>Be warned that this can take a long time depending on how large the corpus is. The <a href=""https://keras.io/models/model/#train_on_batch"" rel=""noreferrer""><code>train_on_batch</code></a> function gives you more control in training and you can vary the batch size or select samples you choose at every step of the training. </p>
",5,6,2972,2018-06-26 09:46:59,https://stackoverflow.com/questions/51039800/keras-word2vec-implementation
gensim function predict output words,"<p>I use the gensim library to create a word2vec model. It contains the function <code>predict_output_words()</code> which I understand as follows:</p>

<p>For example, I have a model that is trained with the sentence: ""Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy.""</p>

<p>and then I use </p>

<p><code>model.predict_output_words(context_words_list=['Anarchism', 'does', 'not', 'offer', 'a', 'fixed', 'body', 'of', 'from', 'a', 'single', 'particular', 'world', 'view', 'instead', 'fluxing'], topn=10)</code>.</p>

<p>In this situation, could I get/predict the correct word or the omitted word 'doctrine'?</p>

<p>Is this the right way? Please explain this function in detail.</p>
","python, tensorflow, nlp, word2vec, gensim","<p>I am wondering if you have seen the documentation of <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer""><code>predict_output_word</code></a>?</p>

<blockquote>
  <p>Report the probability distribution of the center word given the
  context words as input to the trained model.</p>
</blockquote>

<p>To answer your specific question about the word 'doctrine' - it strongly depends if for the words you listed as your context one of the 10 most probable words is 'doctrine'. This means that it must occur relatively frequently in the corpus you use for training of the model. Also, since 'doctrine' does not seem to be one of the very often used words there is a high chance other words will have a higher probability of appearing in the context. Therefore, if you base only on the returned probability of the words given the context you may end up failing to predict 'doctrine' in this case.</p>
",1,3,5876,2018-06-29 16:10:47,https://stackoverflow.com/questions/51105753/gensim-function-predict-output-words
How to I get the similiarity between a word to a document in gensim,"<p>So I have started to learn gensim for both word2vec and doc2vec and it works. The similarity scores actually work really well. For an experiment, however, I wanted to optimize a key word based search algorithm by comparing a single word and getting how similar it is to a piece of text. </p>

<p>What is the best way to do this? I considered averaging the the word vectors of all words in the text (maybe remove fill and stop word first) and and comparing this to the search word? But this really is just intuition, what would be the best way to do this?</p>
","python, search, gensim, word2vec, doc2vec","<p>Averaging all the word-vectors of a longer text is one crude but somewhat effective way to get a single vector for the full text. The resulting vector might then be usefully comparable to single word-vectors. </p>

<p>The <code>Doc2Vec</code> modes that train word-vectors into the same 'space' as the doc-vectors â€“ PV-DM (<code>dm=1</code>), or PV-DBOW if word-training is added (<code>dm=0, dbow_words=1</code>) â€“ could be considered. The doc-vectors closest to a single word-vector might work for your purposes. </p>

<p>Another technique for calculating a 'closeness' of two sets-of-word-vectors is ""Word Mover's Distance"" ('WMD'). It's more expensive to calculate than those techniques that reduce a text to a single vector, because it's essentially considering many possible cost-minimizing ways of correlating the sets-of-vectors. I'm not sure how well it works in the degenerate case of one 'text' being just a single word (or very short phrase), but it could be worth trying. (The method <code>wmd_distance()</code> in gensim offers this.)</p>

<p>I've also seen mention of another calculation, called 'Soft Cosine Similarity', that may be more efficient that WMD but offer similar benefits. It's also now available in gensim; there's a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb"" rel=""nofollow noreferrer"">Jupyter notebook intro tutorial as well</a>. </p>
",1,0,321,2018-07-04 21:18:46,https://stackoverflow.com/questions/51180848/how-to-i-get-the-similiarity-between-a-word-to-a-document-in-gensim
How to fix (do better) text classification model with using word2vec,"<p>I'm the freshman in Machine Learning and Neural Network. I've got the problem with text classification. I use LSTM NN architecture system with Keras library. 
My model every time reach the results about 97%. I got the database with something about 1 million records, where 600k of them are positive and 400k are negative. 
I got also 2 labeled classes as 0 (for negative) and 1 (for positive). My database is split for training database and tests database in relation 80:20. For the NN input, I use Word2Vec trained on PubMed articles.
My network architecure: </p>

<pre><code>model = Sequential()
model.add(emb_layer)
model.add(LSTM(64, dropout =0.5))
model.add(Dense(2))
model.add(Activation(â€˜softmaxâ€™)
model.compile(optimizer=â€™rmspropâ€™, loss=â€™binary_crossentropyâ€™, metrics=[â€˜accuracyâ€™])
model.fit(X_train, y_train, epochs=50, batch_size=32)
</code></pre>

<p>How can I fix (do better) my NN created model in this kind of text classification?</p>
","python, machine-learning, neural-network, keras, word2vec","<p>The problem with which we are dealing here is called overfitting. 
First of all, make sure your input data is properly cleaned. One of the principles of machine learning is: â€˜Garbage In, Garbage Outâ€. Next, you should balance your data collection, for example on 400k positive and 400k negative records. In sequence, the data set should be divided into a training, test and validation set (60%:20%:20%), for example using scikit-learn library, as in the following example:</p>

<pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)
</code></pre>

<p>Then I would use a different neural network architecture and try to optimize the parameters.
Personally, I would suggest using a 2-layer LSTM neural network or a combination of a convolutional and recurrent neural network (faster and reading articles that give better results).</p>

<p><strong>1) 2-layer LSTM:</strong></p>

<pre><code>model = Sequential()
model.add(emb_layer)
model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)
model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))
model.add(Dense(2))
model.add(Activation(â€˜sigmoidâ€™))
</code></pre>

<p>You can try using 2 layers with 64 hidden neurons, add recurrent_dropout parameter. 
The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.</p>

<p><strong>2) CNN + LSTM</strong></p>

<pre><code>model = Sequential()
model.add(emb_layer)
model.add(Convolution1D(32, 3, padding=â€™sameâ€™))
model.add(Activation(â€˜reluâ€™))
model.add(MaxPool1D(pool_size=2))
model.add(Dropout(0.5))
model.add(LSTM(32, dropout(0.5, recurrent_dropout=0.5, return_sequences=True))
model.add(LSTM(64, dropout(0.5, recurrent_dropout=0.5))
model.add(Dense(2))
model.add(Activation(â€˜sigmoidâ€™))
</code></pre>

<p>You can try using combination of a CNN and RNN. In this architecture, the model learns faster (up to 5 times faster).</p>

<p>Then, in both cases, you need to apply optimization, loss function.</p>

<p>A good optimizer for both cases is the ""Adam"" optimizer.</p>

<pre><code>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
</code></pre>

<p>In the last step, we validate our network on the validation set.
In addition, we use callback, which will stop the network learning process, in case when, for example, in 3 more iterations, there are no changes in the accuracy of the classification.</p>

<pre><code>from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(patience=3)

model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])
</code></pre>

<p>We can also control the overfitting using graphs. If you want to see how to do it, check <a href=""https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>If you need further help, let me know in a comment.</p>
",2,0,711,2018-07-04 21:55:15,https://stackoverflow.com/questions/51181152/how-to-fix-do-better-text-classification-model-with-using-word2vec
Applying Bag of words,"<p>Hey I am working with bag of words and I am trying to implement  so suppose I have the corpus below but I don't want to use <code>print( vectorizer.fit_transform(corpus).todense() )</code> as a vocabulary instead I have one create which goes like 
<code>{u'all': 0, u'sunshine': 1, u'some': 2, u'down': 3, u'reason': 4}</code>
How can  I use this vocabulary to generate the matrix?</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

corpus = [
'All my cats in a row',
'When my cat sits down, she looks like a Furby toy!',
'The cat from outer space',
'Sunshine loves to sit like this for some reason.'
]

vectorizer = CountVectorizer()
print( vectorizer.fit_transform(corpus).todense() )
print( vectorizer.vocabulary_ )
</code></pre>
","python, machine-learning, nlp, word2vec","<p>Instantiate your CountVectorizer using your custom vocabulary and then transform your corpus.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

corpus = [
'All my cats in a row',
'When my cat sits down, she looks like a Furby toy!',
'The cat from outer space',
'Sunshine loves to sit like this for some reason.'
]

vocabulary = {u'all': 0, u'sunshine': 1, u'some': 2, u'down': 3, u'reason': 4}

vectorizer = CountVectorizer(vocabulary=vocabulary)

print( vectorizer.transform(corpus).todense() )
[[1 0 0 0 0]
 [0 0 0 1 0]
 [0 0 0 0 0]
 [0 1 1 0 1]]

print( vectorizer.vocabulary_ )
{'all': 0, 'sunshine': 1, 'some': 2, 'down': 3, 'reason': 4}
</code></pre>
",1,1,265,2018-07-07 07:11:25,https://stackoverflow.com/questions/51220868/applying-bag-of-words
word2vec gensim multiple languages,"<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>

<pre><code>model.wv.most_similar(positive = ['man'])
Out[14]: 
[('woman', 0.7380284070968628),
 ('lady', 0.6933152675628662),
 ('monk', 0.6662989258766174),
 ('guy', 0.6513140201568604),
 ('soldier', 0.6491742134094238),
 ('priest', 0.6440571546554565),
 ('farmer', 0.6366012692451477),
 ('sailor', 0.6297377943992615),
 ('knight', 0.6290514469146729),
 ('person', 0.6288090944290161)]
--------------------------------------------
</code></pre>

<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>

<pre><code>model.similarity('man', 'à¤†à¤¦à¤®à¥€')
__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will 
be removed in 4.0.0, use self.wv.similarity() instead).
Out[13]: 0.078265618974427215
</code></pre>

<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>

<p>This is what I am doing here:</p>

<pre><code>#Combining all the words together.
all_reviews=HindiWordsList + EnglishWordsList

#Training FastText model
cpu_count=multiprocessing.cpu_count()
model=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)
model.build_vocab(all_reviews)
model.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)
model.save(""word2vec_combined_50.bin"")
</code></pre>
","python, nlp, artificial-intelligence, word2vec, gensim","<p>First of all, you should really use self.wv.similarity().</p>

<p>I'm assuming there are very close to no words that exist in both between your Hindi corpus and English corpus, since Hindi corpus is in Devanagari and English is in, well, English. Simply adding two corpuses together to make a model does not make sense. Corresponding words in the two languages co-occur in two versions of a document, but not in your word embeddings for Word2Vec to figure out most similar.</p>

<p>Eg. Until your model knows that </p>

<p>Man:Aadmi::Woman:Aurat, </p>

<p>from the word embeddings, it can never make out the </p>

<p>Raja:King::Rani:Queen </p>

<p>relation. And for that, you need <em>some</em> anchor between the two corpuses.
Here are a few suggestions that you can try out:</p>

<ol>
<li>Make an independent Hindi corpus/model</li>
<li>Maintain and lookup data of a few English->Hindi word pairs that you have will have to create manually.</li>
<li>Randomly replace input document words with their counterparts from the corresponding document while training</li>
</ol>

<p>These might be enough to give you an idea. You can also look into <a href=""https://github.com/google/seq2seq"" rel=""nofollow noreferrer"">seq2seq</a> if you want only want to do translations. You can also read the <a href=""https://www.learndatasci.com/tutorials/intro-to-word-embeddings-problems-theory/"" rel=""nofollow noreferrer"">Word2Vec theory</a> in detail to understand what it does.</p>
",3,4,7610,2018-07-08 15:46:00,https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages
What&#39;s the maximum euclidean distance between 2 hyperpoints in word2vec algorithm?,"<p>I have been considering to use Word2vec for a problem.
I know that you can use cosine distance which means the minimum distance can be 0 if the hyperpoints are identical or 1 because cosine spans from [-1,1] in case of maximum.
The same applies for minimum in euclidean distance.
My question is in practice what is the maximum euclidean distance two said words can achieve while using word2vec to project them in the same hyperspace ? Can it be estimated mathematically ? is it theoretically infinite ?</p>
","math, machine-learning, nlp, deep-learning, word2vec","<p>The training process doesn't necessarily bound where a word-vector winds up, so I believe the euclidean-distance between two words could become arbitrarily large. </p>

<p>But, they'd only get arbitrarily large with arbitrarily many training passes, and perhaps only on certain extreme training corpuses. The normal variety of language and limited number of training passes means that in practice vectors don't get <em>too</em> far from the origin point.</p>

<p>It's common to unit-normalize the word-vectors, so that they all have a magnitude of 1.0 (and thus are on the ""unit-hypersphere""), before making word-to-word comparisons. If you've done this normalization:</p>

<ul>
<li><p>while the euclidean-distance and cosine-distance will be different values, the <em>rank order</em> of nearest-neighbors will be the same no matter which you use</p></li>
<li><p>the maximum distance between any two vectors would be 2, for points diametrically opposite each other on the hypersphere</p></li>
</ul>
",1,0,2425,2018-07-12 13:16:35,https://stackoverflow.com/questions/51306721/whats-the-maximum-euclidean-distance-between-2-hyperpoints-in-word2vec-algorith
Can&#39;t load glove.6B.300d.txt,"<p>I'm trying to load glove vectors, with the following code</p>

<pre><code>en_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)
</code></pre>

<p>and I unexpectedly get the following error.</p>

<pre><code> File ""/home/k/Desktop/Work/Vector explorer/word2vec-explorer/vec_test_loader.py"", line 55, in make_model
en_model = KeyedVectors.load_word2vec_format(model_path, binary=is_bin)
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 1119, in load_word2vec_format
limit=limit, datatype=datatype)
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/utils_any2vec.py"", line 175, in _load_word2vec_format
vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/utils_any2vec.py"", line 175, in &lt;genexpr&gt;
vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>

<p>Can someone help?</p>
",word2vec,"<p>Gensim need more information about <code>model_path</code>, we have to append two number at the first line which the first indicates how many numbers of words vocabulary we have and the second  indicates the number of dimension of word embedding, it looks like below:</p>

<pre><code>101 300
the 1.0 2.1 -1.3 ...
I   1.1 0.2 -0.3 ...
.
.
.
</code></pre>

<p>you can try to use one line code as below:</p>

<pre><code>python -m gensim.scripts.glove2word2vec --input  glove.840B.300d.txt --output glove.840B.300d.w2vformat.txt
</code></pre>

<p>Or you can use my code as reference below:</p>

<pre><code>import gensim
import os
import shutil
import hashlib
from sys import platform

def getFileLineNums(filename):
    f = open(filename, 'r')
    count = 0
    for line in f:
        count += 1
    return count


def prepend_line(infile, outfile, line):
    with open(infile, 'r') as old:
        with open(outfile, 'w') as new:
            new.write(str(line) + ""\n"")
            shutil.copyfileobj(old, new)

def prepend_slow(infile, outfile, line):
    with open(infile, 'r') as fin:
        with open(outfile, 'w') as fout:
            fout.write(line + ""\n"")
            for line in fin:
                fout.write(line)

def load(filename):
    num_lines = getFileLineNums(filename)
    gensim_file = 'glove_model.txt'
    gensim_first_line = ""{} {}"".format(num_lines, 300)
    # Prepends the line.
    if platform == ""linux"" or platform == ""linux2"":
        prepend_line(filename, gensim_file, gensim_first_line)
    else:
        prepend_slow(filename, gensim_file, gensim_first_line)

    model = gensim.models.KeyedVectors.load_word2vec_format(gensim_file)
    return model
model = load(your_model_path)
</code></pre>
",7,3,5682,2018-07-13 10:45:17,https://stackoverflow.com/questions/51323344/cant-load-glove-6b-300d-txt
Using Word2Vec for polysemy solving problems,"<p>I have some questions about Word2Vec:</p>

<ol>
<li><p>What determines the dimension of the result model vectors?</p></li>
<li><p>What is elements of this vectors?</p></li>
<li><p>Can I use Word2Vec for polysemy solving problems (state = administrative unit vs state = condition), if I already have texts for every meaning of words?</p></li>
</ol>
","nlp, word2vec","<p>(1) You pick the desired dimensionality, as a meta-parameter of the model. Rigorous projects with enough time may try different sizes, to see what works best for their qualitative evaluations. </p>

<p>(2) Individual dimensions/elements of each word-vector (floating-point numbers),  in vanilla word2vec are not easily interpretable. It's only the arrangement of words as a whole that has usefulness â€“ placing similar words near each other, and making relative directions (eg ""towards 'queen' from 'king'"") match human intuitions about categories/continuous-properties. And, because the algorithms use explicit randomization, and optimized multi-threaded operation introduces thread-scheduling randomness to the order-of-training-examples, even the exact same data can result in different (but equally good) vector-coordinates from run-to-run.</p>

<p>(3) Basic word2vec doesn't have an easy fix, but there's a bunch of hints of polysemy in the vectors, and research work to do more to disambiguate contrasting senses. </p>

<p>For example, generally more-polysemous word-tokens wind up with word-vectors that are some combination of their multiple senses, and (often) of a smaller-magnitude than less-polysemous words. </p>

<p>This <a href=""https://dl.acm.org/citation.cfm?id=2390645"" rel=""nofollow noreferrer"">early paper</a> used multiple representations per word to help discover polysemy. Similar later papers like <a href=""https://arxiv.org/abs/1610.07569v1"" rel=""nofollow noreferrer"">this one</a> use clustering-of-contexts to discover polysemous words then relabel them to give each sense its own vector. </p>

<p><a href=""https://arxiv.org/abs/1601.03764v2"" rel=""nofollow noreferrer"">This paper</a> manages an impressive job of detecting alternate senses via postprocessing of normal word2vec vectors. </p>
",3,1,1247,2018-07-13 17:57:36,https://stackoverflow.com/questions/51330549/using-word2vec-for-polysemy-solving-problems
How to sentence Tokenize a list of Paragraphs in Python?,"<p>I am learning word2vec technique at the moment and got stuck at sentence tokenizing my text data. Hopefully, someone can help me work out how to do it properly.</p>

<p>So, my data is a bunch of complaint notes of our customers. When I loaded my data into a python list, it became something like:</p>

<pre><code>text = ['this is the first sentence of the first paragraph. and this is the second sentence.','some random text in the second paragraph. and another test sentence.','here is the third paragraph. and this is another sentence','I have run out of text here. I am learning python and deep learning.','another paragraph with some random text. the this is a learning sample.','I need help implementing word2vec. this all sounds exciting.','it''s sunday and I shoudnt be learning in the first place. it''s nice and sunny here.']
</code></pre>

<p>I tried some of the most frequently used Sentence Tokenizer methods in the community which all return this error:</p>

<blockquote>
  <p>TypeError: expected string or bytes-like object</p>
</blockquote>

<p>Eventually, I found this:</p>

<pre><code>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
sentences = tokenizer.tokenize(text[:5][4]) 
sentences
</code></pre>

<p>This sort of works but I can't work out what index to put in the [][]s e.g. :5 &amp; 4 to get the entire dataset (all the paragraphs) back tokenized as sentences. </p>

<p>Sorry if my question is vague, please ask if you need clarifications.</p>

<p>Many thanks  </p>
","python, deep-learning, tokenize, word2vec, sentence","<p>You can use <a href=""https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize"" rel=""nofollow noreferrer""><strong><code>nltk.tokenize.word_tokenize()</code></strong></a> in a list comprehension as in:</p>

<pre><code>In [112]: from nltk.tokenize import word_tokenize
In [113]: tokenized = [word_tokenize(sent) for sent in text]
</code></pre>

<p>which outputs:</p>

<pre><code>[['this',
  'is',
  'the',
  'first',
  'sentence',
  'of',
  'the',
  'first',
  'paragraph',
  '.',
  'and',
  'this',
  'is',
  'the',
  'second',
  'sentence',
  '.'],
 ['some',
  'random',
  'text',
  'in',
  'the',
  'second',
  'paragraph',
  .
  .
  .
  .
  ]]
</code></pre>
",3,1,1930,2018-07-15 08:12:36,https://stackoverflow.com/questions/51346551/how-to-sentence-tokenize-a-list-of-paragraphs-in-python
Remove first word and then take the word as a index like one hot encode vector pandas,"<p>I have a word2vec dataframe like this which saved from save_word2vec_format using Gensim under txt file. After using pandas to read this file. (Picture below). How to delete the first word and make them as an index? I want to have a dataframe like one hot encoding vector dataframe. This is my txt file <a href=""https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing</a>
<a href=""https://i.sstatic.net/stOjk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/stOjk.png"" alt=""enter image description here""></a></p>
","pandas, dataframe, word2vec","<p>I think need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>read_csv</code></a> with omit first row, change separator to <code>\s+</code> for one or more whitespaces, set first column to index and set default columns names to <code>RangeIndex</code>, last transpose by <code>T</code>:</p>

<pre><code>df = pd.read_csv('model.txt', sep='\s+', index_col=0, header=None, skiprows=1).T
print (df.head())

0       the       and        of         a        to        in        he  \
1 -0.058613  0.015442 -0.158179  0.140175  0.093452  0.018156  0.119811   
2 -0.167606 -0.107773 -0.029066 -0.206769 -0.091758 -0.089092 -0.154339   
3  0.050763 -0.017081 -0.124401  0.155085  0.175548 -0.029413  0.246189   
4  0.283456  0.208988  0.110836 -0.007077  0.265104  0.023497 -0.027724   
5  0.152869 -0.006580 -0.009774  0.116188  0.039773  0.047682  0.008068   

0        wa        it         i    ...         ammy       mim  candyman  \
1  0.044857  0.351965  0.480889    ...     0.036848  0.060897  0.072883   
2 -0.113168 -0.195455 -0.007680    ...    -0.008903 -0.024123 -0.023799   
3  0.039933  0.143591  0.205823    ...     0.002832  0.014112  0.011426   
4 -0.074092  0.075550 -0.089214    ...     0.003451  0.012912  0.016158   
5 -0.107139  0.040009 -0.013390    ...    -0.000931 -0.006203  0.000539   

0  washboiler  mincepie     ruben    croome    mamlet  postnotes   bettina  
1    0.040233  0.048775  0.059252  0.029014  0.047536   0.034878  0.043068  
2   -0.013842 -0.015706 -0.023821 -0.014749 -0.013498  -0.011608 -0.019654  
3    0.006556  0.012816  0.004323 -0.006120  0.006841   0.008062  0.006986  
4    0.011206  0.010511  0.012700  0.006781  0.007779   0.008678  0.016355  
5   -0.003435 -0.003693 -0.003387 -0.002963 -0.003910  -0.001301 -0.003683  

[5 rows x 31849 columns]
</code></pre>
",1,1,44,2018-07-25 08:18:11,https://stackoverflow.com/questions/51514060/remove-first-word-and-then-take-the-word-as-a-index-like-one-hot-encode-vector-p
word2vec for dictionary of words,"<p>I need to generate word2vec array for a dictionary of words. The dictionary looks something like this </p>

<pre><code>test={0: 'tench, Tinca tinca',
 1: 'goldfish, Carassius auratus',
 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',
 3: 'tiger shark, Galeocerdo cuvieri',
 4: 'hammerhead, hammerhead shark'}
</code></pre>

<p>The loop should go through each line, check if the word exists in the model, if yes then store the vector in an array otherwise check the next word in the line. If none of the words are present in the gensim model, then it should do nothing (array is initialised with zeros)
However if a word doesn't exist in the pre trained model, then it raises this exception:</p>

<blockquote>
  <p>KeyError: ""word 'Galeocerdo cuvieri' not in vocabulary""</p>
</blockquote>

<p>What should be the ideal loop that also has the exception in order to bypass the error raised?
This is my starting code:</p>

<pre><code> import gensim
 model = gensim.models.KeyedVectors.load_word2vec_format('/home/shikhar /Downloads/GoogleNews-vectors-negative300.bin',binary=True) 
 array=np.zeros((4,300)) 
 for i in test:
     synonyms=test[i].split(',')
</code></pre>
","python, nlp, gensim, word2vec","<p>why don't try this</p>

<pre><code>vectors= list()
for i in test:
    flag=True
    synonyms=test[i].split(',')
    for k in synonyms:
        try:
            vectors.append(model[k]])
            flag = False
            break
        except KeyError as e:
            print(e)
            continue
    if flag:
        vectors.append(# Insert your array with zeroes here)               
</code></pre>

<p>I'm assuming that you need all the vectors in a list</p>
",1,2,1586,2018-07-25 08:57:25,https://stackoverflow.com/questions/51514825/word2vec-for-dictionary-of-words
gensim word2vec - update model data,"<p>I have an issue similar to the one discussed here - <a href=""https://stackoverflow.com/questions/40727093/gensim-word2vec-updating-word-embeddings-with-newcoming-data"">gensim word2vec - updating word embeddings with newcoming data</a></p>

<p>I have the following code that saves a model as <strong>text8_gensim.bin</strong></p>

<pre><code>sentences = word2vec.Text8Corpus('train/text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5,sg=0, window=8, iter=15, sample=1e-4,alpha=0.05,cbow_mean=1,negative=25)
model.save(""./savedModel/text8_gensim.bin"")
</code></pre>

<p>Here is the code that adds more data to the saved model (after loading it)</p>

<pre><code>fname=""savedModel/text8_gensim.bin""
model = word2vec.Word2Vec.load(fname)
model.epochs=15

#Custom words
docs = [""start date"", ""end date"", ""eft date"",""termination date""]
model.build_vocab(docs, update=True)
model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
model.wv.similarity('start','eft')
</code></pre>

<p>The model loads fine; however when I try to call <strong>model.wv.similarity</strong> function I get the following error</p>

<p><strong>KeyError: ""word 'eft' not in vocabulary""</strong></p>

<p>Am I missing something here?</p>
","gensim, word2vec, word-embedding","<p>Those <code>docs</code> aren't in the right format: each text should be a list-of-string-tokens, not a string. </p>

<p>And, the same <code>min_count</code> threshold will apply to incremental updates: words less frequent that that threshold will be ignored. (Since a <code>min_count</code> higher than 1 is almost always a good idea, a word that appears only once in any update will never be added to the model.)</p>

<p>Incrementally adding words introduces lots of murky issue with unclear proper choices with regard to model quality, balancing the effects of early-vs-late training, management of the <code>alpha</code> learning-rate, and so forth. It won't necessarily improve your model; with the wrong choices it could make it worse, by adjusting some words with your new texts in ways that move them out-of-compatible-alignment with earlier-batch-only words.</p>

<p>So be careful and always check with a repeatable automated quantitative quality check that your changes are helping. (The safest approach is to retrain with old and new texts in one combined corpus, so that all words get trained against one another equally on all data.)</p>
",2,1,1267,2018-07-26 20:49:50,https://stackoverflow.com/questions/51547315/gensim-word2vec-update-model-data
Clustering with word2vec and Kmeans,"<p>I'm trying to do a clustering with word2vec and Kmeans, but it's not working.</p>
<p>Here part of my data:</p>
<pre><code>demain fera chaud Ã  paris pas marseille
mauvais exemple ce n est pas un clichÃ© mais il faut comprendre pourquoi aussi
il y a plus de travail Ã  Paris c est d ailleurs pour cette raison qu autant de gens&quot;,
mais s il y a plus de travail, il y a aussi plus de concurrence
s agglutinent autour de la capitale
</code></pre>
<p>Script:</p>
<pre><code>import nltk
import pandas
import pprint
import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn import metrics
from gensim.models import Word2Vec
from nltk.cluster import KMeansClusterer
from sklearn.metrics import adjusted_rand_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF

dataset = pandas.read_csv('text.csv', encoding = 'utf-8')

comments = dataset['comments']

verbatim_list = no_duplicate.values.tolist()

min_count = 2
size = 50
window = 4

model = Word2Vec(verbatim_list, min_count=min_count, size=size, window=window)

X = model[model.vocab]

clusters_number = 28
kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)

assigned_clusters = kclusterer.cluster(X, assign_clusters=True)

words = list(model.vocab)
for i, word in enumerate(words):  
    print (word + &quot;:&quot; + str(assigned_clusters[i]))

kmeans = cluster.KMeans(n_clusters = clusters_number)
kmeans.fit(X)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

clusters = {}
for commentaires, label in zip(verbatim_list, labels):
    try:
        clusters[str(label)].append(verbatim)
    except:
       clusters[str(label)] = [verbatim]
pprint.pprint(clusters)
</code></pre>
<p>Output:</p>
<blockquote>
<p>Traceback (most recent call last):</p>
<p>File &quot;kmwv.py&quot;, line 37, in </p>
<p>X = model[model.vocab]</p>
<p>AttributeError: 'Word2Vec' object has no attribute 'vocab'</p>
</blockquote>
<p>I need a clustering that works with word2vec, but every time I try something, I have this error. Is there any way to do a clustering with word2vec?</p>
","python, python-3.x, cluster-analysis, k-means, word2vec","<p>As Davide said, try this:</p>

<pre><code>X = model[model.wv.vocab]
</code></pre>
",6,1,6648,2018-07-30 09:07:08,https://stackoverflow.com/questions/51590459/clustering-with-word2vec-and-kmeans
How I can extract matrixes WI and WO from gensim word2vec?,"<p>CBOW word2vec scheme look like this:</p>

<p><a href=""https://i.sstatic.net/gmgo6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gmgo6.png"" alt=""enter image description here""></a></p>

<p>How I can extract matrixes WI and WO from <code>gensim.models.word2vec.Word2Vec</code>?
I found only these fields in gensim w2v model:</p>

<p><code>gensim.models.word2vec.Word2Vec.trainables.syn1neg</code></p>

<p>and</p>

<p><code>gensim.models.word2vec.Word2Vec.vw.syn1neg.vectors</code></p>

<p>Can I make an assumption that <code>syn1neg</code> is WI, and WO = <code>vectors</code> - <code>syn1neg</code>?</p>

<p>Why this code</p>

<pre><code>sentences = [['car', 'tree', 'chip2'], ['chip1', 'sugar']]
model = Word2Vec(sentences, min_count=1, size = 5)
</code></pre>

<p>give <code>Word2Vec.trainables.syn1neg</code> matrix with zero elements only?</p>

<p>For 30MB dataset <code>Word2Vec.trainables.syn1neg</code> matrix also contain zero elements only, log is here:</p>

<p><a href=""https://pastebin.com/cKfxv2zz"" rel=""nofollow noreferrer"">gensim log</a></p>
","python, gensim, word2vec","<p>The <code>w2v_model.wv.vectors</code> is what was formerly called ""syn0"", and serves as the ""projection weights"" which essentially map a one-hot word-encoding into <em>N</em> dimensions. In your diagram, that's <em>WI</em>.</p>

<p>The <code>w2v_model.trainables.syn1neg</code> is the hidden-to-output weights for negative-sampling mode, what your diagram labels <em>WO</em>.</p>
",1,3,615,2018-07-30 12:38:19,https://stackoverflow.com/questions/51594165/how-i-can-extract-matrixes-wi-and-wo-from-gensim-word2vec
Adding additional words in word2vec or Glove (maybe using gensim),"<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
","nlp, gensim, word2vec, glove","<p>I don't know an <em>easy</em> way. </p>

<p>In particular, word-vectors that weren't co-trained together won't have compatible/comparable coordinate-spaces. (There's no one right place for a word â€“ just a relatively-good place compared to the other words that are in the same model.)</p>

<p>So, you can't just append the missing words from another model: you'd need to transform them into compatible locations. Fortunately, it seems to work to use some set of shared anchor-words, present in both word-vector-sets, to learn a transformation â€“ then apply that the words you want to move over.</p>

<p>There's a class, <code>[TranslationMatrix][1]</code>, and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">demo notebook</a> in gensim showing this process for language-translation (an application mentioned in the original word2vec papers). You could concievably use this, combined with the ability to <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">append extra vectors to a gensim <code>KeyedVectors</code></a> instance, to create a new set of vectors with a superset of the words in either of your source models.</p>
",4,2,2579,2018-07-30 20:52:57,https://stackoverflow.com/questions/51602111/adding-additional-words-in-word2vec-or-glove-maybe-using-gensim
What is the semantic relationship expected between word vectors which are scalar multiples of each other in word2vec?,"<p>Let's say you have a word vector for the word <code>queen</code>. Some of its scalar multiples would be <code>x = queen + queen</code> , <code>y = queen + queen + queen</code> and <code>n * queen</code> for any real value of n ( so we're also considering non-integer values of n such as in <code>0.83 * queen</code> ).</p>

<p>Consider x to be the word most similar to the vector queen + queen according to the cosine similarity between a simple mean of the projection weight vectors of the most similar word and the vector queen + queen.</p>

<p>Consider y to be the word most similar to the vector queen + queen + queen by the same method.</p>

<p>Then what is the semantic relationship expected between the words <code>x</code>, <code>y</code> and <code>queen</code>? I know these vectors will all have the same ratio between the dimensional values within the vector, but I'm having a hard time figuring out how to read this in terms of word meaning. </p>

<p>My intution says that I'll get something in another context that has a position on that context similar to queen. For instance, a queen's ""wealth"" may be significantly larger than a queen's ""beauty"". So I'll get another word in another context that has the same wealth/beauty balance as ""queen"".</p>

<p>So let's say I'm moving from Royal titles ( queen, king, princess... ) to the Forbes list ( Jeff Bezos, Bill Gates, Warren Buffet... ) when I multiply queen by n.</p>

<p>queen * n = someone on the Forbes list that has the same wealth/beauty balance as a queen ( very wealthy, but not very pretty )</p>

<p>princess * n = someone on the Forbes list that has the same wealth/beauty balance as a princess ( moderately wealthy, but very pretty )</p>

<p>However this is just a wild theory, I have no clue about how to systematically prove this is real. </p>
","machine-learning, nlp, pca, word2vec, linguistics","<p>The words that most cosine-similar to to <code>wv['queen']</code> will be the exact same that are most cosine-similar to <code>n * wv['queen']</code>, for any <code>n</code>, because cosine-similarity is unaffected by vector magnitude. So, your assumption is wrong. </p>

<p>If you were to use euclidean-distance instead of cosine-similarity, on the raw (not unit-normalized) word vectors, you might find some other interesting relationships... but that's not a typical way to use/compare word-vectors, so you'd have to experiment &amp; I have no expectations of what you might find or whether it would be useful. </p>

<p>In general, the raw non-unit-normalized word-vectors tend to have a higher-magnitude for words that have a single narrow sense (all contexts they appear in are very similar), while words with many senses and varied contexts tend to have smaller magnitudes. But I'm not sure you can count on this from much. Once word-vectors are normalized to unit-length â€“ and thus all words are on the same 'unit sphere' â€“ then the <em>rank order</em> of nearest-neighbors will be same by either cosine-distance or euclidean-distance (even though the magnitudes of the distance/similarity numbers won't be identical or proportionate at each rank). </p>
",2,1,173,2018-07-31 10:11:31,https://stackoverflow.com/questions/51610905/what-is-the-semantic-relationship-expected-between-word-vectors-which-are-scalar
Sharing memory for gensim&#39;s KeyedVectors objects between docker containers,"<p>Following <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">related question solution</a> I created docker container which loads GoogleNews-vectors-negative300 KeyedVector inside docker container and load it all to memory</p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
word_vectors.most_similar('stuff')
</code></pre>

<p>Also I have another Docker container which provides REST API which loads this model with </p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
</code></pre>

<p>And I observe that fully loaded container takes more than 5GB of memory and each gunicorn worker takes 1.7 GB of memory.</p>

<pre><code>CONTAINER ID        NAME                        CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS
acbfd080ab50        vectorizer_model_loader_1   0.00%               5.141GiB / 15.55GiB   33.07%              24.9kB / 0B         32.9MB / 0B         15
1a9ad3dfdb8d        vectorizer_vectorizer_1     0.94%               1.771GiB / 15.55GiB   11.39%              26.6kB / 0B         277MB / 0B          17
</code></pre>

<p>However, I expect that all this processes share same memory for KeyedVector, so it only takes 5.4 GB shared between all containers.</p>

<p>Have someone tried to achieve that and succeed?</p>

<p>edit: 
I tried following code snippet and it indeed share same memory across different containers.</p>

<pre><code>import mmap
from threading import Semaphore

with open(""data/GoogleNews-vectors-negative300.bin"", ""rb"") as f:
    # memory-map the file, size 0 means whole file
    fileno = f.fileno()
    mm = mmap.mmap(fileno, 0, access=mmap.ACCESS_READ)
    # read whole content
    mm.read()
    Semaphore(0).acquire()
    # close the map
    mm.close()
</code></pre>

<p>So the problem that <code>KeyedVectors.load(model_path, mmap='r')</code> don't share memory</p>

<p>edit2:
Studying gensim's source code I see that <code>np.load(subname(fname, attrib), mmap_mode=mmap)</code> is called to open memmaped file. Following code sample shares memory across multiple container.</p>

<pre><code>from threading import Semaphore

import numpy as np

data = np.load('data/native_format.bin.vectors.npy', mmap_mode='r')
print(data.shape)
# load whole file to memory
print(data.mean())
Semaphore(0).acquire()
</code></pre>
","python, mmap, gensim, word2vec","<p>After extensive debugging I figured out that mmap works as expected for numpy arrays in <code>KeyedVectors</code> object.</p>

<p>However, KeyedVectors have other attributes like <code>self.vocab</code>, <code>self.index2word</code> and <code>self.index2entity</code> which are not shared and consumes ~1.7 GB of memory for each object.</p>
",3,1,1055,2018-07-31 14:41:18,https://stackoverflow.com/questions/51616074/sharing-memory-for-gensims-keyedvectors-objects-between-docker-containers
Can I interpret doc2vec components?,"<p>I am solving a binary text classification problem with corporate filings. Using Doc2Vec embeddings of length 100 with LightGBM is producing great results. However, for this project it would be very valuable to approximate a thematic meaning for at least one of the components. Ideally, this would be a feature ranked with high importance by LightGBM explained anecdotally with a few examples.</p>

<p>Has anyone attempted this, or should interpretation be off the table for a high-dimensional model with this level of complexity?</p>
","nlp, word2vec, word-embedding, doc2vec, lightgbm","<p>The individual dimensions of a <code>Doc2Vec</code> representation should not be considered independent, interpretable features. They're only useful in concert with each other, and the exact directions aligned with individual coordinate-axes may not be strongly meaningful in any human-describable sense.</p>

<p>However, <em>neighborhoods</em> of the space may loosely fit describable themes, and certain <em>directions</em> (not specifically parallel with coordinate-axes) may loosely fit semantic themes. </p>

<p>But to characterize those, you might try to find the centroid points of groups-of-related-documents, or discovered clusters, and compare the relative distances/directions between those centroids.  </p>
",0,1,371,2018-08-06 17:48:05,https://stackoverflow.com/questions/51713078/can-i-interpret-doc2vec-components
Word2vec - get rank of similarity,"<p>Given I got a word2vec model (by gensim), I want to get the rank similarity between to words.
For example, let's say I have the word ""desk"" and the most similar words to ""desk"" are:</p>

<blockquote>
  <ol>
  <li>table 0.64</li>
  <li>chair 0.61</li>
  <li>book 0.59</li>
  <li>pencil 0.52</li>
  </ol>
</blockquote>

<p>I want to create a function such that:</p>

<blockquote>
  <p>f(desk,book) = 3
  Since book is the 3rd most similar word to desk.
  Does it exists? what is the most efficient way to do this?</p>
</blockquote>
","python, python-3.x, nlp, gensim, word2vec","<p>You can use the <code>rank(entity1, entity2)</code> to get the distance - same as the index.</p>

<pre><code>model.wv.rank(sample_word, most_similar_word)
</code></pre>

<hr>

<p>A separate function as given below won't be necessary here. Keeping it for information sake.</p>

<p>Assuming you have the list of words and their vectors in a list of tuples, returned by <code>model.wv.most_similar(sample_word)</code> as shown</p>

<pre><code>[('table', 0.64), ('chair', 0.61), ('book', 0.59), ('pencil', 0.52)]
</code></pre>

<p>The following function accepts the sample word and the most similar word as params, and returns the index or rank (eg. [2]) if it's present in the output</p>

<pre><code>def rank_of_most_similar_word(sample_word, most_similar_word):
    l = model.wv.most_similar(sample_word)
    return [x+1 for x, y in enumerate(l) if y[0] == most_similar_word]

sample_word = 'desk'
most_similar_word = 'book'
rank_of_most_similar_word(sample_word, most_similar_word)
</code></pre>

<p>Note: use <code>topn=x</code> to get the top x most similar words while using <code>model.wv.most_similar()</code>, as suggested in the comments.</p>
",2,3,2021,2018-08-08 13:09:31,https://stackoverflow.com/questions/51747613/word2vec-get-rank-of-similarity
DeprecationWarning in Gensim `most_similar`?,"<p>While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?</p>
<p>Currently, I am getting the following issue.</p>
<p><strong>DeprecationWarning: Call to deprecated <code>most_similar</code> (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be treated as <code>np.int32 == np.dtype(int).type</code>.
if np.issubdtype(vec.dtype, np.int):</strong></p>
<p>Please help to curb this issue? Any help is appreciated.</p>
<p>The code what, I have tried is as follows.</p>
<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')
</code></pre>
","python, python-3.x, gensim, word2vec","<p>It's a warning  which that it's about to become obsolete and non-functional.</p>

<blockquote>
  <p>Usually things are deprecated for a few versions giving anyone using them enough time to move to the new method before they are removed.</p>
</blockquote>

<p>They've moved <code>most_similar</code> to <a href=""https://radimrehurek.com/gensim/models/deprecated/word2vec.html"" rel=""noreferrer""><code>wv</code></a></p>

<p>So <code>most_simliar()</code> should look something like:</p>

<pre><code>model.wv.most_similar('hamlet')
</code></pre>

<p><a href=""http://pydoc.net/gensim/3.2.0/gensim.models.word2vec/"" rel=""noreferrer"">src ref</a></p>

<p>Hope this helps</p>

<p>Edit : using <code>wv.most_similar()</code></p>

<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
similarities = model.wv.most_similar('hamlet')
for word , score in similarities:
    print(word , score)
</code></pre>
",7,4,9925,2018-08-10 18:11:40,https://stackoverflow.com/questions/51791964/deprecationwarning-in-gensim-most-similar
"Glove word embedding model parameters using tex2vec in R, and display training output (epochs) after every n iterations","<p>I am using text2vec package in R for training word embedding (Glove Model) as:</p>

<pre><code>library(text2vec)
library(tm)

prep_fun = tolower
tok_fun = word_tokenizer
tokens = docs %&gt;%  # docs: a collection of text documents  
prep_fun %&gt;% 
tok_fun

it = itoken(tokens, progressbar = FALSE)

stopword &lt;- tm::stopwords(""SMART"")
vocab = create_vocabulary(it,stopwords=stopword) 

vectorizer &lt;- vocab_vectorizer(vocab)

tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 6)

x_max &lt;- min(50,max(10,ceiling(length(vocab$doc_count)/100)))
glove_model &lt;- GlobalVectors$new(word_vectors_size = 200, vocabulary = vocab, x_max = x_max,learning_rate = 0.1) 

word_vectors &lt;- glove_model$fit_transform(tcm, n_iter = 1000, convergence_tol = 0.001)
</code></pre>

<p>When I run this code I get the following output:
<a href=""https://i.sstatic.net/e7TBg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e7TBg.png"" alt=""enter image description here""></a></p>

<p>My questions are:</p>

<ol>
<li>Is it possible to have output after every n iterations, i.e. output for epoch 50, 100, 150 and so on.</li>
<li>Any suggestion for optimal values for word_vectors_size, x_max and learning_rate? for example for 10,000 documents, what is the best value for those parameters? </li>
</ol>

<p>I appreciate your response.</p>

<p>Many thanks,
Sam</p>
","r, word2vec, text2vec, glove","<p>There is a member of the <code>GlobalVectors</code> class called <code>n_dump_every</code>. You can set it to some number and the history of word embeddings will be saved. Then it can be retrieved with <code>get_history()</code> function</p>

<pre><code>glove_model &lt;- GlobalVectors$new(word_vectors_size = 200, vocabulary = vocab, x_max = 100,learning_rate = 0.1) 
glove_model$n_dump_every = 10
word_vectors &lt;- glove_model$fit_transform(tcm, n_iter = 1000, convergence_tol = 0.001)
trace = glove_model$get_history()
</code></pre>

<p>Regarding second question - </p>

<ul>
<li>you may try to vary learning rate a bit (usually decrease), but default one should be ok (keep track of the value of cost function).</li>
<li>the more data you have the larger value you can provide for <code>word_vectors_size</code>. For wikipedia size 300 is usually enough. For smaller datasets you may start with 20-50. You really need to experiment with this.</li>
</ul>
",0,0,1022,2018-08-14 00:51:28,https://stackoverflow.com/questions/51832333/glove-word-embedding-model-parameters-using-tex2vec-in-r-and-display-training-o
word2vec - find a word by a specific vector,"<p>I trained a gensim Word2Vec model.
Let's say I have a certain vector and I want the find the word it represents - what is the best way to do so?</p>

<p>Meaning, for a specific vector:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>I want to get a word:</p>

<pre><code> 'computer' = model.vec2word(vec)
</code></pre>
","python-3.x, nlp, gensim, word2vec","<p>Word-vectors are generated through an iterative, approximative process â€“ so shouldn't be thought of as precisely right (even though they do have exact coordinates), just ""useful within certain tolerances"".</p>

<p>So, there's no lookup of exact-word-for-exact-coordinates. Instead, in gensim <code>Word2Vec</code> and related classes there's <code>most_similar()</code>, which gives the known words <em>closest</em> to given known-words or vector coordinates, in ranked order, with the cosine-similarities. So if you've just trained (or loaded) a full <code>Word2Vec</code> model into the variable <code>model</code>, you can get the closest words to your vector with:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
similars = model.wv.most_similar(positive=[vec])
print(similars)
</code></pre>

<p>If you just want the single closest word, it'd be in <code>similars[0][0]</code> (the first position of the top-ranked tuple).</p>
",2,3,2700,2018-08-15 06:56:45,https://stackoverflow.com/questions/51854220/word2vec-find-a-word-by-a-specific-vector
how to assign weights to articles in the corpus for generating word embedding (e.g. word2vec)?,"<p>there are certain articles in the corpus that I found much more important than other articles (for instance I like their wording more). As a result, I would like to increase their ""weights"" in the entire corpus during the process of generating word vectors. Is there a way to implement this? The current solution that I can think of is to copy the more important articles multiple times, and add them to the corpus. However, will this work for the word embedding process? And is there a better way to achieve this? Many thanks!</p>
","word2vec, corpus, word-embedding","<p>The word2vec library with which I am most familiar, in gensim for Python, doesn't have a feature to overweight certain texts. However, your idea of simply repeating the more important texts should work. </p>

<p>Note though that:</p>

<ul>
<li><p>it'd probably work better if the texts don't repeat consecutively in your corpus - spreading out the duplicated contexts so that they're encountered in an interleaved fashion with other diverse usage examples</p></li>
<li><p>the algorithm really benefits from diverse usage examples â€“ repeating the same rare examples 10 times is nowhere near as good as 10 naturally-subtly-contrasting usages, to induce the kinds of continuous gradations-of-meaning that people want from word2vec</p></li>
<li><p>you should be sure to test your overweighting strategy, with a quantitative quality score related to your end purpose, to be sure it's helping as you hope. It might be extra code/training-effort for negligible benefit, or even harm some word vectors' quality.</p></li>
</ul>
",1,1,501,2018-08-15 14:08:39,https://stackoverflow.com/questions/51860339/how-to-assign-weights-to-articles-in-the-corpus-for-generating-word-embedding-e
How to set the number of iterations in PySpark Word2vec model?,"<p>My code:</p>

<pre><code> word2Vec = Word2Vec(vectorSize = 100, minCount = 100, inputCol = 'token', 
 outputCol = 'word2vec', seed=123)
</code></pre>

<p>I tried the keyword numIterations,<br>
but it is invalid</p>

<p>From Pyspark Documentation:</p>

<pre><code>setNumIterations(int numIterations)
</code></pre>

<p>Sets number of iterations (default: 1), which should be smaller than or equal to the number of partitions.  </p>

<p>Thanks for the help.</p>
","python, apache-spark, word2vec","<p>The parameter seems to be <code>maxIter</code>: give that a try.</p>

<pre><code>  final val maxIter: IntParam = new IntParam(this, ""maxIter"", 
     ""maximum number of iterations (&gt;= 0)"", ParamValidators.gtEq(0))
</code></pre>
",1,2,564,2018-08-15 17:38:50,https://stackoverflow.com/questions/51863736/how-to-set-the-number-of-iterations-in-pyspark-word2vec-model
How to handle &lt;UKN&gt; tokens in text generation,"<p>In my text generation dataset, I have converted all infrequent words into the token  (unknown word), as suggested by most text-generation literature.</p>

<p>However, when training an RNN to take in part of a sentence as input and predict the rest of the sentence, I am not sure how I should stop the network from generating  tokens.
When the network encounters an unknown (infrequent) word in the training set, what should its output be?</p>

<p><strong>Example:</strong><br>
Sentence: <code>I went to the mall and bought a &lt;ukn&gt; and some groceries</code><br>
Network input: <code>I went to the mall and bought a</code><br>
Current network output: <code>&lt;unk&gt; and some groceries</code><br>
Desired network output: <code>??? and some groceries</code><br></p>

<p>What should it be outputting instead of the <code>&lt;unk&gt;</code>? </p>

<p>I don't want to build a generator that outputs words it does not know.</p>
","machine-learning, neural-network, nlp, word2vec, recurrent-neural-network","<p>A RNN will give you a sampling of tokens that are most likely to appear next in your text. In your code you choose the token with the highest probability, in this case Â«unkÂ». </p>

<p>In this case you can omit the Â«uknÂ» token and simply take the <strong>next most likely token</strong> that the RNN suggests based on the probability values that it renders.  </p>
",4,-1,2309,2018-08-19 01:16:07,https://stackoverflow.com/questions/51913706/how-to-handle-ukn-tokens-in-text-generation
Train neural network with sentences of different length in a QA system,"<p>I'm trying to implement a QA system following the instructions showed on this <a href=""https://arxiv.org/abs/1602.03609"" rel=""noreferrer"">paper</a>. I've correctly imported some datasets and converted the words in vectors with the word2vec method. After the word embedding there is the need to insert the questions and the answers in a CNN. What should be the size of the input Tensor given that each question/answer has a different length? (each question/answer is an array of vectors).</p>

<p><em>Excerpt from paper:</em></p>

<p><a href=""https://i.sstatic.net/arzx8.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/arzx8.png"" alt=""enter image description here""></a></p>

<p><strong><em>q_emb</strong> is the question after the word embedding and <strong>r_w_k</strong> is a word vector of length <strong>d</strong>.</em></p>

<p>Which is the right value of <strong><em>M</em></strong> (the length of the Q/A) that should be used? Can you please show me some methods to solve this issue or simply give me some help? Thank you</p>
","neural-network, word2vec, nlp-question-answering","<p>Determine the maximum question/answer vector array length and make your input tensor of shape <code>(num_samples, max_qa_length, word_embedding_size)</code>. For questions shorter than <code>max_qa_length</code>, pad them with zero vectors at the end.</p>
",3,5,372,2018-08-20 17:36:45,https://stackoverflow.com/questions/51935905/train-neural-network-with-sentences-of-different-length-in-a-qa-system
&#39;word&#39; not in Vocabulary in a corpus with words shown in a single list only in gensim library,"<p>Hello Community Members,</p>

<p>At present, I am implementing the Word2Vec algorithm.</p>

<p>Firstly, I have extracted the data (sentences), break and split the sentences into tokens (words), remove the punctuation marks and store the tokens in a single list. The list basically contain the words. Then I have calculated the frequency of words and then computed it occurrences in terms of frequency. It results a list. </p>

<p>Next, I am trying to load the model using gensim. However, I am facing a problem. The problem is about <code>the word is not in the vocabulary</code>. The code snippet, whatever I have tried is as follows.</p>

<pre><code>import nltk, re, gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2= (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't', 's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop=sw+sw1+sw2
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()
def word_embedding():
    for i in range(len(words)):
        model = Word2Vec(words, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
        model.init_sims(replace = True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word , score)
word_embedding()
</code></pre>

<p>Note: I am using Python 3.7 in Windows OS. From the <code>syntax of gensim</code>, it is suggested to use sentences and split into tokens and apply the same to build and train the model. My question is that how to apply the same to a corpus with single list containing only words. I have specified the words also using list, i.e. [words], during the training of the model.       </p>
","python-3.x, nltk, gensim, word2vec, nltk-book","<p>The first parameter passed to <code>Word2Vec</code> expects an list of sentences. You're passing a list of words </p>

<pre><code>import nltk
import re
import gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords


def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1 = (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2 = (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't',
            's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop = sw + sw1 + sw2
    words = [w for w in words if not w in stop]


preprocessing()


def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()


def word_embedding():
    for i in range(len(words)):
        print(type(words))
        #pass words as a list.
        model = Word2Vec([words], size=100, sg=1, window=3,
                        min_count=1, iter=10, workers=4)
        model.init_sims(replace=True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word, score)


word_embedding()
</code></pre>

<p>hope this helps :)</p>
",2,1,1343,2018-08-21 09:23:24,https://stackoverflow.com/questions/51945520/word-not-in-vocabulary-in-a-corpus-with-words-shown-in-a-single-list-only-in-g
h2o aggregate method &quot;none&quot; mapping unknown words to NAN and not vector,"<p>I am currently using h2o.ai to perform some NLP. I have a trained model for my corpus in Word2Vec and have successfully aggregated a number of records with the method ""Average"". The problem comes in when I want to create features for my DRF model by using this w2v model to create a bag of words for each entry. When I use the aggregate method ""none"" the vectors are returned in a single column containing NaN's where The records begin and end, however the unknown words in the model are also being mapped to NaN and not the the unknown word vector. This is stopping me from reorganizing the vectors into a bag of words for each record because the record separation association is lost due to the extra and unpredictably entered NaNs. Is there a fix for this? </p>

<p>I am currently going to use the original tokenized list to make an index of the original double NaN structure that is used to deliminate between records and then recombine my vectors based off of this. Just wanted to throw this out there to see if anyone else is dealing with this or if there is some type of fix in place that I cannot find on the interwebs.</p>

<pre><code>DATA = pd.read_sql(sql, conn1)

steps = [
    (r'[\n\t\â€™\â€“\â€\â€œ\!~`\""@#\$%\^\&amp;\*()_+\{\}|:&lt;&gt;\?\-=\[\]\\;\',./\d]', ' '), 

    (r'\s+', ' ')
    ]

steps = [ (re.compile(a), b) for (a, b) in steps ] 

def do_steps(anarr):
    for pattern,replacement in steps:
        anarr = pattern.sub(replacement,anarr)
    return anarr

DATA.NARR = DATA.NARR.apply(do_steps)

train_hdata = h2o.H2OFrame(DATA).ascharacter()
train_narr = train_hdata[""NARR""]
train_key = train_hdata[""KEY""]
train_tokens_narr = train_narr.tokenize(split=' ')

train_vecs = w2v.transform(train_tokens_narr, aggregate_method='NONE')
VECS = train_vecs.as_data_frame()
df = train_tokens_narr.as_data_frame()
B=(VECS.isnull()&amp;df.isnull())
idx = B[B['C1'] == True].index.tolist()
X = []
X.append('')
j=0
for i in tqdm(range(len(VECS.C1)-1)):
    if i in idx:
        X[j]= X[j][:-2]
        j+=1
        X.append('')
    else:
        X[j]= X[j] + str(VECS.C1[i])[:6] + ', '

s = pd.DataFrame({""C1"":X})
print(s)
</code></pre>

<p>The above is the current code looking to take some records and encode them with the word2vec model for a bag of words. The bottom portion is a draft loop that I am using to put the correct vectors with the correct records. Let me know if I need to clarify.</p>
","python, word2vec, h2o","<p>Unfortunately the functionality to distinguish between words that are missing from your dictionary and NAs used to demarcate the start and end of a record is not currently available. I've made a jira ticket <a href=""https://0xdata.atlassian.net/browse/PUBDEV-5867"" rel=""nofollow noreferrer"">here</a> to track the issue. Please feel free to comment or update the ticket. </p>
",0,0,138,2018-08-21 18:13:48,https://stackoverflow.com/questions/51954502/h2o-aggregate-method-none-mapping-unknown-words-to-nan-and-not-vector
Is it necessary to mix old corpus and new corpus in updating word2vec model?,"<p>I found it is not explicit in usage  </p>

<pre><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""]]

model = Word2Vec(min_count=1)
model.build_vocab(sentences)  # prepare the model vocabulary
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors
(1, 30)
</code></pre>

<p>the sentences whether should contain the old corpus?</p>
","gensim, word2vec","<p>Your code doesn't show any incremental updating of an old model with new examples. </p>

<p>However, it's never guaranteed that incremental updating (as with <code>build_vocab(new_sentences, update=True)</code> and then <code>train(new_sentences, ...)</code>) necessarily improves the model overall. </p>

<p>The underlying algorithm gets its strength from a large dataset, of subtly-varied usage examples, being trained together in an interleaved fashion. The contrasting examples ""pull"" the model in various ways, sometimes reinforcing each other and sometimes cancelling out, resulting in final word-vector arrangements that are useful for other purposes. </p>

<p>Let's say you then do an incremental update with texts that are not the same as the original training data. (And after all, they must be meaningfully different, or else you wouldn't bother with more training.) During that new training, only the words affected by the new (possibly-smaller) dataset are changing. And they're changing just to be better at the new text examples. Any words (or senses-of-words) that only appeared in earlier data aren't being updated... and so the new training unavoidably pulls current words out of the balanced relationship with older words that existed after joint training. </p>

<p>In the extreme in some neural-network models, such new-data training can lead to ""<a href=""https://en.wikipedia.org/wiki/Catastrophic_interference"" rel=""nofollow noreferrer"">catastrophic interference</a>"", making the network much worse at things it once knew. </p>

<p>It might still work out ok, if there's a good overlap of vocabulary, or the right level of re-training and balance of learning-rates is chosen... but there are no hard-and-fast rules for picking the parameters/processes that make sure such 'tuning' works. You have to monitor the quality and optimize it yourself. </p>

<p>The safest, most robust course, when significant new data arrives, is to re-train the model from scratch using all available data â€“ discarding the old model (because coordinates int he new model may not necessarily be comparable with older coordinates). It may be the case that starting this new model with vectors/weights from the old model may help it reach quality/stability sooner than starting from scratch â€“ but still wouldn't guarantee coordinate-compatibility, or necessarily make it safe to leave out any older data. </p>
",1,0,194,2018-08-24 18:11:54,https://stackoverflow.com/questions/52009779/is-it-necessary-to-mix-old-corpus-and-new-corpus-in-updating-word2vec-model
"Loss does not decrease during training (Word2Vec, Gensim)","<p>What can cause loss from <code>model.get_latest_training_loss()</code> increase on each epoch?  </p>

<p>Code, used for training: </p>

<pre><code>class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0

        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch))
        model.save(savepath)
        print(
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch ... "", sep=""\n""
            )
        if os.path.isfile(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))):
            print(""Previous model deleted "")
            os.remove(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))) 
        self.epoch += 1
        print(""Model loss:"", model.get_latest_training_loss())

    def train():

        ### Initialize model ###
        print(""Start training Word2Vec model"")

        workers = multiprocessing.cpu_count()/2

        model = Word2Vec(
            DocIter(),
            size=300, alpha=0.03, min_alpha=0.00025, iter=20,
            min_count=10, hs=0, negative=10, workers=workers,
            window=10, callbacks=[EpochSaver(""./checkpoints"")], 
            compute_loss=True
    )     
</code></pre>

<p>Output: </p>

<p>Losses from epochs (1 to 20): </p>

<pre><code>Model loss: 745896.8125
Model loss: 1403872.0
Model loss: 2022238.875
Model loss: 2552509.0
Model loss: 3065454.0
Model loss: 3549122.0
Model loss: 4096209.75
Model loss: 4615430.0
Model loss: 5103492.5
Model loss: 5570137.5
Model loss: 5955891.0
Model loss: 6395258.0
Model loss: 6845765.0
Model loss: 7260698.5
Model loss: 7712688.0
Model loss: 8144109.5
Model loss: 8542560.0
Model loss: 8903244.0
Model loss: 9280568.0
Model loss: 9676936.0
</code></pre>

<p>What am I doing wrong?</p>

<p>Language arabian. 
As input from DocIter - list with tokens. </p>
","python, gensim, word2vec, loss","<p>Up through gensim 3.6.0, the loss value reported may not be very sensible, only resetting the tally each call to <code>train()</code>, rather than each internal epoch. There are some fixes forthcoming in this issue:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/pull/2135"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/pull/2135</a></p>

<p>In the meantime, the <em>difference</em> between the previous value, and the latest, may be more meaningful. In that case, your data suggest the 1st epoch had a total loss of 745896, while the last had (9676936-9280568=) 396,368 â€“ which may indicate the kind of progress hoped-for. </p>
",6,8,7575,2018-08-27 11:48:18,https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim
Word2Vec Skipgrams - Should couples span sentences?,"<h1>Background</h1>

<p>I am trying to train a Skip-gram word2vec model using negative sampling. from what I understand I need to generate couples (target, context) and a label where 0 = not in context and 1 = in context.</p>

<h1>What I am unsure about:</h1>

<p>Should we make skipgram couples sentence by sentence? or should we flatten the sentences in to one large sentence and generate skipgrams from that? <strong>In other words, should the generated couples span sentences?</strong></p>

<p>The only difference between the two code snippets below is one of them generates couples that span the two sentences like so:</p>

<pre><code>data = ['this is some stuff.', 'I have a cookie.']
</code></pre>

<p><strong>results</strong>:</p>

<pre><code>...SNIP...
[some, have]
[stuff, this]
[stuff, is]
[stuff, some]
[stuff, i]
[stuff, have]
[stuff, a]
[i, is]
[i, some]
[i, stuff]
[i, have]
[i, a]
[i, cookie]
[have, some]
[have, stuff]
...SNIP...
</code></pre>

<p>We can see that there are couples that stretch across sentences</p>

<p>Or we can have couples that don't span sentences:</p>

<pre><code>...SNIP...
[some, stuff]
[stuff, this]
[stuff, is]
[stuff, some]
[i, have]
[i, a]
[i, cookie]
[have, i]
[have, a]
[have, cookie]
...SNIP...
</code></pre>

<h1>What I have done so far.</h1>

<p><strong>Get data</strong></p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train',
                          remove=('headers', 'footers', 'quotes'))
</code></pre>

<p><strong>Initialize some variables</strong></p>

<pre><code>vocabulary_size = 8
window_size = 3 
neg_samples = 0.0
</code></pre>

<p><strong>Flatten the sentences into one large sequence</strong></p>

<pre><code>sents = newsgroups_train.data
tokenizer = Tokenizer(num_words= vocabulary_size, lower=True, filters=filters)
tokenizer.fit_on_texts(sents)
word_index_inv = {v: k for k, v in tokenizer.word_index.items()}
sequences = tokenizer.texts_to_sequences(sents)
couples, labels = skipgrams(list(itertools.chain.from_iterable(sequences)), vocabulary_size=vocabulary_size, window_size=window_size, shuffle=False, negative_samples=neg_samples)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype=""int32"")
word_context = np.array(word_context, dtype=""int32"")
</code></pre>

<h1>OR:</h1>

<p><strong>Split dataset into sentences and generate couples based off each sentence.</strong></p>

<pre><code>sents = [nltk.sent_tokenize(s) for s in newsgroups_train.data]
sents = list(itertools.chain.from_iterable(sents))

tokenizer = Tokenizer(num_words= vocabulary_size, lower=True, filters=filters)
tokenizer.fit_on_texts(sents)
word_index_inv = {v: k for k, v in tokenizer.word_index.items()}
sequences = tokenizer.texts_to_sequences(sents)    
couples = []
labels = []
for seq in sequences:
    c,l = skipgrams(seq, vocabulary_size=vocabulary_size, 
            window_size=window_size, shuffle=False, 
            negative_samples=neg_samples)
    couples.extend(c)
    labels.extend(l)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype=""int32"")
word_context = np.array(word_context, dtype=""int32"")
</code></pre>

<p><strong>Print out our words</strong></p>

<pre><code>for couple in couples:
    print('[{}, {}]'.format(word_index_inv[couple[0]], word_index_inv[couple[1]]))
</code></pre>
","python, keras, word2vec, word-embedding","<p>Usually it doesn't matter that much either way. </p>

<p>Even when libraries/APIs talk about ""sentences"" they really mean ""texts"" which might be multiple sentences.</p>

<p>The worst case, if you wind up with some contexts spanning texts that have no essential relation, is that it adds a little noise to training... which might require a little more training to reach optimality on the other non-noise contexts. But often run-together texts are actually related, from the same original source, so such contexts might still capture real useful patterns, and thus be a net positive compared to smaller text fragments. </p>

<p>You could try it both ways and score the results against each other to see if one or the other is better with your corpus and end task. </p>
",1,0,345,2018-08-27 22:14:28,https://stackoverflow.com/questions/52047625/word2vec-skipgrams-should-couples-span-sentences
Wrong length for Gensim Word2Vec&#39;s vocabulary,"<p>I am trying to train the <code>Gensim Word2Vec</code> model by:</p>

<pre><code>X = train['text']    

model_word2vec = models.Word2Vec(X.values, size=150)
model_word2vec.train(X.values, total_examples=len(X.values), epochs=10)
</code></pre>

<p>after the training, I get a small vocabulary (<code>model_word2vec.wv.vocab</code>) of length <code>74</code> containing only the alphabet's letters.</p>

<p>How could I get the right vocabulary?</p>

<p><strong>Update</strong></p>

<p>I tried this before:</p>

<pre><code>tokenizer = Tokenizer(lower=True)
tokenized_text = tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

model_word2vec.train(sequence, total_examples=len(X.values), epochs=10
</code></pre>

<p>but I got the same wrong vocabulary size.</p>
","nlp, gensim, word2vec","<p>Supply the model with the kind of corpus it needs: a sequence of texts, where each text is a <em>list-of-string-tokens</em>. If you supply it with non-tokenized strings instead, it will think each single character is a token, giving the results you're seeing. </p>
",0,0,142,2018-08-28 15:33:05,https://stackoverflow.com/questions/52061585/wrong-length-for-gensim-word2vecs-vocabulary
How can I group text questions that are similar to each other?,"<p>I have a dataset of 200k questions, and I would like to group them together by similarity/duplicates.</p>

<p>How can I use NLP/machine learning to group these questions with similar intents together?</p>

<p>Given a question and a list of questions, how can I find the question or questions that are similar or duplicates?</p>

<p>Are there any services that can do this?</p>
","machine-learning, nlp, word2vec","<p>Generally, you'd want to convert the questions into a abstract numerical format (such as a single high-dimensional vector, or 'bags of words/vectors'), from which it is then possible to calculate numerical pairwise similarities between questions. </p>

<p>For example: you could turn each question into a simple average of the word-vectors for its individual words. (Those word-vectors might come from your own training corpus, that matches the questions' usage domain exactly, or from some other outside source that's good enough.) </p>

<p>If the word-vectors are 300-dimensional, averaging all the words-vectors of a question together then gives you a 300-dimensional vector for the question. You can then use a typical measure of vector-similarity, such as ""cosine similarity"", to get a number from -1.0 to 1.0 for each pair of questions, with larger values indicating ""more similar"". </p>

<p>Such a simple approach is often a good baseline. Being smarter about dropping some words, or weighting words by their observed significance (eg by ""TF/IDF"" weighting) may improve it. </p>

<p>But there are other ways to get summary vectors that may work better than a simple average. One relatively straightforward algorithm, largely similar to the way word-vectors are created, is called ""Paragraph Vectors"", and is sometimes called in popular libraries (like Python gensim) ""Doc2Vec"". It's not quite a simple average of word-vectors, as much as creating a synthetic word-like token for a full text, which then is trained to be as good as possible at predicting the text's words. Again, once you have a (for example) 300-dimensional text-vector, calculating cosine-similarity can rank question similarities. </p>

<p>There's also an interesting algorithm called ""Word Mover's Distance"", which leaves the texts as variable-sized bags of each constituent word-vector, as if each word-vector was a pile-of-meaning. It then calculates the ""effort"" to move the piles from one text's shape-of-piles, to another text's â€“ and less effort seems to correlate well with humans' sense of text similarity. (However, finding these minimal-shifts is a <em>lot</em> more computationally expensive than simple cosine-similarity â€“ so this works best with short texts, or small corpuses, or when you can massively parallelize the computation.) </p>

<p>Once you have any of these numeric-similarity measures working, then you can also clustering algorithms to find groups of highly-related questions â€“ and often once you have those groups, the most-common words in those groups (as opposed to others), or human editorial work, can name the groups. </p>
",1,-1,892,2018-08-31 21:07:48,https://stackoverflow.com/questions/52122595/how-can-i-group-text-questions-that-are-similar-to-each-other
Using pretrained gensim Word2vec embedding in keras,"<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>
","python, keras, gensim, word2vec, word-embedding","<p>Let's say you have following data that you need  to encode</p>

<pre><code>docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']
</code></pre>

<p>You must then tokenize it using the <code>Tokenizer</code> from Keras like this and find the <code>vocab_size</code></p>

<pre><code>t = Tokenizer()
t.fit_on_texts(docs)
vocab_size = len(t.word_index) + 1
</code></pre>

<p>You can then enocde it to sequences like this</p>

<pre><code>encoded_docs = t.texts_to_sequences(docs)
print(encoded_docs)
</code></pre>

<p>You can then pad the sequences so that all the sequences are of a fixed length</p>

<pre><code>max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
</code></pre>

<p>Then use the word2vec model to make embedding matrix </p>

<pre><code># load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r')
    lines = file.readlines()[1:]
    file.close()
    # create a map of words to vectors
    embedding = dict()
    for line in lines:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = asarray(parts[1:], dtype='float32')
    return embedding

# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = zeros((vocab_size, 100))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = embedding.get(word)
    return weight_matrix

# load embedding from file
raw_embedding = load_embedding('embedding_word2vec.txt')
# get vectors in the right order
embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)
</code></pre>

<p>Once you have the embedding matrix you can use it in <code>Embedding</code> layer like this</p>

<pre><code>e = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=4, trainable=False)
</code></pre>

<p>This layer can be used in making a model like this</p>

<pre><code>model = Sequential()
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)
model.add(e)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)
</code></pre>

<p>All the codes are adapted from <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""noreferrer"">this</a> awesome blog post. follow it to know more about Embeddings using Glove</p>

<p>For using word2vec see <a href=""https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"" rel=""noreferrer"">this</a> post</p>
",18,12,16281,2018-09-01 08:53:27,https://stackoverflow.com/questions/52126539/using-pretrained-gensim-word2vec-embedding-in-keras
Gensim: raise KeyError(&quot;word &#39;%s&#39; not in vocabulary&quot; % word),"<p>I have this code and I have list of article as dataset. Each raw has an article. </p>

<p>I run this code:</p>

<pre><code>import gensim    
docgen = TokenGenerator( raw_documents, custom_stop_words )    
# the model has 500 dimensions, the minimum document-term frequency is 20    
w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)    
print( ""Model has %d terms"" % len(w2v_model.wv.vocab) )    
w2v_model.save(""w2v-model.bin"")    
# To re-load this model, run    
#w2v_model = gensim.models.Word2Vec.load(""w2v-model.bin"")    
    def calculate_coherence( w2v_model, term_rankings ):
        overall_coherence = 0.0
        for topic_index in range(len(term_rankings)):
            # check each pair of terms
            pair_scores = []
            for pair in combinations(term_rankings[topic_index], 2 ):
                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )
            # get the mean for all pairs in this topic
            topic_score = sum(pair_scores) / len(pair_scores)
            overall_coherence += topic_score
        # get the mean score across all topics
        return overall_coherence / len(term_rankings)

import numpy as np    
def get_descriptor( all_terms, H, topic_index, top ):    
    # reverse sort the values to sort the indices    
    top_indices = np.argsort( H[topic_index,:] )[::-1]    
    # now get the terms corresponding to the top-ranked indices    
    top_terms = []    
    for term_index in top_indices[0:top]:    
        top_terms.append( all_terms[term_index] )    
    return top_terms    
from itertools import combinations    
k_values = []    
coherences = []    
for (k,W,H) in topic_models:    
    # Get all of the topic descriptors - the term_rankings, based on top 10 terms
    term_rankings = []    
    for topic_index in range(k):
        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )

    # Now calculate the coherence based on our Word2vec model
    k_values.append( k )
    coherences.append( calculate_coherence( w2v_model, term_rankings ) )
    print(""K=%02d: Coherence=%.4f"" % ( k, coherences[-1] ) )
</code></pre>

<p>I face with this error:</p>

<pre><code>raise KeyError(""word '%s' not in vocabulary"" % word)
</code></pre>

<p>KeyError: u""word 'business' not in vocabulary""</p>

<p>The original code works great with their data set. </p>

<p><a href=""https://github.com/derekgreene/topic-model-tutorial"" rel=""nofollow noreferrer"">https://github.com/derekgreene/topic-model-tutorial</a></p>

<p>Could you help what this error is?</p>
","python, nlp, gensim, word2vec, topic-modeling","<p>It could help answerers if you included more of the information around the error message, such as the multiple-lines of call-frames that will clearly indicate which line of your code triggered the error. </p>

<p>However, if you receive the error <code>KeyError: u""word 'business' not in vocabulary""</code>, you can trust that your <code>Word2Vec</code> instance, <code>w2v_model</code>, never learned the word <code>'business'</code>. </p>

<p>This might be because it didn't appear in the training data the model was presented, or perhaps appeared but fewer than <code>min_count</code> times. </p>

<p>As you don't show the type/contents of your <code>raw_documents</code> variable, or code for your <code>TokenGenerator</code> class, it's not clear why this would have gone wrong â€“ but those are the places to look. Double-check that <code>raw_documents</code> has the right contents, and that individual items inside the <code>docgen</code> iterable-object look like the right sort of input for <code>Word2Vec</code>. </p>

<p>Each item in the <code>docgen</code> iterable object should be a list-of-string-tokens, not plain strings or anything else. And, the <code>docgen</code> iterable must be possible of being iterated-over multiple times. For example, if you execute the following two lines, you should see the same two lists-of-string tokens (looking something like <code>['hello', 'world']</code>:</p>

<pre><code>print(iter(docgen).next())
print(iter(docgen).next())
</code></pre>

<p>If you see plain strings, <code>docgen</code> isn't providing the right kind of items for <code>Word2Vec</code>. If you only see one item printed, <code>docgen</code> is likely a simple single-pass iterator, rather than an iterable object. </p>

<p>You could also enable logging at the <code>INFO</code> level and watch the output during the <code>Word2Vec</code> step carefully, and pay extra attention to any numbers/steps that seem incongruous. (For example, do any steps indicate nothing is happening, or do the counts of words/text-examples seem off?)</p>
",2,1,1603,2018-09-02 17:21:53,https://stackoverflow.com/questions/52139386/gensim-raise-keyerrorword-s-not-in-vocabulary-word
H2O Word2Vec inconsistent vectors,"<p>I have a general question on a specific topic. </p>

<p>I am using the vectors generated by Word2Vec to feed as features into my Distributed Random Forest model for classifying some records. I have millions of records and am receiving new records on a daily basis. Because of the new records coming in I want the new records to be encoded with the same vector model as the previous records. Meaning that the word ""AT"" will be the same vector now and in the future. 
I know that Word2Vec uses a random seed to generate the vectors for the words in the corpus but I want to turn this off. I need to set the seed such that if I train a model on a section of the data today and then again on the same data in the future, I want it to generate the same model with the exact same vectors for each word.
The problem with generating new models and then encoding is that it takes a great deal of time to encode these records and then on top of that my DRF model for classification isn't any good anymore because the vector for the words have changed. So I have to retrain a new DRF.
Normally this would not be an issue since I could just train one model each and then use that forever;However I know that a good practice is to update your packages on the regular. This is a problem for h2o since once you update there is no backward comparability with model generated on previous version.</p>

<p>Are there any sources that I could read on how to set the seed on the Word2Vec model for h2o in python? I am using Python version 3 and h2o version 3.18</p>
","python, word2vec, h2o","<p>word2vec in h2o-3 uses hogwild implementation - the model parameters are updated concurrently from multiple threads and it is not possible to guarantee the reproducibility in this implementation.</p>

<p>How big is your text corpus? At the cost of a slowdown of the model training you could get reproducible result with limiting the algo to use just a single thread (h2o start-up parameter <code>-nthread</code>).</p>
",1,0,181,2018-09-06 18:54:20,https://stackoverflow.com/questions/52210521/h2o-word2vec-inconsistent-vectors
TypeError: ufunc &#39;add&#39; did not contain a loop with signature matching types dtype,"<p>I would like to pass the <code>X_train_word2vec</code> vector as input to <code>Gensim Word2Vec</code> model.
The vector type is <code>numpy.ndarray</code>, at example:</p>

<pre><code>X_train_word2vec[9] = array([   19,     7,     1, 20120,     2,     1,   856,   233,   671,
       1,  1208,  6016,     2,    32,     0,     0,     0,     0, ....)]
</code></pre>

<p>When I run this code:</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
model_word2vec.train(X_train_word2vec,total_examples=X_train_word2vec.shape[0], epochs=10)
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U11') dtype('&lt;U11') dtype('&lt;U11')</code></p>

<p>I have read <a href=""https://stackoverflow.com/questions/44527956/python-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtype"">this</a> post, where the issue is due to different data types in the input array but, in my case, I have all the data of the same type: <code>int</code>.</p>

<p><strong>Update:</strong>
The code before <code>model_Word2Vec</code>:</p>

<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

seq_max_len = 50
X_seq = pad_sequences(sequenza, maxlen=seq_max_len,padding='post',truncating='post',dtype=int)

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_seq, y_cat, test_size=0.2, random_state=123)
</code></pre>
","python, gensim, word2vec","<p>Gensim's <code>Word2Vec</code> requires a corpus of texts â€“ such as in its intializer's 1st argument â€“ that's an iterable sequence object of lists-of-string-tokens. It doesn't take a raw numpy array. </p>

<p>Further, if you do supply a corpus at instantiation, as in your line of code...</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
</code></pre>

<p>...then it will automatically do its vocabulary-building and training steps. You don't need to then call <code>train()</code> explicitly. (And, while it's possible to call <code>train()</code> again, very few users doing very advanced things will need to do so. The usual, safe approach is a single training session on a complete corpus, after which the model is ""done"".)</p>

<p>Finally, <code>train()</code> also expects any corpus as an iterable sequence object of lists-of-string-tokens.</p>

<p>If you supply the right kind of corpus, it's doubtful you'll receive an error like you're getting. </p>
",1,1,874,2018-09-07 10:24:05,https://stackoverflow.com/questions/52220514/typeerror-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtyp
What&#39;s the difference between Skip-gram word2vec and CBOW w2v during training with gensim library?,"<p>For Skip-gram word2vec training samples are obtained as follows:</p>

<pre><code>Sentence: The fox was running across the maple forest
</code></pre>

<p>The word <code>fox</code> give next pairs for training:</p>

<pre><code>fox-run, fox-across, fox-maple, fox-forest
</code></pre>

<p>and etc. for every word. CBOW w2v use reverse approach:</p>

<pre><code>run-fox, across-fox, maple-fox, forest-fox
</code></pre>

<p>or for <code>forest</code> word:</p>

<pre><code>fox-forest, run-forest, across-forest, maple-forest
</code></pre>

<p>So we get all the pairs. What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library, if we do not specify the target word when training in the CBOW-mode? In both cases all pairs of words are used, or not?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>Only skip-gram uses training pairs of the form <code>(context_word)-&gt;(target_word)</code>.</p>

<p>In CBOW, the training examples are <code>(average_of_multiple_context_words)-&gt;(target_word)</code>. So, when the error from a single training example is backpropagated, multiple context-words get the same corrective nudge.</p>
",7,2,1913,2018-09-10 06:22:37,https://stackoverflow.com/questions/52252119/whats-the-difference-between-skip-gram-word2vec-and-cbow-w2v-during-training-wi
Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models,"<p>I build two word embedding (word2vec models) using <code>gensim</code> and save it as (word2vec1 and word2vec2) by using the <code>model.save(model_name)</code> command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as <code>a</code>). </p>

<p>How to compute the degree of similarity (<code>cosine-similarity or similarity</code>) of the extracted top word (say 'a'), for the two word2vec models? Does <code>most_similar()</code> will work in this case efficiently? </p>

<p>I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?</p>

<p>Any idea is deeply appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding","<p>You seem to have the wrong idea about word2vec. It doesn't provide one absolute vector for one word. It manages to find a representation for a word relative to other words. So, for the same corpus, if you run word2vec twice, you will get 2 different vectors for the same word. The meaning comes in when you compare it relative to other word vectors. </p>

<p><code>king</code> - <code>man</code> will always be close(cosine similarity wise) to <code>queen</code> - <code>woman</code> no matter how many time you train it. But they will have different vectors after each train.</p>

<p>In your case, since the 2 models are trained differently, comparing vectors of the same word is the same as comparing two random vectors. You should rather compare the relative relations. Maybe something like: <code>model1.most_similar('dog')</code> vs <code>model2.most_similar('dog')</code></p>

<p>However, to answer your question, if you wanted to compare the 2 vectors, you could do it as below. But the results will be meaningless.</p>

<p>Just take the vectors from each model and manually calculate cosine similarity.</p>

<pre><code>vec1 = model1.wv['computer']
vec2 = model2.wv['computer']
print(np.sum(vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))
</code></pre>
",5,1,2958,2018-09-11 13:43:28,https://stackoverflow.com/questions/52277384/calculation-of-cosine-similarity-of-a-single-word-in-2-different-word2vec-models
What is the operation behind the word analogy in Word2vec?,"<p>According to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">https://code.google.com/archive/p/word2vec/</a>: </p>

<blockquote>
  <p>It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.</p>
</blockquote>

<p>So we can try from the supplied demo script:</p>

<pre><code>+ ../bin/word-analogy ../data/text8-vector.bin
Enter three words (EXIT to break): paris france berlin

Word: paris  Position in vocabulary: 198365

Word: france  Position in vocabulary: 225534

Word: berlin  Position in vocabulary: 380477

                                              Word              Distance
------------------------------------------------------------------------
                                           germany      0.509434
                                          european      0.486505
</code></pre>

<p>Please note that <code>paris france berlin</code> is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in <code>Gensim</code> and try to compute the vectors myself. For example:</p>

<pre><code>&gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(BIGDATA, binary=True)
&gt;&gt;&gt; v = word_vectors['paris'] - word_vectors['france'] + word_vectors['berlin']
&gt;&gt;&gt; word_vectors.most_similar(np.array([v]))
[('berlin', 0.7331711649894714), ('paris', 0.6669869422912598), ('kunst', 0.4056406617164612), ('inca', 0.4025722146034241), ('dubai', 0.3934606909751892), ('natalie_portman', 0.3909246325492859), ('joel', 0.3843030333518982), ('lil_kim', 0.3784593939781189), ('heidi', 0.3782389461994171), ('diy', 0.3767407238483429)]
</code></pre>

<p>So, what is the word analogy actually doing? How should I reproduce it?</p>
","python, gensim, word2vec, word-embedding","<p>You should be clear about exactly which word-vector set you're using: different sets will have a different ability to perform well on analogy tasks. (Those trained on the tiny <code>text8</code> dataset might be pretty weak; the big <code>GoogleNews</code> set Google released would probably do well, at least under certain conditions like discarding low-frequnecy words.)</p>

<p>You're doing the wrong arithmetic for the analogy you're trying to solve. For an analogy ""A is to B as C is to ?"" often written as:</p>

<pre><code>A : B :: C : _?_
</code></pre>

<p>You begin with 'B', subtract 'A', then add 'C'. So the example:</p>

<pre><code>France : Paris :: Italy : _?_
</code></pre>

<p>...gives the formula in your excerpted text:</p>

<pre><code>wv('Paris') - wv('France') + wv('Italy`) = target_coordinates  # close-to wv('Rome')
</code></pre>

<p>And to solve instead:</p>

<pre><code>Paris : France :: Berlin : _?_
</code></pre>

<p>You would try:</p>

<pre><code>wv('France') - wv('Paris') + wv('Berlin') = target_coordinates
</code></pre>

<p>...then see what's closest to <code>target_coordinates</code>. (Note the difference in operation-ordering to your attempt.)</p>

<p>You can think of it as:</p>

<ol>
<li>start at a country-vector ('France')</li>
<li>subtract the (country&amp;capital)-vector ('Paris'). This leaves you with an interim vector that's, sort-of, ""zero"" country-ness, and ""negative"" capital-ness.</li>
<li>add another (country&amp;capital)-vector ('Berlin'). This leaves you with a result vector that's, again sort-of, ""one"" country-ness, and ""zero"" capital-ness. </li>
</ol>

<p>Note also that <code>gensim</code>'s <code>most_similar()</code> takes multiple positive and negative word-examples, to do the arithmetic for you. So you can just do:</p>

<pre><code>sims = word_vectors.most_similar(positive=['France', 'Berlin'], negative=['Paris'])
</code></pre>
",5,5,4769,2018-09-17 09:30:43,https://stackoverflow.com/questions/52364632/what-is-the-operation-behind-the-word-analogy-in-word2vec
Word2Vec is it for word only in a sentence or for features as well?,"<p>I would like to ask more about Word2Vec:</p>

<p>I am currently trying to build a program that check for the embedding vectors for a sentence. While at the same time, I also build a feature extraction using sci-kit learn to extract the lemma 0, lemma 1, lemma 2 from the sentence.</p>

<p>From my understanding;</p>

<p>1) Feature extractions : Lemma 0, lemma 1, lemma 2
2) Word embedding: vectors are embedded to each character (this can be achieved by using gensim word2vec(I have tried it))</p>

<p>More explanation:</p>

<p>Sentence = ""I have a pen"".
Word = token of the sentence, for example, ""have""</p>

<p>1) Feature extraction</p>

<p>""I have a pen"" --> lemma 0:I, lemma_1: have, lemma_2:a.......lemma 0:have, lemma_1: a, lemma_2:pen and so on.. Then when try to extract the feature by using one_hot then will produce:</p>

<pre><code>[[0,0,1],
[1,0,0],
[0,1,0]]
</code></pre>

<p>2) Word embedding(Word2vec)</p>

<p>""I have a pen"" ---> ""I"", ""have"", ""a"", ""pen""(tokenized) then word2vec from gensim will produced matrices for example if using window_size = 2 produced:</p>

<pre><code>[[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345]
]
</code></pre>

<p><strong>The floating and integer numbers are for explanation purpose and original data should vary depending on the sentence. These are just dummy data to explain.*</strong></p>

<p>Questions:</p>

<p>1) Is my understanding about Word2Vec correct? If yes, what is the difference between feature extraction and word2vec?
2) I am curious whether can I use word2vec to get the feature extraction embedding too since from my understanding, word2vec is only to find embedding for each word and not for the features. </p>

<p>Hopefully someone could help me in this.</p>
",word2vec,"<p>It's not completely clear what you're asking, as you seem to have many concepts mixed-up together. (Word2Vec gives vectors per word, not character; word-embeddings are a kind of feature-extraction on words, rather than an alternative to 'feature extraction'; etc. So: I doubt your understanding is yet correct.) </p>

<p>""Feature extraction"" is a very general term, meaning any and all ways of taking your original data (such as a sentence) and creating a numerical representation that's good for other kinds of calculation or downstream machine-learning. </p>

<p>One simple way to turn a corpus of sentences into numerical data is to use a ""one-hot"" encoding of which words appear in each sentence. For example, if you have the two sentences...</p>

<pre><code>['A', 'pen', 'will', 'need', 'ink']
['I', 'have', 'a', 'pen']
</code></pre>

<p>...then you have 7 unique case-flattened words...</p>

<pre><code>['a', 'pen', 'will', 'need', 'ink', 'i', 'have']
</code></pre>

<p>...and you could ""one-hot"" the two sentences as a 1-or-0 for each word they contain, and thus get the 7-dimensional vectors:</p>

<pre><code> [1, 1, 1, 1, 1, 0, 0]  # A pen will need ink
 [1, 1, 0, 0, 0, 1, 1]  # I have a pen
</code></pre>

<p>Even with this simple encoding, you can now compare sentences mathematically: a euclidean-distance or cosine-distance calculation between those two vectors will give you a summary distance number, and sentences with no shared words will have a high 'distance', and those with many shared words will have a small 'distance'. </p>

<p>Other very-similar possible alternative feature-encodings of these sentences might involve counts of each word (if a word appeared more than once, a number higher than <code>1</code> could appear), or weighted-counts (where words get an extra significance factor by some measure, such as the common ""TF/IDF"" calculation, and thus values scaled to be anywhere from 0.0 to values higher than 1.0).</p>

<p>Note that you can't encode a single sentence as a vector that's just as wide as its own words, such as ""I have a pen"" into a 4-dimensional <code>[1, 1, 1, 1]</code> vector. That then isn't comparable to any other sentence. They all need to be converted to the same-dimensional-size vector, and in ""one hot"" (or other simple ""bag of words"") encodings, that vector is of dimensionality equal to the <em>total vocabulary</em> known among all sentences. </p>

<p><code>Word2Vec</code> is a way to turn individual words into ""dense"" embeddings with fewer dimensions but many non-zero floating-point values in those dimensions. This is instead of sparse embeddings, which have many dimensions that are mostly zero. The 7-dimensional sparse embedding of 'pen' alone from above would be:</p>

<pre><code>[0, 1, 0, 0, 0, 0, 0]  # 'pen'
</code></pre>

<p>If you trained a 2-dimensional Word2Vec model, it might instead have a dense embedding like:</p>

<pre><code>[0.236, -0.711]  # 'pen'
</code></pre>

<p>All the 7 words would have their own 2-dimensional dense embeddings. For example (all values made up):</p>

<pre><code>[-0.101, 0.271]   # 'a'
[0.236, -0.711]   # 'pen'
[0.302, 0.293]    # 'will'
[0.672, -0.026]   # 'need'
[-0.198, -0.203]  # 'ink'
[0.734, -0.345]   # 'i'
[0.288, -0.549]   # 'have'
</code></pre>

<p>If you have <code>Word2Vec</code> vectors, then one alternative simple way to make a vector for a longer text, like a sentence, is to average together all the word-vectors for the words in the sentence. So, instead of a 7-dimensional sparse vector for the sentence, like:</p>

<pre><code>[1, 1, 0, 0, 0, 1, 1]  # I have a pen
</code></pre>

<p>...you'd get a single 2-dimensional dense vector like:</p>

<pre><code>[ 0.28925, -0.3335 ]  # I have a pen
</code></pre>

<p>And again different sentences may be usefully comparable to each other based on these dense-embedding features, by distance. Or these might work well as training data for a downstream machine-learning process. </p>

<p>So, this is a form of ""feature extraction"" that uses <code>Word2Vec</code> instead of simple word-counts. There are many other more sophisticated ways to turn text into vectors; they could all count as kinds of ""feature extraction"". </p>

<p>Which works best for your needs will depend on your data and ultimate goals. Often the most-simple techniques work best, especially once you have a lot of data. But there are few absolute certainties, and you often need to just try many alternatives, and test how well they do in some quantitative, repeatable scoring evaluation, to find which is best for your project.</p>
",2,0,1870,2018-09-18 05:07:41,https://stackoverflow.com/questions/52379317/word2vec-is-it-for-word-only-in-a-sentence-or-for-features-as-well
load a file with only its extension name,"<p>I would like to load a file for only it's extension name in gensim. </p>

<p>A normal code would be this:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(""news.bin"")
</code></pre>

<p>But I would like it to auto open any file with "".bin"".</p>

<p>Example:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(***I would like to change this part to only load any .bin***)
</code></pre>

<p>.bin files:</p>

<p>It can be ""news.bin"", ""file.bin"" or ""guess.bin"". As long as it load only the extension. Thank you.</p>
","python, gensim, word2vec","<p>If you want to open <strong>ALL</strong> of them one by one, you can iterate over files in the target directory.    </p>

<p>This is the code example for Python 3:</p>

<pre><code>import os

directory_path = ""/path/to/directory""

for filename in os.listdir(directory_path):
    if filename.endswith("".bin""): 
        file_path = os.path.join(directory_path, filename)
        model = gensim.models.word2vec.Word2Vec.load(file_path)
        # Do whatever you want to do with model
</code></pre>

<p>If you only want to open <strong>ANY ONE</strong> of them, you can break out of the for loop after the first match:</p>

<pre><code>import os

directory_path = ""/path/to/directory""

for filename in os.listdir(directory_path):
    if filename.endswith("".bin""): 
        file_path = os.path.join(directory_path, filename)
        model = gensim.models.word2vec.Word2Vec.load(file_path)
        # Do whatever you want to do with model
        # Break out of the for loop afterwards so it stops iterating
        break
</code></pre>
",1,0,102,2018-09-19 02:14:15,https://stackoverflow.com/questions/52397065/load-a-file-with-only-its-extension-name
Word2vec Skipgram codes,"<p>I build a program and a part of my program has a function to find the windowData with <code>window_size = 2</code></p>

<p><strong>My code:</strong></p>

<pre><code>string = [['I', 'have', 'a', 'pen', 'to', 'use']]

window_size = 2
windowData = []
for lines in string:
    for index,word in enumerate(lines):
        for words in lines[max(index-window_size,0):min(index+window_size,len(string)+1)]:
            if words != word:
                windowData.append([word,words])

print(windowData)
</code></pre>

<p><strong>current output:</strong></p>

<pre><code>[['I', 'have'], ['have', 'I'], ['a', 'I'], ['a', 'have'], ['pen', 'have']]
</code></pre>

<p>From my understanding about the skip-gram it should be something like this, right? (Please correct me if I am wrong)</p>

<p><strong>Expected output:</strong></p>

<pre><code>[['I', 'have'], ['I', 'a'], ['have', 'I'], ['have', 'a'], ['have', 'pen'], ['a', 'have'], ['a', 'I'], ['a', 'pen'],['a', 'to'],  ['pen', 'a'], ['pen', 'have'], ['pen', 'to'], ['pen', 'use'], ['to', 'pen'], ['to', 'a'],['to', 'use'], ['use', 'pen'],['use', 'to']]
</code></pre>

<p>I understand that learning only programming language alone is not enough but I should focus more on the problem solving. If possible, please suggest me some sites as well. Thank you. </p>
","python, word2vec","<p>A few observations:</p>

<ul>
<li><p>It's a bad idea to call a list-of-lists-of-strings by the variable name 'string'; if in fact here it's the kind of list-of-tokenized-texts commonly used in <code>Word2Vec</code>, a name like 'sentences' or 'texts' is clearer.</p></li>
<li><p>You don't want to re-enumerate <code>lines</code> each nested loop, but rather work on the current-item of the outer loop. So the loop over <code>sentences</code> would give a <code>sentence</code>. You'd loop over the <code>sentence</code> to get each <code>word</code>. </p></li>
<li><p>These context-word to target-word pairs are actually a great place to use Python's <em>tuples</em>, essentially tiny immutable lists created at the moment of need - just use parentheses rather than square-brackets. </p></li>
<li><p>You don't need to add one to the length of the <code>sentence</code> when slicing out the end-truncated window, because the length is already the actual count of elements, which is one higher than the last position. But you do need to add one to <code>index + window_size</code>, because the slicing operation (<code>[x:y]</code>) is <em>exclusive</em> of the second value (<em>y</em>). </p></li>
<li><p>If you are in fact intending for this loop to handle many texts, you may not want to return all pairs as one giant list of pairs from all texts. Instead, you might want to return one list-of-pairs per each list-of-words in the input. </p></li>
<li><p>When just starting out and struggling to understand, it helps to use very-descriptive variable names, and break out interim results to separate lines in named variables, for clarity. </p></li>
</ul>

<p>Try this minimally-altered version which reflects those changes:</p>

<pre><code>sentences = [['I', 'have', 'a', 'pen', 'to', 'use']]

window_size = 2
pairs_for_all_sentences = []

for sentence in sentences:
    this_sentence_pairs = []

    for index, target_word in enumerate(sentence): 
        window_words = sentence[max(index - window_size, 0) : min(index + window_size + 1, len(sentence))]

        for window_word in window_words:
            if window_word != target_word:
                this_sentence_pairs.append((window_word, target_word))

    pairs_for_all_sentences.append(this_sentence_pairs)

 print(pairs_for_all_sentences)
</code></pre>

<p>A final note: compared to how real skip-gram pairs are created, this isn't exactly right. While no pair is generated for the word to itself, if the same word appears within the window, a word-to-word pair is created. So in the sentence ""I am very very happy"", two <code>('very', 'very')</code> pairs will actually be trained. </p>
",2,1,545,2018-09-19 05:33:18,https://stackoverflow.com/questions/52398749/word2vec-skipgram-codes
What is vector for specific word in CBOW word2vec?,"<p>Classical CBOW word2vec looks like:</p>

<p><a href=""https://i.sstatic.net/29d91.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/29d91.png"" alt=""CBOW word2vec scheme""></a></p>

<p>What is vector for specific word in this scheme? How it obtains from WI and WO matrixes? Or useful word-vectors obtains only from Skip-gram word2vec?</p>
","nlp, word2vec","<p>With regard to the diagram you've shown, each row in the <em>WI</em> matrix is a word-vector. (After training, when you ask the model for a word like 'cat', it will find out which slot from 0 to <em>V</em> stores 'cat', then return that row of the <em>WI</em> matrix.) </p>

<p><em>WI</em> is initialized with random, low-magnitude vectors. <em>WO</em> is left as zeros at the beginning of training. During training, various rows of <em>WO</em> and <em>WI</em> are repeatedly improved, via back-propagation corrective nudges, to make the network's output layer more predictive of each (context)->(word) training example. </p>

<p>For skip-gram, you can think of the input-layer in this diagram as a one-hot encoding of the single context input-word. For CBOW, you can think of the input layer in this diagram as having the count of each word in the multi-word context as the <em>x<sub>i</sub></em> values â€“ most zero (sparse). In practice in CBOW, each word is looked up in <em>WI</em> and their word-vectors are averaged to create the hidden-layer activation. </p>

<p>Both skip-gram and CBOW work OK to create useful word-vectors inside <em>WI</em>.</p>
",1,0,216,2018-09-19 07:47:01,https://stackoverflow.com/questions/52400735/what-is-vector-for-specific-word-in-cbow-word2vec
AttributeError: module &#39;boto&#39; has no attribute &#39;plugin&#39;,"<p>I'm running a VM on Google Cloud Platform using Jupyter notebook with word2vec models. I have the following code snippet:</p>

<pre><code>from gensim.models import Word2Vec
amazon_word2vec = Word2Vec(model, min_count=1, size=100)
</code></pre>

<p>And it results in the error:</p>

<pre><code>AttributeError: module 'boto' has no attribute 'plugin'
</code></pre>

<p>What is the solution to the above problem?</p>
","python, google-cloud-platform, jupyter-notebook, google-compute-engine, word2vec","<pre><code>pip install google-compute-engine
</code></pre>

<p>install google compute engine and restart your vm and it works fine.</p>
",6,2,1910,2018-09-19 21:09:48,https://stackoverflow.com/questions/52414249/attributeerror-module-boto-has-no-attribute-plugin
How to store word vectors embeddings?,"<p>I'm very new to <code>NLP</code> and <code>Deep Learning</code> field and want to understand that after vectorization of a whole corpus using <code>Word2Vec</code>, Do I need to store the word vector values locally?
If yes I want to make a chatbot for <code>android.</code> Can anyone please guide me for this?</p>
","deep-learning, lstm, word2vec, opennlp, word-embedding","<p>word2vec embeddings can be saved:</p>

<ul>
<li>in first layers of your deep model. It's rare approach, because in this case you can't use this word2vec for other tasks.</li>
<li>as independent file on disk. It's more viable apporach for most use cases.</li>
</ul>

<p>I'd suggest to use gensim framework for training of word2vec. Here you can learn more how to train word2vec and save them to disk: <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Particularly, saving is performed via: </p>

<pre><code>model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(""word2vec.model"")
</code></pre>

<p>Training of chatbot is much more difficult problem. I can try to suggest you a possible workflow, but you should to clarify what type of chatbot do you have in mind? E.g. should it answer on any question (open domain)? Should it generate answers or it will have predefined answers only? </p>
",3,1,2760,2018-09-20 08:51:14,https://stackoverflow.com/questions/52421121/how-to-store-word-vectors-embeddings
How to clear vocab cache in DeepLearning4j Word2Vec so it will be retrained everytime,"<p>Thanks in advance.
I am using Word2Vec in DeepLearning4j.</p>

<p>How do I clear the vocab cache in Word2Vec. This is because I want it to retrain on a new set of word patterns every time I reload Word2Vec. For now, it seems that the vocabulary of the previous set of word patterns persists and I get the same result even though I changed my input training file. </p>

<p>I try to reset the model, but it doesn't work. Codes:-</p>

<p>Word2Vec vec = new Word2Vec.Builder()
                .minWordFrequency(1)
                .iterations(1)
                .layerSize(4)
                .seed(1)
                .windowSize(1)
                .iterate(iter)
                .tokenizerFactory(t)
             .resetModel(true)
             .limitVocabularySize(1)
             .build();</p>

<p>Anyone can help? </p>
","java, neural-network, word2vec, deeplearning4j","<p>If you want to retrain (this is called <em>training</em>), I understand that you just want to completely ignore previous learned model (vocabulary, words vector, ...). To do that you should create another Word2Vec object and fit it with new data. You should use an other instance for <em>SentenceIterator</em> and <em>Tokenizer</em> classes so. Your problem could be the way you change your input training files. </p>

<p>It should be ok if you just change the <em>SentenceIterator</em>, i.e : </p>

<pre><code>SentenceIterator iter = new CollectionSentenceIterator(DataFetcher.getFirstDataset());
Word2Vec vec = new Word2Vec.Builder()
            .iterate(iter)
            ....
            .build();

vec.fit();

vec.wordsNearest(""clear"", 10); // you will see results from first dataset

SentenceIterator iter2 = new CollectionSentenceIterator(DataFetcher.getSecondDataset());
vec =  new Word2Vec.Builder()
    .iterate(iter2)
    ....
    .build();

vec.fit();

vec.wordsNearest(""clear"", 10); // you will see results from second dataset, without any first dataset implication
</code></pre>

<p>If you run the code twice and you changed your input data between executions (let's say A and then B) you shouldn't have the same results. If so that's mean your model learned the same thing with input data A and B.</p>

<p>If you want to update training (this is called <em>inference</em>), I mean use previous learned model and new data to update this model, then you should use <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecUptrainingExample.java"" rel=""nofollow noreferrer"">this example</a> from dl4j examples.</p>
",0,0,251,2018-09-24 02:36:17,https://stackoverflow.com/questions/52472053/how-to-clear-vocab-cache-in-deeplearning4j-word2vec-so-it-will-be-retrained-ever
Why Gensim most similar in doc2vec gives the same vector as the output?,"<p>I am using the following code to get the ordered list of user posts.</p>

<pre><code>model = doc2vec.Doc2Vec.load(doc2vec_model_name)
doc_vectors = model.docvecs.doctag_syn0
doc_tags = model.docvecs.offset2doctag

for w, sim in model.docvecs.most_similar(positive=[model.infer_vector('phone_comments')], topn=4000):
        print(w, sim)
        fw.write(w)
        fw.write("" ("")
        fw.write(str(sim))
        fw.write("")"")
        fw.write(""\n"")

fw.close()
</code></pre>

<p>However, I am also getting the vector <code>""phone comments""</code> (that I use to find nearest neighbours) in like 6th place of the list. Is there any mistake I do in the code? or is it a issue in Gensim (becuase the vector cannot be a neighbour of itself)?</p>

<p><strong>EDIT</strong></p>

<p>Doc2vec model training code</p>

<pre><code>######Preprocessing
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^1-9a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key.replace(' ', '_')
    docs.append(analyzedDocument(words, tags.split(' ')))

sentences = []  # Initialize an empty list of sentences
######Get n-grams
#Get list of lists of tokenised words. 1 sentence = 1 list
for item in docs:
    sentences.append(item.words)

#identify bigrams and trigrams (trigram_sentences_project) 
trigram_sentences_project = []
bigram = Phrases(sentences, min_count=5, delimiter=b' ')
trigram = Phrases(bigram[sentences], min_count=5, delimiter=b' ')

for sent in sentences:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)

paper_count = 0
for item in trigram_sentences_project:
    docs[paper_count] = docs[paper_count]._replace(words=item)
    paper_count = paper_count+1

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 5, workers = 4, iter = 20)

#Save the trained model for later use to take the similarity values
model_name = user_defined_doc2vec_model_name
model.save(model_name)
</code></pre>
","nlp, data-mining, gensim, word2vec, doc2vec","<p>The <code>infer_vector()</code> method expects a list-of-tokens, just like the <code>words</code> property of the text examples (<code>TaggedDocument</code> objects, usually) that were used to train the model. </p>

<p>You're supplying a simple string, <code>'phone_comments'</code>, which will look to <code>infer_vector()</code> like the list <code>['p', 'h', 'o', 'n', 'e', '_', 'c', 'o', 'm', 'm', 'e', 'n', 't', 's']</code>. Thus your origin vector for the <code>most_similar()</code> is probably garbage. </p>

<p>Further, you're not getting back the input <code>'phone_comments'</code>, you're getting back the different string <code>'phone comments'</code>. If that's a tag-name in the model, then that must have been a supplied <code>tag</code> during model training. Its superficial similarity to <code>phone_comments</code> may be meaningless - they're different strings. </p>

<p>(But it may also hints that your training had problems, too, and trained the text that should have been <code>words=['phone', 'comments']</code> as <code>words=['p', 'h', 'o', 'n', 'e', ' ', 'c', 'o', 'm', 'm', 'e', 'n', 't', 's']</code> instead.)</p>
",1,0,538,2018-09-24 19:26:58,https://stackoverflow.com/questions/52486070/why-gensim-most-similar-in-doc2vec-gives-the-same-vector-as-the-output
How reliable is the Elbow curve in finding K in K-Means?,"<p>So I was trying to use the Elbow curve to find the value of optimum 'K' (number of clusters) in K-Means clustering. </p>

<p>The clustering was done for the average vectors (using Word2Vec) of a text column in my dataset (1467 rows). But looking at my text data, I can clearly find more than 3 groups the data can be grouped into.</p>

<p>I read the reasoning is to have a small value of k while keeping the Sum of Squared Errors (SSE) low. Can somebody tell me how reliable the Elbow Curve is? 
Also if there's something I'm missing.</p>

<p>Attaching the Elbow curve for reference. I also tried plotting it up to 70 clusters, exploratory.<a href=""https://i.sstatic.net/l4odz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l4odz.png"" alt=""enter image description here""></a>.</p>

<p><a href=""https://i.sstatic.net/0cagC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0cagC.png"" alt=""enter image description here""></a></p>
","python, r, cluster-analysis, k-means, word2vec","<p>The &quot;elbow&quot; is not even well defined so how can it be reliable?</p>
<p>You can &quot;normalize&quot; the values by the expected dropoff from splitting the data into k clusters and it will become a bit more readable.
For example, the Calinski and Harabasz (1974) <a href=""http://docode.techyoung.cn/calinski_harabasz/chi.pdf"" rel=""nofollow noreferrer"">variance ratio criterion</a>. It is essentially a rescaled version that makes <em>much</em> more sense.</p>
",2,0,1661,2018-09-26 09:41:01,https://stackoverflow.com/questions/52514645/how-reliable-is-the-elbow-curve-in-finding-k-in-k-means
Variable sentence length for LSTM using word2vec as inputs on tensorflow,"<p>I am building an LSTM Model using word2vec as an input. I am using the tensorflow framework. I have finished word embedding part, but I am stuck with LSTM part.</p>

<p>The issue here is that I have different sentence lengths, which means that I have to either do padding or use dynamic_rnn with specified sequence length. I am struggling with both of them.</p>

<ol>
<li><p>Padding.
The confusing part of padding is when I do padding. My model goes like</p>

<p>word_matrix=model.wv.syn0<br>
X = tf.placeholder(tf.int32, shape)<br>
data = tf.placeholder(tf.float32, shape)<br>
data = tf.nn.embedding_lookup(word_matrix, X)  </p></li>
</ol>

<p>Then, I am feeding sequences of word indices for word_matrix into X. I am worried that if I pad zero's to the sequences fed into X, then I would incorrectly keep feeding unnecessary input (word_matrix[0] in this case).</p>

<p>So, I am wondering what is the correct way of 0 padding. It would be great if you let me know how to implement it with tensorflow.</p>

<ol start=""2"">
<li>dynamic_rnn
For this, I have declared a list containing all the lengths of sentences and feed those along with X and y at the end. In this case, I cannot feed the inputs as batch though. Then, I have encountered this error (ValueError: as_list() is not defined on an unknown TensorShape.), which seems to me that sequence_length argument only accepts list? (My thoughts might be entirely incorrect though).</li>
</ol>

<p>The following is my code for this.</p>

<pre><code>X = tf.placeholder(tf.int32)
labels = tf.placeholder(tf.int32, [None, numClasses])
length = tf.placeholder(tf.int32)

data = tf.placeholder(tf.float32, [None, None, numDimensions])
data = tf.nn.embedding_lookup(word_matrix, X)

lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)
lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)
initial_state=lstmCell.zero_state(batchSize, tf.float32)
value, _ = tf.nn.dynamic_rnn(lstmCell, data, sequence_length=length,
                             initial_state=initial_state, dtype=tf.float32)
</code></pre>

<p>I am so struggling with this part so that any help would be very much appreciated.</p>

<p>Thank you in advance.</p>
","python, tensorflow, lstm, word2vec","<p>Tensorflow does not support variable length Tensor. So when you declare a Tensor, the list/numpy array should have a uniform shape.</p>

<ol>
<li><p>From your 1st part, what I understand is that you were already able to pad the zeros in the last time steps of the sequence length. Which is what the ideal situation should be. Here is how it should look for a batch size of 4, <strong>max</strong> sequence length 10 and 50 hidden units -></p>

<p><code>[4,10,50]</code> would be the size of your whole batch, but internally, it may be shaped like this when you try to visualize the paddings -></p>

<pre><code>`[[5+5pad,50],[10,50],[8+2pad,50],[9+1pad,50]`
</code></pre>

<p>Each pad would represent a sequence length of 1 with hidden state size 50 Tensor. All filled with nothing but zeroes. Look at <a href=""https://stackoverflow.com/questions/34141430/tensorflow-tensor-reshape-and-pad-with-zeros"">this question</a> and <a href=""https://stackoverflow.com/questions/43928642/how-does-tensorflow-pad-work"">this one</a> to know more about how to pad manually.</p></li>
<li><p>You will use dynamic rnn for the exact reason that you do not want to compute it on the padding sequences. The <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">tf.nn.dynamic_rnn</a> api will ensure that by passing the <code>sequence_length</code> argument.</p>

<p>For the above example, that argument will be: <code>[5,10,8,9]</code> for the example above. You can compute it by summing the non-zero entities for each batch component. A simple way to compute that would be:</p>

<pre><code>data_mask = tf.cast(data, tf.bool)
data_len = tf.reduce_sum(tf.cast(data_mask, tf.int32), axis=1)
</code></pre>

<p>and pass it in the <code>tf.nn.dynamic_rnn</code> api:</p>

<pre><code>tf.nn.dynamic_rnn(lstmCell, data, sequence_length=data_len, initial_state=initial_state)
</code></pre></li>
</ol>
",2,3,640,2018-09-27 07:33:12,https://stackoverflow.com/questions/52531740/variable-sentence-length-for-lstm-using-word2vec-as-inputs-on-tensorflow
Merging layers on Keras (dot product),"<p>I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it for hours and trying a lot of unsuccessful solutions.</p>

<p><a href=""https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"" rel=""noreferrer"">https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa</a></p>

<p>The step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.</p>

<p>I've seem many discussions about it, and the majority of answers was the you need to use the Functional API of Keras to merge layers now. But the problem is, I'm a total beginner in Keras and have no idea how to translate my code from Sequential to Functional, here's the code that the author used (and I copied):</p>

<pre><code>from keras.layers import Merge
from keras.layers.core import Dense, Reshape
from keras.layers.embeddings import Embedding
from keras.models import Sequential

# build skip-gram architecture
word_model = Sequential()
word_model.add(Embedding(vocab_size, embed_size,
                         embeddings_initializer=""glorot_uniform"",
                         input_length=1))
word_model.add(Reshape((embed_size, )))

context_model = Sequential()
context_model.add(Embedding(vocab_size, embed_size,
                  embeddings_initializer=""glorot_uniform"",
                  input_length=1))
context_model.add(Reshape((embed_size,)))

model = Sequential()
model.add(Merge([word_model, context_model], mode=""dot""))
model.add(Dense(1, kernel_initializer=""glorot_uniform"", activation=""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer=""rmsprop"")

# view model summary
print(model.summary())

# visualize model structure
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, 
rankdir='TB').create(prog='dot', format='svg'))
</code></pre>

<p>And when I run the block, the following error is shown:</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-79-80d604373468&gt; in &lt;module&gt;()
----&gt; 1 from keras.layers import Merge
      2 from keras.layers.core import Dense, Reshape
      3 from keras.layers.embeddings import Embedding
      4 from keras.models import Sequential
      5 

ImportError: cannot import name 'Merge'
</code></pre>

<p>What I'm asking here is some guidance on how to transform this Sequential into a Functional API structure.</p>
","python, tensorflow, keras, word2vec, word-embedding","<p>This did indeed change. For a dot product, you can now use the <code>dot</code> layer:</p>

<pre><code>from keras.layers import dot
...
dot_product = dot([target, context], axes=1, normalize=False)
...
</code></pre>

<p>You have to set the <code>axis</code> parameter according to your data, of course. If you set <code>normalize=True</code>, this gives the cosine proximity. For more information, see <a href=""https://keras.io/layers/merge/#dot_1"" rel=""noreferrer"">the documentation</a>.</p>

<p>To learn about the functional API to Keras, there is a good <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""noreferrer"">guide to the functional API</a> in the documentation. It's not difficult to switch if you already understand the sequential API.</p>
",7,5,11957,2018-09-27 17:30:06,https://stackoverflow.com/questions/52542275/merging-layers-on-keras-dot-product
loop each element and print word2vec vectors based on the element,"<p>I would like to print each vector of a word by using gensim word2vec.</p>

<p>Here is my code:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
a = [[""man"", ""eater"", ""king""]]
model = Word2Vec(a, size =100, window=1, min_count=1)
model.build_vocab(a, update=True)
model.train(a, total_examples=1, epochs=1)

"""""" I know that I could use:
    for x in a:
        print(model.wv[x])

    This is not I intended because I wanted to know whether is there a possibility
    to loop each element in a list and return it into the model.wv[element] by printing 
    every each element by using only 1 line of code""""""

#For example my intended way
print(model.wv[x for x in a])
</code></pre>

<p>However, this is list comprehensive method and it wont work. I also tried </p>

<pre><code>print(model.wv[lambda x:x, a])
</code></pre>

<p>But still dont work. Could anyone please tell me how can I print every each word vector 
without using the for loop way? I just want it to be 1 line for the print part.
If it is still not clear, please let me know.  </p>
","python, printing, word2vec","<p>Yes, you can use a list comprehension here, but not in the format you described:</p>

<p><code>[print(model.wv[x]) for x in a]</code></p>
",1,1,1009,2018-10-03 01:28:32,https://stackoverflow.com/questions/52618479/loop-each-element-and-print-word2vec-vectors-based-on-the-element
&#39;Word2Vec&#39; object has no attribute &#39;index2word&#39;,"<p>I'm getting this error ""AttributeError: 'Word2Vec' object has no attribute 'index2word'"" in following code in python. Anyone knows how can I solve it?
Acctually ""tfidf_weighted_averaged_word_vectorizer"" throws the error. ""obli.csv"" contains line of sentences. 
Thank you.</p>

<pre><code>from feature_extractors import tfidf_weighted_averaged_word_vectorizer

    dataset = get_data2()
    corpus, labels = dataset.data, dataset.target
    corpus, labels = remove_empty_docs(corpus, labels)
    # print('Actual class label:', dataset.target_names[labels[10]])

    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,
                                                                            labels,
                                                                            test_data_proportion=0.3)
    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(train_corpus)


    vocab = tfidf_vectorizer.vocabulary_
        tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train,
                                                                          tfidf_vectors=tfidf_train_features,
                                                                          tfidf_vocabulary=vocab,
                                                                          model=model,
                                                                          num_features=100)



    def get_data2():

        obli = pd.read_csv('db/obli.csv').values.ravel().tolist()
        cl0 = [0 for x in range(len(obli))]

        nonObli = pd.read_csv('db/nonObli.csv').values.ravel().tolist()
        cl1 = [1 for x in range(len(nonObli))]

        all = obli + nonObli


        db =  Db(all,cl0 + cl1)
        db.data = all
        db.target = cl0 + cl1

        return db
</code></pre>
","python-3.x, text-mining, word2vec","<p>This is code from chapter 4 of Text Analytics for Python by Dipanjan Sarkar.</p>

<p>index2word in gensim has been moved since that text was published.</p>

<p>Instead of <code>model.index2word</code> you should use <code>model.wv.index2word</code>.</p>
",9,2,6148,2018-10-03 14:14:25,https://stackoverflow.com/questions/52629055/word2vec-object-has-no-attribute-index2word
How to evaluate Word2Vec model,"<p>Hi have my own corpus and I train several Word2Vec models on it.
What is the best way to evaluate them one against each-other and choose the best one? (Not manually obviously - I am looking for various measures).</p>

<p>It worth noting that the embedding is for items and not word, therefore I can't use any existing benchmarks.</p>

<p>Thanks!</p>
","python, nlp, word2vec, embedding, word-embedding","<p>There's no generic way to assess token-vector quality, if you're not even using real words against which other tasks (like the popular analogy-solving) can be tried. </p>

<p>If you have a custom ultimate task, you have to devise your own repeatable scoring method. That will likely either be some subset of your actual final task, or well-correlated with that ultimate task. Essentially, whatever ad-hoc method you may be using the 'eyeball' the results for sanity should be systematized, saving your judgements from each evaluation, so that they can be run repeatedly against iterative model improvements. </p>

<p>(I'd need more info about your data/items and ultimate goals to make further suggestions.)</p>
",9,9,10821,2018-10-04 11:22:37,https://stackoverflow.com/questions/52645459/how-to-evaluate-word2vec-model
Find cosine distance for all pairs of word2vec encodings without using nested loops,"<p>I need to calculate and store <b>cosine distances for all pairs of words of a word2vec encoding</b>. Each word is represented as a 4 * 1 vector stored in a pandas dataframe, with each element in the contunuous range [1, 9].
I need to store the result in a pandas dataframe so that it can be accessed in constant time. 
<br><br>
I am unable to use the apply function of pandas library/lambda. Using nested loops will take approx. 9 hours (according to tqdm).</p>

<pre><code>word     word1    word2    word3 ...
word1    d11      d12      d13...
word2    d21      d22      d23...
word3    d31      d32      d33...
.
.
.
</code></pre>
","python, pandas, nlp, word2vec","<p>If you were to use something like the Python <code>gensim</code> library to load a pre-existing vector set (in the original word2vec.c format) into its <code>KeyedVectors</code> representation, then the raw vectors will be in a numpy array in its <code>vectors</code> property. For example:</p>

<pre><code>kv = KeyedVectors.load_word2vec_format('word_vectors.bin', binary=True)
print(kv.vectors.shape)
</code></pre>

<p>You could then use a library function like <code>scikit-learn</code>'s <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html"" rel=""nofollow noreferrer""><code>pairwise_distances()</code></a> to compute the distance matrix:</p>

<pre><code>from sklearn.metrics import pairwise_distances
distances = pairwise_distances(kv.vectors, metric=""cosine"")
</code></pre>

<p>Because the <code>sklearn</code> routine uses optimized native math routines, it will likely be a lot faster than your initial loops-in-pure-Python approach. Note, though, that the resulting distances matrix may be huge!</p>

<p>(You can find out which words are in which <code>kv.vectors</code> slots via the list in <code>kv.index2entity</code>, or look up the slot for a word via the dict in <code>kv.vocab</code>.)</p>
",1,3,1016,2018-10-05 10:47:45,https://stackoverflow.com/questions/52663908/find-cosine-distance-for-all-pairs-of-word2vec-encodings-without-using-nested-lo
Doc2Vec: Similarity Between Coded Documents and Unseen Documents,"<p>I have a sample of ~60,000 documents.  We've hand coded 700 of them as having a certain type of content.  Now we'd like to find the ""most similar"" documents to the 700 we already hand-coded.  We're using gensim doc2vec and I can't quite figure out the best way to do this.</p>

<p>Here's what my code looks like:</p>

<pre><code>cores = multiprocessing.cpu_count()

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
        epochs=10, workers=cores, dbow_words=1, train_lbls=False)

all_docs = load_all_files() # this function returns a named tuple
random.shuffle(all_docs)
print(""Docs loaded!"")
model.build_vocab(all_docs)
model.train(all_docs, total_examples=model.corpus_count, epochs=5)
</code></pre>

<p>I can't figure out the right way to go forward.  Is this something that doc2vec can do?  In the end, I'd like to have a ranked list of the 60,000 documents, where the first one is the ""most similar"" document.</p>

<p>Thanks for any help you might have!  I've spent a lot of time reading the gensim help documents and the various tutorials floating around and haven't been able to figure it out.</p>

<p>EDIT: I can use this code to get the documents most similar to a short sentence:</p>

<pre><code>token = ""words associated with my research questions"".split()
new_vector = model.infer_vector(token)
sims = model.docvecs.most_similar([new_vector])
for x in sims:
    print(' '.join(all_docs[x[0]][0]))
</code></pre>

<p>If there's a way to modify this to instead get the documents most similar to the 700 coded documents, I'd love to learn how to do it!</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Your general approach is reasonable. A few notes about your setup:</p>

<ul>
<li>you'd have to specify <code>epochs=10</code> in your <code>train()</code> call to truly get 10 training passes â€“ and 10 or more is most common in published work</li>
<li><code>sample</code>-controlled downsampling helps speed training and often improves vector quality as well, and the value can become more aggressive (smaller) with larger datasets</li>
<li><code>train_lbls</code> is not a parameter to <code>Doc2Vec</code> in any recent <code>gensim</code> version</li>
</ul>

<p>There are several possible ways to interpret and pursue your goal of ""find the 'most similar' documents to the 700 we already hand-coded"". For example, for a candidate document, how should its similarity to the set-of-700 be defined - as a similarity to one summary 'centroid' vector for the full set? Or as its similarity to any one of the documents? </p>

<p>There are a couple ways you could obtain a single summary vector for the set:</p>

<ul>
<li><p>average their 700 vectors together</p></li>
<li><p>combine all their words into one synthetic composite document, and <code>infer_vector()</code> on that document. (But note: texts fed to <code>gensim</code>'s optimized word2vec/doc2vec routines face an internal implementation limit of 10,000 tokens â€“ excess words are silently ignored.)</p></li>
</ul>

<p>In fact, the <code>most_similar()</code> method can take a list of multiple vectors as its 'positive' target, and will automatically average them together before returning its results. So if, say, the 700 document IDs (tags used during training) are in the list <code>ref_docs</code>, you could try...</p>

<pre><code>sims = model.docvecs.most_similar(positive=ref_docs, topn=0)
</code></pre>

<p>...and get back a ranked list of all other in-model documents, by their similarity to the average of all those <code>positive</code> examples. </p>

<p>However, the alternate interpretation, that a document's similarity to the reference-set is its highest similarity to any one document inside the set, might be better for your purpose. This could especially be the case if the reference set itself is varied over many themes â€“ and thus not well-summarized by a single average vector.</p>

<p>You'd have to compute these similarities with your own loops. For example, roughly:</p>

<pre><code>sim_to_ref_set = {}
for doc_id in all_doc_ids:
    sim_to_ref_set[doc_id] = max([model.docvecs.similarity(doc_id, ref_id) for ref_id in ref_docs])
sims_ranked = sorted(sim_to_ref_set.items(), key=lambda it:it[1], reverse=True)
</code></pre>

<p>The top items in <code>sims_ranked</code> would then be those most-similar to any item in the reference set. (Assuming the reference-set ids are also in <code>all_doc_ids</code>, the 1st 700 results will be the chosen docs again, all with a self-similarity of <code>1.0</code>.)</p>
",0,0,3418,2018-10-07 21:18:10,https://stackoverflow.com/questions/52693004/doc2vec-similarity-between-coded-documents-and-unseen-documents
Need help in creating an appropriate model to predict semantic similarity between two sentences,"<p>I am new to ML field and trying my hands on creating a model which will predict semantic similarity between two sentences.
I am using following approach:</p>

<p>1.Using word2vec model in gensim package vectorise each word present in the sentences in question</p>

<p>2.Calculate the average vector for all words in every sentence/document </p>

<pre><code>import numpy as np
from scipy import spatial

index2word_set = set(model.wv.index2word)

def avg_feature_vector(sentence, model, num_features, index2word_set):
    words = sentence.split()
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    for word in words:
        if word in index2word_set:
            n_words += 1
            feature_vec = np.add(feature_vec, model[word])
    if (n_words &gt; 0):
        feature_vec = np.divide(feature_vec, n_words)
    return feature_vec
</code></pre>

<p>3.Next calculate cosine similarity between these two average vectors</p>

<pre><code>s1_afv = avg_feature_vector('this is a sentence', model=model, 
num_features=300, index2word_set=index2word_set)
s2_afv = avg_feature_vector('this is also sentence', model=model, 
num_features=300, index2word_set=index2word_set)
sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)
print(sim)
</code></pre>

<p>Reference stackoverflow question:
<a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>

<p>Help needed for the following challenge:</p>

<p>As I want to create a model which would predict semantic similarity between two sentences, I am not quite sure about:</p>

<p>1.Which model would be best suited for this problem</p>

<p>2.Next more importantly how to train that model? </p>

<p>Should I create a matrix where each row will contain two sentences: 
sen1 and sen2 and I would vectorise them and calculate cosine similarity(as per the above mentioned approach)</p>

<p>Then for training data:</p>

<p>X_Train: avg vectors for sen1 and sen2 and their cosine similarity value</p>

<p>y_Train(prediction) : a set of binary values(1 or similar if cosine similarity > 0.7 and 0 otherwise)</p>

<p>I am quite confused whether my approach is correct and how to put a proper approach in the form of a working codebase.</p>

<p>Internet and materials available online are my only teachers to learn ML; thus requesting your guidance in help clearing my gap in understanding and help in coming up with a good working model for my problem.</p>
","python, machine-learning, nlp, data-modeling, word2vec","<p>Your general approach is reasonable. An average of the word-vectors in a sentence often works OK as a rough summary vector of the sentence. (There are many other possible techniques which might do better, but that's a good easy start.)</p>

<p>You can use someone else's pre-trained word-vectors, but if you have a good large training set of text from your domain, those word-vectors may work better. You should look for a tutorial on how to train your own word-vectors with <code>gensim</code>. For example, there'a a demo Jupyter notebook <code>word2vec.ipynb</code> included with it, in its <code>docs/notebooks</code> directory, which you can also view online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb</a></p>

<p>Your current <code>avg_feature_vector()</code> function has a number of problems. In particular:</p>

<ul>
<li><p>if you pass in the <code>model</code>, it already includes within it the fixed <code>index2word</code> list, and an already-determined number-of-dimensions â€“ so no need to pass those in redundantly</p></li>
<li><p>you're looping over all words in the model, rather than just the ones in your sentence, so not calculating just based on your sentence</p></li>
<li><p>there are better, more pythonic ways to do the various array math operations you're attempting - including in the <code>numpy</code> library a simple <code>mean()</code> function that will spare you the adding/dividing of creating the average</p></li>
</ul>

<p>You may want to fix those problems, as an exercise, but you could also use utility methods on the word-vectors <code>model</code> instead. In particular, look at <code>n_similarity()</code> - it specifically takes two sets-of-words, automatically averages each set, then reports the similarity-value (closer to 1.0 for more-similar, closer to -1.0 for least-similar) between the two sets. See:</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.n_similarity"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.n_similarity</a></p>

<p>So if you had two sentences (as strings) in <code>sent1</code> and <code>sent2</code>, and a set of word-vectors (either just-trained by you, or loaded from elsewhere) in <code>kv_model</code>, you could get the sentence's similarity via:</p>

<pre><code>kv_model.n_similarity(sent1.split(), sent2.split())
</code></pre>

<p>(You might still get errors if any of the word-tokens aren't known by the model.)</p>

<p>Whether you actually create average vectors for different sentences and store them in some list/dict/dataframe/etc, or simply remember the pairwise-similarities somewhere, will depend on what you want to do next. </p>

<p>And, after you have the basics working on this simple measure of text-similarity, you could look into other techniques. For example, another way to compare two texts using word-vectors â€“ but not via the simple average â€“ is called ""Word Mover's Distance"". (It's quite a bit slower to calculate, though.) </p>

<p>Another technique for collapsing texts into a single vector, for the purposes of comparison, is available in <code>gensim</code> as <code>Doc2Vec</code> â€“ it works a lot like <code>Word2Vec</code> but also creates vectors-per-longer-text, instead of just vectors-per-individual-word.</p>
",3,4,1112,2018-10-09 15:22:01,https://stackoverflow.com/questions/52724444/need-help-in-creating-an-appropriate-model-to-predict-semantic-similarity-betwee
Keras - Translation from Sequential to Functional API,"<p>I've been following Towards Data Science's tutorial about word2vec and skip-gram models, but I stumbled upon a problem that I cannot solve, despite searching about it a lot and trying multiple unsuccessful solutions.</p>

<p><a href=""https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"" rel=""nofollow noreferrer"">https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa</a></p>

<p>The step that it shows you how to build the skip-gram model architecture seems deprecated because of the use of the Merge layer from keras.layers.</p>

<p>What I tried to do was translate his piece of code - which is implemented in the Sequential API of Keras - to the Functional API to solve the deprecation of the Merge layer, by replacing it with the keras.layers.Dot layer. However, I'm still stuck in this step of merging the two models (word and context) into the final model, whose architecture must be like this:</p>

<p><a href=""https://i.sstatic.net/eMt8A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eMt8A.png"" alt=""Skip-gram model summary and architecture""></a></p>

<p>Here's the code that the author used:</p>

<pre><code>from keras.layers import Merge
from keras.layers.core import Dense, Reshape
from keras.layers.embeddings import Embedding
from keras.models import Sequential

# build skip-gram architecture
word_model = Sequential()
word_model.add(Embedding(vocab_size, embed_size,
                         embeddings_initializer=""glorot_uniform"",
                         input_length=1))
word_model.add(Reshape((embed_size, )))

context_model = Sequential()
context_model.add(Embedding(vocab_size, embed_size,
                  embeddings_initializer=""glorot_uniform"",
                  input_length=1))
context_model.add(Reshape((embed_size,)))

model = Sequential()
model.add(Merge([word_model, context_model], mode=""dot""))
model.add(Dense(1, kernel_initializer=""glorot_uniform"", activation=""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer=""rmsprop"")
</code></pre>

<p>And here is my attempt to translate the Sequential code implementation into the Functional one:</p>

<pre><code>from keras import models
from keras import layers
from keras import Input, Model

word_input = Input(shape=(1,))
word_x = layers.Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform')(word_input)
word_reshape = layers.Reshape((embed_size,))(word_x)

word_model = Model(word_input, word_reshape)    

context_input = Input(shape=(1,))
context_x = layers.Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform')(context_input)
context_reshape = layers.Reshape((embed_size,))(context_x)

context_model = Model(context_input, context_reshape)

model_input = layers.dot([word_model, context_model], axes=1, normalize=False)
model_output = layers.Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')

model = Model(model_input, model_output)
</code></pre>

<p>However, when executed, the following error is returned:</p>

<blockquote>
  <p>ValueError: Layer dot_5 was called with an input that isn't a symbolic
  tensor. Received type: . Full
  input: [,
  ]. All inputs to
  the layer should be tensors.</p>
</blockquote>

<p>I'm a total beginner to the Functional API of Keras, I will be grateful if you could give me some guidance in this situation on how could I input the context and word models into the dot layer to achieve the architecture in the image.              </p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You are passing <code>Model</code> instances to the layer, however as the error suggests you need to pass Keras Tensors (i.e. outputs of layers or models) to layers in Keras. You have two option here. One is to use the <code>.output</code> attribute of the <code>Model</code> instance like this:</p>

<pre><code>dot_output = layers.dot([word_model.output, context_model.output], axes=1, normalize=False)
</code></pre>

<p>or equivalently, you can use the output tensors directly:</p>

<pre><code>dot_output = layers.dot([word_reshape, context_reshape], axes=1, normalize=False)
</code></pre>

<p>Further, you need to apply the <code>Dense</code> layer which is followed on the <code>dot_output</code> and pass instances of <code>Input</code> layer as inputs of <code>Model</code>. Therefore:</p>

<pre><code>model_output = layers.Dense(1, kernel_initializer='glorot_uniform',
                            activation='sigmoid')(dot_output)

model = Model([word_input, context_input], model_output)
</code></pre>
",3,3,1118,2018-10-10 16:05:02,https://stackoverflow.com/questions/52744467/keras-translation-from-sequential-to-functional-api
Why the output of model.wv.similarity() in Word2Vec results different with model.wv.similar()?,"<p>I have trained a Word2Vec model and I am trying to use it.
When I input the most similar words of â€˜åŠ¨åŠ›', I got the output like this:</p>

<pre><code>åŠ¨åŠ›ç³»ç»Ÿ 0.6429724097251892
é©±åŠ¨åŠ› 0.5936785936355591
åŠ¨èƒ½ 0.5788494348526001
åŠ¨åŠ›è½¦ 0.5579575300216675
å¼•æ“Ž 0.5339343547821045
æŽ¨åŠ¨åŠ› 0.5152761936187744
æ‰­åŠ› 0.501279354095459
æ–°åŠ¨åŠ› 0.5010953545570374
æ”¯æ’‘åŠ› 0.48610919713974
ç²¾ç¥žåŠ›é‡ 0.47970670461654663
</code></pre>

<p>But the problem is that if I input <code>model.wv.similarity('åŠ¨åŠ›','åŠ¨åŠ›ç³»ç»Ÿ')</code> I got the result 0.0, which is not equal with </p>

<pre><code>0.6429724097251892
</code></pre>

<p>what confused me more was that when I got the next similarity of word 'åŠ¨åŠ›' and word 'é©±åŠ¨åŠ›', it showed</p>

<pre><code>3.689349e+19
</code></pre>

<p>So why ? Did I make misunderstanding with the similarity? I need someone to tell me!!
And the code is:</p>

<pre><code>res = model.wv.most_similar('åŠ¨åŠ›')
for r in res:
    print(r[0],r[1])
print(model.wv.similarity('åŠ¨åŠ›','åŠ¨åŠ›ç³»ç»Ÿ'))
print(model.wv.similarity('åŠ¨åŠ›','é©±åŠ¨åŠ›'))
print(model.wv.similarity('åŠ¨åŠ›','åŠ¨èƒ½'))
</code></pre>

<p>output:</p>

<pre><code>åŠ¨åŠ›ç³»ç»Ÿ 0.6429724097251892
é©±åŠ¨åŠ› 0.5936785936355591
åŠ¨èƒ½ 0.5788494348526001
åŠ¨åŠ›è½¦ 0.5579575300216675
å¼•æ“Ž 0.5339343547821045
æŽ¨åŠ¨åŠ› 0.5152761936187744
æ‰­åŠ› 0.501279354095459
æ–°åŠ¨åŠ› 0.5010953545570374
æ”¯æ’‘åŠ› 0.48610919713974
ç²¾ç¥žåŠ›é‡ 0.47970670461654663
0.0
3.689349e+19
2.0
</code></pre>
",word2vec,"<p>I have written a function to replace the <code>model.wv.similarity</code> method. </p>

<pre><code>def Similarity(w1,w2,model):
    A = model[w1]; B = model[w2]
    return sum(A*B)/(pow(sum(pow(A,2)),0.5)*pow(sum(pow(B,2)),0.5)
</code></pre>

<p>Where <code>w1</code> and <code>w2</code> are the words you input, <code>model</code> is the Word2Vec model you have trained.</p>
",2,0,2022,2018-10-17 06:16:27,https://stackoverflow.com/questions/52848365/why-the-output-of-model-wv-similarity-in-word2vec-results-different-with-model
"Gensim Keywords, how to load a german model?","<p>I'm try to get started with the gensim library. My goal is pretty simple. I want to use the keywords extraction provided by gensim on a german text. Unfortunately, i'm failing hard.</p>

<p>Gensim comes with a keywords extraction build in, it is build on TextRank. While the results look good on english text, it seems not to work on german. I simple installed gensim via pypi and used it out of the box. Well such AI Products are usually driven by a model. My guess is that gensim comes with a english model. A word2vec model for german is available on a <a href=""https://github.com/devmount/GermanWordEmbeddings"" rel=""nofollow noreferrer"">github page</a>.</p>

<p>But here i'm stuck, i can't find a way how the summarization module of gensim, which provides the <a href=""https://radimrehurek.com/gensim/summarization/keywords.html"" rel=""nofollow noreferrer"">keywords function</a> i'm looking for, can work with a external model.</p>

<p>So the basic question is, how do i load the german model and get keywords from german text?</p>

<p>Thanks</p>
","nlp, keyword, gensim, word2vec","<p>There's nothing in the <code>gensim</code> docs, or the <a href=""https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"" rel=""nofollow noreferrer"">original TextRank paper</a> (from 2004), suggesting that algorithm requires a Word2Vec model as input. (Word2Vec was 1st published around 2013.) It just takes word-tokens. </p>

<p>See examples of its use in the tutorial notebook that's included with <code>gensim</code>:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb</a></p>

<p>I'm not sure the same algorithm would work as well on German text, given the differing importance of compound words. (To my eyes, TextRank isn't very impressive with English, either.) You'd have to check the literature to see if it still gives respected results. (Perhaps some sort of extra stemming/intraword-tokenizing/canonicalization would help.)</p>
",0,0,2328,2018-10-21 11:14:26,https://stackoverflow.com/questions/52914701/gensim-keywords-how-to-load-a-german-model
Word2VecKeyedVectors object does not support item assignment,"<p>I think I understand the error, that you cannot change a certain index of certain types of data, however, I don't understand where in the process this is happening at.  </p>

<p>I'm trying to take a string (tweet), split it into words, run the words through the w2v dictionary (if they exist in the vocab), and then sum the values.  I don't understand why and where I'm assigning an WordToVector item (w2v) and receiving this error. </p>

<pre><code> n['new'] = n['tweet'].apply(lambda x: np.sum([x for w2v[i] in x.split() if i in w2v.vocab]))

'Word2VecKeyedVectors' object does not support item assignment
</code></pre>
","python, pandas, list-comprehension, word2vec","<p>I can't see the rest of your code, but just looking at this part, the error seems to come from the inner for loop:</p>

<pre><code>[x for w2v[i] in x.split() if i in w2v.vocab]
</code></pre>

<p>It appears that the w2v object is of type 'Word2VecKeyedVectors'. So you're trying to assign to w2v[i] each word you find on your x.split(). And this gives the error</p>

<pre><code>'Word2VecKeyedVectors' object does not support item assignment
</code></pre>
",1,1,697,2018-10-30 23:28:11,https://stackoverflow.com/questions/53074268/word2veckeyedvectors-object-does-not-support-item-assignment
How doc2vec creates vector for sentence,"<p>I am working on Doc2vec for text classification. It is creating a vector for a sentence with some given size (e.g.: 100, length of vector). I am not able to understand how it creates vector of that length.</p>

<p>i am following <a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment"" rel=""nofollow noreferrer"">this link</a>. in here they are creating a vector for sentence which will be saved in the doc2v model, i can't use this model for new data(production data) to test as there is no vector for new sentence. Error showing for new data </p>

<blockquote>
  <p>KeyError: ""tag 'Test_2028' not seen in training corpus/invalid""</p>
</blockquote>
","python, machine-learning, data-science, word2vec, doc2vec","<p>If you've created a <code>gensim</code> <code>Doc2Vec</code> model with your training data, it will only know trained vectors for the document tags that were present in the training data. </p>

<p>However, there's also the method <code>infer_vector()</code> which can infer a compatible document-vector for a new text. The new text should be tokenized the same as the training data, and passed as a list-of-string-tokens to <code>infer_vector()</code>.</p>
",0,-1,652,2018-10-31 03:47:39,https://stackoverflow.com/questions/53075994/how-doc2vec-creates-vector-for-sentence
How do I get word2vec to load a string? problemï¼š&#39;dict&#39; object has no attribute &#39;_load_specials&#39;,"<p>I have a problem when using word2vec and lstm, the code is:</p>

<pre><code>def input_transform(string):
    words=jieba.lcut(string)
    words=np.array(words).reshape(1,-1)
    model=Word2Vec.load('lstm_datamodel.pkl')
    combined=create_dictionaries(model,words)
    return combined

def lstm_predict(string):
    print ('loading model......')
    with open('lstm_data.yml', 'r') as f:
        yaml_string = yaml.load(f)
    model = model_from_yaml(yaml_string)

    print ('loading weights......')
    model.load_weights('lstm_data.h5')
    model.compile(loss='binary_crossentropy',
              optimizer='adam',metrics=['accuracy'])
    data=input_transform(string)
    data.reshape(1,-1)
    #print data
    result=model.predict_classes(data)
    if result[0][0]==1:
        print (string,' positive')
    else:
        print (string,' negative')
</code></pre>

<p>and the error is:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Python36\lib\site-packages\gensim\models\word2vec.py"", line 1312, in load
model = super(Word2Vec, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\base_any2vec.py"", line 1244, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\base_any2vec.py"", line 603, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 423, in load
obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 211, in &lt;module&gt;
lstm_predict(string)
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 191, in lstm_predict
data=input_transform(string)
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 177, in input_transform
model=Word2Vec.load('lstm_datamodel.pkl')
File ""C:\Python36\lib\site-packages\gensim\models\word2vec.py"", line 1323, in load
return load_old_word2vec(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 153, in load_old_word2vec
old_model = Word2Vec.load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 1618, in load
model = super(Word2Vec, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\old_saveload.py"", line 88, in load
obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'enter code here
</code></pre>

<p>I am sorry for including so much code. </p>

<p>This is my first time to ask on StackOverflow, and I have tried my very best to find the answer on my own, but failed. So can you help me? Thank you very much!</p>
","python-3.x, lstm, word2vec","<p>The error is occurring on the line...</p>

<pre><code>model=Word2Vec.load('lstm_datamodel.pkl')
</code></pre>

<p>...so all the other/later code you've supplied is irrelevant and superfluous. </p>

<p>The suffix of your filename, <code>lstm_datamodel.pkl</code>, suggests it may have been created via Python's <code>pickle()</code> facility. The <code>gensim</code> <code>Word2Vec.load()</code> method only expects to load models that were saved by the module's own <code>save()</code> routine, <em>not</em> any pickled object. </p>

<p>The <code>gensim</code> native <code>save()</code> does make use of pickle for some of its saving, but not all, and thus wouldn't expect a fully-pickled object in the file provided.</p>

<p>This might be cause of your problem. You could try instead a load based entirely on Python <code>pickle</code>:</p>

<pre><code>model = pickle.load('lstm_datamodel.pkl')
</code></pre>

<p>Alternatively, if you can reconstruct the model in the file, but be sure to save it via the native gensim <code>model.save(filename)</code>, that might also resolve the problem. </p>
",0,0,547,2018-11-05 16:11:24,https://stackoverflow.com/questions/53158087/how-do-i-get-word2vec-to-load-a-string-problem-dict-object-has-no-attribute
How to use scraped data from website to Word2vec Gensim,"<p>I'm pretty new to MySQL, Gensim, and Word2Vec, and I'm still learning how to use by working on my personal project.</p>
<p>I have data that I got by doing web scraping so it's not hard coded.
(I used Instagram account to get hashtag data from several post, so my data is
Instagram hashtags)</p>
<p>I'm trying to use that data in this code below:</p>
<pre><code>import pymysql.cursors
import re
from gensim.models import Word2Vec

# Connect to the database
connection = pymysql.connect(host=secrets[0],
user=username,
password=password,
db='test',
charset='charsetExample',
cursorclass=pymysql.cursors.DictCursor)

try:
    # connection to database
    with connection.cursor() as cursor:
    # cursor is iterator / 'Select' - caption is column 
     # post is the table 
     cursor.execute(&quot;SELECT caption FROM posts LIMIT 1000&quot;)
     data = cursor.fetchall()
     # list of captions
      captions = [d['caption'].lower() for d in data]
     # hashtags = [re.findall(r&quot;#([A-Za-z_0-9]+)&quot;, caption) for caption in captions]
    # hashtags = [hashtag for hashtag in hashtags if hashtag != []]
    model = Word2Vec(captions, min_count=1)
    model = Word2Vec(hashtags) 
    res = model.wv.most_similar(&quot;fitness&quot;)

    print(captions)
    print(res)

finally:
    connection.close()
</code></pre>
<p>This is the part that I'm working on and not really sure how to do:</p>
<pre><code>res = model.wv.most_similar(&quot;fitness&quot;)
</code></pre>
<p>For now I was trying to use <code>most_similar()</code> method to see how it works.
What I'm trying to do is in the <code>most_similar(&quot;value&quot;)</code> I want to use my data
which will be each hashtags that I got by scraping the Instagram website as the value.</p>
<p>Thank you!</p>
","python, mysql, gensim, word2vec","<p>OK, so, You have to train the word2vec model yourself. What you have to do is make sure that your hashtags are actually without <code>#</code> sign and lowercase.</p>

<p>Now, group the hashtags by post. So, if some post had hashtags <code>#red</code>, <code>#Wine</code>, <code>#party</code>, you should make a list from it to look like: <code>[red, wine, party]</code>. Repeat that for every post and save a list from every post to a new list. So the output from this should be list of lists: <code>[[red, wine, party], [post_2_hashtags], ...]</code>. Now you can input that to the word2vec model and train it with this line:</p>

<pre><code>model = gensim.models.Word2Vec(
    documents,
    size=150,
    window=10,
    min_count=2,
    workers=10)
model.train(documents, total_examples=len(documents), epochs=10)
model.save(""word2vec.model"")
</code></pre>

<p><code>documents</code> is a list of lists created in the previous step. You can then load the model with <code>model = gensim.models.Word2Vec.load(""word2vec.model"")</code>. And the rest is the same. You still use <code>most_similar()</code> method to get the most similar word (in this case, hashtag). </p>

<p>The only thing you have to be aware of is the vector size (<code>size</code> parameter in <code>word2vec.model</code>). You define it before training. If you have a lot of data, you set it to be a bigger number, and if you have smaller amount of data, set it to smaller number. But this is something you have to figure out since you're the only one that can see the data you have. Try playing with the <code>size</code> parameter and evaluate the model with <code>most_similar()</code> method. </p>

<p>I hope this is clear enough :)</p>
",0,0,393,2018-11-07 10:05:24,https://stackoverflow.com/questions/53187257/how-to-use-scraped-data-from-website-to-word2vec-gensim
Python - Data Encoding Vector To Word,"<p>I have a code that converts word to vector. Below is my code:</p>

<pre><code># word_to_vec_demo.py

from gensim.models import word2vec
import logging

logging.basicConfig(format='%(asctime)s : \
%(levelname)s : %(message)s', level=logging.INFO)

sentences = [['In', 'the', 'beginning', 'Abba','Yahweh', 'created', 'the',
'heaven', 'and', 'the', 'earth.', 'And', 'the', 'earth', 'was',
'without', 'form,', 'and', 'void;', 'and', 'darkness', 'was',
'upon', 'the', 'face', 'of', 'the', 'deep.', 'And', 'the',
'Spirit', 'of', 'Yahweh', 'moved', 'upon', 'the', 'face',  'of',
'the', 'waters.']]

model = word2vec.Word2Vec(sentences, size=10, min_count=1)

print(""Vector for \'earth\' is: \n"")
print(model.wv['earth'])

print(""\nEnd demo"")
</code></pre>

<p>The output is </p>

<pre><code>Vector for 'earth' is: 

[-0.00402722  0.0034133   0.01583795  0.01997946  0.04112177  0.00291858
-0.03854967  0.01581967 -0.02399057  0.00539708]
</code></pre>

<p>Is it possible to encode from array of vector to words? If yes, how will I implement it in Python?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>You can use the <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.similar_by_vector.html"" rel=""nofollow noreferrer"">similar_by_vector()</a> method from your model to find the top-N most similar words by vector.
Hope this helps.</p>
",2,0,240,2018-11-12 15:12:41,https://stackoverflow.com/questions/53265028/python-data-encoding-vector-to-word
Why does Spark&#39;s Word2Vec return a vector?,"<p>Running the <a href=""https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java"" rel=""nofollow noreferrer"">Spark's example for Word2Vec</a>, I realized that it takes in an array of string and gives out a vector. My question is, shouldn't it return a matrix instead of a vector? I was expecting one vector per input word. But it returns one vector period!</p>

<p>Or maybe it should have accepted string, instead of an array of strings (one word) as input. Then, yeah sure, it could return one vector as output. But accepting an array of strings and returning one single vector does not make sense to me.</p>

<p><strong>[UPDATE]</strong></p>

<p>Per @Shaido's request, here's the code with my minor change to print the schema for the output:</p>

<pre><code>public class JavaWord2VecExample {
    public static void main(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .appName(""JavaWord2VecExample"")
                .getOrCreate();

        // $example on$
        // Input data: Each row is a bag of words from a sentence or document.
        List&lt;Row&gt; data = Arrays.asList(
                RowFactory.create(Arrays.asList(""Hi I heard about Spark"".split("" ""))),
                RowFactory.create(Arrays.asList(""I wish Java could use case classes"".split("" ""))),
                RowFactory.create(Arrays.asList(""Logistic regression models are neat"".split("" "")))
        );
        StructType schema = new StructType(new StructField[]{
                new StructField(""text"", new ArrayType(DataTypes.StringType, true), false, Metadata.empty())
        });
        Dataset&lt;Row&gt; documentDF = spark.createDataFrame(data, schema);

        // Learn a mapping from words to Vectors.
        Word2Vec word2Vec = new Word2Vec()
                .setInputCol(""text"")
                .setOutputCol(""result"")
                .setVectorSize(7)
                .setMinCount(0);

        Word2VecModel model = word2Vec.fit(documentDF);
        Dataset&lt;Row&gt; result = model.transform(documentDF);

        for (Row row : result.collectAsList()) {
            List&lt;String&gt; text = row.getList(0);
            System.out.println(""Schema: "" + row.schema());
            Vector vector = (Vector) row.get(1);
            System.out.println(""Text: "" + text + "" =&gt; \nVector: "" + vector + ""\n"");
        }
        // $example off$

        spark.stop();
    }
}
</code></pre>

<p>And it prints:</p>

<pre><code>Schema: StructType(StructField(text,ArrayType(StringType,true),false), StructField(result,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
Text: [Hi, I, heard, about, Spark] =&gt; 
Vector: [-0.0033279924420639875,-0.0024428479373455048,0.01406305879354477,0.030621735751628878,0.00792500376701355,0.02839711122214794,-0.02286271695047617]

Schema: StructType(StructField(text,ArrayType(StringType,true),false), StructField(result,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
Text: [I, wish, Java, could, use, case, classes] =&gt; 
Vector: [-9.96453288410391E-4,-0.013741840076233658,0.013064394239336252,-0.01155538750546319,-0.010510949650779366,0.004538436819400106,-0.0036846946126648356]

Schema: StructType(StructField(text,ArrayType(StringType,true),false), StructField(result,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
Text: [Logistic, regression, models, are, neat] =&gt; 
Vector: [0.012510885251685977,-0.014472834207117558,0.002779599279165268,0.0022389178164303304,0.012743516173213721,-0.02409198731184006,0.017409833287820222]
</code></pre>

<p>Please correct me if I'm wrong, but the input is an array of strings and the output is a single vector. And I was expecting each word to be mapped into a vector.</p>
","java, apache-spark, machine-learning, word2vec, apache-spark-ml","<p>This is an attempt to justify the rationale of Spark here, and it should be read as a complement to the nice <em>programming</em> explanation already provided as an answer...</p>

<p>To start with, how exactly individual word embeddings should be combined is not in principle a feature of the Word2Vec model itself (which is about, well, <em>individual</em> words), but an issue of concern to ""higher order"" models, such as <a href=""https://github.com/klb3713/sentence2vec"" rel=""noreferrer"">Sentence2Vec</a>, Paragraph2Vec, <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">Doc2Vec</a>, <a href=""https://wikipedia2vec.github.io/wikipedia2vec/"" rel=""noreferrer"">Wikipedia2Vec</a> etc (you could name a few more, I guess...).</p>

<p>Having said that, it turns out indeed that a very first approach in combining word vectors in order to get vector representations of larger pieces of text (phrases, sentences, tweets etc) is indeed to simply average the vector representations of the constituent words, as Spark ML does. </p>

<p>Starting from the practitioner community, we have:</p>

<p><a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">How to concatenate word vectors to form sentence vector</a> (SO answer):</p>

<blockquote>
  <p>There are at least three common ways to combine embedding vectors; (a)
  summing, (b) summing &amp; averaging or (c) concatenating. [...] See <code>gensim.models.doc2vec.Doc2Vec</code>, <code>dm_concat</code> and
  <code>dm_mean</code> - it allows you to use any of those three options</p>
</blockquote>

<p><a href=""https://medium.com/@premrajnarkhede/sentence2vec-evaluation-of-popular-theories-part-i-simple-average-of-word-vectors-3399f1183afe"" rel=""noreferrer"">Sentence2Vec : Evaluation of popular theoriesâ€Šâ€”â€ŠPart I (Simple average of word vectors)</a> (blog post):</p>

<blockquote>
  <p>So whatâ€™s first thing that comes to your mind when you have word
  vectors and need to calculate sentence vector.</p>
  
  <p>Just average them?</p>
  
  <p>Yes thatâ€™s what we are going to do here.
  <a href=""https://i.sstatic.net/878in.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/878in.png"" alt=""enter image description here""></a></p>
</blockquote>

<p><a href=""https://github.com/stanleyfok/sentence2vec"" rel=""noreferrer"">Sentence2Vec</a> (Github repo):</p>

<blockquote>
  <p>Word2Vec can help to find other words with similar semantic meaning.
  However, Word2Vec can only take 1 word each time, while a sentence
  consists of multiple words. To solve this, I write the Sentence2Vec,
  which is actually a wrapper to Word2Vec. To obtain the vector of a
  sentence, I simply get the averaged vector sum of each word in the
  sentence.</p>
</blockquote>

<p>It certainly seems that, at least for practitioners, this simple averaging of the individual word vectors is far from unexpected. </p>

<p>An expected counter-argument here is that blog posts and SO answers are arguably not <em>that</em> credible sources; what about the <em>researchers</em> and the relevant <em>scientific literature</em>? Well, it turns out that this simple averaging is far from uncommon here, too:</p>

<p>From <a href=""https://arxiv.org/abs/1405.4053"" rel=""noreferrer"">Distributed Representations of Sentences and Documents</a> (Le &amp; Mikolov, Google, ICML 2014):</p>

<p><a href=""https://i.sstatic.net/dstJK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dstJK.png"" alt=""enter image description here""></a></p>

<p>From <a href=""http://www.aclweb.org/anthology/S17-2100"" rel=""noreferrer"">NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment analysis</a> (SemEval 2017, section 2.1.2):</p>

<p><a href=""https://i.sstatic.net/7MHAH.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7MHAH.png"" alt=""enter image description here""></a></p>

<hr>

<p>It should be clear by now that the particular design choice in Spark ML is far from arbitrary, or even uncommon; I have blogged about what certainly seem as <em>absurd</em> design choices in Spark ML (see <a href=""https://www.nodalpoint.com/spark-classification/"" rel=""noreferrer"">Classification in Spark 2.0: â€œInput validation failedâ€ and other wondrous tales</a>), but it seems that this is not such a case...</p>
",8,6,3215,2018-11-13 02:08:17,https://stackoverflow.com/questions/53272749/why-does-sparks-word2vec-return-a-vector
Using Word2Vec functions inside of a UDF in Apache Spark (v2.3.1),"<p>I have a dataframe which consists of two columns, one an Int and the other 
a String:</p>

<pre><code>+-------------+---------------------+
|user_id      |token                |
+-------------+---------------------+
|          419|                 Cake|
|          419|            Chocolate|
|          419|               Cheese|
|          419|                Cream|
|          419|                Bread|
|          419|                Sugar|
|          419|               Butter|
|          419|              Chicken|
|          419|               Baking|
|          419|             Grilling|
+-------------+---------------------+
</code></pre>

<p>I need to find the 250 closest tokens in the Word2Vec vocabulary, for each token in the ""token"" column. I attempted to use the <code>findSynonymsArray</code> method in a udf:</p>

<pre><code>def getSyn( w2v : Word2VecModel ) = udf { (token : String) =&gt; w2v.findSynonymsArray(token, 10)}
</code></pre>

<p>However, this udf causes <code>NullPointerException</code> when used with <code>withColumn</code>. This exception occurs even if token is hard-coded, and regardless of whether code is run locally or in cluster mode. I used a try-catch inside the udf to catch the null pointer, and it is being raised on every row. </p>

<p>I have queried the dataframe for null values, there are none in either column. </p>

<p>I also tried extracting the words and vectors from the <code>Word2VecModel</code> with <code>getVectors</code>, running my udf on the words on this dataframe, and doing an inner join with my dataframe. The same exception is raised.</p>

<p>I would greatly appreciate any help.</p>
","scala, apache-spark, user-defined-functions, word2vec, apache-spark-ml","<p>This is an expected outcome <code>Word2VecModel</code> is a distributed model, and its methods are implemented using <code>RDD</code> operations. Because of that, it cannot be used inside <code>udf</code>, <code>map</code> or any other executor-side code.</p>

<p>If you want to compute synonyms for the whole <code>DataFrame</code> you'll can try to do it manually.</p>

<ul>
<li>Load the model directly as <code>DataFrame</code> as shown for example in <a href=""https://stackoverflow.com/q/34448456/"">using Word2VecModel.transform() does not work in map function</a></li>
<li>Transform the input data.</li>
<li>Join both using <a href=""https://stackoverflow.com/q/43938672"">approximate join</a> or cross product and filter the result.</li>
</ul>
",3,2,716,2018-11-14 04:15:52,https://stackoverflow.com/questions/53293114/using-word2vec-functions-inside-of-a-udf-in-apache-spark-v2-3-1
vocab size versus vector size in word2vec,"<p>I have a data with 6200 sentences(which are triplets of form ""sign_or_symptoms diagnoses Pathologic_function""), however the unique words(vocabulary) in these sentence is 181, what would be the appropriate vector size to train a model on the sentences with such low vocabulary. Is there any resource or research on appropriate vector size depending on vocabulary size?</p>
","word2vec, word-embedding","<p>The best practice is to test it against your true end-task. </p>

<p>That's an incredibly small corpus and vocabulary-size for word2vec. It might not be appropriate at all, as it gets its power from large, varied training sets. </p>

<p>But on the bright side, you can run lots of trials with different parameters very quickly!</p>

<p>You absolutely can't use a vector dimensionality as large as your vocabulary (181), or even really very close. In such a case, the model is certain to 'overfit' â€“ just memorizing the effects of each word in isolation, with none of the necessary trading-off 'tug-of-war', forcing words to be nearer/farther to each other, that creates the special value/generality of word2vec models. </p>

<p>My very loose rule-of-thumb would be to investigate dimensionalities around the square-root of the vocabulary size. And, multiples-of-4 tend to work best in the underlying array routines (at least when performance is critical, which it might not be with such a tiny data set). So I'd try 12 or 16 dimensions first, and then explore other lower/higher values based on some quantitative quality evaluation on your real task. </p>

<p>But again, you're working with a dataset so tiny, unless your 'sentences' are actually really long, word2vec may be a very weak technique for you without more data.</p>
",1,0,1397,2018-11-20 05:44:36,https://stackoverflow.com/questions/53386911/vocab-size-versus-vector-size-in-word2vec
Clustering ip-addresses on domain names,"<p>I have an ip-network which is basically a list of sequential ip-addresses. From this list I want to cluster ranges of ip-addresses into independent entities. I want to give each IP in the range a set of properties like time to live, nameservers and domain names associated with it. </p>

<p>I then want to determine the distance between each IP-address and its neighbors and start clustering based on shortest distance.</p>

<p>My question lies in the distance function. TTL is a number so that should not be a problem. Nameservers and domain names are strings however, how would you represent those as numbers in a vector?</p>

<p>Basically if 2 IP-addresses have the same nameserver or very similar domain names (equal 2LD) you want them to have a smaller distance. I've looked into something like word2vec but can't really find a useful implementation.</p>
","python, ip, ip-address, word2vec, hierarchical-clustering","<p>I would try using difflib like this.</p>

<pre><code>from difflib import SequenceMatcher

def similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()
</code></pre>

<p>Then you can call the function against each set of names to get a similarity score and group them based on that. </p>

<pre><code>similarity(""server1"",""server1"")
1.0

similarity(""Server1"",""Server2"")
0.8571428571428571

similarity(""foo"",""bar"")
0.0
</code></pre>
",1,1,362,2018-11-21 16:33:57,https://stackoverflow.com/questions/53416627/clustering-ip-addresses-on-domain-names
what is workers parameter in word2vec in NLP,"<p>in below code .
i didn't understand the meaning of workers parameter .
model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)</p>
","python, machine-learning, nlp, word2vec","<p>workers = use this many worker threads to train the model (=faster training with multicore machines).</p>

<p>If your system is having 2 cores, and if you specify workers=2, then data will be trained in two parallel ways.</p>

<p>By default , worker = 1 i.e,  no parallelization</p>
",4,1,7694,2018-11-21 17:07:54,https://stackoverflow.com/questions/53417258/what-is-workers-parameter-in-word2vec-in-nlp
How to sentence embed from gensim Word2Vec embedding vectors?,"<p>I have a <code>pandas</code> dataframe containing descriptions. I would like to cluster descriptions based on meanings usign <code>CBOW</code>. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using <code>gensim</code> as so:</p>

<pre><code>from gensim.models import Word2Vec

vocab = pd.concat((df['description'], df['more_description']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
</code></pre>

<p>I am however a bit confused now on how to replace the full sentences from my <code>df</code> with document vectors of equal dimensions.</p>

<p>For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though <code>gensim</code>, so that I could say something like this:</p>

<pre><code>df['description'].apply(model.vectorize)
</code></pre>
","python-3.x, gensim, word2vec, word-embedding, doc2vec","<p>I think you are looking for sentence embedding. There are a lot ways of generating sentence embedding from word embeddings. You may find this useful: <a href=""https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings"">https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings</a></p>
",1,1,4581,2018-11-22 12:26:22,https://stackoverflow.com/questions/53430997/how-to-sentence-embed-from-gensim-word2vec-embedding-vectors
"Python Calculating similarity between two documents using word2vec, doc2vec","<p>I am trying to calculate similarity between two documents which are comprised of more than thousands sentences.</p>

<p>Baseline would be calculating cosine similarity using BOW.</p>

<p>However, I want to capture more of semantic difference between documents.</p>

<p>Hence, I built word embedding and calculated documents similarity by generating document vectors by simply averaging all the word vectors in each of documents and measure cosine similarity between these documents vectors. </p>

<p>However, since the size of each input document is rather big, the results I get from using the method above are very similar to simple BOW cosine similarity.</p>

<p>I have two questions, </p>

<p>Q1. I found gensim module offers soft cosine similarity. But I am having hard time understanding the difference from the methods I used above, and I think it may not be the mechanism to calculate similarity between million pairs of documents.</p>

<p>Q2. I found Doc2Vec by gensim would be more appropriate for my purpose. But I recognized that training Doc2Vec requires more RAM than I have (32GB) (the size of my entire documents is about 100GB). Would there be any way that I train the model with small part(like 20GB of them) of entire corpus, and use this model to calculate pairwise similarities of entire corpus?
If yes, then what would be the desirable train set size, and is there any tutorial that I can follow?</p>
","python, similarity, gensim, word2vec, doc2vec","<p>Ad Q1: If the similarity matrix contains the cosine similarities of the word embeddings (which it more or less does, see Equation 4 in <a href=""http://www.aclweb.org/anthology/S17-2051"" rel=""nofollow noreferrer"">SimBow at SemEval-2017 Task 3</a>) and if the word embeddings are L2-normalized, then the SCM (Soft Cosine Measure) is equivalent to averaging the word embeddings (i.e. your baseline). For a proof, see Lemma 3.3 in <a href=""https://arxiv.org/pdf/1808.09407.pdf"" rel=""nofollow noreferrer"">the Implementation Notes for the SCM</a>. My Gensim implementation of the SCM (<a href=""https://github.com/RaRe-Technologies/gensim/pull/1827"" rel=""nofollow noreferrer"">1</a>, <a href=""https://github.com/RaRe-Technologies/gensim/pull/2016"" rel=""nofollow noreferrer"">2</a>) additionally sparsifies the similarity matrix to keep the memory footprint small and to regularize the embeddings, so you will get slightly different results compared to vanilla SCM. If embedding averaging gives you similar results to simple BOW cosine similarity, I would question the quality of the embeddings.</p>

<p>Ad Q2: Training a Doc2Vec model on the entire dataset for one epoch is equivalent to training a Doc2Vec model on smaller segments of the entire dataset, one epoch for each segment. Just be aware that Doc2Vec uses document ids as a part of the training process, so you must ensure that the ids are still unique after the segmentation (i.e. the first document of the first segment must have a different id than the first document of the second segment).</p>
",1,2,1530,2018-11-25 12:25:40,https://stackoverflow.com/questions/53467414/python-calculating-similarity-between-two-documents-using-word2vec-doc2vec
"word2vec: user-level, document-level embeddings with pre-trained model","<p>I am currently developing a Twitter content-based recommender system and have a word2vec model pre-trained on 400 million tweets.</p>

<p>How would I go about using those word embeddings to create a document/tweet-level embedding and then get the user embedding based on the tweets they had posted? </p>

<p>I was initially intending on averaging those words in a tweet that had a word vector representation and then averaging the document/tweet vectors to get a user vector but I wasn't sure if this was optimal or even correct. Any help is much appreciated.</p>
","python, twitter, nlp, word2vec, word-embedding","<p>Averaging the vectors of all the words in a short text is one way to get a summary vector for the text. It often works OK as a quick baseline. (And, if all you have, is word-vectors, may be your main option.) </p>

<p>Such a representation might sometimes improve if you did a weighted average based on some other measure of relative term importance (such as TF-IDF), or used raw word-vectors (before normalization to unit-length, as pre-normalization raw magnitudes can sometimes hints at strength-of-meaning). </p>

<p>You could create user-level vectors by averaging all their texts, or by (roughly equivalently) placing all their authored words into a pseudo-document and averaging all those words together. </p>

<p>You might retain more of the variety of a user's posts, especially if their interests span many areas, by first clustering their tweets into N clusters, then modeling the user as the N centroid vectors of the clusters. Maybe even the N varies per user, based on how much they tweet or how far-ranging in topics their tweets seem to be. </p>

<p>With the original tweets, you could also train up per-tweet vectors using an algorithm like 'Paragraph Vector' (aka 'Doc2Vec' in a library like Python gensim.) But, that can have challenging RAM requirements with 400 million distinct documents. (If you have a smaller number of users, perhaps they can be the 'documents', or they could be the predicted classes of a FastText-in-classification-mode training session.)</p>
",2,1,513,2018-11-30 21:40:14,https://stackoverflow.com/questions/53565271/word2vec-user-level-document-level-embeddings-with-pre-trained-model
Different sized vectors in word2vec,"<p>I am trying to generate three different sized output vectors namely 25d, 50d and 75d. I am trying to do so by training the same dataset using the word2vec model. I am not sure how I can get three vectors of different sizes using the same training dataset. Can someone please help me get started on this? I am very new to machine learning and word2vec. Thanks</p>
","python, vector, word2vec","<p>You run the code for one model three times, each time supplying a different <code>vector_size</code> parameter to the model initialization.</p>
",1,0,34,2018-12-03 01:15:30,https://stackoverflow.com/questions/53586254/different-sized-vectors-in-word2vec
What is the meaning of &quot;size&quot; of word2vec vectors [gensim library]?,"<p>Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?</p>

<p>But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?</p>
","python, gensim, word2vec, word-embedding","<p>It is <em>not</em> the case that ""[word2vec] aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary"". </p>

<p>Rather, given a certain target dimensionality, like say 100, the Word2Vec algorithm gradually trains word-vectors of 100-dimensions to be better and better at its training task, which is predicting nearby words. </p>

<p>This iterative process tends to force words that are related to be ""near"" each other, in rough proportion to their similarity - and even further the various ""directions"" in this 100-dimensional space often tend to match with human-perceivable semantic categories. So, the famous ""wv(king) - wv(man) + wv(woman) ~= wv(queen)"" example often works because ""maleness/femaleness"" and ""royalty"" are vaguely consistent regions/directions in the space. </p>

<p>The individual dimensions, alone, don't mean anything. The training process includes randomness, and over time just does ""whatever works"". The meaningful directions are not perfectly aligned with dimension axes, but angled through all the dimensions. (That is, you're not going to find that a <code>v[77]</code> is a gender-like dimension. Rather, if you took dozens of alternate male-like and female-like word pairs, and averaged all their differences, you might find some 100-dimensional vector-dimension that is suggestive of the gender direction.)</p>

<p>You can pick any 'size' you want, but 100-400 are common values when you have enough training data. </p>
",5,2,3213,2018-12-03 05:29:29,https://stackoverflow.com/questions/53587960/what-is-the-meaning-of-size-of-word2vec-vectors-gensim-library
Online updating Word2Vec,"<p>I've got a problem with online updating my Word2Vec model.</p>

<p>I have a document and build model by it. But this document can update with new words, and I need to update vocabulary and model in general.</p>

<p>I know that in gensim 0.13.4.1 we can do this</p>

<p>My code:</p>

<pre><code>model = gensim.models.Word2Vec(size=100, window=10, min_count=5, workers=11, alpha=0.025, min_alpha=0.025, iter=20)
model.build_vocab(sentences, update=False)

model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)

model.save('model.bin')
</code></pre>

<p>And after this I have new words. For e.x.:</p>

<pre><code>sen2 = [['absd', 'jadoih', 'sdohf'], ['asdihf', 'oisdh', 'oiswhefo'], ['a', 'v', 'b', 'c'], ['q', 'q', 'q']]

model.build_vocab(sen2, update=True)
model.train(sen2, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>What's wrong and how can I solve my problem?</p>
","python, nlp, gensim, word2vec","<p>Your model is set to ignore words with fewer than 5 occurrences: <code>min_count=5</code>. It will, in fact, require at least 5 occurrences in a single <code>build_vocab()</code> call. (It won't remember there were 3 before, then see 2 new occurrences, then train on all 5. It needs all 5 or more in one batch.) </p>

<p>If you're only calling your update with the tiny dataset shown, no new words will make the cut. </p>

<p>More generally, if at all possible, you should retrain the whole model with all old and new data. That will ensure equal influence is given to old and new words, and any words are treated properly according to their combined frequency. Making small incremental updates to a <code>Word2Vec</code> model risks pulling newer words, or old words that continue to reappear, out of meaningful arrangement with older words that were only trained in the original (or earlier) batches. (Only words that go through the same interleaved training cycles are fully positionally adjusted with respect to each other.)</p>
",2,0,901,2018-12-04 10:03:24,https://stackoverflow.com/questions/53610331/online-updating-word2vec
Different results of Gensim Word2Vec Model in two editors for the same source code in same environment and platform?,"<p>I am trying to apply the word2vec model implemented in the library gensim 3.6 in python 3.7, Windows 10 machine. I have a list of sentences (each sentences is a list of words) as an input to the model after performing preprocessing.</p>

<p>I have computed the results (obtaining 10 most similar words of a given input word using <code>model.wv.most_similar</code>) in <code>Anaconda's Spyder</code> followed by <code>Sublime Text</code> editor. </p>

<p>But, I am getting different results for the same source code executed in two editors.</p>

<p>Which result should <strong>I need to choose and Why</strong>?</p>

<p>I am specifying the screenshot of the results obtained by running the same code in both spyder and sublime text. The input word for which I need to obtain 10 most similar word is <code>#universe#</code></p>

<p>I am really confused how to choose the results, on what basis? Also, I have started learning Word2Vec recently.</p>

<p>Any suggestion is appreciated.</p>

<p>Results Obtained in Spyder:</p>

<p><a href=""https://i.sstatic.net/v87AW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v87AW.png"" alt=""enter image description here""></a></p>

<p>Results Obtained using Sublime Text:
<a href=""https://i.sstatic.net/JPN0X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JPN0X.png"" alt=""enter image description here""></a></p>
","python, python-3.x, gensim, word2vec","<p>The Word2Vec algorithm makes use of randomization internally. Further, when (as is usual for efficiency) training is spread over multiple threads, some additional order-of-presentation randomization is introduced. These mean that two runs, even in the exact same environment, can have different results. </p>

<p>If the training is effective â€“ sufficient data, appropriate parameters, enough training passes â€“ all such models should be of similar quality when doing things like word-similarity, even though the actual words will be in different places. There'll be some jitter in the relative rankings of words, but the results should be broadly similar. </p>

<p>That your results are vaguely related to <code>'universe'</code> but not impressively so, and that they vary so much from one run to another, suggest there may be problems with your data, parameters, or quantity of training. (We'd expect the results to vary a little, but not that much.)</p>

<p>How much data do you have? (Word2Vec benefits from lots of varied word-usage examples.)</p>

<p>Are you retaining rare words, by making <code>min_count</code> lower than its default of 5? (Such words tend not to get good vectors, and also wind up interfering with the improvement of nearby words' vectors.)</p>

<p>Are you trying to make very-large vectors? (Smaller datasets and smaller vocabularies can only support smaller vectors. Too-large vectors allow 'overfitting', where idiosyncracies of the data are memorized rather than generalized patterns learned. Or, they allow the model to continue improving in many different non-competitive directions, so model end-task/similarity results can be very different from run-to-run, even though each model is doing about-as-well as the other on its <em>internal</em> word-prediction tasks.)</p>

<p>Have you stuck with the default <code>epochs=5</code> even with a small dataset? (A large, varied dataset requires fewer training passes - because all words appear many times, all throughout the dataset, anyway. If you're trying to squeeze results from thinner data, more <code>epochs</code> may help a little â€“ but not as much as more varied data would.)</p>
",1,0,1457,2018-12-04 17:59:28,https://stackoverflow.com/questions/53618906/different-results-of-gensim-word2vec-model-in-two-editors-for-the-same-source-co
Find similarity with doc2vec like word2vec,"<p>Is there a way to find similar docs like we do in word2vec</p>

<p>Like:</p>

<pre><code>  model2.most_similar(positive=['good','nice','best'],
    negative=['bad','poor'],
    topn=10)
</code></pre>

<p>I know we can use infer_vector,feed them to have similar ones, but I want to feed many positive and negative examples as we do in word2vec.</p>

<p>is there any way we can do that! thanks !</p>
","python, nlp, gensim, word2vec, doc2vec","<p>The doc-vectors part of a <code>Doc2Vec</code> model works just like word-vectors, with respect to a <code>most_similar()</code> call. You can supply multiple doc-tags or full vectors inside both the <code>positive</code> and <code>negative</code> parameters. </p>

<p>So you could call...</p>

<pre><code>sims = d2v_model.docvecs.most_similar(positive=['doc001', 'doc009'], negative=['doc102'])
</code></pre>

<p>...and it should work. The elements of the <code>positive</code> or <code>negative</code> lists could be doc-tags that were present during training, or raw vectors (like those returned by <code>infer_vector()</code>, or your own averages of multiple such vectors). </p>
",1,0,295,2018-12-05 08:51:11,https://stackoverflow.com/questions/53628382/find-similarity-with-doc2vec-like-word2vec
doc2vec: measurement of performance and &#39;workers&#39; parameter,"<p>I have an awfully large corpora as input to my doc2vec training, around 23mil documents streamed using an iterable function. I was wondering if it were at all possible to see the development of my training progress, possibly through finding out which iteration its currently on, words per second or some similar metric.</p>

<p>I was also wondering how to speed up the performance of doc2vec, other than reducing the size of the corpus. I discovered the <em>workers</em> parameter and I'm currently training on 4 processes; the intuition behind this number was that multiprocessing cannot take advantage of virtual cores. I was wondering if this was the case for the doc2vec <em>workers</em> parameter or if I could use 8 workers instead or even potentially higher (I have a quad-core processor, running Ubuntu).</p>

<p>I have to add that using the unix command <code>top -H</code> reports only around a 15% CPU usage per python process using 8 workers and around 27% CPU usage per process on 4 workers.</p>
","python, nlp, multiprocessing, word2vec, doc2vec","<p>If you enable logging at the INFO level you should see copious progress output. Following <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer""><code>gensim</code>'s Doc2Vec tutorial</a>, that'd look like</p>
<pre class=""lang-py prettyprint-override""><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
</code></pre>
<p>The optimal throughput for gensim's <code>Word2Vec</code> or <code>Doc2Vec</code> models is often at some level of <code>workers</code> between 3 and 12, but never more than the number of processor cores available. (There's a further optimization that's especially helpful for machines with many more cores, if you use a specific on-disk corpus format, that's available in the most recent 3.6.0 gensim release â€“ see the <a href=""https://github.com/RaRe-Technologies/gensim/releases/tag/3.6.0"" rel=""nofollow noreferrer"">release notes</a> for more info.)</p>
<p>If you're seeing such low utilization on a 4-core, 4-worker setup, the bottleneck might be your corpus iterator. If it's doing any complicated IO or regex-based text-processing, then often the training worker threads are idle waiting for the one master corpus-iterator thread to produce more text, limiting overall utilization &amp; efficiency.</p>
<p>You should try to do the complicated stuff once, and re-write the tagged/tokenized results to disk as a more simple file. Then read that with a very simple line-and-space-delimited iterator for the actual model training.</p>
<p>(If your 4 cores actually support more virtual cores, it's possible that some <code>workers</code> value up to 8 might achieve higher throughput... but only trial-and-error, with your specific model parameters, can currently find your local optimum. The optimal value can vary with other parameters like <code>size</code>, <code>window</code>, <code>negative</code>, etc.)</p>
",4,3,1614,2018-12-05 19:13:51,https://stackoverflow.com/questions/53639236/doc2vec-measurement-of-performance-and-workers-parameter
How to obtain embedded representation of single test instance after training,"<p>The first layer of my RNN is embedded layer as follows.</p>

<pre><code>visible = Input(shape=(250,)) 
embed=Embedding(vocab_size,50)(visible)     
x2=keras.layers.GRU(224, return_sequences=False)(embed)
predictions=Dense(1, activation='sigmoid')(x2)
</code></pre>

<p>I train this network and predict some output. </p>

<p>Now after training, I test it on my test data which is very straight forward. </p>

<p>I want to know the exact embeddings of my test data or for the sake of implementation, embeddings of any of my test sequence after the model has been trained. I want to use those embedding in some other application. Is there any way to extract those embedding for my test sequence data after the training?</p>

<p>Try: Let's say my model is m, the I obtain the embedding weights as follows.</p>

<pre><code>embeddings = m.layers[1].get_weights()
</code></pre>

<p>I also have a reverse dictionary <code>reverse_char_map</code> as follows.</p>

<pre><code>{1: 'c',
 2: 'C',
 3: '(',
 4: ')',
 5: 'O',
 6: '=',
 7: '1',
 8: '2',
 9: 'N',
 10: '3',
 11: 'n',
 12: '[',
 13: ']',
 14: 'S',
 15: '4',
 16: '-',
 17: 'l',
 18: '+',
 19: 'H',
 20: '5',
 21: 'F',
 22: '.',
 23: '#',
 24: 's',
 25: 'o',
 26: '6',
 27: 'P',
 28: 'B',
 29: 'r',
 30: 'a',
 31: '7',
 32: 'e',
 33: 'I',
 34: 'i',
 35: '8',
 36: 'u',
 37: 'K',
 38: '9',
 39: 'R',
 40: '%',
 41: '0',
 42: 'Z',
 43: 'h',
 44: 'L',
 45: 'A',
 46: 't',
 47: 'd',
 48: 'G',
 49: 'M',
 50: 'g',
 51: 'U',
 52: 'b',
 53: 'T',
 54: 'W',
 55: 'p',
 56: 'V'}
</code></pre>

<p>I don't know how to use <code>embeddings</code> with <code>reverse_char_map</code> to obtain embeddings for one of the test input sequence.</p>
","tensorflow, machine-learning, keras, nlp, word2vec","<p>You can name the above layers, like so:</p>

<pre><code>visible = Input(shape=(250,), name='visible') 
embed=Embedding(vocab_size,50,name='embed')(visible)     
x2=keras.layers.GRU(224, return_sequences=False, name='x2')(embed)
predictions=Dense(1, activation='sigmoid', name='final')(x2)
</code></pre>

<p>Then, the model composed of these layers (let's call it m1), may be used to initialize a new model, where you can refer to these layers, like so:</p>

<pre><code>def evaluation_model(training_model):
    visible = training_model.get_layer('visible')
    embed = training_model.get_layer('embed')
    output = embed(visible)
    m = Model(inputs=[visible], outputs=[output]
    m.compile(..)
    return m

em = evaluation_model(m1)
</code></pre>

<p>Alternatively, you could simply pop the final few layers of your initial model, or also have it output the embeddings but without any loss.</p>
",1,0,201,2018-12-06 06:43:49,https://stackoverflow.com/questions/53645910/how-to-obtain-embedded-representation-of-single-test-instance-after-training
Spacy: what algorithm is used for word vectors?,"<p>The question is clear, just would like to know what algo is used: CBOW, Skipgram, SGNS, Glove? Thanks</p>
","nlp, word2vec, spacy","<p>Vectors are included as part of a model, so there's no fixed algorithm, though in practice most use GloVe. You can check by looking at the model detail page, like <a href=""https://spacy.io/models/en#section-en_core_web_md"" rel=""nofollow noreferrer"">this one</a> for the medium sized English corpus.</p>
",1,1,401,2018-12-09 11:59:06,https://stackoverflow.com/questions/53692086/spacy-what-algorithm-is-used-for-word-vectors
Best tool for text representation to deep learning,"<p>so I wanna ask you which is the best tool used to prepare my text to deep learning?</p>

<p>What is the difference between <code>Word2Vec</code>, <code>Glove</code>, <code>Keras</code>, <code>LSA</code>...</p>
","keras, deep-learning, word2vec, lsa, glove","<p>You should use a pre-trained embedding to represent the sentence into a vector or a matrix. There are a lot of sources where you can find pre-trained embeddings that use different dataset (for instance all the Wikipedia) to train their models. These models can have different length, but normally each word is represented with 100 or 300 dimensions.</p>

<p><a href=""http://vectors.nlpl.eu/repository/"" rel=""nofollow noreferrer"">Pre-trained embeddings</a>
<a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">Pre-trained embeddings 2</a></p>
",1,-2,51,2018-12-11 10:49:39,https://stackoverflow.com/questions/53722492/best-tool-for-text-representation-to-deep-learning
How to run word2vec on Windows using gensim,"<p>A couple of years ago, a previous developer for my team wrote the following Python code calling word2vec, passing in a training file and the location of an output file. He worked on Linux. I have been asked to get this running on a Windows machine. Bearing in mind <em>I know next to no Python</em>, I have installed Gensim which I'm guessing implements word2vec now, but do not know how to rewrite the code to use the library rather than the executable which it doesnt seem possible to compile on a Windows box. Could someone help me update this code please?</p>

<pre><code>#!/usr/bin/env python3

import os
import csv
import subprocess
import shutil

from gensim.models import word2vec

def train_word2vec(trainFile, output):
    # run word2vec:
    subprocess.run([""word2vec"", ""-train"", trainFile, ""-output"", output,
                    ""-cbow"", ""0"", ""-window"", ""10"", ""-size"", ""100""],
                   shell=False)
    # Remove some invalid unicode:
    with open(output, 'rb') as input_,\
         open('%s.new' % output, 'w') as new_output:
        for line in input_:
            try:
                print(line.decode('utf-8'), file=new_output, end='')
            except UnicodeDecodeError:
                print(line)
                pass
    shutil.move('%s.new' % output, output)

def main():
    train_word2vec(""c:/temp/wc/test1_BigF.txt"", ""c:/temp/wc/test1_w2v_model.txt"")

if __name__ == '__main__':
    main()
</code></pre>
","python, python-3.x, gensim, word2vec","<p>I think the core of what you're after looks something like this:</p>

<pre><code>import sys

from gensim.models.word2vec import Word2Vec

def train_word2vec(trainFile, output):
    # compile word arrays for each sentence of input vocab
    sentences = list(line.split() for line in open(trainFile))

    # effective executable invocation of original code (included for reference)
    # word2vec -train {trainFile} -output {output} -cbow 0 -window 10 -size 100

    # invocation via word2vec module with (mostly) equivalent params
    model = Word2Vec(sentences, size=100, window=10, min_count=1, workers=4)

    # save generated model        
    model.save(output)

if __name__ == '__main__':
    train_word2vec(sys.argv[1], sys.argv[2])
</code></pre>

<p>Save as <code>train.py</code> and invoke as follows:</p>

<pre><code>python train.py input.txt output.txt
</code></pre>

<p>A few things to note:</p>

<ul>
<li>There's different capitalisation used for names of the module (<code>word2vec</code>) and the imported class (<code>Word2Vec</code>). It <em>will</em> break if you mix them up.</li>
<li>I've not found/included an equivalent for the command line <code>-cbow 0</code> argument. I'd guess this indicates a preference for the Skip-gram algorithm over CBOW, but would need someone with more <code>gensim</code> experience than me to advise on its ramifications - or indeed those of leaving it out.</li>
<li>Nor have I included (or attempted to reproduce) the Unicode removal logic of the original. The generated model output is largely binary data, so taken 'as is' it (a) falls over pretty much straight away and (b) leaves me rather in the dark as to what it's even trying to achieve.</li>
</ul>

<p>Hope this helps a little anyway.</p>
",1,0,822,2018-12-12 11:49:30,https://stackoverflow.com/questions/53742400/how-to-run-word2vec-on-windows-using-gensim
Similarity measure using vectors in gensim,"<p>I have a pair of word and semantic types of those words. I am trying to compute the relatedness measure between these two words using semantic types, for example: word1=king, type1=man, word2=queen, type2=woman
we can use gensim word_vectors.most_similar to get 'queen' from 'king-man+woman'. However, I am looking for similarity measure between vector represented by 'king-man+woman' and 'queen'.</p>

<p>I am looking for a solution to above (or)
way to calculate vector that is representative of 'king-man+woman' (and)
calculating similarity between two vectors using vector values in gensim (or)
 way to calculate simple mean of the projection weight vectors(i.e king-man+woman)</p>
","gensim, word2vec","<p>You should look at the source code for the gensim <code>most_similar()</code> method, which is used to propose answers to such analogy questions. Specifically, when you try...</p>

<pre><code>sims = wv_model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>...the top result will (in a sufficiently-trained model) often be 'queen' or similar. So, you can look at the source code to see exactly how it calculates the target combination of <code>wv('king') - wv('man') + wv('woman')</code>, before searching all known vectors for those closest vectors to that target. See...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L486"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L486</a></p>

<p>...and note that the local variable <code>mean</code> is the combination of the <code>positive</code> and <code>negative</code> values provided.</p>

<p>You might also find other methods there useful, either directly or as models for your own code, such as <code>distances()</code>...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L934"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L934</a></p>

<p>...or <code>n_similarity()</code>...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L1005"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L1005</a></p>
",1,0,1674,2018-12-15 11:34:03,https://stackoverflow.com/questions/53791972/similarity-measure-using-vectors-in-gensim
Value of alpha in gensim word-embedding (Word2Vec and FastText) models?,"<p>I just want to know the effect of the value of alpha in gensim <code>word2vec</code> and <code>fasttext</code> word-embedding models? I know that alpha is the <code>initial learning rate</code> and its default value is <code>0.075</code> form Radim blog.</p>

<p>What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.</p>

<p>Why this imprecise result for same data with two popular models with different precision? Does the value of <code>alpha</code> plays such a crucial role during building of the model?</p>

<p>Any suggestion is appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding, fasttext","<p>The default starting <code>alpha</code> is <code>0.025</code> in gensim's Word2Vec implementation. </p>

<p>In the stochastic gradient descent algorithm for adjusting the model, the effective <code>alpha</code> affects how strong of a correction to the model is made after each training example is evaluated, and will decay linearly from its starting value (<code>alpha</code>) to a tiny final value (<code>min_alpha</code>) over the course of all training. </p>

<p>Most users won't need to adjust these parameters, or might only adjust them a little, after they have a reliable repeatable way of assessing whether a change improves their model on their end tasks. (I've seen starting values of <code>0.05</code> or less commonly <code>0.1</code>, but never as high as your reported <code>0.5</code>.)</p>
",5,4,4505,2018-12-17 12:36:39,https://stackoverflow.com/questions/53815402/value-of-alpha-in-gensim-word-embedding-word2vec-and-fasttext-models
How to use a list in word2vec.similarity,"<p>I have a word2vec model using pre-trained GoogleNews-vectors-negative300.bin. The model works fine and I can get the similarities between the two words. For example:</p>

<pre><code>word2vec.similarity('culture','friendship')

0.2732939
</code></pre>

<p>Now, I want to use list elements instead of the words. For example, suppose that I have a list which its name is ""tag"". and the first two elements in the first row are culture and friendship. So, tag[0,0]= culture, and tag[0,1]=friendship.
I use the following code which gives me an error:</p>

<pre><code>word2vec.similarity(tag[0,0],tag[0,1])
</code></pre>

<p>the ""tag"" list is a <code>numpy.ndarray</code></p>

<p>the error is:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 992, in similarity
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 337, in __getitem__
    return self.get_vector(entities)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 455, in get_vector
    return self.word_vec(word)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 452, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word ' friendship' not in vocabulary""
</code></pre>
","python, gensim, word2vec","<p>I think there are some leading spaces in your word ' friendship'.</p>

<p>Could you try this:</p>

<pre><code>word2vec.similarity(tag[0,0].strip(),tag[0,1].strip())
</code></pre>
",0,0,401,2018-12-20 04:59:23,https://stackoverflow.com/questions/53862627/how-to-use-a-list-in-word2vec-similarity
How does Word2Vec ensure that antonyms will be far apart in the vector space,"<p>Broadly speaking the training of word2vec is a process in which words that are often in the same context are clustered together in the vector space.
We start by randomly shuffling the words on the plane and then with each iteration more and more clusters form. 
I think I understood this but how can we assure that the words that are antonyms or rarely appear in the same context don't end up in clusters that are close by?  Also how can we know that words that are more irrelevant are farther away than word that are less irrelevant.</p>
","machine-learning, nlp, word2vec","<p>To elaborate somewhat on Novak's response:</p>

<p>You seem to regard <code>word2vec</code> as a tool to evaluate semantic <em>meaning</em>.  Although much of the result is correlated with meaning, that is <em>not</em> the functionality of <code>word2vec</code>.  Rather, it indicates contextual correlation, which is (somewhat loosely) regarded as ""relevance"".</p>

<p>When this ""relevance"" is applied to certain problems, <em>especially</em> when multiple ""relevance"" hits are required to support a reportable result, <em>then</em> the overall effect is often useful to the problem at hand.</p>

<p>For your case, note that a word and its antonym will often appear near one another, for literary contrast or other emphasis.  As such, they are contextually <em>quite</em> relevant to one another.  Unless you have some pre-processing that can identify and appropriately alter various forms of negation, you will see such pairs often in your vectorization -- as is appropriate to the tool.</p>
",2,2,1108,2018-12-21 11:11:59,https://stackoverflow.com/questions/53883789/how-does-word2vec-ensure-that-antonyms-will-be-far-apart-in-the-vector-space
Pretrained (Word2Vec) embedding in Neural Networks,"<p>If I have to use pretrained word vectors as embedding layer in Neural Networks (eg. say CNN), How do I deal with index 0?</p>

<p><strong>Detail:</strong> </p>

<p>We usually start with creating a zero numpy 2D array. Later we fill in the indices of words from the vocabulary. 
The problem is, 0 is already the index of another word in our vocabulary (say, 'i' is index at 0). Hence, we are basically initializing the whole matrix filled with 'i' instead of empty words. So, how do we deal with padding all the sentences of equal length?</p>

<p>One easy pop-up in mind is we can use the another digit=numberOfWordsInVocab+1 to pad. But wouldn't that take more size? [Help me!]</p>
","python, tensorflow, nlp, artificial-intelligence, word2vec","<blockquote>
  <p>One easy pop-up in mind is we can use the another digit=numberOfWordsInVocab+1 to pad. But wouldn't that take more size? </p>
</blockquote>

<p>Nope! That's the same size. </p>

<pre><code>a=np.full((5000,5000), 7)
a.nbytes
200000000

b=np.zeros((5000,5000))
b.nbytes
200000000
</code></pre>

<p>Edit: Typo</p>
",1,2,404,2018-12-25 14:46:37,https://stackoverflow.com/questions/53923344/pretrained-word2vec-embedding-in-neural-networks
W2VTransformer: Only works with one word as input?,"<p>Following reproducible script is used to compute the accuracy of a Word2Vec classifier with the <code>W2VTransformer</code> wrapper in gensim:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)

# Set random seed
np.random.seed(0)

# Tokenize text
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
# Get labels
y_train = data.label

train_input = [x[0] for x in X_train]

# Train W2V Model
model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1)
clf.fit(model.transform(train_input), y_train)

text_w2v = Pipeline(
    [('features', model),
     ('classifier', clf)])

score = text_w2v.score(train_input, y_train)
score
</code></pre>

<blockquote>
  <p>0.80000000000000004</p>
</blockquote>

<p>The problem with this script is that it <strong>only</strong> works when <code>train_input = [x[0] for x in X_train]</code>, which essentially is always the first word only. 
Once change to <code>train_input = X_train</code> (or <code>train_input</code> simply substituted by <code>X_train</code>), the script returns:</p>

<blockquote>
  <p>ValueError: cannot reshape array of size 10 into shape (10,10)</p>
</blockquote>

<p>How can I solve this issue, i.e. how can the classifier work with more than one word of input?</p>

<p><strong>Edit:</strong></p>

<p>Apparently, the W2V wrapper can't work with the variable-length train input, as compared to D2V. Here is a working D2V version:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess, lemmatize
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')

np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
score = accuracy_score(y_train,y_pred)
print(score)
</code></pre>
","scikit-learn, gensim, word2vec","<p>This is technically not an answer, but cannot be written in comments so here it is. There are multiple issues here:</p>

<ul>
<li><p><code>LogisticRegression</code> class (and most other scikit-learn models) work with 2-d data <code>(n_samples, n_features)</code>. </p>

<p>That means that it needs a collection of 1-d arrays (one for each row (sample), in which the elements of array contains the feature values).  </p>

<p>In your data, a single word will be a 1-d array, which means that the single sentence (sample) will be a 2-d array. Which means that the complete data (collection of sentences here) will be a collection of 2-d arrays. Even in that, since each sentence can have different number of words, it cannot be combined into a single 3-d array. </p></li>
<li><p>Secondly, the <code>W2VTransformer</code> in gensim looks like a scikit-learn compatible class, but its not. It tries to follows ""scikit-learn API conventions"" for defining the methods <code>fit()</code>, <code>fit_transform()</code> and <code>transform()</code>. They are <strong>not compatible</strong> with scikit-learn <code>Pipeline</code>. </p>

<p>You can see that the input param requirements of <code>fit()</code> and <code>fit_transform()</code> are different. </p>

<ul>
<li><p><a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html#gensim.sklearn_api.w2vmodel.W2VTransformer.fit"" rel=""nofollow noreferrer""><code>fit()</code></a>:</p>

<blockquote>
  <p><strong>X (iterable of iterables of str)</strong> â€“ The input corpus. </p>
  
  <p>X can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from
  disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec
  module for such examples.</p>
</blockquote></li>
<li><p><a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html#gensim.sklearn_api.w2vmodel.W2VTransformer.fit_transform"" rel=""nofollow noreferrer""><code>fit_transform()</code></a>:</p>

<blockquote>
  <p><strong>X (numpy array of shape [n_samples, n_features])</strong> â€“ Training set.</p>
</blockquote></li>
</ul></li>
</ul>

<p>If you want to use scikit-learn, then you will need to have the 2-d shape. You will need to ""somehow merge"" word-vectors for a single sentence to form a 1-d array for that sentence. That means that you need to form a kind of sentence-vector, by doing:</p>

<ul>
<li>sum of individual words</li>
<li>average of individual words</li>
<li>weighted averaging of individual words based on frequency, tf-idf etc.</li>
<li>using other techniques like sent2vec, paragraph2vec, doc2vec etc.</li>
</ul>

<p><strong>Note</strong>:- I noticed now that <a href=""https://stackoverflow.com/questions/53997148/implementing-gensim-wrapper-into-sklearn"">you were doing this thing based on <code>D2VTransformer</code></a>. That should be the correct approach here if you want to use sklearn.</p>

<p>The issue in that question was this line (since that question is now deleted):</p>

<pre><code>X_train = vectorizer.fit_transform(X_train)
</code></pre>

<p>Here, you overwrite your original <code>X_train</code> (list of list of words) with already calculated word vectors and hence that error. </p>

<p>Or else, you can use other tools / libraries (keras, tensorflow) which allow sequential input of variable size. For example, LSTMs can be configured here to take a variable input and an ending token to mark the end of sentence (a sample).</p>

<p><strong>Update</strong>:</p>

<p>In the above given solution, you can replace the lines:</p>

<pre><code>model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
</code></pre>

<p>with </p>

<pre><code>pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_train)
</code></pre>

<p>No need to fit and transform separately, since <code>pipeline.fit()</code> will automatically do that.</p>
",2,-1,1907,2019-01-01 19:47:37,https://stackoverflow.com/questions/53998446/w2vtransformer-only-works-with-one-word-as-input
How to build a propper H2O word2vec training_frame,"<p>How do I build a H2O word2vec training_frame that distinguishes between different document/sentences etc.?</p>

<p>As far as I can read from the very limited documentation I have found, you simply supply one long list of words? Such as</p>

<pre><code>'This' 'is' 'the' 'first' 'This' 'is' 'number' 'two'
</code></pre>

<p>However it would make sense to be able to distinguish â€“ ideally something like this:</p>

<pre><code>Name   | ID
This   | 1
is     | 1
the    | 1
first  | 1
This   | 2
is     | 2
number | 2
two    | 2
</code></pre>

<p>Is that possible?</p>
","word2vec, h2o","<p>word2vec is a type of unsupervised learning: it turns string data into numbers. So to do a classification you need to do a two-step process:</p>

<ul>
<li>word2vec for strings to numbers</li>
<li>any supervised learning technique for numbers to categories</li>
</ul>

<p>The <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/word2vec.html"" rel=""nofollow noreferrer"">documentation</a> contains links to a categorization example in each of <a href=""https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.word2vec.craigslistjobtitles.R"" rel=""nofollow noreferrer"">R</a> and <a href=""https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/word2vec_craigslistjobtitles.ipynb"" rel=""nofollow noreferrer"">Python</a>. <a href=""https://github.com/h2oai/h2o-tutorials/tree/master/h2o-world-2017/nlp"" rel=""nofollow noreferrer"">This tutorial</a> shows the same process on a different data set (and there should be a H2O World 2017 video that goes with that).</p>

<p>By the way, in your original example, you don't just supply the words; the sentences are separated by NA. If you give <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.tokenize.html"" rel=""nofollow noreferrer"">h2o.tokenize()</a> a vector of sentences, it will make this format for you. So your example would actually be:</p>

<blockquote>
  <p>'This' 'is' 'the' 'first' NA 'This' 'is' 'number' 'two'</p>
</blockquote>
",3,2,234,2019-01-11 14:09:47,https://stackoverflow.com/questions/54148173/how-to-build-a-propper-h2o-word2vec-training-frame
What is the stochastic aspect of Word2Vec?,"<p>I'm vectorizing words on a few different corpora with Gensim and am getting results that are making me rethink how Word2Vec functions. My understanding was that Word2Vec was deterministic, and that the position of a word in a vector space would not change from training to training. If ""My cat is running"" and ""your dog can't be running"" are the two sentences in the corpus, then the value of ""running"" (or its stem) seems necessarily fixed.</p>

<p>However, I've found that that value indeed does vary across models, and words keep changing where they are on a vector space when I train the model. The differences are not always hugely meaningful, but they do indicate the existence of some random process. What am I missing here?</p>
","nlp, gensim, word2vec","<p>This is well-covered in the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Gensim FAQ</a>, which I quote here:</p>

<blockquote>
  <h3>Q11: I've trained my <code>Word2Vec</code>/<code>Doc2Vec</code>/etc model repeatedly using the exact same text corpus, but the vectors are different each time. Is there a bug or have I made a mistake? (*2vec training non-determinism)</h3>
  
  <p><strong>Answer:</strong> The *2vec models (word2vec, fasttext, doc2vecâ€¦) begin with random initialization, then most modes use additional randomization
  during training. (For example, the training windows are randomly
  truncated as an efficient way of weighting nearer words higher. The
  negative examples in the default negative-sampling mode are chosen
  randomly. And the downsampling of highly-frequent words, as controlled
  by the <code>sample</code> parameter, is driven by random choices. These
  behaviors were all defined in the original Word2Vec paper's algorithm
  description.)</p>
  
  <p>Even when all this randomness comes from a
  pseudorandom-number-generator that's been seeded to give a
  reproducible stream of random numbers (which gensim does by default),
  the usual case of multi-threaded training can further change the exact
  training-order of text examples, and thus the final model state.
  (Further, in Python 3.x, the hashing of strings is randomized each
  re-launch of the Python interpreter - changing the iteration ordering
  of vocabulary dicts from run to run, and thus making even the same
  string-of-random-number-draws pick different words in different
  launches.)</p>
  
  <p>So, it is to be expected that models vary from run to run, even
  trained on the same data. There's no single ""right place"" for any
  word-vector or doc-vector to wind up: just positions that are at
  progressively more-useful distances &amp; directions from other vectors
  co-trained inside the same model. (In general, only vectors that were
  trained together in an interleaved session of contrasting uses become
  comparable in their coordinates.) </p>
  
  <p>Suitable training parameters should yield models that are roughly as
  useful, from run-to-run, as each other. Testing and evaluation
  processes should be tolerant of any shifts in vector positions, and of
  small ""jitter"" in the overall utility of models, that arises from the
  inherent algorithm randomness. (If the observed quality from
  run-to-run varies a lot, there may be other problems: too little data,
  poorly-tuned parameters, or errors/weaknesses in the evaluation
  method.)</p>
  
  <p>You can try to force determinism, by using <code>workers=1</code> to limit
  training to a single thread â€“ and, if in Python 3.x, using the
  <code>PYTHONHASHSEED</code> environment variable to disable its usual string hash
  randomization. But training will be much slower than with more
  threads. And, you'd be obscuring the inherent
  randomness/approximateness of the underlying algorithms, in a way that
  might make results more fragile and dependent on the luck of a
  particular setup. It's better to tolerate a little jitter, and use
  excessive jitter as an indicator of problems elsewhere in the data or
  model setup â€“ rather than impose a superficial determinism.</p>
</blockquote>
",3,3,1164,2019-01-13 00:26:23,https://stackoverflow.com/questions/54165109/what-is-the-stochastic-aspect-of-word2vec
Doc2Vec: infer most similar vector from ConcatenatedDocvecs,"<p>I am generating a Doc2Vec embedding of a Pandas DataFrame by following the guidance provided <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
import gensim.models.doc2vec
from collections import OrderedDict
import pandas as pd
import numpy as np

cube_embedded =  # pandas cube
# convert the cube to documents
alldocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(cube_embedded.values.tolist())]

# train models
simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
]

for d2v_model in simple_models:
    d2v_model.build_vocab(alldocs)
    d2v_model.train(alldocs, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)

models_by_name = OrderedDict((str(d2v_model), d2v_model) for d2v_model in simple_models)
models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])
models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])
</code></pre>

<p>Given a document vector V, if I try to infer the most similar documents to the document vector V from a ConcatenatedDocvecs model, I get the following error:</p>

<pre><code>V = np.random.rand(200)
models_by_name['dbow+dmc'].docvecs.most_similar([V])

AttributeError: 'ConcatenatedDocvecs' object has no attribute 'most_similar'
</code></pre>

<p>Of course, I cannot use the simple models to infer similar documents as the produced vector embeddings have size 100 (and not 200 as the concatenated vectors do).</p>

<p>How can I get the list of most similar documents to a document vector from a ConcatenatedDocvecs model?</p>
","python, gensim, word2vec, doc2vec","<p>The <code>ConcatenatedDocvecs</code> is a simple utility wrapper class that lets you access the concatenation of a tag's vectors in multiple underlying <code>Doc2Vec</code> models. It exists to make it a little easier to reproduce some of the analysis in the original 'ParagraphVector' paper.</p>

<p>It doesn't reproduce all the functionality of a <code>Doc2Vec</code> model (or set of keyed-vectors), so can't directly hep you with the <code>most_similar()</code> you want to perform. </p>

<p>You could instead do a most-similar operation within each of the constituent models, then combine the two similarity measures (per neighbor) â€“ such as by averaging them â€“ to get a usable similarity-like value for the combined model (and then re-sort on that). I suspect, but am not sure, such a value from the two 100d models would behave very much like a a true cosine-similarity from the concatenated 200d model.</p>

<p>Alternatively, instead of using <code>ConcatenatedDoc2Vec</code> wrapper class (which only creates and returns the concatenated 200d vectors when requested), you could look at the various <code>KeyedVectors</code> class in gensim, and use (or adapt) one to be filled with all the concatenated 200d vectors from the two constituent models. Then, its <code>most_similar()</code> would work.</p>
",1,0,936,2019-01-14 17:15:22,https://stackoverflow.com/questions/54186233/doc2vec-infer-most-similar-vector-from-concatenateddocvecs
Gensim pretrained model similarity,"<p>Problem :</p>

<p>Im using glove pre-trained model with vectors to retrain my model with a specific domain say #cars, after training I want to find similar words within my domain but I got words not in my domain corpus, I believe it's from glove's vectors. </p>

<pre><code>model_2.most_similar(positive=['spacious'],    topn=10)

[('bedrooms', 0.6275501251220703),
 ('roomy', 0.6149100065231323),
 ('luxurious', 0.6105825901031494),
 ('rooms', 0.5935696363449097),
 ('furnished', 0.5897485613822937),
 ('cramped', 0.5892841219902039),
 ('courtyard', 0.5721820592880249),
 ('bathrooms', 0.5618442893028259),
 ('opulent', 0.5592212677001953),
 ('expansive', 0.555268406867981)]
</code></pre>

<p>Here I expect something like leg-room, car's spacious features mentioned in the domain's corpus. How can we exclude the glove vectors while having similar vectors?</p>

<p>Thanks  </p>
","python, vector, nlp, gensim, word2vec","<p>There may not be enough info in a simple set of generic word-vectors to filter neighbors by domain-of-use. </p>

<p>You could try using a mixed-weighting: combine the similarities to <code>'spacious'</code>, and to <code>'cars'</code>, and return the top results in that combination â€“ and it might help a little. </p>

<p>Supplying more than one <code>positive</code> word to the <code>most_similar()</code> method might approximate this. If you're sure of some major sources of interference/overlap, you might even be able to use <code>negative</code> word examples, similar to how word2vec finds candidate answers for analogies (though this might also suppress useful results that are legitimately related to both domains, like <code>'roomy'</code>). For example:</p>

<pre><code>candidates = vec_model.most_similar(positive=['spacious', 'car'], 
                                    negative=['house'])
</code></pre>

<p>(Instead of using single words like 'car' or 'house' you could also try using vectors combined from many words that define a domain.)</p>

<p>But a sharp distinction sounds like a research project, rather than something easily possible with off-the-shelf libraries/vectors â€“ and may requires more sophisticated approaches and datasets. </p>

<p>You could also try using a set of vectors trained only on a dataset of text from the domain of interest â€“ thus ensuring the vocabulary, and senses, of words are all in that domain. </p>
",1,0,440,2019-01-16 10:52:02,https://stackoverflow.com/questions/54215456/gensim-pretrained-model-similarity
faster way of reading word2vec txt in python,"<p>I have a standard word2vec output which is a .txt file formatted as follows:</p>

<pre><code>[number of words] [dimension (300)]
word1 [300 float numbers separated by spaces]
word2 ...
</code></pre>

<p>Now I want to read at most <code>M</code> word representations out of this file. A simple way is to loop the first <code>M+1</code> lines in the file, and store the <code>M</code> vectors into a numpy array. But this is super slow, is there a faster way?</p>
","text, binary, word2vec","<p>What do you mean, ""is super slow""? Compared to what? </p>

<p>Because it's a given text format, there's no way around reading the file line-by-line, parsing the floats, and assigning them into a usable structure. But you might be doing things very inefficiently â€“ without seeing your code, it's hard to tell. </p>

<p>The <code>gensim</code> library in Python includes classes for working with word-vectors in this format. And, its routines include an optional <code>limit</code> argument for reading just a certain number of vectors from the front of a file. For example, this will read the 1st 1000 from a file named <code>vectors.txt</code>:</p>

<pre><code>word_vecs = KeyedVectors.load_word2vec_format('word-vectors.txt', 
                                              binary=False,
                                              limit=1000)
</code></pre>

<p>I've never noticed it as being a particularly slow operation, even when loading something like the 3GB+ set of word-vectors Google released. (If it does seem super-slow, it could be you have insufficient RAM, and the attempted load is relying on virtual memory paging â€“ which you never want to happe with a random-access data structure like this.)</p>

<p>If you then save the vectors in <code>gensim</code>'s native format, via <code>.save()</code>, and if the constituent numpy arrays are large enough to be saved as separate files, then you'd have the option of using <code>gensim</code>'s native <code>.load()</code> with the optional <code>mmap='r'</code> argument. This would entirely skip any parsing of the raw on-disk numpy arrays, just memory-mapping them into addressable space â€“ making <code>.load()</code> complete very quickly. Then, as ranges of the array are accessed, they'd be paged into RAM. You'd still be paying the cost of reading-from-disk all the data â€“ but incrementally, as needed, rather than in a big batch up front. </p>

<p>For example...</p>

<pre><code>word_vecs.save('word-vectors.gensim')
</code></pre>

<p>...then later...</p>

<pre><code>word_vecs2 = KeyedVectors.load('word_vectors.gensim', mmap='r')
</code></pre>

<p>(There's no 'limit' option for the native <code>.load()</code>.)</p>
",1,1,1015,2019-01-17 18:54:58,https://stackoverflow.com/questions/54242521/faster-way-of-reading-word2vec-txt-in-python
Combining/adding vectors from different word2vec models,"<p>I am using gensim to create Word2Vec models trained on large text corpora. I have some models based on StackExchange data dumps. I also have a model trained on a corpus derived from English Wikipedia. </p>

<p>Assume that a vocabulary term is in both models, and that the models were created with the same parameters to Word2Vec. Is there any way to combine or add the vectors from the two separate models to create a single new model that has the same word vectors that would have resulted if I had combined both corpora initially and trained on this data?</p>

<p>The reason I want to do this is that I want to be able to generate a model with a specific corpus, and then if I process a new corpus later, I want to be able to add this information to an existing model rather than having to combine corpora and retrain everything from scratch (i.e. I want to avoid reprocessing every corpus each time I want to add information to the model). </p>

<p>Are there builtin functions in gensim or elsewhere that will allow me to combine models like this, adding information to existing models instead of retraining?</p>
","python, gensim, word2vec, training-data, corpus","<p>Generally, only word vectors that were trained together are meaningfully comparable. (It's the interleaved tug-of-war during training that moves them to relative orientations that are meaningful, and there's enough randomness in the process that even models trained on the same corpus will vary in where they place individual words.)</p>

<p>Using words from both corpuses as guideposts, it is possible to learn a transformation from one space A to the other B, that tries to move those known-shared-words to their corresponding positions in the other space. Then, applying that same transformation to the words in A that <em>aren't</em> in B, you can find B coordinates for those words, making them comparable to other native-B words.</p>

<p>This technique has been used with some success in word2vec-driven language translation (where the guidepost pairs are known translations), or as a means of growing a limited word-vector set with word-vectors from elsewhere. Whether it'd work well enough for your purposes, I don't know. I imagine it could go astray especially where the two training corpuses use shared tokens in wildly different senses. </p>

<p>There's a class, <code>TranslationMatrix</code>, that may be able to do this for you in the <code>gensim</code> library. See:</p>

<p><a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/translation_matrix.html</a></p>

<p>There's a demo notebook of its use at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb</a></p>

<p>(Whenever practical, doing a full training on a mixed-together corpus, with all word examples, is likely to do better.)</p>
",2,1,3036,2019-01-17 20:27:57,https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models
How to measure the accuracy of Word2vec model Trained on another language?,"<p>I've trained a word2vec model not for English but for an Asian language 'Sinhala'. in the later phase, I'm going to use this trained model to get the sentence similarities in order to detect plagiarism in Sinhala documents.
Please explain to me how to measure the accuracy of the trained model.I'm a university student. I have no previous knowledge of these things.</p>
","gensim, word2vec","<p>There is no universal measure of word2vec model quality or 'accuracy'. </p>

<p>The commonly-reported ""accuracy"" is typically based on a set of english-language analogy questions that were used by Google in their original word2vec paper (and included in their source code release). See for example:</p>

<p><a href=""https://github.com/tmikolov/word2vec/blob/master/questions-words.txt"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/questions-words.txt</a></p>

<p>To make a similar calculation for another language, you'd need to supply a similar set of evaluation questions for that language. I don't know of any collection of such questions for Sinhalese, or other languages, so you may have to find or create it yourself. (You could create an alternate file in the same format, and use the existing evaluation methods, specifying your alternate file.) </p>
",4,1,869,2019-01-20 14:18:14,https://stackoverflow.com/questions/54277363/how-to-measure-the-accuracy-of-word2vec-model-trained-on-another-language
Doc2vecC predicting vectors for unseen documents,"<p>I have trained a set of documents using Doc2vecc.</p>

<pre><code>https://github.com/mchen24/iclr2017
</code></pre>

<p>I am trying to generate the embedding vector for the unseen documents.I have trained the documents as mentioned in the go.sh.</p>

<pre><code>""""""
time ./doc2vecc -train ./aclImdb/alldata-shuf.txt -word 
wordvectors.txt -output docvectors.txt -cbow 1 -size 100 -window 10 - 
negative 5 -hs 0 -sample 0 -threads 4 -binary 0 -iter 20 -min-count 10 
-test ./aclImdb/alldata.txt -sentence-sample 0.1 -save-vocab 
alldata.vocab
""""""
</code></pre>

<p>I get the docvectors.txt  and wordvectors.txt for the train set. Now from here how do I generate vectors for unseen test using the same model without retraining.</p>
","machine-learning, nlp, word2vec, doc2vec","<p>As far as I can tell, the author (<a href=""https://github.com/mchen24"" rel=""nofollow noreferrer"">https://github.com/mchen24</a>) of that <code>doc2vecc.c</code> code (and paper) just made minimal changes to some example 'paragraph vector' code that was itself a minimal change to the original Google/Mikolov <code>word2vec.c</code> (<a href=""https://github.com/tmikolov/word2vec/blob/master/word2vec.c"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/word2vec.c</a>). </p>

<p>Neither the 'paragraph vector' changes nor the subsequent <code>doc2vecc</code> changes appear to include any functionality for inferring vectors for new documents.</p>

<p>Because these are unsupervised algorithms, for some purposes it may be appropriate to calculate the document-vectors for some downstream classification task, for both training and test texts, in the same combined bulk training. (Your ultimate goals may in fact have unlabeled examples to help learn the document-vectorization, even if your classifier should be trained an evaluated on some subset of known-label texts.)</p>
",1,1,628,2019-01-23 15:20:58,https://stackoverflow.com/questions/54330445/doc2vecc-predicting-vectors-for-unseen-documents
how to merge two Word2Vec File,"<p>I created my model using Word2Vec.
But the results were not good.
So I want to add a word.
The code I created the first time
Creation is possible, but can not be added.
Please tell me how can add.</p>

<p>createModel.py</p>

<pre><code>token = loadCsv(""test_data"")
embeddingmodel = []
for i in range(len(token)):
temp_embeddingmodel = []
for k in range(len(token[i][0])):
    temp_embeddingmodel.append(token[i][0][k])
embeddingmodel.append(temp_embeddingmodel)

embedding = Word2Vec(embeddingmodel, size=300, window=5, min_count=3, iter=100, sg=1,workers=4, max_vocab_size = 360000000)
embedding.save('post.embedding')
</code></pre>

<p>loadWord2Vec.py</p>

<pre><code>tokens = W2V.tokenize(sentence)
embedding = Convert2Vec('Data/post.embedding', tokens)
zero_pad = W2V.Zero_padding(embedding, Batch_size, Maxseq_length, Vector_size)
</code></pre>

<p>Tell me how to add or merge the results of Word2Vec</p>
","python, tensorflow, word2vec","<p>There's no easy way to merge two <code>Word2Vec</code> models. </p>

<p>Only word-vectors that were trained together are ""in the same space"" and thus comparable. </p>

<p>The best policy would be to combine the two training corpuses of texts, and train a new model on the combined data, thus obtaining word-vectors for all words from the same training session. </p>
",2,1,406,2019-01-24 01:46:06,https://stackoverflow.com/questions/54338261/how-to-merge-two-word2vec-file
Is there a difference between training small data multiple times and large data once when training a model?,"<p>I already have a model that has trained 130,000 sentences.</p>

<p>I want to categorize sentences with bidirectional lstm.
We plan to use this service.
However, the model must continue to be trained throughout the service.</p>

<p>so i Think
Until the accuracy of the model increases
I will look at the sentences that the model has categorized and I will answer them myself.</p>

<p>I will train sentence to answer.</p>

<p>Is there a difference between training the sentences one by one and training them by merge them into one file?</p>

<p>Every time I give a sentence
One by one training
Does it matter?</p>
","python, tensorflow, lstm, word2vec","<p>Yes, there is a difference. Suppose, you have a dataset of 10,000 sentences.</p>

<ul>
<li>If you are training one sentence at each time, then optimization will take place at every sentence ( backpropagation ). This consumes more time and memory and is not a good choice. This is not possible if you have a large dataset. Computing gradient on each instance is noisy and the speed of convergence is less.</li>
<li>If you are training in batches, suppose the batch size is 1000, then you have 10 batches. These batches together go in the network and thus gradients are computed over these batches. Hence, the gradients receive enough noise to converge at the global minima rather than the local minima. Also, it is memory efficient and converges faster.</li>
</ul>

<p>You can check out answers from <a href=""https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data"">here</a>, <a href=""https://datascience.stackexchange.com/questions/16609/benefits-of-stochastic-gradient-descent-besides-speed-overhead-and-their-optimiz"">here</a> and <a href=""https://datascience.stackexchange.com/questions/17501/what-is-a-batch-in-machine-learning"">here</a>.</p>
",1,0,162,2019-01-25 05:38:08,https://stackoverflow.com/questions/54359519/is-there-a-difference-between-training-small-data-multiple-times-and-large-data
How does word2vec work to find sentence similarity?,"<p>I am using word2vec/doc2vec to find text similarities of two documents.
I studied that word2vec works on two approaches :</p>

<ul>
<li>CBOW : which predicts words on the basis of its context</li>
<li>Skipgram : which predicts context on the basis of the word</li>
</ul>

<p>But I am stuck at understanding how these two approaches works in calculating the text similarities. 
Also which one is the better approach for the current task.</p>
","nlp, data-science, word2vec","<p>Word vectors just model individual words. </p>

<p>But, you can then use these per-word vectors to create vectors for larger texts, or similarity-calculations between larger texts. </p>

<p>A simple way to turn a text into a single fixed-width vector is to average the word-vectors of all the text's words. (This could also be a weighted average, based on some ideas of individual words' importance.) This sort of text-vector can often work well as a quick and simple baseline. For two texts, the cosine similarity of the two averages-of-all-their-word-vectors is then the similarity of two texts. </p>

<p>An algorithm like <code>Doc2Vec</code> (aka ""Paragraph Vector"") is an alternative way to get a vector for a text. It doesn't strictly combine word-vectors, but rather uses a process like what is used to create word-vectors to create per-text vectors instead. </p>

<p>If just working with word-vectors, another option for text-to-text similarity is ""Word Mover's Distance"" (WMD). Rather than averaging all word-vectors together, to create a single vector for the text, the WMD measure treats all the words of a text as ""piles of meaning"" at their various word-vectors' coordinates. The distance between texts is how much effort is required to ""move"" the mass of one text's word-vectors to the other's. It's expensive (since each such pairwise calc is an optimization problem among many possible word-to-word shifts) but retains a bit more distinction than just collapsing a text into a single summary vector. </p>
",2,0,641,2019-01-25 06:24:46,https://stackoverflow.com/questions/54359963/how-does-word2vec-work-to-find-sentence-similarity
Tracking loss and embeddings in Gensim word2vec model,"<p>I'm pretty new to Gensim and I'm trying to train my first model using word2vec model. I see that all the parameters are pretty straightforward and easy to understand, however I don't know how to track the loss of the model to see the progress. Also, I would like to be able to get the embeddings after each epoch so that I can also <em>show</em> that the predictions also get more <em>logical</em> with after each epoch. How can I do that?</p>

<p>OR, is it better to train for <em>iter=1</em> each time and save the loss and embeddings after each epoch? Sounds not too efficient.</p>

<p>Not much to show with the code but still posting it below:</p>

<pre><code>model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999)
</code></pre>
","gensim, word2vec","<p><code>gensim</code> allows us to use <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""noreferrer"">callbacks</a> for such purposes.</p>

<p>Example:</p>

<pre><code>from gensim.models.callbacks import CallbackAny2Vec

class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(""Model loss:"", model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))

""""""
prepare datasets etc.
... 
...
""""""

monitor = MonitorCallback([""word"", ""I"", ""less""])  # monitor with demo words
model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999, 
             callbacks=[monitor])
</code></pre>

<ul>
<li>now there's some <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwjQg4zzm5PgAhUqpIsKHTcqBHUQFjAAegQIChAB&amp;url=https%3A%2F%2Fgithub.com%2FRaRe-Technologies%2Fgensim%2Fissues%2F1172&amp;usg=AOvVaw1f0_kOzCBG1EupM1GkAUPM"" rel=""noreferrer"">issues</a> with <code>get_latest_training_loss</code> - may be it's incorrect (bad luck, for now github is down, can't check). I've tested this code and loss increases - looks weird.</li>
<li>may be you prefer <code>logging</code> - gensim is <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""noreferrer"">fitted</a> for it.</li>
</ul>
",12,4,4387,2019-01-29 14:03:15,https://stackoverflow.com/questions/54422810/tracking-loss-and-embeddings-in-gensim-word2vec-model
How to do prediction using trained and stored tensorflow model,"<p>I have an existing trained model (specifically tensorflow word2vec <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</a>). I restore the existing model well enough:</p>

<pre><code>model1 = tf.train.import_meta_graph(""models/model.meta"")
model1.restore(sess, tf.train.latest_checkpoint(""model/""))
</code></pre>

<p>But I don't know how to use the newly loaded (and trained) model to make predictions. How do I do predictions with a restored model?</p>

<p>Edit:</p>

<p>model code from the official tensorflow repo <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>
","python, tensorflow, word2vec","<p>Based on how you are loading the checkpoint I assume this should be the best way to use it for inference.</p>

<p>Load the placeholders:</p>

<pre><code>input = tf.get_default_graph().get_tensor_by_name(""Placeholders/placeholder_name:0"")
....
</code></pre>

<p>Load the op you use to perform prediction:</p>

<pre><code>prediction = tf.get_default_graph().get_tensor_by_name(""SomewhereInsideGraph/prediction_op_name:0"")
</code></pre>

<p>Create a session, execute the prediction op, and feed data in the placeholders.</p>

<pre><code>sess = tf.Session()
sess.run(prediction, feed_dict={input:input_data})
</code></pre>

<p>On the other hand, what I prefer to do is always create have the whole model creation inside a constructor of a class. Then, what I would do is the following:</p>

<pre><code>tf.reset_default_graph()
model = ModelClass()
loader = tf.train.Saver()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
loader.restore(sess, path_to_checkpoint_dir)
</code></pre>

<p>Since you want to load the embeddings from a trained word2vec model in another model, you should do something like:</p>

<pre><code>embeddings_new_model = tf.Variable(...,name=""embeddings"")
embedding_saver = tf.train.Saver({""embeddings_word2vec"": embeddings_new_model})
with tf.Session() as sess:
    embedding_saver.restore(sess, ""word2vec_model_path"")
</code></pre>

<p>Assuming that the embeddings variable in the word2vec model is named <code>embeddings_word2vec</code>.</p>
",1,0,477,2019-01-30 23:25:32,https://stackoverflow.com/questions/54451105/how-to-do-prediction-using-trained-and-stored-tensorflow-model
Layer size in gensim&#39;s word2vec,"<p>When I start training my word2vec model, I am presented with the warning</p>

<blockquote>
  <p>consider setting layer size to a multiple of 4 for greater performance</p>
</blockquote>

<p>That sounds neat, but I can't find any reference to a <code>layer</code> argument or similar in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the documentation</a>. </p>

<p>So how can I increase the layer size, and how can I determine a good value?</p>
","python, python-3.x, nlp, gensim, word2vec","<p>The layer size simply means the size (dimension) of the word vectors which can be set with the <code>size</code> parameter. The default value is 100 but you can for example try with 128 to have a multiple of 4. The best size depends on your training data and has to be determined empirically. In general, more data means that you can go for a bigger size.</p>
",7,3,1887,2019-02-06 17:44:35,https://stackoverflow.com/questions/54559615/layer-size-in-gensims-word2vec
Understanding gensim word2vec&#39;s most_similar,"<p>I am unsure how I should use the most_similar method of gensim's Word2Vec. Let's say you want to test the tried-and-true example of: <em>man stands to king as woman stands to X</em>; find X. I thought that is what you could do with this method, but from the results I am getting I don't think that is true.</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">The documentation</a> reads:</p>

<blockquote>
  <p>Find the top-N most similar words. Positive words contribute
  positively towards the similarity, negative words negatively.</p>
  
  <p>This method computes cosine similarity between a simple mean of the
  projection weight vectors of the given words and the vectors for each
  word in the model. The method corresponds to the word-analogy and
  distance scripts in the original word2vec implementation.</p>
</blockquote>

<p>I assume, then, that <code>most_similar</code> takes the positive examples and negative examples, and tries to find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones. Is that correct?</p>

<p>Additionally, is there a method that allows us to map the relation between two points to another point and get the result (cf. the man-king woman-X example)?</p>
","python, python-3.x, nlp, gensim, word2vec","<p>You can view exactly what <code>most_similar()</code> does in its source code:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485</a></p>
<p>It's not quite &quot;find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones&quot;. Rather, as described in the original word2vec papers, it performs vector arithmetic: adding the positive vectors, subtracting the negative, then from that resulting position, listing the known-vectors closest to that angle.</p>
<p>That is sufficient to solve <code>man : king :: woman :: ?</code>-style analogies, via a call like:</p>
<pre><code>sims = wordvecs.most_similar(positive=['king', 'woman'], 
                             negative=['man'])
</code></pre>
<p>(You can think of this as, &quot;start at 'king'-vector, add 'woman'-vector, subtract 'man'-vector, from where you wind up, report ranked word-vectors closest to that point (while leaving out any of the 3 query vectors).&quot;)</p>
",8,3,14132,2019-02-07 18:48:10,https://stackoverflow.com/questions/54580260/understanding-gensim-word2vecs-most-similar
How to use word2vec2tensor in gensim?,"<p>I am following the following gensim tutorial to transform my word2vec model to tensor.
Link to the tutorial: <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/word2vec2tensor.html</a></p>

<p>More specifically, I ran the following command</p>

<pre><code>python -m gensim.scripts.word2vec2tensor -i C:\Users\Emi\Desktop\word2vec\model_name -o C:\Users\Emi\Desktop\word2vec
</code></pre>

<p>However, I get the following error for the above command.</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>When I use <code>model.wv.save_word2vec_format(model_name)</code> to save my model (as mentioned in the following link: <a href=""https://github.com/RaRe-Technologies/gensim/issues/1847"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1847</a>) and then use the above command I get the following error.</p>

<pre><code>ValueError: invalid vector on line 1 (is this really the text format?)
</code></pre>

<p>Just wondering if I have made any mistakes in the syntax of the commads. Please let me know how to resolve this issue.</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec","<p>I was able to solve the issue by using the following code:</p>

<pre><code>model = gensim.models.keyedvectors.KeyedVectors.load(file_name)

max_size = len(model.wv.vocab)-1
w2v = np.zeros((max_size,model.layer1_size))

if not os.path.exists('projections'):
    os.makedirs('projections')

with open(""projections/metadata.tsv"", 'w+') as file_metadata:

    for i, word in enumerate(model.wv.index2word[:max_size]):

        #store the embeddings of the word
        w2v[i] = model.wv[word]

        #write the word to a file 
        file_metadata.write(word + '\n')

sess = tf.InteractiveSession()
with tf.device(""/cpu:0""):
    embedding = tf.Variable(w2v, trainable=False, name='embedding')
tf.global_variables_initializer().run()
saver = tf.train.Saver()
writer = tf.summary.FileWriter('projections', sess.graph)
config = projector.ProjectorConfig()
embed= config.embeddings.add()
embed.tensor_name = 'embedding'
embed.metadata_path = 'metadata.tsv'
projector.visualize_embeddings(writer, config)
saver.save(sess, 'projections/model.ckpt', global_step=max_size)
</code></pre>
",2,5,1284,2019-02-11 04:31:22,https://stackoverflow.com/questions/54623993/how-to-use-word2vec2tensor-in-gensim
How Word2Vec works? Python,"<p>I've got a question about gensim <strong>Word2Vec</strong> and documentation doesn't help me.</p>

<p>For example in my block of text I have some sentences like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
         ...
</code></pre>

<p>And in some time I have a new sentence like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;
</code></pre>

<p>How can I detect this situation? (of course Word4 is in dictionary too)</p>

<p>My solutions:
1). I tried to find most similar words for each and see - if the next word is in this is - OK, otherwise - I can find Word4. I mean I will do:</p>

<pre><code>model.most_similar('&lt;Word_i&gt;')
or
model.similar_by_vector('&lt;Word_i&gt;')
</code></pre>

<p>And in top of answer list I will get Word_i+1. But it doesn't work!
Because I thought that the words in the sentence after training will have quite similar coordinates and in top list Word_i+1 will be for Word_i.
But it's wrong. When I checked this solution and trained by all corpus of text I had a situation when Word_2 wasn't in top list for Word_1! My explanation that not the near words have quite similar coordinates, but words with contextual proximity have quite similar coordinates, it's not the same..</p>

<p>2). So my second solution is using <strong>doesnt_match()</strong>, which takes a list of words, and reports the one word which is furthest from the average of all the words.</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;'.split()))
</code></pre>

<p>And yes - in this case the answer will be Word4! (so I detect this word)
But if I do it with:</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;'.split()))
</code></pre>

<p>The answer will be Word2 (for example). And if I again will explore top words for Word1 and Word3 I won't see Word2 in this lists, but this sentence (Word1 Word2 Word3) is normal.</p>

<p>So how can I detect it?</p>
","python, python-3.x, machine-learning, nlp, word2vec","<p>I'm not sure that I understand what's the question here but I'll try to explain word2vec concept and what does <code>most_similar</code> return and hopefully it will be beneficial.</p>

<p>So, let's consider situation that there are two sentences: <code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;</code> and <code>&lt;Word1&gt; &lt;Word4&gt; &lt;Word3&gt;</code>. When creating the word2vec model, we take the same number of words left and right of the target  (current) word and construct tuples like: (target_word, proximity_word). Let's say we want to observe the situation when the target word is the middle word. So for sentence1 we'll get <code>(&lt;Word2&gt;, &lt;Word1&gt;)</code> and <code>(&lt;Word2&gt;, &lt;Word3&gt;)</code> and for sentence2 we'll get <code>(&lt;Word4&gt;, &lt;Word1&gt;)</code> and <code>(&lt;Word4&gt;, &lt;Word3&gt;)</code>. This way we tell the model that <code>&lt;Word1&gt;</code> and <code>&lt;Word3&gt;</code> are in the context of the . Similarly, <code>&lt;Word1&gt;</code> and <code>&lt;Word3&gt;</code> are in the context of <code>&lt;Word4&gt;</code>. What that means? We can conclude that the <code>&lt;Word2&gt;</code> and <code>&lt;Word4&gt;</code> are in some way similar.</p>

<p>So if you call <code>most_similar(&lt;Word2&gt;)</code> you will not get  or  but  because word 2 and 4 are appearing in the same context. That said, you cannot expect that if you have the sentence <code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;</code> and call <code>most_similar(&lt;Word3)</code> to get vector of <code>&lt;Word4&gt;</code>. Instead you'll get some word that has appeared in the context of words 1, 2 and 4 (this context window depends on the size we specify before training). I hope this has been helpful and makes word2vec clearer.</p>
",0,0,330,2019-02-11 08:56:14,https://stackoverflow.com/questions/54626897/how-word2vec-works-python
Expected input to torch Embedding layer with pre_trained vectors from gensim,"<p>I would like to use pre-trained embeddings in my neural network architecture. The pre-trained embeddings are trained by gensim. I found <a href=""https://stackoverflow.com/a/49802495/1150683"">this informative answer</a> which indicates that we can load pre_trained models like so:</p>

<pre><code>import gensim
from torch import nn

model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors)
emb = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors))
</code></pre>

<p>This seems to work correctly, also on 1.0.1. My question is, that I don't quite understand what I have to feed into such a layer to utilise it. Can I just feed the tokens (segmented sentence)? Do I need a mapping, for instance token-to-index? </p>

<p>I found that you can access a token's vector simply by something like</p>

<pre><code>print(weights['the'])
# [-1.1206588e+00  1.1578362e+00  2.8765252e-01 -1.1759659e+00 ... ]
</code></pre>

<p>What does that mean for an RNN architecture? Can we simply load in the tokens of the batch sequences? For instance:</p>

<pre><code>for seq_batch, y in batch_loader():
    # seq_batch is a batch of sequences (tokenized sentences)
    # e.g. [['i', 'like', 'cookies'],['it', 'is', 'raining'],['who', 'are', 'you']]
    output, hidden = model(seq_batch, hidden)
</code></pre>

<p>This does not seem to work so I am assuming you need to convert the tokens to its index in the final word2vec model. Is that true? I found that you can get the indices of words by using the word2vec model's <code>vocab</code>:</p>

<pre><code>weights.vocab['world'].index
# 147
</code></pre>

<p>So as an input to an Embedding layer, should I provide a tensor of <code>int</code> for a sequence of sentences that consist of a sequence of words? Example use with dummy dataloader (cf. example above) and dummy RNN welcome.</p>
","vector, pytorch, gensim, word2vec, recurrent-neural-network","<p>The <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding"" rel=""nofollow noreferrer"">documentation</a> says the following</p>

<blockquote>
  <p>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</p>
</blockquote>

<p>So if you want to feed in a sentence, you give a <code>LongTensor of</code> indices, each corresponding to a word in the vocabulary, which the <code>nn.Embedding</code> layer will map into word vectors going forward. </p>

<p>Here's an illustration</p>

<pre class=""lang-py prettyprint-override""><code>test_voc = [""ok"", ""great"", ""test""]
# The word vectors for ""ok"", ""great"" and ""test""
# are at indices, 0, 1 and 2, respectively.

my_embedding = torch.rand(3, 50)
e = nn.Embedding.from_pretrained(my_embedding)

# LongTensor of indicies corresponds to a sentence,
# reshaped to (1, 3) because batch size is 1
my_sentence = torch.tensor([0, 2, 1]).view(1, -1)

res = e(my_sentence)
print(res.shape)
# =&gt; torch.Size([1, 3, 50])
# 1 is the batch dimension, and there's three vectors of length 50 each
</code></pre>

<p>In terms of RNNs, next you can feed that tensor into your RNN module, e.g</p>

<pre class=""lang-py prettyprint-override""><code>lstm = nn.LSTM(input_size=50, hidden_size=5, batch_first=True)
output, h = lstm(res)
print(output.shape)
# =&gt; torch.Size([1, 3, 5])
</code></pre>

<p>I also recommend you look into <a href=""https://github.com/pytorch/text"" rel=""nofollow noreferrer"">torchtext</a>. It can automatate some of the stuff you will have to do manually otherwise.</p>
",1,4,2841,2019-02-12 17:28:57,https://stackoverflow.com/questions/54655604/expected-input-to-torch-embedding-layer-with-pre-trained-vectors-from-gensim
Missing words in word2vec vocabulary,"<p>I am training word2vec on my own text-corpus using mikolov's implementation from <a href=""https://github.com/tmikolov/word2vec/blob/master/word2vec.c"" rel=""nofollow noreferrer"">here</a>. Not all unique words from the corpus get a vector even though I have set the min-count to 1. Are there any parameters I may have missed, that might be the reason not all unique words get a vector? What else might be the reason?</p>

<p>To test word2vecs behavior I have written the following script providing a text file with 20058 sentences and 278896 words (all words and punctuation are space separated and there is one sentence per line). </p>

<pre><code>import subprocess


def get_w2v_vocab(path_embs):
    vocab = set()
    with open(path_embs, 'r', encoding='utf8') as f:
        next(f)
        for line in f:
            word = line.split(' ')[0]
            vocab.add(word)
    return vocab - {'&lt;/s&gt;'}


def train(path_corpus, path_embs):
    subprocess.call([""./word2vec"", ""-threads"", ""6"", ""-train"", path_corpus,
                     ""-output"", path_embs, ""-min-count"", ""1""])


def get_unique_words_in_corpus(path_corpus):
    vocab = []
    with open(path_corpus, 'r', encoding='utf8') as f:
        for line in f:
            vocab.extend(line.strip('\n').split(' '))
    return set(vocab)

def check_equality(expected, actual):
    if not expected == actual:
        diff = len(expected - actual)
        raise Exception('Not equal! Vocab expected: {}, Vocab actual: {}, Diff: {}'.format(len(expected), len(actual), diff))
    print('Expected vocab and actual vocab are equal.')



def main():
    path_corpus = 'test_corpus2.txt'
    path_embs = 'embeddings.vec'
    vocab_expected = get_unique_words_in_corpus(path_corpus)
    train(path_corpus, path_embs)
    vocab_actual = get_w2v_vocab(path_embs)
    check_equality(vocab_expected, vocab_actual)


if __name__ == '__main__':
    main()
</code></pre>

<p>This script gives me the following output:</p>

<pre><code>Starting training using file test_corpus2.txt
Vocab size: 33651
Words in train file: 298954
Alpha: 0.000048  Progress: 99.97%  Words/thread/sec: 388.16k  Traceback (most recent call last):
  File ""test_w2v_behaviour.py"", line 44, in &lt;module&gt;
    main()
  File ""test_w2v_behaviour.py"", line 40, in main
    check_equality(vocab_expected, vocab_actual)
  File ""test_w2v_behaviour.py"", line 29, in check_equality
    raise Exception('Not equal! Vocab expected: {}, Vocab actual: {}, Diff: {}'.format(len(expected), len(actual), diff))
Exception: Not equal! Vocab expected: 42116, Vocab actual: 33650, Diff: 17316

</code></pre>
",word2vec,"<p>As long as you're using Python, you might want to use the <code>Word2Vec</code> implementation in the <code>gensim</code> package. It does everything the original Mikolov/Google<code>word2vec.c</code> does, and more, and is usually performance-competitive. </p>

<p>In particular, it won't have any issues with UTF-8 encoding â€“ while I'm not sure the Mikolov/Google <code>word2vec.c</code> handles UTF-8 correctly. And, that may be a source of your discrepancy.</p>

<p>If you need to get to the bottom of your discrepancy, I would suggest:</p>

<ul>
<li><p>have your <code>get_unique_words_in_corpus()</code> also tally/report the total number of non-unique words its tokenization creates. If that's not the same as the <code>298954</code> reported by <code>word2vec.c</code>, then the two processes are clearly not working from the same baseline understanding of what 'words' are in the source file.</p></li>
<li><p>find some words, or at least one representative word, that your token-count expects to be in the final model, and isn't. Review those for any common characteristic â€“ including in context in the file. That will probably reveal why the two tallies differ.</p></li>
</ul>

<p>Again, I suspect something UTF-8 related, or perhaps related to other implementation-limits in <code>word2vec.c</code> (such as a maximum word-lenght) that are not mirrored in your Python-based word tallies. </p>
",1,0,1645,2019-02-13 13:14:11,https://stackoverflow.com/questions/54671075/missing-words-in-word2vec-vocabulary
Is there a way to remove a word from a KeyedVectors vocab?,"<p>I need to remove an invalid word from the vocab of a ""gensim.models.keyedvectors.Word2VecKeyedVectors"". </p>

<p>I tried to remove it using <code>del model.vocab[word]</code>, if I print the <code>model.vocab</code> the word disappeared, but when I run <code>model.most_similar</code> using other words the word that I deleted is still appearing as similar. 
So how can I delete a word from <code>model.vocab</code> in a way that affect the <code>model.most_similar</code> to not bring it?</p>
","gensim, word2vec, embedding, glove","<p>There's no existing method supporting the removal of individual words. </p>

<p>A quick-and-dirty workaround might be to, at the same time as removing the <code>vocab</code> entry, noting the <code>index</code> of the existing vector (in the underlying large vector array), and also changing the string in the <code>kv_model.index2entity</code> list at that index to some plug value (like say, <code>'***DELETED***'</code>). </p>

<p>Then, after performing any <code>most_similar()</code>, discard any entries matching <code>'***DELETED***'</code>. </p>
",2,2,1205,2019-02-14 19:26:53,https://stackoverflow.com/questions/54697748/is-there-a-way-to-remove-a-word-from-a-keyedvectors-vocab
How to handle words that are not in word2vec&#39;s vocab optimally,"<p>I have a list of ~10 million sentences, where each of them contains up to 70 words.</p>

<p>I'm running gensim word2vec on every word, and then taking the simple average of each sentence. The problem is that I use min_count=1000, so a lot of words are not in the vocab. </p>

<p>To solve that, I intersect the vocab array (that contains about 10000 words) with every sentence, and if there's at least one element left in that intersection, it returns its the simple average, otherwise, it returns a vector of zeros.</p>

<p>The issue is that calculating every average takes a very long time when I run it on the whole dataset, even when splitting into multiple threads, and I would like to get a better solution that could run faster.</p>

<p>I'm running this on an EC2 r4.4xlarge instance.</p>

<p>I already tried switching to doc2vec, which was way faster, but the results were not as good as word2vec's simple average.</p>

<pre><code>word2vec_aug_32x = Word2Vec(sentences=sentences, 
                        min_count=1000, 
                        size=32, 
                        window=2,
                        workers=16, 
                        sg=0)

vocab_arr = np.array(list(word2vec_aug_32x.wv.vocab.keys()))

def get_embedded_average(sentence):
    sentence = np.intersect1d(sentence, vocab_arr)
    if sentence.shape[0] &gt; 0:
        return np.mean(word2vec_aug_32x[sentence], axis=0).tolist()
    else:
        return np.zeros(32).tolist()

pool = multiprocessing.Pool(processes=16)

w2v_averages = np.asarray(pool.map(get_embedded_average, np.asarray(sentences)))
pool.close()
</code></pre>

<p>If you have any suggestions of different algorithms or techniques that have the same purpose of sentence embedding and could solve my problem, I would love to read about it.</p>
","python, numpy, optimization, gensim, word2vec","<p>You could use <a href=""https://github.com/facebookresearch/fastText"" rel=""noreferrer"">FastText</a> instead of Word2Vec. FastText is able to embed out-of-vocabulary words by looking at subword information (character ngrams). Gensim also has a FastText implementation, which is very easy to use:</p>

<pre><code>from gensim.models import FastText

model = FastText(sentences=training_data, size=128, ...)

word = 'hello' # can be out of vocabulary
embedding = model[word] # fetches the word embedding
</code></pre>
",10,5,10102,2019-02-15 12:15:36,https://stackoverflow.com/questions/54709178/how-to-handle-words-that-are-not-in-word2vecs-vocab-optimally
Mapping word vector to the most similar/closest word using spaCy,"<p>I am using spaCy as part of a topic modelling solution and I have a situation where I need to map a derived word vector to the ""closest"" or ""most similar"" word in a vocabulary of word vectors.</p>

<p>I see gensim has a function (WordEmbeddingsKeyedVectors.similar_by_vector) to calculate this, but I was wondering if spaCy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?</p>
","nlp, spacy, word2vec, word-embedding","<p>After a bit of experimentation, I found a scikit function (cdist in scikit.spatial.distance) that finds a ""close"" vector in a vector space to the input vector. </p>

<pre><code># Imports
from scipy.spatial import distance
import spaCy

# Load the spacy vocabulary
nlp = spacy.load(""en_core_web_lg"")

# Format the input vector for use in the distance function
# In this case we will artificially create a word vector from a real word (""frog"")
# but any derived word vector could be used
input_word = ""frog""
p = np.array([nlp.vocab[input_word].vector])

# Format the vocabulary for use in the distance function
ids = [x for x in nlp.vocab.vectors.keys()]
vectors = [nlp.vocab.vectors[x] for x in ids]
vectors = np.array(vectors)

# *** Find the closest word below ***
closest_index = distance.cdist(p, vectors).argmin()
word_id = ids[closest_index]
output_word = nlp.vocab[word_id].text
# output_word is identical, or very close, to the input word
</code></pre>
",12,12,9013,2019-02-15 21:43:21,https://stackoverflow.com/questions/54717449/mapping-word-vector-to-the-most-similar-closest-word-using-spacy
How to use pretrained word2vec vectors in doc2vec model?,"<p>I am trying to implement doc2vec, but I am not sure how the input for the model should look like if I have pretrained word2vec vectors.</p>

<p>The problem is, that I am not sure how to theoretically use pretrained word2vec vectors for doc2vec. I imagine, that I could prefill the hidden layer with the vectors and the rest of the hidden layer fill with random numbers</p>

<p>Another idea is to use the vector as input for word instead of a one-hot-encoding but I am not sure if the output vectors for docs would make sense.</p>

<p>Thank you for your answer!</p>
","python, machine-learning, nlp, word2vec, doc2vec","<p>You might think that <code>Doc2Vec</code> (aka the 'Paragraph Vector' algorithm of Mikolov/Le) requires word-vectors as a 1st step. That's a common belief, and perhaps somewhat intuitive, by analogy to how humans learn a new language: understand the smaller units before the larger, then compose the meaning of the larger from the smaller. </p>

<p>But that's a common misconception, and <code>Doc2Vec</code> doesn't do that. </p>

<p>One mode, pure PV-DBOW (<code>dm=0</code> in gensim), doesn't use conventional per-word input vectors at all. And, this mode is often one of the fastest-training and best-performing options. </p>

<p>The other mode, PV-DM (<code>dm=1</code> in gensim, the default) does make use of neighboring word-vectors, in combination with doc-vectors in a manner analgous to word2vec's CBOW mode â€“ but any word-vectors it needs will be trained-up simultaneously with doc-vectors. They are <em>not</em> trained 1st in a separate step, so there's not a easy splice-in point where you could provide word-vectors from elsewhere. </p>

<p>(You can mix skip-gram word-training into the PV-DBOW, with <code>dbow_words=1</code> in gensim, but that will train word-vectors from scratch in an interleaved, shared-model process.)</p>

<p>To the extent you could pre-seed a model with word-vectors from elsewhere, it wouldn't necessarily improve results: it could easily send their quality sideways or worse. It might in some lucky well-managed cases speed model convergence, or be a way to enforce vector-space-compatibility with an earlier vector-set, but not without extra gotchas and caveats that <em>aren't</em> a part of the original algorithms, or well-described practices. </p>
",4,0,1643,2019-02-19 09:11:19,https://stackoverflow.com/questions/54762478/how-to-use-pretrained-word2vec-vectors-in-doc2vec-model
What is the input format for word2vec features in SVM classification task?,"<p>I am doing a binary classification task using linear SVM in scikit learn. I use nominal features and word vectors. I obtained the word vectors using the pretrained Google word2vec, however, I am not sure how SVM can handle word vectors as a feature.<br>
It seems that I need to ""split"" each vector in 300 separate features (=300 vector dimensions), because I can't pass the vector as a whole to SVM. But that doesn't seem right, as the vector should be treated as one feature.<br>
What would be the correct way to represent a vector in this case?</p>
","python, classification, svm, word2vec","<h2>Vector of many features</h2>

<p>From the perspective of an SVM, each dimension of a word vector would be a separate numeric feature - each dimension in that vector represents a numeric metric representing something different.</p>

<p>The same applies for non-SVM classifiers. For example, if you'd have a neural network, and your input features were that word vector of length 300 and (for the sake of a crude example) a bit stating whether that word was capitalized, then you'd concatenate those things and would have 301 numbers as your input; you'd treat that feature just as each of the 300 dimensions.</p>
",1,3,1075,2019-02-23 21:20:35,https://stackoverflow.com/questions/54846314/what-is-the-input-format-for-word2vec-features-in-svm-classification-task
gensim word2vec print log loss,"<p>how to print to log (file or stout) the loss of each epoch in the training phase, when using gensim word2vec model. </p>

<p>I tried :</p>

<pre><code> logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
 logging.root.setLevel(level=logging.INFO)
</code></pre>

<p>But I didn't saw any loss printing.</p>
","python, gensim, word2vec","<p>You can get the latest training loss of a word2vec model with the method <code>get_latest_training_loss()</code>. If you want to print the loss after every epoch you can add a callback that does this. For example:</p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

model = Word2Vec(common_texts, size=100, window=5, min_count=1, 
                 compute_loss=True, callbacks=[callback()])
</code></pre>

<p>However, the loss is computed in a cumulative way (i.e. the loss that gets printed after each epoch is the total loss of all epochs so far). See <a href=""https://stackoverflow.com/a/52067942/10921263"">gojomo's answer here</a> for more explanation.</p>
",13,6,10112,2019-02-26 15:05:38,https://stackoverflow.com/questions/54888490/gensim-word2vec-print-log-loss
Word2Vec vocab results in just letters and symbols,"<p>I'm new to Word2Vec and I am trying to cluster words based on their similarity.  To start I am using nltk to separate the sentences and then using the resulting list of sentences as the input into Word2Vec.  However, when I print the vocab, it is just a bunch of letters, numbers and symbols rather than words. To be specific, an example of one of the letters is ""&lt; gensim.models.keyedvectors.Vocab object at 0x00000238145AB438>, 'L':""</p>

<pre><code># imports needed and logging
import gensim
from gensim.models import word2vec
import logging

import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
with open('C:\\Users\\Freddy\\Desktop\\Thesis\\Descriptions.txt','r') as f_open:
    text = f_open.read()
arr = []

sentences = nltk.sent_tokenize(text) # this gives a list of sentences

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)

model = word2vec.Word2Vec(sentences, size = 300)

print(model.wv.vocab)
</code></pre>
","python, python-3.x, tokenize, gensim, word2vec","<p>As the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">tutorial</a> and the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">documentation</a> for <code>Word2Vec</code> class suggests the constructor of the class requires list of lists of words as the first parameter (or iterator of iterators of words in general):</p>

<blockquote>
  <p><strong>sentences</strong> (iterable of iterables, optional) â€“ The sentences iterable can be simply a list of lists of tokens, but for larger
  corpora,...</p>
</blockquote>

<p>I believe before feeding in <code>sentences</code> into <code>Word2Vec</code> you need to use <code>words_tokenize</code> on each of the sentences changing the crucial line to:</p>

<pre><code>sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]
</code></pre>

<p><strong>TL;DR</strong></p>

<p>You get letters as your ""words"" because <code>Word2Vec</code> treats strings corresponding to sentences as iterables containing words. Iterating over strings results in the sequence of letters. These letters are used as the basis for the model learning (instead of intended words).</p>

<p>As the ancient saying goes: <em>trash in - trash out</em>.</p>
",5,2,2751,2019-02-28 16:02:09,https://stackoverflow.com/questions/54929726/word2vec-vocab-results-in-just-letters-and-symbols
Word2Vec time complexity,"<p>I have googled this issue but I cannot find any reliable solution (some sources gives log(V) some log(V/2). But what is the time complexity of the word2vec model with the following parameters:</p>

<p><code>Word2Vec(corpus, size=4000, window=30, min_count=1, workers=50, iter=100, alpha=0.0001)</code></p>

<p>I have a vocabulary that equals to 10000 words (unique words). </p>
","python, time-complexity, big-o, gensim, word2vec","<p>Without a formal analysis/proof, in practice and in the default 'negative sampling' case, execution time is chiefly determined by the size of the corpus, and grows roughly linearly with the size of the corpus. The number of unique words (vocabulary size V) isn't a major factor.</p>
<p>Gensim's implementation uses a binary-search over a vocabulary-sized array to achieve the sampling of negative examples, so its time complexity might technically be:</p>
<pre><code>O(N * log(V))
</code></pre>
<ul>
<li>where N is the total corpus size and</li>
<li>V is the unique-words vocabulary count.</li>
</ul>
<p>But this particular <em>O(log(V))</em> operation is, in practice, often faster than the memory-hungry O(1) sampling lookup used by the original Google/Mikolov word2vec.c â€“ probably due to improved CPU cache efficiency.</p>
<p>So with the defaults:</p>
<ul>
<li>If one corpus is twice as long, in words, than another, then it will take about twice as long to train the model on the larger corpus.</li>
<li>But if one corpus is the same size, in words, as another, but with a vocabulary twice as big, you probably won't notice much change in runtime.</li>
</ul>
<p>In the non-default <code>hierarchical softmax</code> case â€“ <code>hs=1, negative=0</code> â€“ words are encoded differently, and have longer encodings as the size of the vocabulary grows, and this increases the average number of training operations per corpus word â€“ by a factor of <em>log(V)</em>, I believe, so we again technically have a *<em>O(N * log(V))</em> time-complexity.</p>
<p>But, this vocabulary-driven increase tends to be more significant, in practice, than the one inside negative-sampling's binary-search-based sampling.</p>
<p>So if you have two corpuses of the same length, but one has twice the number of unique words, you very well may notice a longer runtime, in hierarchical-softmax mode.</p>
",6,4,2413,2019-03-01 18:39:02,https://stackoverflow.com/questions/54950481/word2vec-time-complexity
Desired distribution of weights in word embedding vectors,"<p>I am training my own embedding vectors as I'm focused on an academic dataset (WOS); whether the vectors are generated via word2vec or fasttext doesn't particularly matter. Say my vectors are 150 dimensions each. I'm wondering what the desired distribution of weights within a vector ought to be, if you averaged across an entire corpus's vectors?  </p>

<p>I did a few experiments while looking at the distributions of a sample of my vectors and came to these conclusions (uncertain as to how absolutely they hold): </p>

<p>If one trains their model with too few epochs then the vectors don't change significantly from their initiated values (easy to see if you start you vectors as weight 0 in every category). Thus if my weight distribution is centered around some point (typically 0) then I've under-trained my corpus. </p>

<p>If one trains their model with too few documents/over-trains then the vectors show significant correlation with each other (I typically visualize a random set of vectors and you can see stripes where all the vectors have weights that are either positive or negative). </p>

<p>What I imagine is a single ""good"" vector has various weights across the entire range of -1 to 1. For any single vector it may have significantly more dimensions near -1 or 1. However, the weight distribution of an entire corpus would balance out vectors that randomly have more values towards one end of the spectrum or another, so that the weight distribution of the entire corpus is approximately evenly distributed across the entire corpus. Is this intuition correct? </p>
","nlp, word2vec, fasttext","<p>I'm unfamiliar with any research or folk wisdom about the desirable ""weights of the vectors"" (by which I assume you mean the individual dimensions). </p>

<p>In general, since the individual dimensions <em>aren't</em> strongly interpretable, I'm not sure you could say much about how any one dimension's values should be distributed. And remember, our intuitions from low-dimensional spaces (2d, 3d, 4d) often don't hold up in high-dimensional spaces. </p>

<p>I've seen two interesting, possibly relevant observations in research:</p>

<ul>
<li><p>some have observed that the raw trained vectors for words with singular meanings tend to have a larger magnitude, and those with many meanings have smaller magnitudes. A plausible explanation for this would be that word-vectors for polysemous word-tokenss are being pulled in different directions for the multiple contrasting meanings, and thus wind up ""somewhere in the middle"" (closer to the origin, and thus of lower magnitude). Note, though, that most word-vector-to-word-vector comparisons <em>ignore</em> the magnitudes, by using cosine-similarity to only compare angles (or largely equivalently, by normalizing all vectors to unit length before comparisons). </p></li>
<li><p>A paper ""All-but-the-Top: Simple and Effective Postprocessing for Word Representations"" by Mu, Bhat, &amp; Viswanath <a href=""https://arxiv.org/abs/1702.01417v2"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1702.01417v2</a> has noted that the average of all word-vectors that were trained together tends to biased in a certain direction from the origin, but that removing that bias (and other commonalities in the vectors) can result in improved vectors for many tasks. In my own personal experiments, I've observed that the magnitude of that bias-from-origin seems correlated with the number of <code>negative</code> samples chosen - and that choosing the extreme (and uncommon) value of just 1 negative sample makes such a bias negligible (but might not be best for overall quality or efficiency/speed of training). </p></li>
</ul>

<p>So there <em>may</em> be useful heuristics about vector quality from looking at the relative distributions of vectors, but I'm not sure any would be sensitive to individual dimensions (except insofar as those happen to be the projections of vectors onto a certain axis). </p>
",1,0,669,2019-03-01 19:43:21,https://stackoverflow.com/questions/54951312/desired-distribution-of-weights-in-word-embedding-vectors
train Word2vec model using Gensim,"<p>this is my code.it reads reviews from an excel file (rev column) and make a list of list.</p>

<p>xp is like this</p>

<pre><code>[""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""]
</code></pre>

<p>but when use list for model, it gives me error""TypeError: 'float' object is not iterable"".i don't know where is my problem.
Thanks.</p>

<pre><code>xp=[]
import gensim 
import logging
import pandas as pd 
file = r'FileNamelast.xlsx'
df = pd.read_excel(file,sheet_name='FileNamex')
pages = [i for i in range(0,1000)]


for page in  pages:

 text =df.loc[page,[""rev""]]
 xp.append(text[0])


model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, 
workers=10)
model.train(xp,total_examples=len(xp),epochs=10)
</code></pre>

<p>this is what i got.TypeError: 'float' object is not iterable</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-aa34c0e432bf&gt; in &lt;module&gt;()
     14 
     15 
---&gt; 16 model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, workers=10)
     17 model.train(xp,total_examples=len(xp),epochs=10)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in __init__(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
    765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
    766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 767             fast_version=FAST_VERSION)
    768 
    769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in __init__(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
    758 
--&gt; 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
    760             self.train(
    761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in build_vocab(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    934         """"""
    935         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
    937         self.corpus_count = corpus_count
    938         self.corpus_total_words = total_words

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in scan_vocab(self, sentences, corpus_file, progress_per, workers, trim_rule)
   1569             sentences = LineSentence(corpus_file)
   1570 
-&gt; 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
   1572 
   1573         logger.info(

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1552                     sentence_no, total_words, len(vocab)
   1553                 )
-&gt; 1554             for word in sentence:
   1555                 vocab[word] += 1
   1556             total_words += len(sentence)

TypeError: 'float' object is not iterable
</code></pre>
","python-3.x, gensim, word2vec","<p>The <code>sentences</code> corpus argument to <code>Word2Vec</code> should be an iterable sequence of lists-of-word-tokens.</p>

<p>Your reported value for <code>xp</code> is actually a list with one long string in it:</p>

<pre class=""lang-py prettyprint-override""><code>[
  ""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""
]
</code></pre>

<p>I don't see how this would give the error you've reported, but it's definitely wrong, so should be fixed. You should perhaps print <code>xp</code> just before you instantiate <code>Word2Vec</code> to be sure you know what it contains. </p>

<p>A true list, with each item being a list-of-string-tokens, would work. So if <code>xp</code> were the following that'd be correct:</p>

<pre class=""lang-py prettyprint-override""><code>    [
      ['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],
      ['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],
      [ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']
    ]

</code></pre>

<p>Note, however:</p>

<ul>
<li><code>Word2Vec</code> doesn't do well with toy-sized datasets. So while this tiny setup may be helpful to check for basic syntax/format issues, don't expect realistic results until you're training with many hundreds-of-thousands of words.</li>
<li>You don't need to call <code>train()</code> if you already supplied your corpus at instantiation, as you have. The model will do all steps automatically. (If, on the other hand, you don't supply your corpus, you'd then have to call <em>both</em> <code>build_vocab()</code> and <code>train()</code>.) If you enable logging at the INFO level all the steps happening behind the scenes will be clearer.</li>
</ul>
",2,0,2731,2019-03-09 08:10:46,https://stackoverflow.com/questions/55075312/train-word2vec-model-using-gensim
Use word2vec word embeding as feature vector for text classification (simlar to count vectorizer/tfidf feature vector),"<p>I am trying to perform some text classification using machine learning and for that I have extracted feature vectors from the per-processed textual data using simple bag of words approach(count vectorizer) and tfidf vectorizer.  </p>

<p>Now I want to use word2vec i.e. word embedding as my feature vector similar as that of count vectorizer/tfidf vectorizer where I should be able to learn vocabulary from the train data and transform or fit the test data with the learned vocab but I can't find a way to implement that.  </p>

<pre><code>//I need something like this with word2vec

count = CountVectorizer()
train_feature_ vector =count.fit_transform(train_data)
test_feature_vector = count.fit(test_data)

//So I can train my model like this
mb = MultinomialNB()
mb.fit(train_feature_vector,y_train)
acc_score = mb.score(test_feature_vector,y_test)
print(""Accuracy ""+str(acc_score))
</code></pre>
","machine-learning, scikit-learn, word2vec, text-classification, word-embedding","<p>You first should understand what Word Embeddings are. When you apply a CountVectorizer or TfIdfVectorizer what you get is a <strong>sentence</strong> representation in a sparse way, commonly known as a One Hot encoding. The word embeddings representation are used to represent a <strong>word</strong> in a high dimensional space of real numbers.</p>

<p>Once you get your per word representation there are some ways to do this, check:<a href=""https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence"">How to get vector for a sentence from the word2vec of tokens in sentence</a></p>
",1,-1,2050,2019-03-11 07:06:19,https://stackoverflow.com/questions/55096725/use-word2vec-word-embeding-as-feature-vector-for-text-classification-simlar-to
Combine Word Embeddings with with topic-word distribution from LDA for text summarization,"<p>Im a newbie in NLP and i was wondering if it is a good idea to summarize a document that has already been classified into a certain topic through methods such as LDA by considering the Word Embedding retrieved from Word2Vec and the topic-word distribution that has already been generated, to come up with a sentence scoring algorithm. Does this sound like a good approach for creating a summary of a document?</p>
","nlp, word2vec, lda, summarization","<p>I would like to suggest you this <a href=""https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1"" rel=""nofollow noreferrer"">post</a>.</p>
<p>Instead of using Skip-Thought Encoder on the Step 4, you could use pre-trained Word2Vec model from <a href=""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"" rel=""nofollow noreferrer"">Google</a> or <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"" rel=""nofollow noreferrer"">Facebook</a> (check <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">FastText</a> documentation to see how to parse second model or to choose another language).</p>
<p>In general, you will have next steps:</p>
<ol>
<li>Text cleaning (delete numbers, but leave punctuation).</li>
<li>Language detection (to define and delete stopwords, and use appropriate version of Word2Vec model).</li>
<li>Sentence tokenization (after that you could delete punctuation).</li>
<li>Tokens encoding (with chosen Word2Vec model).</li>
<li>Clustering obtained tokens with <a href=""https://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow noreferrer"">Kmeans</a> (you should specify number of clusters - it will be equal to number of sentences in the future summary).</li>
<li>Obtaining summaries (one sentence of the summary is a middle sentence of the one cluster, look original post for more details and code samples).</li>
</ol>
<p>I hope it will help. Good luck! :)</p>
",0,-1,605,2019-03-11 17:20:05,https://stackoverflow.com/questions/55107184/combine-word-embeddings-with-with-topic-word-distribution-from-lda-for-text-summ
Can I preserve the random state of a doc2vec mode for each document I want to infer by infering all documents at the same time?,"<p>is there a way to infer multiple documents at the same time to preserve the random state of the model using Gensim Doc2Vec?</p>

<p>The function infer_vector is defined as</p>

<pre><code>infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)Â¶
</code></pre>

<p>where doc_words (list of str) â€“ A document for which the vector representation will be inferred. And I could not find any opther option to infer multiple documents at the same time.</p>
","gensim, word2vec, doc2vec","<p>There's no current option to infer multiple documents at once. It's one of many wishlist improvements for <code>infer_vector()</code> (collected in an <a href=""https://github.com/RaRe-Technologies/gensim/issues/515"" rel=""nofollow noreferrer"">open issue</a>), but there's no work in progress or targeted release for that to arrive. </p>

<p>I'm not sure what you mean by ""preserve the random state of the model"". The main motivations for batching that I can see would be user convenience, or added performance via multithreading. </p>

<p>If what you really want is deterministic inference, see an <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q12-ive-used-doc2vec-infer_vector-on-a-single-text-but-the-resulting-vector-is-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-doc2vec-inference-non-determinism"" rel=""nofollow noreferrer"">answer in the Gensim FAQ which explains why deterministic <code>Doc2Vec</code> inference isn't necessarily a good idea</a>. (It also includes a link to an issue with some ideas for how to force it, if you're determined to do that despite the good reasons not to.)</p>
",1,0,304,2019-03-14 18:30:43,https://stackoverflow.com/questions/55169721/can-i-preserve-the-random-state-of-a-doc2vec-mode-for-each-document-i-want-to-in
Use Word2vec to determine which two words in a group of words is most similar,"<p>I am trying to use the python wrapper around Word2vec. I have a word embedding or group of words which can be seen below and from them I am trying to determine which two words are most similar to each other. </p>

<p>How can I do this?</p>

<p>['architect', 'nurse', 'surgeon', 'grandmother', 'dad']</p>
","python, word2vec","<p>@rylan-feldspar's answer is generally the correct approach and will work, but you could do this a bit more compactly using standard Python libraries/idioms, especially <code>itertools</code>, a list-comprehension, and sorting functions. </p>

<p>For example, first use <code>combinations()</code> from <code>itertools</code> to generate all pairs of your candidate words:</p>

<pre class=""lang-py prettyprint-override""><code>from itertools import combinations
candidate_words = ['architect', 'nurse', 'surgeon', 'grandmother', 'dad']
all_pairs = combinations(candidate_words, 2)
</code></pre>

<p>Then, decorate the pairs with their pairwise similarity:</p>

<pre class=""lang-py prettyprint-override""><code>scored_pairs = [(w2v_model.wv.similarity(p[0], p[1]), p)
                for p in all_pairs]
</code></pre>

<p>Finally, sort to put the most-similar pair first, and report that score &amp; pair:</p>

<pre class=""lang-py prettyprint-override""><code>sorted_pairs = sorted(scored_pairs, reverse=True)
print(sorted_pairs[0])  # first item is most-similar pair
</code></pre>

<p>If you wanted to be compact but a bit less readable, it could be a (long) ""1-liner"":</p>

<pre class=""lang-py prettyprint-override""><code>print(sorted([(w2v_model.wv.similarity(p[0], p[1]), p) 
              for p in combinations(candidate_words, 2)
             ], reverse=True)[0])
</code></pre>

<p><strong>Update:</strong></p>

<p>Integrating @ryan-feldspar's suggestion about <code>max()</code>, and going for minimality, this should also work to report the best pair (but not its score):</p>

<pre class=""lang-py prettyprint-override""><code>print(max(combinations(candidate_words, 2),
          key=lambda p:w2v_model.wv.similarity(p[0], p[1])))
</code></pre>
",3,5,2251,2019-03-15 17:49:18,https://stackoverflow.com/questions/55188209/use-word2vec-to-determine-which-two-words-in-a-group-of-words-is-most-similar
Word2Vec: Word not in vocabulary even though its in corpus,"<pre><code>test = pd.read_csv('test.csv')
train = pd.read_csv('train.csv')
</code></pre>

<p><a href=""https://i.sstatic.net/iQtnV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iQtnV.png"" alt=""Image""></a></p>

<pre><code>def prep_corpus():
    sentences = []
    for x in test['title']:
        sentences.append(x.strip().split())

    for x in train['title']:
        sentences.append(x.strip().split())

    return sentences

corpus = prep_corpus()
</code></pre>

<p>Corpus is list of sentences where one sentence is one list of words:</p>

<p><a href=""https://i.sstatic.net/bW5dj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bW5dj.png"" alt=""Image""></a></p>

<pre><code>word_model = Word2Vec(corpus, workers = 2,sg=1, iter = 5)

word_model['maybelline', 'clear'].shape
</code></pre>

<p>I have a word vector that seems to work:</p>

<p><a href=""https://i.sstatic.net/e7LVg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e7LVg.png"" alt=""Image""></a></p>

<p>However, when I try to do word_model['intensity], I get the error message: ""word 'intensity' not in vocabulary""</p>

<p>This is despite the fact that the word intensity is in the corpus list. It appears once in the test.</p>

<p>I checked the corpus list by integrating through it and found the index of the sentence containing 'intensity'</p>

<p>I also checked the dataframe and found it inside:</p>

<p><a href=""https://i.sstatic.net/Qonbv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qonbv.png"" alt=""Image""></a></p>

<p>There are also some words that are in the corpus list, but not in the <strong>word2vec</strong> vocab. </p>

<p>I tried using both <strong>cbow</strong> and <strong>skipgram</strong> and trying different epochs of 1,5,15.</p>

<p>In all scenarios, I still encounter this error. How do I solve this problem?</p>
","machine-learning, word2vec","<p>It's likely you're using the <code>gensim</code> <code>Word2Vec</code> implementation. </p>

<p>That implementation, like the original <code>word2vec.c</code> code, enforces a default <code>min_count</code> for words of 5. Words with fewer than 5 examples will be ignored. In general, this greatly improves the quality of the remaining word-vectors. </p>

<p>(Words with just one, or a few, usage example don't get strong word-vectors themselves, as there is insufficient variety to reflect their real meanings in the larger language, and their few examples influence the model far less than other words with more examples. But, since there tend to many, many such words with few examples, in total they wind up diluting/interfering-with the learning the model could do, on other words with plentiful examples.)</p>

<p>You can set <code>min_count=1</code> to retain such words, but compared to discarding those rare words: </p>

<ul>
<li>the rare words' vectors will be poor</li>
<li>the rare words' presence will make the model much larger and noticeably slower to train</li>
<li>the vectors for other more-common words will be marginally worse</li>
</ul>
",2,0,956,2019-03-17 15:27:25,https://stackoverflow.com/questions/55208693/word2vec-word-not-in-vocabulary-even-though-its-in-corpus
How does back propagation in CNN works for pre-trained embedding in text classification,"<p>How do the loss function works in case of pretrained word2vec embeddings as the weights are not updated during training.Then how do the backward pass works and what does it update for prediction?</p>
","conv-neural-network, word2vec, pre-trained-model","<p>The loss is a calculation of the probability outputs and actual classes, this calculation is not dependent on the training status of any subsequent layers. By status I mean <em>trainable==True</em> or <em>trainable==False</em>.</p>

<p>Back-propagation of the loss is used in combination with the learning rate to <strong>adjust the weights of a layer only if the status is <em>trainable==True</em></strong>. The error can still pass through these layers if there are trainable layers in between non-trainable layers. Any layers in your model that are ""trainable"" will update after each training step.</p>

<p>When using Word2Vec, the weights are imported into the embedding layer and many times are ""frozen"" so as not to update while the rest of the parameters are trained. Towards the end of this process, however, sometimes unfreezing these weights can allow for better results.</p>
",0,0,330,2019-03-26 14:25:55,https://stackoverflow.com/questions/55359538/how-does-back-propagation-in-cnn-works-for-pre-trained-embedding-in-text-classif
How to specify an input with a list of arrays to Embedding layer in Keras?,"<p>I'm trying to do some word-level text generation and stuck with the foloowing problem:</p>

<p>My input looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>   tokenized_seq = [[w2v_model.wv.vocab[word].index for word in w2v_data[i]] for i in range(len(w2v_data))]
   x_seq = []
   y_seq = []

   for seq in tokenized_seq:
      x_seq.append(seq[:-1])
      y_seq.append([seq[-1]])
</code></pre>

<p>So, I'm going along the sequence (encoded words usnig word2vec) with the rolling window of the fixed size (tokenized _seq is a list of sequence with fixed length).</p>

<p>Look the example:  </p>

<p>Code block:</p>

<pre class=""lang-py prettyprint-override""><code>print(x_seq[0], '-&gt;', y_seq[0])  
print(' '.join([w2v_model.wv.index2word[i] for i in x_seq[0]]), '-&gt;', w2v_model.wv.index2word[y_seq[0].pop()]) 
</code></pre>

<p>Output:</p>

<pre><code>[608, 1661, 1, 4260, 1, 3, 2978, 741, 0, 153, 740, 1, 12004] -&gt; [109]
Ñ‡Ð°ÑÑ‚ÑŒ Ð¿ÐµÑ€Ð²Ð°Ñ . i . â€” eh bien , mon prince . gÃªnes -&gt; et
</code></pre>

<p>So, then, I'm trying to input all above to Embedding layer.</p>

<pre class=""lang-py prettyprint-override""><code>model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=emdedding_size,
                    input_length=avg_sent_len-1,
                    weights=[predtrained_weights]
                    trainable=False))

model.add(Bidirectional(LSTM(units=128)))

model.add(Dense(units=vocab_size, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_seq, y_seq,
                   epochs=10,
                   batch_size=128,
                   validation_split=0.2,
                   verbose=2)

</code></pre>

<p>Embedding parameters are:</p>

<pre><code>predtrained_weights = w2v_model.wv.vectors
vocab_size, emdedding_size = w2v_model.wv.vectors.shape
</code></pre>

<p><code>avg_sent_len</code> is the len of each seqence in <code>x_seq</code></p>

<p>The model compiles well, but when fitting I get the following error:</p>

<pre><code>ValueError: Error when checking target: expected dense_40 to have shape (31412,) but got array with shape (223396,) 
</code></pre>

<p>(31412,) is <code>vocab_size</code>
223396 is <code>x_seq</code> or <code>y_seq</code> length (number of input sequences)
So, could anybody help me?</p>
","python, tensorflow, keras, word2vec, word-embedding","<p>You input <code>x_seq</code> should be one numpy array of shape <code>(batch_size, seq_len)</code>. Try adding <code>x_seq = np.array(x_seq)</code>.</p>
",0,1,1707,2019-04-02 09:05:49,https://stackoverflow.com/questions/55470854/how-to-specify-an-input-with-a-list-of-arrays-to-embedding-layer-in-keras
Keras input specification for word2vec vectors,"<p>I read all the other answers regarding this topic, but my use case is slightly different.</p>

<p>I have a numpy array of shape (800,128,1). Each element in the 800 elements stores a word2vec embedding of shape (128,1). Now I wanted to send this as input with batch size 64 to a Keras model with first layer as Input layer. I'm getting the following error:</p>

<pre><code>expected party to have 2 dimensions, but got array with shape (800, 128, 1)
</code></pre>

<p>I understand that the input layer requires 2 dimensions, but which two? Or should I specify input shape on my own as three dimensional?</p>

<p>The input layer is currently this:</p>

<pre><code>Input(shape = (embedding_size, ), name = 'party')
</code></pre>
","python, tensorflow, keras, word2vec, word-embedding","<p>The shape of input is <code>(embedding_size,)</code>, where <code>embedding_size</code> is presumably 128. So the input is expected to be an array of shape <code>(batch_size, embedding_size)</code>, not <code>(batch_size, embedding_size, 1)</code>. You need to reshape your array to omit the last dimension of size 1.  </p>
",0,-1,382,2019-04-02 10:36:17,https://stackoverflow.com/questions/55472683/keras-input-specification-for-word2vec-vectors
What is the input file format for the function word2vec from package word2vec?,"<p>I am trying to do my own word embedding using the package word2vec (<a href=""https://pypi.org/project/word2vec/"" rel=""nofollow noreferrer"">https://pypi.org/project/word2vec/</a>). 
However, I can't find the file format of the input file for the function ""word2vec"".</p>

<p>I tried .txt format and pickle file but neither does work.</p>

<p>For example, where corpus.txt has been made with the Windows Notepad and contains ""I am a foo bar corpus test""</p>

<pre><code>import word2vec
word2vec.word2vec(""corpus.txt"", ""corpus.bin"", size=100, verbose=True)
</code></pre>

<p>I would have expected:</p>

<pre><code>Vocab size: 7
Words in train file: 7
</code></pre>

<p>as in the example here : <a href=""https://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb"" rel=""nofollow noreferrer"">https://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb</a></p>

<p>but got only</p>

<pre><code>Vocab size: 1
Words in train file: 0
</code></pre>

<p>Does anyone knows which type/format of file this function accepts ? </p>

<p>Thank you in advance !</p>
","python, word2vec","<p>There's a good chance your specific results are because most word2vec implementations discard all words that appear fewer than some <em>minimum-count</em> value, usually 5. (Word2Vec doesn't create good vectors for such rare words, and their presence usually interferes with better vectors for other more-common words, so discarding them is usually a good idea on real-sized corpuses.)</p>

<p>So a toy-sized input file, of just 7 words each appearing once, leaves nothing but (maybe) one synthetic word. </p>

<p>Because that PyPI package appears to be a thin wrapper around the <code>word2vec.c</code> code originally released by Google, you could probably refer to <a href=""https://github.com/tmikolov/word2vec"" rel=""nofollow noreferrer"">that code</a> to learn more details about formats/usage. </p>

<p>But, you could also use the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer""><code>Word2Vec</code> implementation in the Gensim library</a> - a far more common choice when using Python, with much more documentation &amp; flexibility.</p>
",0,0,545,2019-04-05 14:51:21,https://stackoverflow.com/questions/55538104/what-is-the-input-file-format-for-the-function-word2vec-from-package-word2vec
Does node2vec support negative edge weights?,"<p>Does node2vec provide support for edges with negative weights? I have an edgelist with several edges which are negative valued, but I'm strangely getting ZeroDivisionError on running the code. There are no zero edges, however, I checked. </p>

<p>Edit: was asked to share code. I've made no changes to the original repo, so I'm pasting here the exact lines throwing the error.</p>

<pre><code>unnormalized_probs = []
    for dst_nbr in sorted(G.neighbors(dst)):
        if dst_nbr == src:
            unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)
        elif G.has_edge(dst_nbr, src):
            unnormalized_probs.append(G[dst][dst_nbr]['weight'])
        else:
            unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)
    norm_const = sum(unnormalized_probs)
    normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]
</code></pre>

<p>Getting the ZeroDivisionError error in the last line.
My edgelist which goes as input to this, is written as follows:</p>

<pre><code>0 0 1
234 11 -2
12 0 -1
</code></pre>

<p>The zero-valued nodes aren't a problem, they weren't before when I was running the code on positive node values.</p>
","python, python-3.x, graph, nodes, word2vec","<p>I figured this out. The weight values (stored in unnormalized probabilities) are being added to get a value called 'norm_const', which is then dividing the unnormalized probs. So since they're being added, possibility of zero happening arises, hence zero division error.</p>
",0,3,507,2019-04-08 03:55:18,https://stackoverflow.com/questions/55566027/does-node2vec-support-negative-edge-weights
How are word vectors co-trained with paragraph vectors in doc2vec DBOW?,"<p>I don't understand how word vectors are involved at all in the training process with gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">doc2vec</a> in DBOW mode (<code>dm=0</code>). I know that it's disabled by default with <code>dbow_words=0</code>. But what happens when we set <code>dbow_words</code> to 1?</p>

<p>In my understanding of DBOW, the context words are predicted directly from the paragraph vectors. So the only parameters of the model are the <code>N</code> <code>p</code>-dimensional paragraph vectors plus the parameters of the classifier.</p>

<p>But multiple sources hint that it is possible in DBOW mode to co-train word and doc vectors. For instance:</p>

<ul>
<li>section 5 of <a href=""https://www.aclweb.org/anthology/W16-1609"" rel=""nofollow noreferrer"">An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</a></li>
<li>this SO answer: <a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors/30337118#30337118"">How to use Gensim doc2vec with pre-trained word vectors?</a></li>
</ul>

<p>So, how is this done? <strong>Any clarification would be much appreciated!</strong></p>

<p>Note: for DM, the paragraph vectors are averaged/concatenated with the word vectors to predict the target words. In that case, it's clear that words vectors are trained simultaneously with document vectors. And there are <code>N*p + M*q + classifier</code> parameters (where <code>M</code> is vocab size and <code>q</code> word vector space dim).</p>
","gensim, word2vec, doc2vec","<p>If you set <code>dbow_words=1</code>, then skip-gram word-vector training is added the to training loop, interleaved with the normal PV-DBOW training. </p>

<p>So, for a given target word in a text, 1st the candidate doc-vector is used (alone) to try to predict that word, with backpropagation adjustments then occurring to the model &amp; doc-vector. Then, a bunch of the surrounding words are each used, one at a time in skip-gram fashion, to try to predict that same target word â€“ with the followup adjustments made.</p>

<p>Then, the next target word in the text gets the same PV-DBOW plus skip-gram treatment, and so on, and so on. </p>

<p>As some logical consequences of this:</p>

<ul>
<li><p>training takes longer than plain PV-DBOW - by about a factor equal to the <code>window</code> parameter</p></li>
<li><p>word-vectors overall wind up getting more total training attention than doc-vectors, again by a factor equal to the <code>window</code> parameter</p></li>
</ul>
",2,1,641,2019-04-09 11:46:29,https://stackoverflow.com/questions/55592142/how-are-word-vectors-co-trained-with-paragraph-vectors-in-doc2vec-dbow
permission denied error while reading the GoogleNews-vectors-negative300.bin file,"<p>I am trying to read different language encoding models like golve, fasttext and word3vec and detecting the sarcasm but I am unable to read google's language encoding file. It's giving permission denied error. what should I do?</p>

<p>I tried different encoding and giving all permission to the file as well but still no luck</p>

<pre><code>EMBEDDING_FILE = 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding=""ISO-8859-1""))
embed_size = 300
word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
embedding_matrix = np.zeros((nb_words, embed_size))
for word, i in word_index.items():
    if i &gt;= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector


PermissionError                           Traceback (most recent call last)
&lt;ipython-input-10-5d122ae40ef0&gt; in &lt;module&gt;
      1 EMBEDDING_FILE = 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
      2 def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')
----&gt; 3 embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding=""ISO-8859-1""))
      4 embed_size = 300
      5 word_index = tokenizer.word_index

PermissionError: [Errno 13] Permission denied: 'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'
</code></pre>
","python, nlp, jupyter-notebook, word2vec","<p>You would likely get the same IO-related error no matter how you try, or for what purpose, you try to open the file â€“ so this isn't really a question about <code>nlp</code>, or <code>word2vec</code>, or even <code>jupyter-notebook</code>. </p>

<p>Note that sometimes errors that we'd consider other things get reported as ""permission"" problems - because at some level, you can't do that to that <em>kind</em> of path, or file.</p>

<p>You've specified the file path as <code>'C:/Users/Abhishek/Documents/sarcasm/GoogleNews-vectors-negative300.bin/'</code>, with a trailing <code>/</code> that usually indicates something is a directory. That could be a problem.</p>

<p>Also, I believe this particular file is usually 3+ GB in size - and some DOS-descended filesystems, or a Python interpreter which is only 32-bit, might have problems handling files over certain sizes like 2GB or 4GB.</p>
",2,1,1374,2019-04-11 20:40:35,https://stackoverflow.com/questions/55640826/permission-denied-error-while-reading-the-googlenews-vectors-negative300-bin-fil
Using word2vec in a sentence,"<p>I'm trying to generate the probability of a given sentence to be be correct. </p>

<p>I have word2vec for each token in the language and I want to predict the probability of the sentence to be correct. I'm unable to create a suitable model. How can I proceed ?</p>
","machine-learning, nlp, word2vec, machine-translation","<p>Word-vectors alone won't help you do this. </p>

<p>While their similarities and relative orientations are trained by predicting word-cooccurrences, the vectors alone aren't a clear guide to words which co-occur. And the word-vectors definitely don't encode rules-of-grammatical-usage, as mere proximity, not proper ordering, is the usual training input. </p>

<p>That said, if you happened to be using the Python <code>gensim</code> implementation of <code>Word2Vec</code>, and if you train a full model yourself (as opposed to using off-the-shelf pre-trained vectors), that whole model will, in some modes, support a <code>score()</code> method that grades a set of sentences on how well they conform with the model's expectations. It <em>won't</em> tell you whether a text is ""correct"", just whether it's ""familiar"" â€“ and was 1st proposed/added as a possible way of applying multiple contrasting <code>Word2Vec</code> models to aid in classification problems. You can read more about this method, and find links to the research paper that motivated it and a demo usage, in the method documentation:</p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score</a></p>
",1,-1,56,2019-04-13 19:39:21,https://stackoverflow.com/questions/55669189/using-word2vec-in-a-sentence
Is it normal that there is no data access synchronization when training the neural network by several threads?,"<p>I looked at the <a href=""https://code.google.com/archive/p/word2vec/source/default/source"" rel=""nofollow noreferrer"">classic word2vec sources</a>, and, if I understood correctly, there is no data access synchronization when training the neural network by several threads (synchronization for matrixes syn0, syn1, syn1neg). Is it normal practice for training, or is it a bug?</p>
","machine-learning, neural-network, nlp, word2vec","<p>Perhaps counterintuitively, it's normal. A pioneering work on this was the 'Hogwild' paper in 2011:</p>

<p><a href=""https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent"" rel=""nofollow noreferrer"">https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent</a></p>

<p>Its abstract:</p>

<blockquote>
  <p>Stochastic Gradient Descent (SGD) is a popular algorithm that can
  achieve state-of-the-art performance on a variety of machine learning
  tasks. Several researchers have recently proposed schemes to
  parallelize SGD, but all require performance-destroying memory locking
  and synchronization. This work aims to show using novel theoretical
  analysis, algorithms, and implementation that SGD can be implemented
  <em>without any locking</em>. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility
  of overwriting each other's work. We show that when the associated
  optimization problem is sparse, meaning most gradient updates only
  modify small parts of the decision variable, then Hogwild achieves a
  nearly optimal rate of convergence. We demonstrate experimentally that
  Hogwild outperforms alternative schemes that use locking by an order
  of magnitude.</p>
</blockquote>

<p>It turns out SGD is more slowed by synchronized access than by threads overwriting each others' work... and some results even seem to hint that in practice, the extra ""interference"" may be a net benefit for optimization progress. </p>
",1,0,30,2019-04-15 11:16:54,https://stackoverflow.com/questions/55688024/is-it-normal-that-there-is-no-data-access-synchronization-when-training-the-neur
how to speed up gensim word2vec initialization with pre proccessed corpus?,"<p>i am training multiple word2vec models on the same corpus. (i am doing this to study the variation in learned word vectors)</p>

<p>i am using this tutorial as reference: <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a></p>

<p>it is suggested that by default gensim.models.word2vec will iterate over the corpus at least twice. once for initialization and then again for training (iterating the number of epochs specified) </p>

<p>since i am always using the same corpus, i want to save time by initializing only once, and providing the same initialization as input to all successive models.</p>

<p>how can this be done?</p>

<p>this is my current setting:</p>

<pre class=""lang-py prettyprint-override""><code>subdirectory = 'corpus_directory'
for i in range(10):
    sentences = MySentences(subdirectory) # a memory-friendly iterator
    model = gensim.models.Word2Vec(sentences, min_count=20, size=100, workers=4)
    model.train(sentences, total_examples=model.corpus_count, epochs=1)
    word_vectors = model.wv
    fname = 'WV{}.kv'
    word_vectors.save(fname.format(i))
</code></pre>

<p>where MySentences is defined similarly to the tutorial:
(i made a slight change, so the order of corpus sentences would be shuffled with each initialization)</p>

<pre class=""lang-py prettyprint-override""><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.file_list = [fname for fname in os.listdir(dirname) if fname.endswith('.txt')]
        random.shuffle(self.file_list)

    def __iter__(self):
        for article in self.file_list:
            for line in open(os.path.join(self.dirname, article)):
                yield line.split()
</code></pre>
","gensim, word2vec","<p>If you supply a corpus of <code>sentences</code> to the class-instantiation, as your code has done, you don't need to call <code>train()</code>. It will already have done that automatically, and your second <code>train()</code> is redundant. (I recommend doing all such operations with logging enabled at the INFO level, and review the lgos after each run to understand what is happening â€“ things like two full start-to-finish trainings should stick out in the logs.)</p>

<p>The case where you would call <code>train()</code> explicitly is if you want more control over the interim steps. You leave the <code>sentences</code> out of the class-instantiation, but then it is required for you to perform two explicit steps: both one call to <code>build_vocab()</code> (for initial vocabulary scan) and then one call to <code>train()</code> (for actual multi-epoch training). </p>

<p>In that case, you can use gensim's native <code>.save()</code> to save the model <em>after</em> the vocabulary-discovery, to have a model that's ready for re-training and doesn't need to report that step. </p>

<p>So, you could re-load that vocabulary-built model multiple times, to different variables, to train in different ways. For some of the model's meta-parameters â€“ like <code>window</code> or even <code>dm</code> mode â€“ you can even tamper directly with their values on a model after vocabulary-building to try different variants. </p>

<p>However, if there are any changes to the corpus's words/word-frequencies, or to other parameters that affect the initialization that happens during <code>build_vocab()</code> (like vector <code>size</code>), then the initialization will be out of sync with the configuration you're trying, and you could get strange errors. </p>

<p>In such a case, the best course is to repeat the <code>build_vocab()</code> step entirely. (You could also look into the source code to see the individual steps performed by <code>build_vocab()</code>, and just patch/repeat the initialization steps that are needed, but that requires strong familiarity with the code.)</p>
",1,0,461,2019-04-16 10:07:28,https://stackoverflow.com/questions/55705634/how-to-speed-up-gensim-word2vec-initialization-with-pre-proccessed-corpus
Where to find a pretrained doc2vec model on Wikipedia or large article dataset like Google news?,"<p>Am struggling with training wikipedia dump on doc2vec model, not experienced in setting up a server as a local machine is out of question due to the ram it requires to do the training. I couldnt find a pre trained model except outdated copies for python 2.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>I'm not aware of any publicly-available standard gensim <code>Doc2Vec</code> models trained on Wikipedia. </p>
",1,0,283,2019-04-19 05:02:37,https://stackoverflow.com/questions/55756841/where-to-find-a-pretrained-doc2vec-model-on-wikipedia-or-large-article-dataset-l
How to add magnitude or value to a vector in Python?,"<p>I am using this function to calculate distance between 2 vectors a,b, of size 300, word2vec, I get the distance between 'hot' and 'cold' to be equal 1. </p>

<p>How to add this value (1) to a vector, becz i thought simply new_vec=model['hot']+1, but when I do the calc dist(new_vec,model['hot'])=17? </p>

<pre><code>import numpy
def dist(a,b):
  return numpy.linalg.norm(a-b)


a=model['hot']
c=a+1
dist(a,c)

17
</code></pre>

<p>I expected dist(a,c) will give me back 1!</p>
","numpy, linear-algebra, word2vec","<p>You should review what the <a href=""https://en.wikipedia.org/wiki/Norm_(mathematics)"" rel=""nofollow noreferrer"">norm</a> is. In the case of numpy, the default is to use the L-2 norm (a.k.a the Euclidean norm). When you add 1 to a vector, the call is to add 1 to all of the elements in the vector.</p>

<pre><code>&gt;&gt; vec1 = np.random.normal(0,1,size=300)
&gt;&gt; print(vec1[:5])
... [ 1.18469795  0.04074346 -1.77579852  0.23806222  0.81620881]

&gt;&gt; vec2 = vec1 + 1
&gt;&gt; print(vec2[:5])
... [ 2.18469795  1.04074346 -0.77579852  1.23806222  1.81620881]
</code></pre>

<p>Now, your call to <code>norm</code> is saying <code>sqrt( (a1-b1)**2 + (a2-b2)**2 + ... + (aN-bN)**2 )</code> where <code>N</code> is the length of the vector and <code>a</code> is the first vector and <code>b</code> is the second vector <em>(and ai being the ith element in a)</em>. Since <code>(a1-b1)**2 == (a2-b2)**2 == ... == (aN-bN)**2 == 1</code> we expect this sum to produce N which in your case is 300. So <code>sqrt(300) = 17.3</code> is the expected answer. </p>

<pre><code>&gt;&gt; print(np.linalg.norm(vec1-vec2))
... 17.320508075688775
</code></pre>

<p>To answer the question, ""How to add a value to a vector"": you have done this correctly. If you'd like to add a value to a specific element then you can do <code>vec2[ix] += value</code> where <code>ix</code> indexes the element that you wish to add. If you want to add a value uniformly across all elements in the vector that will change the norm by 1, then add <code>np.sqrt(1/300)</code>.</p>

<p>Also possibly relevant is a more commonly used distance metric for word2vec vectors: the <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine distance</a> which measures the angle between two vectors.</p>
",2,0,2054,2019-04-25 07:37:09,https://stackoverflow.com/questions/55844129/how-to-add-magnitude-or-value-to-a-vector-in-python
Replace random word by similarity with word2vec,"<p>I would like to replace a random word from a sentence by the most similar word from word2vec, for example a word from the sentence <code>question = 'Can I specify which GPU to use?'</code>. </p>

<p>I used this recursive method because with the split function, some words (like <code>to</code>) are not in word2vecmodel: </p>

<pre><code>import gensim.models.keyedvectors as word2vec
import random as rd

model = word2vec.KeyedVectors.load_word2vec_format('/Users/nbeau/Desktop/Word2vec/model/GoogleNews-vectors-negative300.bin', binary=True)

def similar_word(sentence, size):
    pos_to_replace = rd.randint(0, size-1)
    try:
        similarity = model.most_similar(positive = [sentence[pos_to_replace]])
        similarity = similarity[0][0]
    except KeyError:
        similarity, pos_to_replace = test(sentence, size)
        return similarity, pos_to_replace
    return similarity, pos_to_replace

question = question.split()
size = len(question)
similarity, pos_to_replace = similar_word(question, size)
sentence[pos_to_replace] = similarity
</code></pre>

<p>I would like to know if there is a better method to avoid the words which are not in the word2vec model.</p>
","python-3.x, word2vec","<p>A few thoughts:</p>

<ul>
<li><p>If <code>kv_model</code> is your <code>KeyedVectors</code> model, you can do <code>'to' in kv_model</code> to test if a word is present, rather than trying but then catching the <code>KeyError</code>. But being optimistic &amp; catching the error is a common idiom as well!</p></li>
<li><p>Your recursion won't necessarily exit: if the supplied text contains no known words, it will keep recursively trying endlessly (or perhaps when some call-depth implementation limit is reached). Also, it may try the same word many times. </p></li>
</ul>

<p>I'd suggest using a loop rather than recursion, and using Python's <code>random.shuffle()</code> method to create a single random permutation of all potential indexes. Then, try each in turn, returning as soon as a replacement is possible, or indicating failure if no replacement was possible.</p>

<p>Keeping your same method return-signature:</p>

<pre><code>def similar_word(sentence):
    indexes = range(len(sentence))
    random.shuffle(indexes)
    for i in indexes:
        if sentence[i] in kv_model:
            return model.most_similar(sentence[i], topn=1)[0][0], i
    return None, -1  # no replacement was possible
</code></pre>

<p>(But separate from your question: if 100% of the time, the result of the function is used to perform a replacement, I'd just move the replacement inside the function, mutating the passed-in <code>sentence</code>. And the function could report how many replacements it made: <code>0</code> for failure, <code>1</code> for the usual case â€“ and perhaps in the future could accept a parameter to request more than 1 replacement.)</p>
",1,0,474,2019-04-29 11:04:33,https://stackoverflow.com/questions/55902017/replace-random-word-by-similarity-with-word2vec
Can word2vec model be used for words also as training data instead of sentences,"<p>In Word2vec can we use words instead of sentences for model training</p>

<p>Like below code gberg_sents is sentence tokens
model = Word2Vec(sentences=gberg_sents,size=64,sg=1,window=10,min_count=5,seed=42,workers=8)</p>

<p>Like this can we use word tokens also</p>
","word2vec, nlp","<p>No, word2vec is trained with a language modeling objective, i.e., it predicts what words appear in surrounding of other words. For this, your training data need to be actual sentences that show how the words are used in context. It is actually the context of the words that gives you the information that is captured in the embeddings.  </p>
",1,0,44,2019-04-30 03:57:23,https://stackoverflow.com/questions/55913762/can-word2vec-model-be-used-for-words-also-as-training-data-instead-of-sentences
How can I train the word2vec model on my own corpus in R?,"<p>I would like to train the word2vec model on my own corpus using the <code>rword2vec</code> package in R. </p>

<p>The <code>word2vec</code> function that is used to train the model requires a <code>train_file</code>. The package's documentation in R simply notes that this is the training text data, but doesn't specify how it can be created. </p>

<p>The training data used in the example on GitHub can be downloaded here:
<a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">http://mattmahoney.net/dc/text8.zip</a>. I can't figure out what type of file it is. </p>

<p>I've looked through the README file on the <a href=""https://github.com/mukul13/rword2vec"" rel=""nofollow noreferrer"">rword2vec GitHub page</a> and checked out the official word2vec page on <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google Code</a>.</p>

<p>My corpus is a <code>.csv</code> file with about 68,000 documents. File size is roughly 300MB. I realize that training the model on a corpus of this size might take a long time (or be infeasible), but I'm willing to train it on a subset of the corpus. I just don't know how to create the <code>train_file</code> required by the function.</p>
","r, word2vec, word-embedding, nlp","<p>After you unzip text8, you can open it with a text editor.  You'll see that it is one long document.  You will need to decide how many of your 68,000 documents you want to use for training and whether you want to concatenate them together of keep them as separate documents.  See <a href=""https://datascience.stackexchange.com/questions/11077/using-several-documents-with-word2vec"">https://datascience.stackexchange.com/questions/11077/using-several-documents-with-word2vec</a></p>
",2,3,686,2019-04-30 23:41:25,https://stackoverflow.com/questions/55929977/how-can-i-train-the-word2vec-model-on-my-own-corpus-in-r
Word vectors from a whole doc2vec model vs. word vectors from a particular document,"<p>I trained a gensim's Doc2Vec model with default word2vec training (dm=1). I can get the word vectors from the global model in model.wv.vectors.
But the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">documentation</a> says that the same word (""leaves"" in the example) won't have the same vector depending of the document context where it appear.</p>

<p>So I'm a bit confused : in the model.wv.vectors, will the word ""leaves"" by example, have the same vector for all the documents used to train the model (that may be contradictory with what I understand from the documentation) ? If not, how to get the word vectors from a particular document ?</p>
","gensim, word2vec, doc2vec","<p>That documentation is misleading. The word-token <code>'leaves'</code> will have only one word-vector in that model. </p>

<p>I'm guessing the author of that comment <em>may</em> have meant that during model-training in PV-DM mode (<code>dm=1</code>), the training-predictions would be influenced by a combination of the word-vector and the 'floating' doc-vector for that text (and other neighboring word-vectors within the context-window). But still, the one word just has the one vector, and the description there is confused.</p>
",0,0,275,2019-05-01 16:37:26,https://stackoverflow.com/questions/55939511/word-vectors-from-a-whole-doc2vec-model-vs-word-vectors-from-a-particular-docum
gensim Word2Vec - how to apply stochastic gradient descent?,"<p>To my understanding, batch (vanilla) gradient descent makes one parameter update for all training data. Stochastic gradient descent (SGD) allows you to update parameter for each training sample, helping the model to converge faster, at the cost of high fluctuation in function loss. </p>

<p><a href=""https://i.sstatic.net/0LGA5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0LGA5.png"" alt=""enter image description here""></a></p>

<p>Batch (vanilla) gradient descent sets <code>batch_size=corpus_size</code>.</p>

<p>SGD sets <code>batch_size=1</code>.</p>

<p>And mini-batch gradient descent sets <code>batch_size=k</code>, in which <code>k</code> is usually 32, 64, 128...</p>

<p>How does gensim apply SGD or mini-batch gradient descent? It seems that <code>batch_words</code> is the equivalent of <code>batch_size</code>, but I want to be sure. </p>

<p>Is setting <code>batch_words=1</code> in gensim model equivalent to applying SGD?</p>
","nlp, gensim, word2vec, gradient-descent, stochastic","<p>No, <code>batch_words</code> in <code>gensim</code> refers to the size of work-chunks sent to worker threads. </p>

<p>The <code>gensim</code> <code>Word2Vec</code> class updates model parameters after each training micro-example of <code>(context)-&gt;(target-word)</code> (where <code>context</code> might be a single word, as in skip-gram, or the mean of several words, as in CBOW). </p>

<p>For example, you can review this optimized <code>w2v_fast_sentence_sg_neg()</code> cython function for skip-gram with negative-sampling, deep in the <code>Word2Vec</code> training loop:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/460dc1cb9921817f71b40b412e11a6d413926472/gensim/models/word2vec_inner.pyx#L159"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/460dc1cb9921817f71b40b412e11a6d413926472/gensim/models/word2vec_inner.pyx#L159</a></p>

<p>Observe that it is considering exactly one target-word (<code>word_index</code> parameter) and one context-word (<code>word2_index</code>), and updating both the word-vectors (aka 'projection layer' <code>syn0</code>) and the model's hidden-to-output weights (<code>syn1neg</code>) before it might be called again with a subsequent single <code>(context)-&gt;(target-word)</code> pair. </p>
",0,0,1866,2019-05-02 11:10:59,https://stackoverflow.com/questions/55951158/gensim-word2vec-how-to-apply-stochastic-gradient-descent
Words missing from trained word2vec model vocabulary,"<p>I am currently working with python where I train a Word2Vec model using sentences that I provide. Then, I save and load the model to get the word embedding of each and every word in the sentences that were used to train the model. However, I get the following error.</p>

<blockquote>
  <p>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""</p>
</blockquote>

<p>whereas, one of the sentences provided during training is as follows.</p>

<pre><code>sportsteam n1985_chicago_bears teamplaysincity city chicago
</code></pre>

<p>Hence I would like to know why some words are missing from the vocabulary, despite being trained on those words from that sentence corpus. </p>

<p><strong>Training the word2vec model on own corpus</strong></p>

<pre><code>import nltk
import numpy as np
from termcolor import colored
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.decomposition import PCA


#PREPARING DATA

fname = '../data/sentences.txt'

with open(fname) as f:
    content = f.readlines()

# remove whitespace characters like `\n` at the end of each line
content = [x.strip() for x in content]


#TOKENIZING SENTENCES

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

#TRAINING THE WORD2VEC MODEL

model = Word2Vec(sentences)

words = list(model.wv.vocab)
model.wv.save_word2vec_format('model.bin')
</code></pre>

<p><strong>Sample sentences from sentences.txt</strong></p>

<pre><code>sportsteam hawks teamplaysincity city atlanta
stadiumoreventvenue honda_center stadiumlocatedincity city anaheim
sportsteam ducks teamplaysincity city anaheim
sportsteam n1985_chicago_bears teamplaysincity city chicago
stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta
stadiumoreventvenue united_center stadiumlocatedincity city chicago
...
</code></pre>

<p>There are 1860 such lines in the <code>sentences.txt</code> file, each containing exactly 5 words and no stop words.</p>

<p>After saving the model, I tried to load it from a different python file within the same directory as the saved <code>model.bin</code> as shown below.</p>

<p><strong>Loading the saved model.bin</strong></p>

<pre><code>import nltk
import numpy as np
from gensim import models

w = models.KeyedVectors.load_word2vec_format('model.bin', binary=True)
print(w['n1985_chicago_bears'])
</code></pre>

<p>However, I end up with the following error</p>

<pre><code>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""
</code></pre>

<p>Is there a way to get the word embedding for each and every word in the trained sentence corpus using the same method?</p>

<p>Any suggestions in this regard will be much appreciated.</p>
","python, tensorflow, nltk, gensim, word2vec","<p>The default <code>min_count=5</code> for Word2Vec implementation of gensim and looks like the token you are looking for <code>n1985_chicago_bears</code> occurs less then 5 times in your corpus. Change your min count appropriately. </p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Method signature:</a></p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None,
  corpus_file=None, size=100, alpha=0.025, window=5, min_count=5,
  max_vocab_size=None, sample=0.001, seed=1, workers=3,
  min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75,
  cbow_mean=1, hashfxn=, iter=5, null_word=0,
  trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,
  callbacks=(), max_final_vocab=None)</p>
</blockquote>

<pre><code>content = [
    ""sportsteam hawks teamplaysincity city atlanta"",
    ""stadiumoreventvenue honda_center stadiumlocatedincity city anaheim"",
    ""sportsteam ducks teamplaysincity city anaheim"",
    ""sportsteam n1985_chicago_bears teamplaysincity city chicago"",
    ""stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta"",
    ""stadiumoreventvenue united_center stadiumlocatedincity city chicago""
]

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

model = Word2Vec(sentences, min_count=1)
print (model['n1985_chicago_bears'])
</code></pre>
",5,4,4869,2019-05-08 04:40:58,https://stackoverflow.com/questions/56033651/words-missing-from-trained-word2vec-model-vocabulary
What&#39;s the major difference between glove and word2vec?,"<p>What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?</p>
","machine-learning, nlp, word2vec, word-embedding, glove","<p>Yes, they're both ways to train a word embedding. They both provide the same core output: one vector per word, with the vectors in a useful arrangement. That is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions.</p>
<p>Word2Vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus.</p>
<p>GloVe works to fit vectors to model a giant word co-occurrence matrix built from the corpus.</p>
<p>Working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (When I've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.)</p>
<p>I'm more familiar with Word2Vec, and my impression is that Word2Vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (For example, using a small-versus-large <code>window</code> parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. Different downstream applications may prefer word-vectors that skew one way or the other.)</p>
<p>Conversely, some proponents of GLoVe tout that it does fairly well without needing metaparameter optimization.</p>
<p>You probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors.</p>
",24,28,18423,2019-05-10 06:10:19,https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec
Mapping doc2vec paragraph representation to its class tag post-training,"<p>I have trained Doc2Vec paragraph embeddings on text documents using the <code>Doc2Vec</code> module in Python's <code>gensim</code> package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see <a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">this link</a> for details):</p>

<pre><code>def tag_docs(docs, col):
    tagged = docs.apply(lambda r: TaggedDocument(words=simple_preprocess(r[col]), tags=[r.label]), axis=1)
    return tagged
</code></pre>

<p>However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:</p>

<pre><code>print(model.docvecs.count)
</code></pre>

<p>My question is as follows: I trained the model of <code>n</code> classes of documents, yielding <code>n</code> document vectors in <code>model.docvecs</code>. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?</p>
","python, gensim, word2vec, text-classification, doc2vec","<p>If <code>classA</code> was one of the document-tags you provided during training, then <code>model.docvecs['classA']</code> will return the single doc-vector that was learned for that tag from training. </p>

<p>If you have another new vector â€“ for example one inferred on new text via <code>model.infer_vector(words)</code>, then you can get a list of which learned doc-vectors in the model are closest via <code>model.docvecs.most_similar(positive=[new_vector])</code>.</p>

<p>If your true aim to classify new documents into one (or more) of these classes, then taking the top <code>most_similar()</code> result is one crude way to do that. </p>

<p>But  having reduced all classes to just a single summary vector (the one vector learned for that tag), then taking just the one nearest-neighbor of a new-document, may not perform well. It somewhat forces an assumption that that classes are very simple shapes in the n-dimensional space. </p>

<p>For classification, you may want to let all documents get individual vectors (not based on their known classes, or in addition to their known classes), then train a separate classifier on that set of (doc-vector, label) labeled-data. That could discover finer-grained, and oddly-shaped boundaries between the classes. </p>
",1,0,329,2019-05-10 10:59:45,https://stackoverflow.com/questions/56076298/mapping-doc2vec-paragraph-representation-to-its-class-tag-post-training
IndexError - Implementing the test of CBOW with pytorch,"<p>I am not very used to python &amp; machine learning code. I am testing on pytorch CBOW test, but it says something about Index Error. Can anyone help?</p>

<pre><code># model class
class CBOW(nn.Module):

    ...
    def get_word_embedding(self, word):
    word = torch.cuda.LongTensor([word_to_ix[word]])
    return self.embeddings(word).view(1, -1)

# test method
def test_cbow(model, train_words, word_to_ix):
    # test word similarity
    word_1 = train_words[2] #randomly chosen word
    word_2 = train_words[3] #randomly chosen word

    word_1_vec = model.get_word_embedding(word_1)[0].cpu()
    word_2_vec = model.get_word_embedding(word_2)[0].cpu()

    print(word_1_vec)
    print(word_2_vec)


    word_similarity = (word_1_vec.dot(word_2_vec) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))).data.numpy()[0]
    print(""Similarity between '{}' &amp; '{}' : {:0.4f}"".format(word_1, word_2, word_similarity))

# executing the test
test_cbow(model, train_words, word_to_ix)
</code></pre>

<p>BELOW IS THE RESULT:</p>

<pre><code>tensor([ 0.8978,  1.0713, -1.6856, -1.0967, -0.0114,  0.4107, -0.4293, -0.7351,
         0.4410, -1.5937, -1.3773,  0.7744,  0.0739, -0.3263,  1.0342,  1.0420,
        -1.1333,  0.4158,  1.1316, -0.0141, -0.8383,  0.2544, -2.2409, -1.1858,
         0.2652, -0.3232,  0.1287, -1.5274,  0.3199, -2.1822,  0.9464, -0.6619,
         1.1549,  0.5276,  0.0849, -0.1594, -1.7922,  1.3567, -0.4376, -0.9093,
         1.0701,  1.5373, -1.3277, -1.1833,  1.8070, -0.0551, -0.8439,  1.5236,
        -0.3890, -0.2306, -0.7392, -1.6435,  0.4485,  0.8988, -0.5958, -0.6989,
         1.6123, -1.6668,  0.0583,  0.6698, -0.6998,  1.1942,  0.6355,  0.7437,
        -1.0006, -0.5398,  1.3197,  1.3696, -0.3221,  0.9004,  0.6268,  0.0221,
         0.0269, -1.7966, -1.6153, -0.1695, -0.0339, -0.5145,  1.5744, -0.3388,
        -0.9617,  0.6750, -1.1334,  0.0377,  1.1123,  1.1002, -0.3605,  0.2105,
        -1.6570,  1.3818,  0.9183,  0.0274,  0.9072,  0.8414,  0.3424,  0.2199,
         1.6546, -0.1357,  1.1291, -0.5309], grad_fn=&lt;CopyBackwards&gt;)
tensor([-0.6263, -0.5639,  2.1590, -0.3659,  0.2862, -0.4542, -0.4825, -0.1776,
        -0.4242,  0.9525,  0.7138, -0.3107,  1.8733, -0.3406,  0.0277,  1.6775,
         2.1893,  2.0332,  0.7185,  0.0050, -0.1627, -0.1113,  1.0444,  1.4057,
         0.2183,  0.3405,  0.0930,  1.2428, -0.0740,  0.3991, -0.2722,  1.4980,
         0.9207,  0.5008, -1.9297,  0.5600,  1.6416,  1.1550,  0.1440,  0.0739,
        -0.7465, -0.2458,  0.9217,  0.7156, -1.2558, -0.9891, -0.7313,  0.8501,
        -1.2851, -0.3068, -0.0796,  0.9361,  0.0927, -1.2988,  0.7422,  0.1388,
         1.3895, -0.7935,  0.4008, -0.1338,  1.5563,  0.5864,  0.6606, -0.2341,
         0.1218, -0.7313,  0.5073, -0.2941,  0.0316, -2.5356, -0.0885,  2.5765,
         0.2090,  0.2819, -0.0386,  0.7986,  2.1165, -0.0271, -0.2987,  0.2905,
         0.0149,  0.2403,  0.0752, -1.5535,  0.3794,  2.0638,  1.0603,  0.0703,
        -0.3643, -1.5671, -0.4736, -1.3035,  0.6583,  0.2531,  0.9829, -0.6025,
        -0.8148, -0.3457, -0.7339,  0.6758], grad_fn=&lt;CopyBackwards&gt;)
</code></pre>

<hr>

<p>What I am also confused is that I have to convert cuda datatype to numpy, since I used cuda in get_word_embedding method. Is adding .cpu() to convert the datatype correct?</p>

<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-68-39d73aa6e0de&gt; in &lt;module&gt;()
     17     print(""Similarity between '{}' &amp; '{}' : {:0.4f}"".format(word_1, word_2, word_similarity))
     18 
---&gt; 19 test_cbow(model, train_words, word_to_ix)

&lt;ipython-input-68-39d73aa6e0de&gt; in test_cbow(model, train_words, word_to_ix)
     14     print(type(word_1_vec))
     15 
---&gt; 16     word_similarity = (word_1_vec.dot(word_2_vec) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))).data.numpy()[0]
     17     print(""Similarity between '{}' &amp; '{}' : {:0.4f}"".format(word_1, word_2, word_similarity))
     18 

**IndexError: too many indices for array**
</code></pre>
","python, pytorch, word2vec","<p>In your code, <code>word_similarity</code> is not an array, so you can't access it's 0th element. Just modify your code as:</p>

<pre><code>word_similarity = (word_1_vec.dot(word_2_vec) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))).data.numpy()
</code></pre>

<p>You can also use:</p>

<pre><code>word_similarity = (word_1_vec.dot(word_2_vec) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))).item()
</code></pre>

<p>Here, the <code>.item()</code> in PyTorch would directly give you a float value since the similarity value is a scalar.</p>

<blockquote>
  <p>When to use .cpu()?</p>
</blockquote>

<p>To convert a cuda tensor to cpu tensor, you need to use <code>.cpu()</code>. You cannot directly convert a cuda tensor to numpy. You have to convert to cpu tesnor first and then call <code>.numpy()</code>.</p>
",1,0,162,2019-05-11 21:42:46,https://stackoverflow.com/questions/56094478/indexerror-implementing-the-test-of-cbow-with-pytorch
Python gensim create word2vec model from vectors (in ndarray),"<p>I have a ndarray with words and their corresponding vector (with the size of 100 per word).
For example:</p>

<pre><code>Computer 0.11 0.41 ... 0.56
Ball     0.31 0.87 ... 0.32
</code></pre>

<p>And so on.</p>

<p>I want to create a word2vec model from it:</p>

<pre><code>model = load_from_ndarray(arr)
</code></pre>

<p>How can it be done? I saw </p>

<blockquote>
  <p>KeyedVectors</p>
</blockquote>

<p>but it only takes file and not array</p>
","python, nlp, gensim, word2vec","<pre><code>from gensim.models import KeyedVectors
words = myarray[:,0]
vectors = myarray[:,1:]
model = KeyedVectors(vectors.shape[1])
model.add(words, vectors)
</code></pre>

<p>if you want you can then save it</p>

<pre><code>model.save('mymodel')
</code></pre>

<p>and later just load it</p>

<pre><code>model = KeyedVectors.load('mymodel')
</code></pre>
",3,3,1879,2019-05-14 10:47:33,https://stackoverflow.com/questions/56128701/python-gensim-create-word2vec-model-from-vectors-in-ndarray
Use Word2Vec to build a sense embedding,"<p>I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.</p>
<p>I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:</p>
<h3>Dataset</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;sentence&gt;
  &lt;text&gt;word1 word2 word3&lt;/text&gt;
  &lt;annotations&gt;
    &lt;annotation anchor=word1 lemma=lemma1&gt;tag1&lt;/annotation&gt;
    &lt;annotation anchor=word2 lemma=lemma2&gt;tag2&lt;/annotation&gt;
    &lt;annotation anchor=word3 lemma=lemma3&gt;tag3&lt;/annotation&gt;
  &lt;annotations&gt;
&lt;/sentence&gt;
</code></pre>
<p>Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:</p>
<h3>Embedding</h3>
<pre><code>lemma1_tag1 dim 1 dim 2 dim 3 ... dim n
lemma2_tag2 dim 1 dim 2 dim 3 ... dim n
lemma3_tag3 dim 1 dim 2 dim 3 ... dim n
</code></pre>
<p>I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each <code>anchor</code> with the respective <code>lemma1_tag1</code> (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:</p>
<h3>Corpus Example</h3>
<pre><code>let just list most_recent_01730444a headline_06344461n
</code></pre>
<p>Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:</p>
<h3>Vocabulary Example</h3>
<pre><code>lemma1_tag1
lemma2_tag2
</code></pre>
<p>So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:</p>
<h3>Terminal emulation</h3>
<pre><code>./word2vec -train data/test.txt -output data/embeddings.vec -size 300 -window 7 -sample 1e-3 -hs 1 -negative 0 -iter 10 -min-count 1 -read-vocab data/dictionary.txt -cbow 1
</code></pre>
<h3>Output</h3>
<pre><code>Starting training using file data/test.txt
Vocab size: 80
Words in train file: 20811
</code></pre>
<p>The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format <code>lemma1_tag1</code> because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!</p>
","python, gensim, word2vec, word-embedding","<p>Both the original <code>word2vec.c</code> from Google, and gensim's <code>Word2Vec</code>, handle words with underscores just fine. </p>

<p>If both are looking at your input file, and both reporting just 80 unique words where you're expecting 100,000-plus, there's probably something wrong with your input-file. </p>

<p>What does <code>wc data/test.txt</code> report?</p>
",1,1,268,2019-05-16 11:12:27,https://stackoverflow.com/questions/56167224/use-word2vec-to-build-a-sense-embedding
memory Error while training word2vec: hierarchical softmax,"<p>I am training word embeddings with word2vec and with some Parameter configurations I Always get a Memory error. I do this for the whole english Wikipedia and using PathLineSentence for the Input (more than 11 GB). I do not really know, which Parameter Setting is wrong, but I see a trend: When hierachical softmax is set to 1 I get an error - so what is the Default value of hs - 0 or 1? Is it also possible to do hs and negative sampling?</p>

<p>It would be great when you please help me with my Questions and what I can do to handle my Problems.</p>
","parameters, word2vec","<p>You would typically only want to do negative-sampling or hierarchical-softmax, not both. </p>

<p>The default is negative-sampling, equivalent to if you explicitly specified <code>negative=5, hs=0</code>. If you enable hierarchical-softmax, you should disable negative-sampling, for example: <code>hs=1, negative=0</code>. </p>

<p>If you're getting a memory error, the most common causes (if you otherwise have a reasonable amount of RAM) are:</p>

<ul>
<li><p>attempts to load an entire larger-than-practical corpus into memory. However, if you're using a good corpus iterator, like <code>PathLineSentence</code>, properly, it will only read data from files as needed</p></li>
<li><p>trying to create a model that's too large - with the largest contributors to model size being the size of the known vocabulary and the vector-size chosen. If I recall correctly, for something like Wikipedia, you can set <code>min_count=100</code> and still wind up with a vocabulary of around 1 million words. With 300-dimensional vectors, that'd require (1 million * 300 dimensions * 4 bytes-per-dimension=) 1.2GB just for the word-vectors, and about that much again for the model's internal weights, and then some smaller amounts of memory for other structures. </p></li>
</ul>

<p>So to address memory problems, consider: (1) get more RAM; (2) ensure the corpus is only being streamed; (3) reduce vector-size; (4) reduce the trained-vocabulary, most commonly by using a larger <code>min_count</code> cutoff to discard rarer words.</p>
",0,0,192,2019-05-17 19:07:37,https://stackoverflow.com/questions/56192236/memory-error-while-training-word2vec-hierarchical-softmax
Why mmap flag reduces memory consumption for single Word2Vec instance,"<p>According to the docs and wikipedia:</p>

<p>mmap allows processes to share same chunk of ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file))
</code></pre>

<p>This model loaded like this takes ~2.2 GB ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file), mmap='r')
</code></pre>

<p>This model loaded like this takes ~1.2 GB ram</p>

<p>Why am I observing such drastic decrease in ram consumption?</p>

<p>Loading multiple models simultaneously, works as expected and models share the ~1 GM memory.  </p>
","ram, gensim, mmap, word2vec","<p>Memory-mapping re-uses the operating system's virtual-memory functionality to use the existing file as the backing-source for a range of addressable memory. </p>

<p>With a single process, it <em>won't</em> necessarily save any memory. Instead, it just:</p>

<ul>
<li><p><em>Delays</em> loading any range-of-the-addresses into RAM, leaving it on disk until requested. If it's never requested, then RAM is never used, so in that particular case it may ""save"" memory.</p></li>
<li><p><em>Allows</em> those loaded ranges to be cheaply discarded if they're not accessed for a while, and the RAM is required for other allocations â€“ because those ranges can be reloaded on demand from disk if ever again needed. So it might ""save"" memory in that case, compared to exhausting RAM or activating other generic virtual-memory that's not aware of the 1:1 relationship with an existing disk file. (Without memory mapping, seldom-used ranges of material in RAM could get written-out to a separate swap file to free space for other allocations â€“ which a wasteful operation, and redundant data, when the data already exists on disk somewhere.)</p></li>
</ul>

<p>Unfortunately, in the common-case of a single-process, and typical operations like a <code>most_similar()</code> which necessarily computes on every single vector, the whole structure will be brought into memory on each <code>most_similar()</code>. There's no net RAM ""savings"" there (though perhaps a slight CPU/IO benefit if other memory pressure would've forced paging-out the loaded ranges). (Whatever approach you're using the sample the ""~2.2 GB"" and ""~1.2 GB"" used-RAM values may not be properly measuring that.)</p>

<p>The main benefit is when using multiple processes that each need to consult the same file's data. If naively loaded into RAM, each process will have its own redundant copy of the same data. If using memory-mapping, you've let the OS know: these multiple arrays-in-address-space, in multiple separate processed, definitionally have the same data (as reflected in the file). No matter how many processes need the data, only one copy of each file-range will ever consume RAM. There, a large savings can be achieved. </p>
",1,0,155,2019-05-21 10:33:38,https://stackoverflow.com/questions/56236404/why-mmap-flag-reduces-memory-consumption-for-single-word2vec-instance
Is it recommended to remove duplicate words in word2vec algorithm?,"<p>I have a data that consists of DNA-sequences, where the words represented as kmers of length 6, and the sentences represented as DNA-sequences. Each DNA-sequence has 80 kmers (words)</p>

<p>The list of kmers I have is around 130,000 kmers, but after removing the duplicate elements, I would have 4500 kmers only. So, this huge gap confused me in regarding removing the duplicate kmers or not. My question is, is it recommended in this case to remove the duplicated kmers in the word2vec algorithm? </p>

<p>Thanks.</p>
","word2vec, dna-sequence","<p>Without an example, it's unclear what you mean by ""removing the duplicate elements"". (Does that mean, when the same token appears twice in a row? Or twice in one ""sentence""? Or, as I'm not familiar with what your data looks like in this domain, something else entirely?)</p>

<p>That you say there are 130,000 tokens in the vocabulary, but then 4,500 later, is also confusing. Typically the ""vocabulary"" size is the number of unique tokens. Removing duplicate tokens couldn't possibly change the number of unique tokens encountered. </p>

<p>In the usual domain of word2vec, natural language, words don't often repeat one-after-another. To the extent they sometimes might â€“ as in say the utterance ""it's very very hot in here"" â€“ it's not really an important enough case that I've noticed anyone commenting about handling that ""very very"" differently than any other two words.</p>

<p>(If a corpus had some artificially-duplicated full-sentences, it <strong>might</strong> be the case that you'd want to try discarding the exact-duplicate-sentences. Word2vec benefits from a <em>variety</em> of different usage-examples. Repeating the same sentence 10 times essentially just overweights those training examples â€“ it's not nearly as good as 10 contrasting, but still valid, examples of the same words' usage.)</p>

<p>You're in a different domain that's not natural language, with different co-occurrence frequencies, and different end-goals. Word2vec might prove useful, but it's unlikely any general rules-of-thumb or recommendations from other domains will be useful. You should test things both ways, evaluate the results on your ultimate task in a robust repeatable way, and choose based on what you discover.</p>
",5,0,1498,2019-05-24 12:03:24,https://stackoverflow.com/questions/56292315/is-it-recommended-to-remove-duplicate-words-in-word2vec-algorithm
Word2Vec with POS not producing expected results?,"<p>I am trying to gauge the impact of part of speech information with Word2Vec embeddings but am not obtaining expected results.</p>

<p>I expected POS included word2vec embeddings to perform better in a machine translation task but it is actually performing worse.</p>

<p>I am creating two sets of embedding off of the same corpus using Gensim, one is normal Word2Vec, the other, I am changing tokens to ""[WORD]__[POS]"".</p>

<p>I am gauging differences in performance by using the embeddings in a Seq2Seq machine translation task. I am evaluating the two approaches with BLEU</p>

<p>This is how I am training the word2vec + POS embeddings with SpaCy:</p>

<pre><code>sentences = []
    for sent in doc.sents:
        tokens = []
        for t in sent:
            tokens += [""{}__{}"".format(t.text, t.pos_)]
        sentences += tokens
    pos_train += [sentences]
</code></pre>

<p>This is my benchmark machine translation model with Keras + Tensorflow:</p>

<pre><code>encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(LATENT_DIM, return_state=True)
_, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
</code></pre>

<p>With BLEU, the Word2Vec+POS approach consistently scores the same as Word2Vec or 0.01-0.02 points below the normal Word2Vec embeddings.</p>

<p>Does anyone know why this might be happening? Is there a gap in my reasoning or expectations?</p>
","keras, nlp, word2vec, word-embedding, seq2seq","<p>I, too, would have expected accurate part-of-speech info to improve translation â€“ but I don't know if others have reported such an improvement. Some (uninformed) conjectures as to why it might not:</p>

<ul>
<li><p>maybe the POS tagging isn't very accurate with regard to one of the languages, or there are some other anomalous challenges specific to your data</p></li>
<li><p>maybe the method of creating composite tokens, with the internal <code>__</code>, is in a few corner cases interfering with evaluation â€“ for example, if the original corpus retains any tokens which already had <code>__</code> in them</p></li>
<li><p>maybe for certain cases of insufficient data, the collision of similar-meaning homographs of different parts-of-speech actually <em>helps</em> thicken the vaguer meaning-to-meaning translation. (For example, maybe given the semantic relatedness of <code>shop_NOUN</code> and <code>shop_VERB</code>, it's better to have 100 colliding examples of <code>shop</code> than 50 of each.)</p></li>
</ul>

<p>Some debugging ideas (in addition to the obvious ""double-check everything""): </p>

<ul>
<li><p>look closely at exactly those test cases where the plain-vs-POS approaches differ in their scoring; see if there are any patterns â€“ like strange tokens/punctuation, nonstandard grammar, etc â€“ giving clues to where the <code>__POS</code> decorations hurt</p></li>
<li><p>try other language pairs, and other (private or public) concordance datasets, to see if elsewhere (or in general) the POS-tagging does help, and there's something extra-challenging about your particular dataset/language-pair</p></li>
<li><p>consider that the multiplication-of-tokens (by splitting homographs into POS-specific variants) has changed the model size &amp; word-distributions, in ways that might interact with other limits (like <code>min_count</code>, <code>max_vocab_size</code>, etc) in ways that modify training. In particular, perhaps the larger-vocabulary POS model should get more training epochs, or a larger word-vector dimensionality, to reflect its larger vocabulary with a lower average number of word-occurrences.</p></li>
</ul>

<p>Good luck!</p>
",2,0,324,2019-05-26 18:08:18,https://stackoverflow.com/questions/56316146/word2vec-with-pos-not-producing-expected-results
Why java.lang.IllegalStateException occurs when running Word2VecExample form Scala Spark mllib?,"<p>I am trying to read a text file in <em>Spark-mllib examples</em> (Word2VecExample) and create it word vectors. I run it by some text files and it gives no error but when reading one of my files, it gives this error and I am really confuse with that because I tried everything such as file format(<em>utf-8</em>) and <em>ASCII</em> characters.
this is my source code:</p>

<pre><code>package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
// $example off$

object Word2VecExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName(""Word2VecExample"")
    conf.setMaster(""local[4]"")
    val sc = new SparkContext(conf)

    val input = sc.textFile(""C:\\Users\\...\\Desktop\\yelp_labelled.txt"").map(line =&gt; line.split("" "").toSeq)

    val word2vec = new Word2Vec()

    val model = word2vec.fit(input)

    val synonyms = model.findSynonyms(""1"", 5)

    for((synonym, cosineSimilarity) &lt;- synonyms) {
      println(s""$synonym $cosineSimilarity"")
    }

    model.save(sc, ""C:\\Users\\...\\Desktop\\Edited1Yelp"")
    val sameModel = Word2VecModel.load(sc, ""C:\\Users\\...\\Edited1Yelp"")

    // $example off$

    sc.stop()
  }
}
</code></pre>

<p>And this is the Error:</p>

<pre><code>Exception in thread ""main"" 19/05/29 18:36:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on DESKTOP-T5EN156:64774 in memory (size: 13.0 KB, free: 2.2 GB)
java.lang.IllegalStateException: 1 not in vocabulary
    at org.apache.spark.mllib.feature.Word2VecModel.transform(Word2Vec.scala:533)
    at org.apache.spark.mllib.feature.Word2VecModel.findSynonyms(Word2Vec.scala:545)
    at org.apache.spark.examples.mllib.Word2VecExample$.main(Word2VecExample.scala:43)
    at org.apache.spark.examples.mllib.Word2VecExample.main(Word2VecExample.scala)
</code></pre>

<p>And line 43 refers to <code>val synonyms = model.findSynonyms(""1"", 5)</code> but I'm sure it's true because I run the program correctly with some other files.</p>
","scala, apache-spark, apache-spark-mllib, word2vec","<p>Based on the Spark example and <a href=""https://spark.apache.org/docs/1.6.2/mllib-feature-extraction.html#model"" rel=""nofollow noreferrer"">documentation</a>, this error raises from <code>findSynonyms(String, int)</code> method. There must be a vector for String parameter in this method and if it doesn't exist a vector for this, the error </p>

<blockquote>
  <p>Java.lang.IllegalStateException.</p>
</blockquote>

<p>appears.
So, in my example there is no vector for the input String parameter ""<em>1</em>"".</p>
",1,1,429,2019-05-29 14:23:20,https://stackoverflow.com/questions/56362831/why-java-lang-illegalstateexception-occurs-when-running-word2vecexample-form-sca
Why are word embeddings with linguistic features (e.g. Sense2Vec) not used?,"<p>Given that embedding systems such as Sense2Vec incorporate linguistic features such as part-of-speech, why are these embeddings not more commonly used?</p>

<p>Across popular work in NLP today, Word2Vec and GloVe are the most commonly used word embedding systems. Despite the fact that they only incorporate word information and does not have linguistic features of the words.</p>

<p>For example, in sentiment analysis, text classification or machine translation tasks, it makes logical sense that if the input incorporates linguistic features as well, performance could be improved. Particular when disambiguating words such as ""duck"" the verb and ""duck"" the noun.</p>

<p>Is this thinking flawed? Or is there some other practical reason why these embeddings are not more widely used.</p>
","nlp, artificial-intelligence, word2vec, word-embedding, sense2vec","<p>It's a very subjective question.  One reason is the pos-tagger itself. Pos-tagger is a probabilistic model which could add to the overall error/confusion.  </p>

<p>For eg. say you have dense representations for <code>duck-NP</code> and <code>duck-VB</code> but during run/inference time your pos-tagger tags 'duck' as something else then you wont even find it. Moreover it also effectively reduces the total number of times your system sees the word <code>duck</code> hence one could argue that representations generated would be weak.  </p>

<p>To top it off the main problem which <code>sense2vec</code> was addressing is contextualisation of word representations which has been solved by contextual representations like <code>BERT</code> and <code>ElMo</code> etc. without producing any of the above problems. </p>
",1,2,286,2019-05-31 02:44:59,https://stackoverflow.com/questions/56388012/why-are-word-embeddings-with-linguistic-features-e-g-sense2vec-not-used
Create a new vector model in gensim,"<p>I already trained a word2vec model with gensim library. For example, my model contains vectors for 2 words: ""new"" and ""york"". However, I also want to train a vector for the word ""new york"", so I transform ""new york"" into ""new_york"" and train a new vector model. Finally, I want to combine 3 vectors: vector of the word ""new"", ""york"" and ""new_york"" into one vector representation for the word ""new york"".</p>

<p>How can I save the new vector value to the model?</p>

<p>I try to assign the new vector to the model but gensim did not allow to assign the new value for vector model.</p>
","python, vector, gensim, word2vec","<p>Word-vectors are generally only comparable to each other if they were trained together.</p>

<p>So, if you want to have vectors for all of 'new', 'york', and 'new_york', you should prepare a corpus which includes them all, in a variety of uses, and train a <code>Word2Vec</code> model from that. </p>
",1,1,162,2019-06-01 17:31:23,https://stackoverflow.com/questions/56408959/create-a-new-vector-model-in-gensim
How to do an accurate food similarity using word2vec and how should i design the word2vec parameters for this task?,"<p>I am dealing with a food similarity problem in which I need to find similar Indian food for a given Indian dish. So, Can anyone help me dealing with this problem effectively? is using word2vec fine for this problem?</p>

<p>For this task, I started with finding vectors for ingredients and then apply tf-idf weighted average on ingredients to get vectors for dishes. I scraped data for ingredients of different dishes then I applied wor2vec but I didn't find satisfactory results.</p>

<pre><code>#Setting values for NN parameters 
num_features = 300    # Word vector dimensionality                      
min_word_count = 3                       
num_workers = 4       # Number of CPUs
context = 10          # Context window size ie. avg recipe size                                                                                
downsampling = 1e-3   # threshold for configuring which 
                      # higher-frequency words are randomly downsampled

#Initializing and training the model 
 model = word2vec.Word2Vec(sentences, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling) '''

#using init_sims to make the model much more memory-efficient.
model.init_sims(replace=True) 

model.most_similar('ginger')
</code></pre>

<p>Output :</p>

<pre><code>[('salt', 0.9999704957008362),
 ('cloves garlic', 0.9999628067016602),
 ('garam masala', 0.9999610781669617),
 ('turmeric', 0.9999603033065796),
 ('onions', 0.999959409236908),
 ('vegetable oil', 0.9999580383300781),
 ('coriander', 0.9999570250511169),
 ('black pepper', 0.9999487400054932),
 ('cumin seeds', 0.999948263168335),
 ('green chile pepper', 0.9999480247497559)]
</code></pre>
","neural-network, deep-learning, word2vec, recipe","<p>Word2vec may be reasonable for this task. You might need more data, or parameter tweaks, to get the best results.</p>

<p>It's not obvious to me what's wrong with your example results, so you should add more detail to your question, and more examples/explanation, about why you're unsatisfied with the results. </p>

<p>If you have certain ideal-results, that you can collect into repeatable model tests, that would help you to tune the model. For example, if you know that ""cinnamon"" should be a better match for ""ginger"" than ""salt"", you would encode that (and dozens or hundreds or thousands of other ""preferred answers"") into an automated evaluation method that could score a model. </p>

<p>Then, you could adjust (""meta-optimize"") model parameters to find a model that scores best on your evaluation. </p>

<ul>
<li>less or more dimensions (<code>size</code>) could help, depending on the richness of your data</li>
<li>a smaller or larger <code>window</code> might help</li>
<li>more training <code>epochs</code> might help</li>
<li>a larger <code>min_count</code> (discarding more low-frequency words) often helps, especially with larger datasets</li>
<li>a more-aggressive <code>sample</code> value (smaller, such as <code>1e-04</code> or <code>1e-05</code>) can help, with very large datasets</li>
<li>non-default <code>ns_exponent</code> values may help, especially for recommendation-applications</li>
</ul>
",1,0,320,2019-06-04 15:44:55,https://stackoverflow.com/questions/56447245/how-to-do-an-accurate-food-similarity-using-word2vec-and-how-should-i-design-the
Sentence iterator to pass to Gensim language model,"<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Your illustrated class <code>MySentences</code> assumes one sentence per line. That might not be the case for your data.</p>
<p>One thing to note is - calling Word2Vec(sentences, iter=1) will run two passes over the sentences iterator (or, in general iter+1 passes; default iter=5). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. These two (or, iter+1) passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and youâ€™re able to initialize the vocabulary some other way:</p>
<pre><code>model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet
model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator
model.train(other_sentences)  # can be a non-repeatable, 1-pass generator
</code></pre>
<p>For example, if you are trying to read dataset stored in a database, your generator function to stream text directly from a database, will throw TypeError:</p>
<pre><code>TypeError: You can't pass a generator as the sentences argument. Try an iterator.
</code></pre>
<p>A generator can be consumed only once and then itâ€™s forgotten. So, you can write a wrapper which has an iterator interface but uses the generator under the hood.</p>
<pre><code>class SentencesIterator():
    def __init__(self, generator_function):
        self.generator_function = generator_function
        self.generator = self.generator_function()

    def __iter__(self):
        # reset the generator
        self.generator = self.generator_function()
        return self

    def __next__(self):
        result = next(self.generator)
        if result is None:
            raise StopIteration
        else:
            return result
</code></pre>
<p>The generator function is stored as well so it can reset and be used in Gensim like this:</p>
<pre><code>from gensim.models import FastText

sentences = SentencesIterator(tokens_generator)
model = FastText(sentences) 
</code></pre>
",5,2,2396,2019-06-05 22:29:49,https://stackoverflow.com/questions/56468865/sentence-iterator-to-pass-to-gensim-language-model
How to put maximum vocabulary frequency in doc2vec,"<p>Doc2vec while creating the vocabulary has possibility to put minimum occurence of the word in documents to be included in vocabulary as parameter <code>min_count</code>.</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=3, epochs=100,workers=8)
</code></pre>

<p>How is it possible to exclude words which appear far too often, with some parameter?</p>

<p>I know that one way is to do this in preprocessing step by manually deleting those words, and counting each, but would be nice to know if there is maybe some built in method to do so, as it gives more space for testing.
Many thanks for the answer.</p>
","python, gensim, word2vec, doc2vec","<p>There's no explicit <code>max_count</code> parameter in gensim's <code>Word2Vec</code>. </p>

<p>If you're sure some tokens are meaningless, you should preprocess your text to eliminate them. </p>

<p>There is also a <code>trim_rule</code> option that can be passed as model instantiation or <code>build_vocab()</code>, where your own function can discard some words; see the gensim docs at:</p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p>

<p>Similarly, you could possibly avoid calling <code>build_vocab()</code> directly, and instead call its substeps â€“ but edit the discovered raw-counts dictionary before the vocabulary is finalized. You would want to consult the source code to do this, and could use the code that discards too-infrequent words as a model for your own new additional code. </p>

<p>The classic <code>sample</code> parameter of <code>Word2Vec</code> also controls a downsampling of high-frequency words, to prevent the model from spending too much relative effort on redundantly training abundant words. The more aggressive (smaller) this value is, the more instances of high-frequency words will be radomly skipped during training. The default of <code>1e-03</code> (<code>0.001</code>) is very conservative; in very-large natural language corpuses I've seen good results up to <code>1e-07</code> (<code>0.0000001</code>) or <code>1e-8</code> (<code>0.00000001</code>) â€“ so in another domain where some lower-meaning tokens are very-frequent, similarly aggressive downsampling is worth trying. </p>

<p>The newer <code>ns_exponent</code> option changes negative sampling to adjust the relative favoring of less-frequent words. The original <code>word2vec</code> work used a fixed value of 0.75, but some research since has suggested other domains, like recommendation systems, might benefit from other values that are more or less sensitive to actual token frequencies. (The relevant paper is linked in the <code>gensim</code> docs for the <code>ns_exponent</code> parameter.)</p>
",3,2,1235,2019-06-06 13:14:00,https://stackoverflow.com/questions/56478384/how-to-put-maximum-vocabulary-frequency-in-doc2vec
Where is word2vec mapping coming from for DBOW doc2vec in gensim implementation?,"<p>I am trying to use gensim for doc2vec and word2vec.</p>

<p>Since PV-DM approach can generate word2vec and doc2vec at the same time,
I thought PV-DM is the right model to use.</p>

<p>So, I created a model using <code>gensim</code> by specifying <code>dm=1</code> for PV-DM</p>

<p>My questions are followings:</p>

<ol>
<li><p>Is it true that word2vec model gets trained along with doc2vec when I call <code>train</code> on Doc2vec object??</p></li>
<li><p>it seems like property <code>wv</code> contains word2vec and available even before training. Is this static version of word2vec?</p></li>
<li><p>I also created DBOW model and noticed that it also contains <code>wv</code>. Is this also the same static version of word2vec that I mentioned in the previous question?</p></li>
</ol>
","gensim, word2vec, doc2vec","<p>(1) Yes, word-vectors are trained simultaneously with doc-vectors in PV-DM mode.</p>

<p>(2) The contents of the <code>wv</code> property before training happens are the randomly-initialized, untrained word-vectors. (As in word2vec, all vectors get random, low-magnitude starting positions.)</p>

<p>(3) In plain PV-DBOW mode (<code>dm=0</code>), because of code-sharing, the <code>wv</code> vectors are still allocated &amp; initialized â€“ but never trained. At the end of PV-DBOW training, the <code>wv</code> word-vectors will be unchanged, and thus random/useless. (They don't participate in training at all.)</p>

<p>If you enable the optional <code>dbow_words=1</code> parameter, then skip-gram word-vector training will be mixed-in with plain PV-DBOW training. This will be done in an interleaved fashion, so each target word (to be predicted) will be used to train a PV-DBOW doc-vector, then neighboring context word-vectors. As a result, the <code>wv</code> word-vectors will be trained, and in the ""same space"" for meaningful comparisons to doc-vectors. </p>

<p>With this option, training will take longer than in plain PV-DBOW (by a factor related to the <code>window</code> size). For any particular end-purpose, the doc-vectors in this mode might be better (if the word-to-word predictions effectively helped to extend the corpus in useful ways) or worse (if the model spending so much effort on word-to-word predictions effectively diluted/overwhelmed other patterns in the full-doc doc-to-word predictions).</p>
",2,2,124,2019-06-06 18:51:00,https://stackoverflow.com/questions/56483468/where-is-word2vec-mapping-coming-from-for-dbow-doc2vec-in-gensim-implementation
"Gensim: Manual generation of training tuples of (target, context, label)","<p>I am asking this question as a lazy researcher who just wants to try out random crazy ideas quickly, without spending a ton of time reinventing wheels. I completely understand these aren't the intended use cases.</p>

<p>To test a number of hypothesis, I would love to</p>

<ul>
<li>generate the (target, context, +1) tuples differently, instead of the default sliding window.</li>
<li>generate the negative samples (target, random_context, -1) tuples based on some rules, instead of from random NCE draws.</li>
</ul>

<p>For example, I can get the parse tree of a sentence and use parent-child relationship to generate tuples, which is a non-linear window(somebody already tried it in NLP research community, hand-coded ofc...). I can also get an antonyms dictionary to lookup and to generate more negative samples in addition to the random ones (not sure, may help with faster convergence).</p>

<p>Are there some private member functions (something that starts with <code>_XX</code>)I can override to achieve these?</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Unfortunately, there are not easy extension-points for changing the (context->word) training-examples, or negative-example sampling. </p>

<p>Of course the full source code is available, and thus anything's possible as patches, or by using the existing code as a starting-point. Practically, however, the key loops/decisions about these steps are only efficiently run from inside the optimized Cython training routines â€“ which are a bit harder to read/adapt/test/deploy.</p>

<p>(There's <a href=""https://github.com/RaRe-Technologies/gensim/issues/1623"" rel=""nofollow noreferrer"">an open issue #1623</a> to re-factor the code to make such related variants of Word2Vec easier to implement. But the project's prior effort to ostensibly meet this need, <a href=""https://github.com/RaRe-Technologies/gensim/pull/1777"" rel=""nofollow noreferrer"">PR #1777</a>, was somewhat of a disaster, adding more layers of indirection and scattering key operations across new classes, without offering the sorts of extension-points that were really needed.)</p>
",1,0,37,2019-06-11 21:01:09,https://stackoverflow.com/questions/56551612/gensim-manual-generation-of-training-tuples-of-target-context-label
Python3 - Doc2Vec: Get document by vector/ID,"<p>I've already built my Doc2Vec model, using around 20.000 files. I'm looking for a way to find the string representation of a given vector/ID, which might be similar to Word2Vec's index2entity. I'm able to get the vector itself, using model['n'], but now I'm wondering whether there's a way to get some sort of string representation of it as well.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>If you want to look up your actual training text, for a given text+tag that was part of training, you should retain that mapping outside the <code>Doc2Vec</code> model. (The model doesn't store training texts â€“ only looking at them, repeatedly, during training.)</p>

<p>If you want to <em>generate</em> a text from a <code>Doc2Vec</code> doc-vector, that's not an existing feature, nor do I know any published work describing a reliable technique for doing so. </p>

<p>There's a speculative/experimental bit of work-in-progress for gensim <code>Doc2Vec</code> that will forward-propagate a doc-vector through the model's neural-network, and report back the most-highly-predicted target words. (This is somewhat the opposite of the way <code>infer_vector()</code> works.)</p>

<p>That <em>might</em>, plausibly, give a sort-of summary text. For more details see this open issue &amp; the attached PR-in-progress:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/2459"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2459</a></p>

<p>Whether this is truly useful or likely to become part of gensim is still unclear. </p>

<p>However, note that such a set-of-words <em>wouldn't</em> be grammatical. (It'll just be the ranked-list of most-predicted words. Perhaps some other subsystem could try to string those words together in a natural, grammatical way.) </p>

<p>Also, the subtleties of whether a concept has many potential associates words, or just one, could greatly affect the ""top N"" results of such a process. Contriving a possible example: there are many words for describing a 'cold' environment. As a result, a doc-vector for a text about something cold might have lots of near-synonyms for 'cold' in the 11th-20th ranked positions â€“ such that the ""total likelihood"" of at least one cold-ish word is very high, maybe higher than any one other word. But just looking at the top-10 most-predicted words might instead list other ""purer"" words whose likelihood isn't so divided, and miss the (more-important-overall) sense of ""coldness"". So, this experimental pseudo-summarization method might benefit from a second-pass that somehow ""coalesces"" groups-of-related-words into their most-representative words, until some overall proportion (rather than fixed top-N) of the doc-vector's predicted-words are communicated. (This process might be vaguely like finding a set of M words whose ""Word Mover's Distance"" to the full set of predicted-words is minimized â€“ though that could be a very expensive search.)</p>
",0,0,276,2019-06-13 14:25:26,https://stackoverflow.com/questions/56582711/python3-doc2vec-get-document-by-vector-id
Copying embeddings for gensim word2vec,"<p>I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:</p>

<pre><code>model_w2v = models.Word2Vec(sg = 1, size=300)
model_w2v.build_vocab(all_tokens)
model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30)
</code></pre>

<p>Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training </p>

<pre><code>model_w2v_new = models.Word2Vec(sg = 1, size=300)
model_w2v_new.build_vocab(all_tokens)
</code></pre>

<p>and set the embeddings of the new Word2Vec equal to the first one</p>

<pre><code>model_w2v_new.wv.vectors = model_w2v.wv.vectors
</code></pre>

<p>Most of the functions work as expected, e.g.</p>

<pre><code>model_w2v.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
model_w2v_new.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
</code></pre>

<p>and</p>

<pre><code>model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
</code></pre>

<p>and</p>

<pre><code>entities_list = list(model_w2v.wv.vocab.keys()).remove('religion')

model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
</code></pre>

<p>However, most_similar doesn't work:</p>

<pre><code>model_w2v.wv.most_similar(positive=['religion'], topn=3)
[('religions', 0.4796232581138611),
 ('judaism', 0.4426296651363373),
 ('theists', 0.43141329288482666)]

model_w2v_new.wv.most_similar(positive=['religion'], topn=3)
&gt;[('roderick', 0.22643062472343445),
&gt; ('nci', 0.21744996309280396),
&gt; ('soviet', 0.20012077689170837)]
</code></pre>

<p>What am I missing? </p>

<p>Disclaimer. I posted this question on <a href=""https://datascience.stackexchange.com/questions/53601/copying-embeddings-for-gensim-word2vec"">datascience.stackexchange</a> but got no response, hoping to have a better luck here. </p>
","gensim, word2vec, word-embedding","<p>Generally, your approach should work. </p>

<p>It's likely the specific problem you're encountering was caused by an extra probing step you took and is not shown in your code, because you had no reason to think it significant: some sort of <code>most_similar()</code>-like operation on <code>model_w2v_new</code> <strong>after</strong> its <code>build_vocab()</code> call but <strong>before</strong> the later, malfunctioning operations.</p>

<p>Traditionally, <code>most_similar()</code> calculations operate on a version of the vectors that has been normalized to unit-length. The 1st time these unit-normed vectors are needed, they're calculated â€“ and then cached inside the model. So, if you then  replace the raw vectors with other values, but don't discard those cached values, you'll see results like you're reporting â€“ essentially random, reflecting the randomly-initialized-but-never-trained starting vector values. </p>

<p>If this is what happened, just discarding the cached values should cause the next <code>most_similar()</code> to refresh them properly, and then you should get the results you expect:</p>

<pre><code>model_w2v_new.wv.vectors_norm = None
</code></pre>
",1,0,557,2019-06-14 03:52:24,https://stackoverflow.com/questions/56591149/copying-embeddings-for-gensim-word2vec
"word not in vocabulary after training gensim word2vec model, why?","<p>So I want to use word-embeddings in order to get some handy dandy cosine similarity values. After creating the model and checking for similarity of the word ""not"" (which is in the data I give the model) it tells me that the word is not in the vocabulary.</p>

<p>Why can't it find the similarity for the word 'not'?</p>

<p>the description data looks as follows:<br>
[['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent', 'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in', 'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a', 'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the', 'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our', 'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look', 'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various', 'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw', 'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions', 'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion', 'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]</p>

<p>Note that I've already tried to give the data as separate sentences instead of separate words.</p>

<pre><code>def word_vec_sim_sum(row):
    description = row.product_description.split()
    description_embedding = gensim.models.Word2Vec([description], size=150,
        window=10,
        min_count=2,
        workers=10,
        iter=10)       
    print(description_embedding.wv.most_similar(positive=""not""))
</code></pre>
","python, gensim, word2vec","<p>You need to lower <code>min_count</code>.</p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">documentation</a>: <em>min_count (int, optional) â€“ Ignores all words with total frequency lower than this.</em> In the data you have provided <code>""not""</code> appears once, so it is ignored. By setting <code>min_count</code> to 1 it works. </p>

<pre><code>import gensim as gensim

data = [['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent',
         'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in',
         'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a',
         'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the',
         'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our',
         'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look',
         'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various',
         'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw',
         'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions',
         'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion',
         'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]


def word_vec_sim_sum(row):
    description = row
    description_embedding = gensim.models.Word2Vec([description], size=150,
                                                   window=10,
                                                   min_count=1,
                                                   workers=10,
                                                   iter=10)
    print(description_embedding.wv.most_similar(positive=""not""))


word_vec_sim_sum(data[0])
</code></pre>

<p>And the output:</p>

<pre><code>[('do', 0.21456070244312286), ('our', 0.1713767945766449), ('can', 0.1561305820941925), ('repair', 0.14236785471439362), ('screw', 0.1322808712720871), ('offers', 0.13223429024219513), ('project', 0.11764446645975113), ('against', 0.08542445302009583), ('various', 0.08226475119590759), ('use', 0.08193354308605194)]
</code></pre>
",4,1,1241,2019-06-14 13:33:49,https://stackoverflow.com/questions/56599306/word-not-in-vocabulary-after-training-gensim-word2vec-model-why
How to dynamically assign the right &quot;size&quot; for Word2Vec?,"<p>The question is two-fold:
1.  How to select the ideal value for <code>size</code>?
2.  How to get the vocabulary size dynamically (per row as I intend) to set that ideal size? </p>

<p>My data looks like the following (example)â€”just one row and one column:</p>

<p>Row 1</p>

<pre><code>{kfhahf}    
Lfhslnf;
.
.
. 
</code></pre>

<p>Row 2</p>

<pre><code>(stdgff  ksshu, hsihf)
asgasf;
.
.
. 
</code></pre>

<p>Etc.</p>

<p>Based on this post: <a href=""https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"">Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class</a> The <code>size</code> parameter should be less than (or equal to?) the vocabulary size. So, I am trying to dynamically assign the size as following:</p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# I do Word2Vec for each row
For item in dataset:
    Tokenized = word_tokenize(item)
    model = Word2Vec([Tokenized], min_count=1)
</code></pre>

<p>I get the vocabulary size here. So I create a second model:</p>

<pre><code>model1 = Word2Vec([Tokenized], min_count=1, size=len(model.wv.vocab))
</code></pre>

<p>This sets the <code>size</code> value to the current vocab value of the current row, as I intended. But is it the right way to do? What is the right size for a small vocabulary text?</p>
","python, python-3.x, nltk, gensim, word2vec","<p>There's no simple formula for the best <code>size</code> - it will depend on your data and purposes.</p>

<p>The best practice is to devise a robust, automatable way to score a set of word-vectors for your purposes â€“ likely with some hand-constructed representative subset of the kinds of judgments, and preferred results, you need. Then, try many values of <code>size</code> (and other parameters) until you find the value(s) that score highest for your purposes. </p>

<p>In the domain of natural language modeling, where vocabularies are at least in the tens-of-thousands of unique words but possibly in the hundreds-of-thousands or millions, typical <code>size</code> values are usually in the 100-1000 range, but very often in the 200-400 range. So you might start a search of alternate values around there, if your task/vocabulary is similar. </p>

<p>But if your data or vocabulary is small, you may need to try smaller values. (Word2Vec really needs large, diverse training data to work best, though.)</p>

<p>Regarding your code-as-shown:</p>

<ul>
<li><p>there's unlikely any point to computing a new <code>model</code> for every <code>item</code> in your dataset (discarding the previous <code>model</code> on each loop iteration). If you want a count of the unique tokens in any one tokenized item, you could use idiomatic Python like <code>len(set(word_tokenize(item)))</code>. Any <code>Word2Vec</code> model of interest would likely need to be trained on the combined corpus of tokens from <strong>all</strong> items.</p></li>
<li><p>it's usually the case that <code>min_count=1</code> makes a model worse than larger values (like the default of <code>min_count=5</code>). Words that only appear once generally can't get good word-vectors, as the algorithm needs multiple subtly-contrasting examples to work its magic. But, trying-and-failing to make useful word-vectors from such singletons tends to take up training-effort and model-state that could be more helpful for other words with adequate examples â€“ so retaining those rare words even makes <strong>other</strong> word-vectors worse. (It is most definitely <strong>not</strong> the case that ""retaining every raw word makes the model better"", though it is almost always the case that ""more real diverse data makes the model better"".)</p></li>
</ul>
",1,0,470,2019-06-14 21:32:01,https://stackoverflow.com/questions/56605373/how-to-dynamically-assign-the-right-size-for-word2vec
How to measure using word vectors,"<p>I'm attempting to understand how bias can be measured using word embeddings. Reading the article <a href=""https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17"" rel=""nofollow noreferrer"">https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17</a>  </p>

<p><a href=""https://i.sstatic.net/VudOT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VudOT.png"" alt=""enter image description here""></a></p>

<p>What is the bias being identified in the above statement ? Is the bias here that a woman cannot be seen as a doctor when a man is involved ?</p>

<p>Is a neutral bias for a either a man or woman being identified is where there is a small difference between woman,doctor man,doctor , represented a vector : $woman + doctor \approx man + doctor$ ?</p>
","machine-learning, deep-learning, word2vec","<p>You would expect that </p>

<pre><code>woman + doctor = man + doctor
</code></pre>

<p>Or rewritten:</p>

<pre><code>woman + doctor - man = doctor
</code></pre>

<p>But since it is 'nurse' in that word embedding space, that is an indicator for bias towards women in healthcare to be percieved as nurses.
Doctors are associated more with men in the corpus from which the embeddings were trained, so it can be concluded that the corpus (and the learned word embedding) has a gender bias.</p>
",0,-1,35,2019-06-15 15:43:46,https://stackoverflow.com/questions/56611841/how-to-measure-using-word-vectors
Converting string tokens into integers,"<p>I am trying to convert tokens of sentences into integers. But it is giving me floats. </p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
model = Word2Vec([tokens], min_count=1, size=1)
</code></pre>

<p>when I do,</p>

<pre><code>print(model[""hello""])
</code></pre>

<p>it gives me,</p>

<pre><code>[-0.3843384]
</code></pre>

<p>I want this to be a positive integer. </p>
","python, python-3.x, nltk, gensim, word2vec","<p>There's no essential reason to use <code>Word2Vec</code> for this. The point of <code>Word2Vec</code> is to map words to multi-dimensional, ""dense"" vectors, with many floating-point coordinates. </p>

<p>Though <code>Word2Vec</code> happens to scan your training corpus for all unique words, and give each unique word an integer position in its internal data-structures, you wouldn't usually make a model of only one-dimension (<code>size=1</code>), or ask the model for the word's integer slot (an internal implementation detail). </p>

<p>If you just need a (string word)->(int id) mapping, the gensim class <code>Dictionary</code> can do that. See:</p>

<p><a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/dictionary.html</a></p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.corpora.dictionary import Dictionary

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
my_vocab = Dictionary([tokens])

print(my_vocab.token2id['hello'])
</code></pre>

<p>Now, if there's actually some valid reason to be using <code>Word2Vec</code> â€“ such as needing the multidimensional vectors for a larger vocabulary, trained on a significant amount of varying text â€“ and your real need is to know <strong>its</strong> internal integer slots for words, you can access those via the internal <code>wv</code> property's <code>vocab</code> dictionary:</p>

<pre><code>print(model.wv.vocab['hello'].index)
</code></pre>
",3,1,3792,2019-06-18 04:53:03,https://stackoverflow.com/questions/56641954/converting-string-tokens-into-integers
How to add numbers with more than one digit to the word2vec-vocabulary,"<p>I am trying to get the embeddings for a list of 1043 nodes with word2vec. When I try to build the vocabulary I find that word2vec takes the list of lists with the nodes and treats them as single digits, eg that ""143"" becomes ""1"",""4"",""3"".</p>

<p>I already tried to have all the numbers as single entries and see wether it is an formatting problem and went with a buil_vocab_from_freq instead of build_vocab, but this also just produces errors (object of type 'int' has no len()).</p>

<p>My code is the following:</p>

<pre><code>from gensim.models import Word2Vec

def generateEmbeddings(all_walks,dimension,min_count):
    model = Word2Vec(min_count = min_count, size = dimension)
    mylist = list(range(1,1043))
    corpus = {}
    j=1
    for i in mylist:
      corpus[str(i)] = j
      j=j+1
    #mylist = [str(i) for i in mylist]
    print(corpus)
    model.build_vocab_from_freq(corpus)
    model.train(mylist, total_examples=model.corpus_count, epochs = 30)
    #if it reaches this point it throws the error ""14 not found in vocabulary""
    print(model.wv.most_similar(positive=['14']))
    return model

print(generateEmbeddings(all_walks,128,2))
</code></pre>

<p>I want to get the embedding for eg. the number ""14"" and not ""1"" as it is by now. Thanks for your help!</p>

<p>//Edit</p>

<p>I managed to fix this, if anybody else is having this specific problem:
you have to format the list as mentioned as [[""1"",""102"",""43""],[""54"",""43""]] etc.
You cant change the old list at runtime (or at least it didnt work the way I did it), so you could create a new list at runtime with</p>

<pre><code>new_list = []
    for i in all_walks:
      temp_list = []
      for j in i:
        temp_list.append(str(j))
      new_list.append(temp_list)
</code></pre>
","python, gensim, word2vec","<p>Per our discussion above, the working approach will feed <code>Word2Vec</code> the kind of corpus it expects â€“ an iterable sequence, where each item is a list of string-tokens. </p>

<p>So, a list-of-lists-of-strings would work, something like...</p>

<pre><code>[
  ['1','2','3'],
  ['1','2','4'],
  ['10','11','12'],
  ['10','14','15','900']
]
</code></pre>

<p>...rather than anything with raw ints in it (like <code>list(range(1, 1043)</code>). </p>
",0,0,162,2019-06-20 13:16:11,https://stackoverflow.com/questions/56686830/how-to-add-numbers-with-more-than-one-digit-to-the-word2vec-vocabulary
How do I use the wikipedia dump as a Gensim model?,"<p>I am trying to use the English Wikipedia dump (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) as my pre-trained word2vec model using <code>Gensim</code>.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model_path = 'enwiki-latest-pages-articles.xml.bz2'
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)
</code></pre>

<p>when I do this, I get</p>

<pre><code>   342     with utils.smart_open(fname) as fin:
    343         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 344         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    345         if limit:
    346             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;mediawiki'
</code></pre>

<p>Do I have to re-download or something?</p>
","python, gensim, word2vec","<p>That dump file includes the actual Wikipedia articles in an XML format â€“ no vectors. The <code>load_word2vec_format()</code> methods only load sets-of-vectors that were trained earlier. </p>

<p>Your <code>gensim</code> installation's <code>docs/notebooks</code> directory includes a number of demo Jupyter notebooks you can run. One of those, <code>doc2vec-wikipedia.ipynb</code>, shows training document-vectors based on the Wikipedia articles dump. (It could be adapted fairly easily to train only word-vectors instead.)</p>

<p>You can also view this notebook online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> </p>

<p>Note that you'll learn more from these if you run them locally, and enable logging at the INFO level. Also, this particular training may take a full day or more to run, and require a machine with 16GB or more or RAM.</p>
",1,0,1496,2019-06-22 12:17:33,https://stackoverflow.com/questions/56715394/how-do-i-use-the-wikipedia-dump-as-a-gensim-model
What are some common ways to get sentence vector from corresponding word vectors?,"<p>I have successfully implemented word2vec model to generate word embedding or word vectors now I require to generate sentence vectors from generated word vectors so that I can feed a neural network  to summarize a text corpus. What are the common approaches to generate sentence vectors from word vectors?</p>
","nlp, word2vec","<p>You can try adding an <code>LSTM/RNN</code> encoder before your actual Neural Network and feed your neural net using hidden states of your encoder( which would act as document representations).</p>

<p>Benefit of doing this is your document embeddings will be trained for your specific task of text summarization.</p>

<p>I don't know what framework you are using otherwise would have helped you with some code to get you started.</p>

<p>EDIT 1: Add code snippet</p>

<pre><code>word_in = Input(shape=(""&lt;MAX SEQ LEN&gt;"",))

emb_word = Embedding(input_dim=""&lt;vocab size&gt;"", output_dim=""&lt;embd_dim&gt;"",input_length=""&lt;MAX SEQ LEN&gt;"", mask_zero=True)(word_in)

lstm = LSTM(units=""&lt;size&gt;"", return_sequences=False,
                                recurrent_dropout=0.5, name=""lstm_1"")(emb_word)
</code></pre>

<p>Add any type of dense layer which takes vectors as inputs.</p>

<p><code>LSTM</code> takes input of shape <code>batch_size * sequence_length * word_vector_dimension</code> and produces output of shape <code>batch_size * rnn_size</code>; which you can use as document embeddings.</p>
",1,2,659,2019-06-24 07:54:22,https://stackoverflow.com/questions/56732102/what-are-some-common-ways-to-get-sentence-vector-from-corresponding-word-vectors
Can I train Word2vec using a Stacked Autoencoder with non-linearities?,"<p>Every time I read about Word2vec, the embedding is obtained with a very simple Autoencoder: just one hidden layer, linear activation for the initial layer, and softmax for the output layer.</p>

<p>My question is: why can't I train some Word2vec model using a stacked Autoencoder, with several hidden layers with fancier activation functions? (The softmax at the output would be kept, of course.)</p>

<p>I never found any explanation about this, therefore any hint is welcome.</p>
","neural-network, nlp, word2vec, autoencoder","<p>Word vectors are noting but hidden states of a neural network trying to get good at something.</p>

<p>To answer your question
Of course you can.</p>

<p>If you are going to do it why not use fancier networks/encoders as well like <code>BiLSTM</code> or <code>Transformers</code>.</p>

<p>This is what people who created things like <code>ElMo</code> and <code>BERT</code> did(though their networks were a lot fancier).</p>
",1,1,150,2019-06-26 18:33:28,https://stackoverflow.com/questions/56779105/can-i-train-word2vec-using-a-stacked-autoencoder-with-non-linearities
How to check the cluster details of a given vector in k-means in sklearn,"<p>I am using the following code to cluster my word vectors using k-means clustering algorithm.</p>

<pre><code>from sklearn import cluster
model = word2vec.Word2Vec.load(""word2vec_model"")
X = model[model.wv.vocab]
clusterer = cluster.KMeans (n_clusters=6)
preds = clusterer.fit_predict(X)
centers = clusterer.cluster_centers_
</code></pre>

<p>Given a word in the word2vec vocabulary (e.g., <code>word_vector = model['jeep']</code>) I want to get its cluster ID and cosine distance to its cluster center.</p>

<p>I tried the following approach.</p>

<pre><code>for i,j in enumerate(set(preds)):
    positions = X[np.where(preds == i)]
    print(positions)
</code></pre>

<p>However, it returns all the vectors in each cluster ID and not exactly what I am looking for.</p>

<p>I am happy to provide more details if needed.</p>
","python, scikit-learn, cluster-analysis, k-means, word2vec","<p>After clustering you get the <code>labels_</code> for all of your input data (in the same order as your input data), i.e. <code>clusterer.labels_[model.wv.vocab['jeep'].index]</code> would give you the cluster to which <code>jeep</code> belongs.</p>

<p>You can calculate the cosine distance with with <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer""><code>scipy.spatial.distance.cosine</code></a></p>

<pre><code>cluster_index = clusterer.labels_[model.wv.vocab['jeep'].index]
print(distance.cosine(model['jeep'], centers[cluster_index]))
&gt;&gt; 0.6935321390628815
</code></pre>

<p><strong>Full code</strong></p>

<p>I don't know how your model looks like but let's use <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer""><code>GoogleNews-vectors-negative300.bin</code></a>.</p>

<pre><code>from gensim.models import KeyedVectors
from sklearn import cluster
from scipy.spatial import distance

model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

# let's use a subset to accelerate clustering
X = model[model.wv.vocab][:40000]

clusterer = cluster.KMeans (n_clusters=6)
preds = clusterer.fit_predict(X)
centers = clusterer.cluster_centers_

cluster_index = clusterer.labels_[model.wv.vocab['jeep'].index]
print(cluster_index, distance.cosine(model['jeep'], centers[cluster_index]))
</code></pre>
",4,4,1524,2019-06-28 02:44:56,https://stackoverflow.com/questions/56800342/how-to-check-the-cluster-details-of-a-given-vector-in-k-means-in-sklearn
Gensim Word2Vec Vocabulary: Unclear output,"<p>I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:</p>

<pre class=""lang-py prettyprint-override""><code>text=['schottky','diode','semiconductors',...]
</code></pre>

<p>So then I tried to use the Word2Vec:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model=Word2Vec(text, size=100, window=5, min_count=5, workers=4)
words=list(model.wv.vocab)

</code></pre>

<p>but the output was like this:</p>

<pre class=""lang-py prettyprint-override""><code>print(words)
['c','h','t','k','d',...]
</code></pre>

<p>I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.</p>

<p>My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.</p>

<p>Does someone know what's the problem? Thanks for your help!</p>

<p>Appendix:</p>

<p>Importing the book</p>

<p>content_text=[]
number_of_inputs=len(os.listdir(path))</p>

<pre><code>    file_to_open=path
open_file=open(file_to_open,'rb')
read_pdf=PyPDF2.PdfFileReader(open_file)
number_of_pages=read_pdf.getNumPages()
page_content=""""
for page_number in range(number_of_pages):
    page = read_pdf.getPage(page_number)
    page_content += page.extractText()
content_text.append(page_content)
</code></pre>
","python, python-3.x, text-mining, gensim, word2vec","<p><code>Word2Vec</code> requires as its <code>sentences</code> parameter a training corpus that is:</p>

<ul>
<li>an iterable sequence (such as a list)</li>
<li>where each item is itself a list of string-tokens</li>
</ul>

<p>If you supply just a list-of-strings, each string is seen as a list-of-one-character-strings, resulting in all the one-letter words you're seeing. </p>

<p>So, use a list-of-lists-of-words, more like:</p>

<pre><code>[
 ['schottky','diode','semiconductors'],
]
</code></pre>

<p>(Note also that you generally won't get interesting <code>Word2Vec</code> results on tiny toy-sized data sets of just a few texts and just dozens to hundreds of words. You need many thousands of unique words, across many dozens of contrasting examples of each word, to induce the useful word-vector arrangements that <code>Word2Vec</code> is known for.)</p>
",1,0,967,2019-07-04 13:49:34,https://stackoverflow.com/questions/56889408/gensim-word2vec-vocabulary-unclear-output
"Dimensions must be equal, but are 1 and 128 for &#39;sampled_softmax_loss/MatMul&#39; (op: &#39;MatMul&#39;) with input shapes: [128,1], [64,128]","<p>Dimensions must be equal, but are 1 and 128 for 'sampled_softmax_loss/MatMul' (op: 'MatMul') with input shapes: [128,1], [64,128].</p>

<pre><code># -*- coding: utf-8 -*-
from __future__ import print_function
import collections
import math
import numpy as np
import random
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
from sklearn.manifold import TSNE
import pickle

sample = open(""/Users/henry/Desktop/Cor.txt"")
words = sample.read() 
print('Data size %d' % len(words))
sample.close()

vocabulary_size = 50000

def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary['UNK']
            unk_count = unk_count + 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
    return data, count, dictionary, reverse_dictionary

data, count, dictionary, reverse_dictionary = build_dataset(words)
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10])
del words  # Hint to reduce memory.


data_index = 0

def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0  # each word pair is a batch, so a training data [context target context] would increase batch number of 2.
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [ skip_window ]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

print('data:', [reverse_dictionary[di] for di in data[:8]])

for num_skips, skip_window in [(2, 1), (4, 2)]:
    data_index = 0
    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
    print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
    print('    batch:', [reverse_dictionary[bi] for bi in batch])
    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])


batch_size = 128
embedding_size = 128 # Dimension of the embedding vector.
skip_window = 1 # How many words to consider left and right.
num_skips = 2 # How many times to reuse an input to generate a label.
# We pick a random validation set to sample nearest neighbors. here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent. 
valid_size = 16 # Random set of words to evaluate similarity on.
valid_window = 100 # Only pick dev samples in the head of the distribution.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64 # Number of negative examples to sample.

graph = tf.Graph()

with graph.as_default():

    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Variables.
    embeddings = tf.Variable(
                        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
                        tf.truncated_normal([vocabulary_size, embedding_size],
                                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
                        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                                    train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
                                            normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
    embeddings_2 = (normalized_embeddings + softmax_weights)/2.0
    norm_ = tf.sqrt(tf.reduce_sum(tf.square(embeddings_2), 1, keep_dims=True))
    normalized_embeddings_2 = embeddings_2 / norm_


num_steps = 100001

with tf.Session(graph=graph) as session:
    if int(tf.VERSION.split('.')[1]) &gt; 11:
        tf.global_variables_initializer().run()
    else:
        tf.initialize_all_variables().run()
    print('Initialized')

    average_loss = 0
    for step in range(num_steps):
        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)
        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
        _, l = session.run([optimizer, loss], feed_dict=feed_dict)
        average_loss += l
        if step % 2000 == 0:
            if step &gt; 0:
                average_loss = average_loss / 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
            print('Average loss at step %d: %f' % (step, average_loss))
            average_loss = 0
        # note that this is expensive (~20% slowdown if computed every 500 steps)
        if step % 10000 == 0:
            sim = similarity.eval()
            for i in range(valid_size):
                valid_word = reverse_dictionary[valid_examples[i]]
                top_k = 8 # number of nearest neighbors
                nearest = (-sim[i, :]).argsort()[1:top_k+1]  # let alone itself, so begin with 1
                log = 'Nearest to %s:' % valid_word
                for k in range(top_k):
                    close_word = reverse_dictionary[nearest[k]]
                    log = '%s %s,' % (log, close_word)
                print(log)
    final_embeddings = normalized_embeddings.eval()
    final_embeddings_2 = normalized_embeddings_2.eval()  # this is better


num_points = 400

tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
two_d_embeddings_2 = tsne.fit_transform(final_embeddings_2[1:num_points+1, :])

with open('2d_embedding_skip_gram.pkl', 'wb') as f:
    pickle.dump([two_d_embeddings, two_d_embeddings_2, reverse_dictionary], f)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-11-e08f5a40ae32&gt;"", line 1, in &lt;module&gt;
    runfile('/Users/liuyang/Desktop/3333sk.py', wdir='/Users/liuyang/Desktop')

  File ""/anaconda3/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""/anaconda3/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/Users/liuyang/Desktop/3333sk.py"", line 129, in &lt;module&gt;
    train_labels, num_sampled, vocabulary_size))

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 1901, in sampled_softmax_loss

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 1429, in _compute_sampled_logits
    acc_ids_2d_int32 = array_ops.reshape(

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 2580, in matmul
    """"""Convert 'x' to IndexedSlices.

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5763, in mat_mul
    `tf.truncatediv(x, y) * y + truncate_mod(x, y) = x`.

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 800, in _apply_op_helper

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3479, in create_op

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1983, in __init__
    This property will return a dictionary for which the keys are nodes with

  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1822, in _create_c_op
    self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,

ValueError: Dimensions must be equal, but are 1 and 128 for 'sampled_softmax_loss/MatMul' (op: 'MatMul') with input shapes: [128,1], [64,128].
</code></pre>

<p>How do I interpret this error?</p>
","python, word2vec","<p>According to the error, the problem is about the dimension of the matrices which are multiplied each other. In order to solve this, the dimensions of the matrices should be <code>[1, 128], [128, 64]</code>.</p>
",1,0,924,2019-07-08 20:11:03,https://stackoverflow.com/questions/56941967/dimensions-must-be-equal-but-are-1-and-128-for-sampled-softmax-loss-matmul-o
Iterate efficiently over a list of strings to get matrix of pairwise WMD distances,"<p>I am trying to generate a matrix of pairwise distances from a list strings (newspaper articles).</p>

<p>WMD distance is not implemented in scipy.spatial.distance.pdist so I hook this implementation: <a href=""https://github.com/src-d/wmd-relax"" rel=""nofollow noreferrer"">https://github.com/src-d/wmd-relax</a> to SpaCy. However, I cannot figure out how to iterate over my list to generate the distance matrix.</p>
","python, matrix, nlp, word2vec, wmd","<p>As per doc:</p>

<pre class=""lang-py prettyprint-override""><code>
import spacy
import wmd
import numpy as np


nlp = spacy.load('en_core_web_md')
nlp.add_pipe(wmd.WMD.SpacySimilarityHook(nlp), last=True)

# given articles is a list of strings
docs = [nlp(article) for article in articles]

# matrix is just a list of lists in terms of Python objects
m = []
for doc1 in docs:
    row = []
    for doc2 in docs:
        # if distance is similarity function
        row.append(doc1.similarity(doc2))
    m.append(row)

result = np.matrix(m)
</code></pre>

<p><a href=""https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.matrix.html"" rel=""nofollow noreferrer"">Numpy matrix doc</a></p>
",0,0,351,2019-07-09 12:13:57,https://stackoverflow.com/questions/56952366/iterate-efficiently-over-a-list-of-strings-to-get-matrix-of-pairwise-wmd-distanc
How wordvector algorithm find similarity between words?,"<p>What is the intuition behind? can someone briefly explain why we extract the output of hidden layer in wordvector  network architecture?</p>
","nlp, recurrent-neural-network, word2vec","<p>The word-vectors typically used are actually from a ""projection layer"" of the neural-network. </p>

<p>That projection-layer essentially converts individual word indexes (""one-hot"" representations, single ints from 0 to V-1, where V is the count of known unique words) into <strong>input</strong> vectors (dense embeddings, with N non-zero continuous dimensions, where N is much smaller than V). </p>

<p>Those input vectors are fed to a shallow neural-network that tries to predict words' neighboring words. </p>

<p>It turns out that when you force those dense-embeddings (and internal neural-network weights) to become better and better at predicting their neighbors, word-vectors for related words get closer to each other, as a general matter of how related they are. </p>

<p>And further, the simultaneous interleaved attempt to do this for all words and training examples tends to also create meaningful ""neighborhoods"" and ""directions"" within the final arrangement â€“ allowing the interesting ""meaning arithmetic"" that lets word-vector operations often solve analogies like <code>man : king :: woman : __?__</code>. </p>
",0,0,25,2019-07-09 13:03:20,https://stackoverflow.com/questions/56953251/how-wordvector-algorithm-find-similarity-between-words
How to cluster and classify word-vectors,"<p>i am currently training a skip-gram model to learn different objects by their description. After i got my word embeddings from  that model i want to cluster these in similar groups and label them.</p>

<p>My idea was to reuse the same model with the same embedding layer and let it learn categroies from their descriptions.</p>

<p>this is my current result:</p>

<p><a href=""https://i.sstatic.net/mCf0T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mCf0T.png"" alt=""https://imgur.com/6xBQ6ng""></a></p>

<p>The the problem is that the new categories are labeled 60, 61 and 62.
The model interprets them as similar and puts them in the same space.</p>

<p>These categories shouldn't be the same and they are not near the vectors they should be.
Am i doing this wrong? How can i reuse my model to cluster and classifiy these objects?</p>

<pre class=""lang-py prettyprint-override""><code>pretrained_vectors_cat =
array([[-0.00703605, -0.00456019, -0.07583138, ..., -0.00803135,
        -0.03794867, -0.03410311],
       [-0.06226502, -0.03059928, -0.07528683, ...,  0.11714505,
         0.01752528, -0.00584977],
       [-0.07654897, -0.04235281, -0.02850686, ...,  0.06900358,
         0.00327334, -0.10425693],
       ...,
       [-0.50258852, -0.57102433, -0.28687169, ..., -0.26322143,
        -0.0910767 ,  0.13004072],
       [-0.53029969,  0.71982554, -0.80099767, ...,  0.75670917,
        -0.61081131,  0.59293241],
       [ 0.22630654, -0.69713363, -0.1661163 , ..., -0.23165715,
         0.18017072, -0.90354915]])

with graph_pretrained.as_default():

    with tf.name_scope('inputs'):
        train_inputs = tf.placeholder(tf.int32, shape=[batch_size], name=""train_inputs"")
        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1], name=""train_labels"")

    with tf.device(device_name):
        with tf.name_scope('embeddings_pretrained'):
            embeddings = tf.get_variable(""embeddings"", initializer=pretrained_vectors_cat)
            embed = tf.nn.embedding_lookup(embeddings, train_inputs)

            embeddings = tf.cast(embeddings, tf.float32)
            embed = tf.cast(embed, tf.float32)

        with tf.name_scope('weights'):
            nce_weights = tf.Variable(tf.truncated_normal(shape=[vocabulary_size_cat, embedding_size],
                                                          stddev=1.0 / math.sqrt(embedding_size)), 
                                      name=""weight_matrix"")

        with tf.name_scope('biases'):
            nce_biases = tf.Variable(tf.zeros([vocabulary_size_cat]), name=""bias_matrix"")

    with tf.name_scope('loss'): 
        loss = tf.reduce_mean(tf.nn.nce_loss(
                weights=nce_weights,
                biases=nce_biases,
                inputs=embed,
                labels=train_labels,
                num_sampled=num_sampled,
                num_classes=vocabulary_size_cat))

    loss_summary = tf.summary.scalar('loss', loss)

    with tf.name_scope('optimizer'):
        optimizer = tf.train.GradientDescentOptimizer(learningrate).minimize(loss)

    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))
    normalized_embeddings = embeddings / norm

    merged = tf.summary.merge_all()

    init = tf.global_variables_initializer()

    saver = tf.train.Saver()

with tf.Session(graph=graph_pretrained, config=session_config) as session:
    # Open a writer to write summaries.
    writer = tf.summary.FileWriter(log_dir + ""/"", session.graph)
    writer_loss = tf.summary.FileWriter(log_dir + ""/loss {}"".format(model_name))
    init.run()
    average_loss = 0

    for step in xrange(num_steps):      
        progbar.update(step)

        batch_inputs, batch_labels = generateCenterContextBatch(batch_size, window_size, new_project_mark)

        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}#, embedding_placeholder: pretrained_vectors}

        run_metadata = tf.RunMetadata()
        _, summary, loss_val = session.run([optimizer, merged, loss],
                                            feed_dict=feed_dict,
                                            run_metadata=run_metadata)

        average_loss += loss_val

        # Add returned summaries to writer in each step.
        writer_loss.add_summary(summary, step)

        # Add metadata to visualize the graph for the last run.
        if step == (num_steps - 1):
            writer_loss.add_run_metadata(run_metadata, 'step%d' % step)

    final_embeddings_category = normalized_embeddings.eval()

    # Save the model for checkpoints.
    saver.save(session, os.path.join(logdir_model"", 'model.ckpt'))

writer_loss.close()
</code></pre>
","python, cluster-analysis, word2vec","<p>Problem solved.</p>

<p>The TensorBoard Projector calculates the cosine distances wrong.
Getting the distances via sklearn results in the correct clusters.</p>

<p>TensorBoard might reduce the original dimensionality from 300 to 200 and computes the distances from the reduced dimensions. So the label ""nearest points in the original space"" is misleading.</p>

<p>--- check: installed_packages <br>
INFO: installed: tensorboard==1.13.1<br>
INFO: installed: tensorflow-gpu==1.13.1<br>
INFO: installed: tensorflow==1.14.0</p>
",1,0,639,2019-07-09 18:34:54,https://stackoverflow.com/questions/56958743/how-to-cluster-and-classify-word-vectors
KMeans clustering multidimensional features,"<p>Is it possible to train a Kmeans ML model using a multidimensional feature matrix?</p>

<p>I'm using sklearn and KmeansClass for clustering, Word2Vec for extracting the bag of words, and TreeTagger for the text pre-processing</p>

<pre><code>from gensim.models import Word2Vec
from sklearn.cluster import KMeans

lemmatized_words = [[""be"", ""information"", ""contract"", ""residential""], [""can"", ""send"", ""package"", ""recovery""]

w2v_model = Word2Vec.load(wiki_path_model)

bag_of_words = [w2v_model.wv(phrase) for phrase in lemmatized_words]

#
#
# bag_of_words = [array([[-0.08796783,  0.08373307,  0.04610106, ...,  0.41964772,
#        -0.1733183 ,  0.09438939],
#       [ 0.11526374,  0.09092105, -0.2086806 , ...,  0.5205145 ,
#        -0.11455593, -0.05190944],
#       [-0.05140354,  0.09938619,  0.07485678, ...,  0.73840886,
#        -0.17298238,  0.09994634],
#       ...,
#       [-0.01144416, -0.17129216, -0.04012141, ...,  0.05281362,
#        -0.23109615,  0.02297313],
#       [-0.08355679,  0.24799444,  0.04348441, ...,  0.27940673,
#        -0.14400786, -0.09187686],
#       [ 0.11022831,  0.11035886,  0.19900796, ...,  0.12891224,
#        -0.09379898,  0.10538024]],dtype=float32)
#       array([[ 1.73330009e-01,  1.26429915e-01, -3.47578406e-01, ...,
#         8.09064806e-02, -3.02738965e-01, -1.61911864e-02],
#       [ 2.47227158e-02, -6.48087710e-02, -1.97364464e-01, ...,
#         1.35158226e-01,  1.72204189e-02, -1.14456110e-01],
#       [ 8.07424933e-02,  2.69261692e-02, -4.22120057e-02, ...,
#         1.01349883e-01, -1.94084793e-01, -2.64464412e-04],
#       ...,
#       [ 1.36009008e-01,  1.50609210e-01, -2.59797573e-01, ...,
#         1.84113771e-01, -6.85161874e-02, -1.04138054e-01],
#       [ 4.83367145e-02,  1.17820159e-01, -2.43335906e-02, ...,
#         1.33836940e-01, -1.55749675e-02, -1.18981823e-01],
#       [-6.68482706e-02,  4.57039356e-01, -2.20365867e-01, ...,
#         2.95841128e-01, -1.55933857e-01,  7.39804050e-03]], dtype=float32)
#       ]
#
#

model = KMeans(algorithm='auto0', max_iter=300, n_clusters=2)

model.fit(bag_of_words)

</code></pre>

<p>I expect that the Kmeans is trained, so I can store the model and use for predictions, but I receive this error message:</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
","python, scikit-learn, nlp, k-means, word2vec","<p>Your problem is in <code>w2v_model.wv(phrase)</code>. Word2vec model, as it name implies, can be applied on the word level. To obtain phrase embeddings, you need to average (or aggregate in some other way) embeddings of all individial words in this phrase. </p>

<p>So you need to replace  </p>

<pre><code>bag_of_words = [w2v_model.wv(phrase) for phrase in lemmatized_words]
</code></pre>

<p>with </p>

<pre><code>import numpy as np
bag_of_words = [np.mean([w2v_model.wv(word) for word in phrase], axis=0) for phrase in lemmatized_words]
</code></pre>

<p>For me, the following code snipped worked OK. It uses <code>KeyedVectors</code> instead of deprecated <code>Word2Vec</code>, but all the rest is the same.</p>

<pre><code>from gensim.models import KeyedVectors
from sklearn.cluster import KMeans
import numpy as np
lemmatized_words = [[""be"", ""information"", ""contract"", ""residential""], [""can"", ""send"", ""package"", ""recovery""]]
w2v_model = KeyedVectors.load_word2vec_format(wiki_path_model, binary=True)  
bag_of_words = np.array([np.mean([w2v_model[word] for word in phrase if word in w2v_model], axis=0) for phrase in lemmatized_words])
print(bag_of_words.shape) # it should give (2, 300) for a 300-dimensional w2v
model = KMeans( max_iter=300, n_clusters=2)
model.fit(bag_of_words)
</code></pre>

<p>Of course, averaging (or other aggregation) discards some information about words, and this information might be meaningful for clustering. But without aggregation, you cannot get comparable phrase embeddings, because different phrases may have different lengths. </p>

<p>If your clustering of average embeddings fails, I would recommend to look for pretrained sentence embeddings (e.g. Google's Universal Sentence Encoder, or maybe embeddings from BERT). </p>
",0,0,759,2019-07-09 20:02:16,https://stackoverflow.com/questions/56959798/kmeans-clustering-multidimensional-features
"In Gensim Word2vec, how to reduce the vocab size of an existing model?","<p>In Gensims word2vec api, I trained a model where I initialized the model with max_final_vocab = 100000 and saved the model using model.save()
(This gives me one .model file, one .model.trainables.syn1neg.npy and one .model.wv.vectors.npy file).</p>

<p>I do not need to train model any further, so I'm fine with using just</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(""train.fr.model"")
kv = model.wv
del model


</code></pre>

<p>the kv variable shown here. I now want to use only the <em>top</em> N (N=40000 in my case) vocabulary items instead of the entire vocabulary. The only way to even attempt cutting down the vocabulary I could find was</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
emb_matrix = np.load(""train.fr.model.wv.vectors.npy"")
emb_matrix.shape
# (100000, 300)
new_emb_matrix = emb_matrix[:40000]
np.save(""train.fr.model.wv.vectors.npy"", new_emb_matrix)
</code></pre>

<p>If I load this model again though, the vocabulary still has length 100000.</p>

<p>I want to reduce the vocabulary of the model or model.wv while retaining a working model. Retraining is not an option.</p>
","gensim, word2vec, vocabulary","<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('train.fr.model', limit=1000)
</code></pre>

<p>Use optional <code>limit</code>parameter to reduce number of vectors that will be loaded from <strong>Word2Vec</strong> model file.</p>
",4,1,2008,2019-07-10 10:37:59,https://stackoverflow.com/questions/56968915/in-gensim-word2vec-how-to-reduce-the-vocab-size-of-an-existing-model
Correctly submitting 3 inputs to a Keras model based on Triplet Loss,"<p>I'm working on a model consisting in 2 parts, as i discussed in <a href=""https://stackoverflow.com/questions/56951787/triplet-loss-on-text-embeddings-with-keras"">this question</a>: the first should take the elements of a triplet (consisting in an anchor, a positive example and a negative example, same principle adopted in FaceNet) and turn them into vectors (word2vec + lstm), while the second should take those vectors and use them to calculate the triplet loss. I started working on some code, here's what i have now:</p>

<hr>

<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
from nltk.tokenize import WordPunctTokenizer
from collections import Counter
from string import punctuation, ascii_lowercase
import regex as re
from tqdm import tqdm
from gensim.models import Word2Vec
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Bidirectional, concatenate, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.layers.normalization import BatchNormalization
from keras.utils import plot_model

# Constants and initial settings
path = 'Datasets/DBLP-ACM/'
tf.compat.v1.set_random_seed(1)
ALPHA = 0.2
TRIPLETS_DATA_FILE = path + 'triplets/random_triplets.csv'
MAX_SEQUENCE_LENGTH = 300
tokenizer = WordPunctTokenizer()
vocab = Counter()

# Tokenize the text
def text_to_wordlist(text, lower=False):
    # Tokenize
    text = tokenizer.tokenize(text)
    # Optional: lower case
    if lower: text = [t.lower() for t in text]
    # Return a list of words
    vocab.update(text)
    return text

# Process data
def process_triplets(list_sentences, lower=False):
    triplet_elements = []
    for text in tqdm(list_sentences):
        txt = text_to_wordlist(text, lower=lower)
        triplet_elements.append(txt)
    return triplet_elements

# Define the custom loss (Triplet Loss)
def triplet_loss(x):
    anchor, positive, negative = x
    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1) 
    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)
    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)
    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)
    return loss

# Build the embedding model 
def build_embedding_model(): # How can i feed the input to the word2vec part?
    # Inputs
    wv_layer = Embedding(nb_words, WV_DIM, mask_zero=False, weights=[wv_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)
    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
    embedded_sequences = wv_layer(embedding_input)
    # BiGRU (aka bidirectional gru, bidirectional LSTM)
    embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)
    x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)
    x = Dropout(0.2)(x)
    x = BatchNormalization()(x)
    # Output
    preds = Dense(1, activation='sigmoid')(x) # Just one output class (dummy)

    # Build the model
    model = Model(inputs=[embedding_input], outputs=preds)
    model.compile(loss='mse', optimizer = ""adam"")

    return model

# Build the entire model
def build_model():
    # Inputs
    anchor_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='anchor_input')
    positive_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='positive_input')
    negative_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='negative_input')

    embedding_model = build_embedding_model()

    # Outputs
    anchor_embedding = embedding_model(anchor_input)
    positive_embedding = embedding_model(positive_input)
    negative_embedding = embedding_model(negative_input)

    merged_output = concatenate([anchor_embedding, positive_embedding, negative_embedding])
    loss = Lambda(triplet_loss, (1,))(merged_output)
    triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=loss)
    triplet_model.compile(loss = 'mean_absolute_error', optimizer = Adam())

    return triplet_model

triplets = pd.read_csv(TRIPLETS_DATA_FILE, error_bad_lines=False, sep=""|"", quotechar=""\"""", encoding=""latin_1"")
list_sentences_anchor = list((triplets[""anchor""].astype(str)).fillna("""").values)
list_sentences_positive = list((triplets[""positive""].astype(str)).fillna("""").values)
list_sentences_negative = list((triplets[""negative""].astype(str)).fillna("""").values)

# Fill an array for anchors, one for positives and one for negatives
anchors = process_triplets(list_sentences_anchor, lower=True)
positives = process_triplets(list_sentences_positive, lower=True)
negatives = process_triplets(list_sentences_negative, lower=True)

model_anchor = Word2Vec(anchors, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)
model_positive = Word2Vec(positives, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)
model_negative = Word2Vec(negatives, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)

word_vectors_anchor = model_anchor.wv
word_vectors_positive = model_positive.wv
word_vectors_negative = model_negative.wv

# Use the embeddings in Keras
MAX_NB_WORDS = max(len(word_vectors_anchor.vocab), len(word_vectors_positive.vocab), len(word_vectors_negative.vocab))

word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}
sequences = [[word_index.get(t, 0) for t in anchor] for anchor in anchors[:len(anchors)]]
# Pad
anchor_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=""pre"", truncating=""post"")
# Create the embedding matrix
WV_DIM = 200
nb_words = min(MAX_NB_WORDS, len(word_vectors_anchor.vocab))
# Initialize the matrix with random numbers
wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0
for word, i in word_index.items():
    if i &gt;= MAX_NB_WORDS: continue
    try: # Words not found in embedding index will be all-zeros
        embedding_vector = word_vectors_anchor[word]
        wv_matrix[i] = embedding_vector
    except: pass  

# Build and fit the model
triplet_model = build_model()
hist = triplet_model.fit([anchor_data, anchor_data, anchor_data], 0, validation_split=0.1, epochs=50, batch_size=256, shuffle=True)
</code></pre>

<hr>

<p>As you will surely see, there's a lot of confusion. Basically, i split the triplets in 3 different pieces, i apply word2vec on each piece and i use the result in the embedding model (i used the same result 3 times just to test if it works, and it doesn't). </p>

<p>The embedding model should compute a vector to be used in the second model, during the fit process, and in the triplet loss. I'm new to Keras and i'm surely doing something wrong here, since i get this error at the moment:</p>

<pre><code>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
</code></pre>

<hr>

<p>This happens in the first line of the triplet loss function itself, and it's probably related to the input format. So the question is: given this code, how can i modify it to correctly accept 3 inputs, producing 3 vectors, and use those vectors in the triplet_model, during the fit?</p>

<p>I'll update the question if i modify the code or i get different errors. </p>
","python, keras, lstm, word2vec, triplet","<p>The loss should not be a Lambda layer. Remove the Lambda layer and update your code such that:</p>

<pre><code>    triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)
    triplet_model.compile(loss = triplet_loss, optimizer = Adam())

</code></pre>

<p>triplet_loss needs to be defined as:</p>

<pre><code>def triplet_loss(y_true, y_pred):
    anchor_vec = y_pred[:, :VECTOR_SIZE]
    positive_vec = y_pred[:, VECTOR_SIZE:2*VECTOR_SIZE]
    negative_vec = y_pred[:, 2*VECTOR_SIZE:]

   ... code ...
</code></pre>

<p>You should then ignore y_true.</p>

<p>Full example of a triple loss function that works:
<a href=""https://colab.research.google.com/drive/1VgOTzr_VZNHkXh2z9IiTAcEgg5qr19y0"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1VgOTzr_VZNHkXh2z9IiTAcEgg5qr19y0</a></p>
",1,0,1235,2019-07-10 15:53:10,https://stackoverflow.com/questions/56974450/correctly-submitting-3-inputs-to-a-keras-model-based-on-triplet-loss
How to check the performance of word embedding,"<p>I have used the gensim Word2Vec model and applied it in my list of documents. Well , the word embedding is getting created. I want to know if Word2Vec is performing well on my list of documents. Is there any metrics to measure that? How will I understand if Word2Vec has really worked well on my document corpus or should I try some different embedding?
Below is the code I have used from gensim.</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(documents , size=150, window=10, min_count=2, sg=1, workers=10)
</code></pre>
","python, word2vec, word-embedding","<p>There's no universal definition of ""performing well"". It depends on your end-goals. </p>

<p>Why do you want to create word-vectors? What value do you expect them to provide? </p>

<p>With the answer to those questions, you can 1st review the results in an informal, ad-hoc fashion: look at some of the words nearest-neighbors (the results of <code>wordvecs.most_similar(query_word)</code>) to see if they make sense to you, for your needs and problem-domain. </p>

<p>But to really test whether your models are doing better over time, as you improve your data or model-parameters, you should form some repeatable, quantitative tests that match your end-goal. (For example: do you need certain pairs of words to be closer to each other than to some third word? Do you use the word-vectors as input to some other classification or info-retrieval process that has some known,desirable results?)</p>

<p>Run those tests, to score the model, then compare one model's score against another. </p>
",0,0,1338,2019-07-11 08:25:54,https://stackoverflow.com/questions/56984758/how-to-check-the-performance-of-word-embedding
How to get document vectors for a given topic in gensim,"<p>I have about 9000 documents and I am using Gensim's <code>doc2vec</code> to embed my documents. My code is as follows:</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I would like to get all the documents related to topic ""deep learning"". i.e. the documents that mainly have content related to deep learning. Is it possible to do this in doc2vec model in gensim?</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec, doc2vec","<p>If there was a document in your training set that was a great example of ""deep learning"" â€“ say, <code>docs[17]</code> â€“ then after successful training you could ask for documents similar to that example document, and that could be roughly what you'd need. For example:</p>

<pre><code>sims = model.docvecs.most_similar(docs[17].tags[0])
</code></pre>

<p>You'd then have in <code>sims</code> a ranked, scored list of the 10 most-similar documents to the <code>tag</code> for the target document. </p>
",1,0,607,2019-07-20 13:17:11,https://stackoverflow.com/questions/57125117/how-to-get-document-vectors-for-a-given-topic-in-gensim
"ValueError: cannot reshape array of size 3800 into shape (1,200)","<p>I am trying to apply word embedding on tweets. I was trying to create a vector for each tweet by taking the average of the vectors of the words present in the tweet as follow:</p>

<pre><code>def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model_w2v[word].reshape((1, size))
            count += 1.
        except KeyError: # handling the case where the token is not in vocabulary

            continue
    if count != 0:
        vec /= count
    return vec
</code></pre>

<p>Next, when I try to Prepare word2vec feature set as follow:</p>

<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 200))
#the length of the vector is 200

for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)

wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape
</code></pre>

<p>I get the following error inside the loop:</p>

<blockquote>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-72aee891e885&gt; in &lt;module&gt;
      4 # wordvec_arrays.reshape(1,200)
      5 for i in range(len(tokenized_tweet)):
----&gt; 6     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)
      7 
      8 wordvec_df = pd.DataFrame(wordvec_arrays)

&lt;ipython-input-31-9e6501810162&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v.wv.__getitem__(word).reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

ValueError: cannot reshape array of size 3800 into shape (1,200)
</code></pre>
</blockquote>

<p>I checked all the available posts in stackOverflow but non of them really helped me.</p>

<p>I tried reshaping the array and it still give me the same error. </p>

<p>My model is:</p>

<pre><code>tokenized_tweet = df['tweet'].apply(lambda x: x.split()) # tokenizing

model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(df['tweet']), epochs=20)
</code></pre>

<p>any suggestions please? </p>
","python, deep-learning, tokenize, word2vec, word-embedding","<p>It looks like the intent of your <code>word_vector()</code> method is to take a list of words, and then with respect to a given <code>Word2Vec</code> model, return the average of all those words' vectors (when present). </p>

<p>To do that, you shouldn't need to do any explicit re-shaping of vectors â€“ or even specification of <code>size</code>, because that's forced by what the model already provides. You could use utility methods from <code>numpy</code> to simplify the code a lot. For example, the <code>gensim</code> <code>n_similarity()</code> method, as part of its comparision of <strong>two</strong> lists-of-words, already does an averaging much like what you're trying, and you can look at its source as a model:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/models/keyedvectors.py#L996"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/models/keyedvectors.py#L996</a></p>

<p>So, while I haven't tested this code, I think your <code>word_vector()</code> method could be essentially replaced with:</p>

<pre><code>import numpy as np

def average_words_vectors(tokens, wv_model):
    vectors = [wv_model[word] for word in tokens 
               if word in wv_model]  # avoiding KeyError
    return np.array(vectors).mean(axis=0)
</code></pre>

<p>(It's sometimes the case that it makes sense to work with vectors that have been normalized to unit-length - as the linked <code>gensim</code> code via applying <code>gensim.matutils.unitvec()</code> to the average. I haven't done this here, as your method hadn't taken that step â€“ but it is something to consider.)</p>

<p>Separate observations about your <code>Word2Vec</code> training code:</p>

<ul>
<li><p>typically words with just 1, 2, or a few occurrences <strong>don't</strong> get good vectors (due to limited number &amp; variety of examples), but <strong>do interfere</strong> with the improvement of other more-common-word vectors. That's why the default is <code>min_count=5</code>. So just be aware: your surviving vectors may get better if you use a default (or even larger) value here, discarding more of the rarer words.</p></li>
<li><p>the dimensions of a ""dense embedding"" like word2vec-vectors aren't really ""independent variables"" (or standalone individually-interpretable ""features"") as implied by your code-comment, even though they may seem that way as separate values/slots in the data. For example, you can't pick one dimension out and conclude, ""that's the foo-ness of this sample"" (like 'coldness' or 'hardness' or 'positiveness' etc). Rather, any of those human-describable meanings tend to be other directions in the combined-space, not perfectly aligned with any of the individual dimensions. You can sort-of tease those out by comparing vectors, and downstream ML algorithms can make use of those complicated/entangled multi-dimensional interactions. But if you think of each dimensions as its own ""feature"" â€“ in any way other than yes, it's technically a single number associated with the item â€“ you may be prone to misinterpreting the vector-space.</p></li>
</ul>
",2,1,3503,2019-07-20 13:42:09,https://stackoverflow.com/questions/57125306/valueerror-cannot-reshape-array-of-size-3800-into-shape-1-200
Training word2vec model streaming data from file and tokenize to sentence,"<p>I need to process a large number of <code>txt</code> files for building a <code>word2vec</code> model.
Now, my txt-files are a bit messy and I need to remove all Â´<code>\n</code>Â´ newlines, read all sentences from my loaded string (txt-file) and then tokenize each sentence for using the word2vec model.</p>

<p>The thing is: I cant read the files line-by-line, cause some sentences do not end after one line. Therefore, I use Â´<code>nltk.tokenizer.tokenize()</code>Â´, which splits the file into sentences.</p>

<blockquote>
  <p>I cant figure out, how to convert a list of strings into a list of list, where each sub-list contains the sentences, while passing it thourgh a generator.</p>
</blockquote>

<p>Or do I actually need to save each sentences into a new file (one sentence per line) to pass it through a generator?</p>

<p>Well, my code looks like this:
Â´<code>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')</code></p>

<pre><code># initialize tokenizer for processing sentences

class Raw_Sentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for file in file_loads: ## Note: file_loads includes directory name of files (e.g. 'C:/Users/text-file1.txt')
            with open(file,'r', encoding='utf-8') as t:     
               # print(tokenizer.tokenize(t.read().replace('\n', ' ')))           
                storage = tokenizer.tokenize(t.read().replace('\n', ' '))
# I tried to temporary store the list of sentences to a list for an iteration
                for sentence in storage:
                    print(nltk.word_tokenize(sentence))
                    yield nltk.word_tokenize(sentence)Â´
</code></pre>

<p>So the goal is: 
load file 1: Â´<code>'some messy text here. And another sentence'</code>Â´
 tokenize into sentences Â´<code>['some messy text here','And another sentence']</code>Â´
and then split each sentence into words Â´<code>[['some','messy','text','here'],['And','another','sentence']]</code>Â´</p>

<p>load file 2: <code>'some other messy text. sentence1. sentence2.'</code>
etc.</p>

<p>and input sentences into word2vec model:
Â´<code>sentences = Raw_Sentences(directory)</code>Â´</p>

<p>Â´<code>model = gensim.models.Word2Vec(sentences)</code>Â´</p>
","python, streaming, nltk, gensim, word2vec","<p>Well... after writing it all down and reconsideration... I assume I solved my own question. <strong>Please correct me if Iam wrong:</strong></p>

<p>To iterate over every sentence created by the nltk punkt sentence tokenizer, one has to pass it directly to the for loop:</p>

<pre><code>def __iter__(self):
    for file in file_loads:
       with open(file,'r') as t:
           for sentence in tokenizer.tokenize(t.read().replace('\n',' ')):
                yield nltk.word_tokenize(sentence) 
</code></pre>

<p>as always, theres also the alternative to  <code>yield gensim.utils.simple_preprocess(sentence, deacc= True)</code> </p>

<p>Feeding that into <code>sentence = Raw_Sentences(directory)</code>  builds a proper working Word2Vec <code>gensim.models.Word2Vec(sentences)</code></p>
",0,0,1398,2019-07-20 14:44:18,https://stackoverflow.com/questions/57125757/training-word2vec-model-streaming-data-from-file-and-tokenize-to-sentence
How to get the nearest documents for a word in gensim in python,"<p>I am using the doc2vec model as follows to construct my document vectors.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I have seen that <strong>gensim doc2vec also includes word vectors</strong>. Suppose I have a word vector created for the word <code>deep learning</code>. My question is; is it possible to get the <code>documents</code> nearest to <code>deep learning</code> word vector in gensim in python?</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec, doc2vec","<p>Some <code>Doc2Vec</code> modes will co-train doc-vectors and word-vectors in the ""same space"". Then, if you have a word-vector for <code>'deep_learning'</code>, you can ask for documents near that vector, and the results may be useful for you. For example:</p>

<pre><code>similar_docs = d2v_model.docvecs.most_similar(
                   positive=[d2v_model.wv['deep_learning']]
               )
</code></pre>

<p>But:</p>

<ul>
<li><p>that's only going to be as good as your model learned <code>'deep_learning'</code> as a word to mean what you think of it as</p></li>
<li><p>a training set of known-good documents fitting the category <code>'deep_learning'</code> (and other categories) could be better - whether you hand-curate those, or try to bootstrap from other sources (like say the Wikipedia category '<a href=""https://en.wikipedia.org/wiki/Category:Deep_learning"" rel=""nofollow noreferrer"">Deep Learning</a>' or other curated/search-result sets that you trust).</p></li>
<li><p>reducing a category to a single summary point (one vector) may not be as good as having a variety of examples â€“ many points - that all fit the category. (Relevant docs may not be a neat sphere around a summary point, but rather populate exotically-shaped regions of the doc-vector high-dimensional space.) If you have a lot of good examples of each category, you could train a classifier to then label, or rank-in-relation-to-trained-categories, any further uncategorized docs.</p></li>
</ul>
",2,1,424,2019-07-22 02:17:04,https://stackoverflow.com/questions/57138453/how-to-get-the-nearest-documents-for-a-word-in-gensim-in-python
Interepretation of word2vec evaluation result,"<p>I have created word embeddings (Word2vec) using my own dataset. I have used Gensim module to create word embeddings. I want to evaluate my word embeddings.</p>

<p>I have used Wordsim353 dataset to evaluate word embeddings. The following code Shows the result of Evaluation. </p>

<p>Code:</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

print(similarities)
</code></pre>

<p>Result:</p>

<pre><code>((0.09410256722489568, 0.3086953732794174), SpearmanrResult(correlation=0.06101508426787973, pvalue=0.5097769955392246), 66.28895184135978)
</code></pre>

<p>How can I interprete the result?</p>

<p>Please help me to interprete the results.</p>
","word2vec, evaluation, word-embedding","<p>The way we evaluate the quality of word embeddings is to see how closely the similarities computed by embeddings match the actual similarities assigned by human judgements.</p>

<p>Your Pearson and Spearmanr's pValue are too high with approximately 0.3 (70%) and 0.5 (50%). I suggest you should use pretrained word embeddings or collect more dataset.</p>

<p>I have strived to evaluate with glove-twitter-25 and received very great pvalue. </p>

<pre><code>import gensim.downloader as api
from gensim.test.utils import datapath

m = api.load(""glove-twitter-25"")
m.evaluate_word_pairs(datapath(""wordsim353.tsv""))
</code></pre>

<p>output:</p>

<pre><code>((0.36409317297819943, pvalue=2.969053896450154e-12), SpearmanrResult(correlation=0.36452011505868487, pvalue=2.788781738485533e-12), 2.26628895184136)
</code></pre>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">evaluate_word_pairs - Gensim module</a></p>
",1,3,982,2019-07-25 09:11:09,https://stackoverflow.com/questions/57198286/interepretation-of-word2vec-evaluation-result
How to turn a list of words into a list of vectors using a pre-trained word2vec model(Google)?,"<p>I am trying to learn word2vec.</p>

<p>I am using the code below to load the Google pre-trained word2vec model in Python 3. But I am unsure how to turn a list such as :[""I"", ""ate"", ""apple""] to a list of vectors (ie how to get vectors from this model?).</p>

<pre><code>import nltk
import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
","python-3.x, gensim, word2vec","<p>You get the vector via idiomatic Python keyed-index-access (brackets). For example:</p>

<pre><code>    wv_apple = model['apple']
</code></pre>

<p>You can create a new list based on some operation on every item of an existing list via an idiomatic Python 'list comprehension' (<code>[expression(x) for x in some_list]</code>), For example:</p>

<pre><code>    words = [""I"", ""ate"", ""apple""]
    vectors = [model[word] for word in words]
</code></pre>
",1,0,622,2019-07-28 20:25:02,https://stackoverflow.com/questions/57244699/how-to-turn-a-list-of-words-into-a-list-of-vectors-using-a-pre-trained-word2vec
How to combine 3D token embeddings into 2D vectors?,"<p>I have a set of strings that I am tokenizing. I am sending each string into the <code>word2vec</code> model in <code>gensim</code>. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (<code>list</code> of <code>list</code> in <code>python</code>). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  </p>

<p>I am sending this 3D into a model in <code>Tensorflow</code> library. I am doing the following,</p>

<p><code>model.add(Embedding(max_features, 128, input_length=maxlen))</code></p>

<p>Here max_features is the size of the token vector i.e. 100 and input_length is also the same. </p>

<p>But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model.</p>
","python, tokenize, gensim, word2vec, word-embedding","<p>The final results of training aren't really ""3D"" in usual <code>Word2Vec</code>/<code>gensim</code> terminology. If you've used <code>Word2Vec</code> with its default <code>vector_size=100</code>, and you had 100 vocabulary words, then you'd have 100 vectors of 100-dimensions each. </p>

<p>(Note: you would never want to create such high-dimensional ""dense embedding"" vectors for such a tiny vocabulary. The essential benefits of such dense representations come from forcing a much-larger set of entities into many-fewer dimensions, so that they are ""compressed"" into subtle, continuous, meaningful relative positions against each other. Giving 100 words a full 100 continuous dimensions, before <code>Word2Vec</code> training, will leave the model prone to severe overfitting. It could in fact then trend towards a ""one-hot""-like encoding of each word, and become very good at the training task without really learning to pack related words near each other in a shared space â€“ which is the usually-desired result of training. In my experience, for 100-dimension vectors, you probably want at least a 100^2 count of vocabulary words. If you really just care about 100 words, then you'd want to use much-smaller vectors â€“ but also remember <code>Word2Vec</code> &amp; related techniques are really meant for ""large data"" problems, with many subtly-varied training examples, and just barely sometimes give meaningful results on toy-sized data.)</p>

<p>The 100 vectors of 100-dimensions each are internally stored inside the <code>Word2Vec</code> model (&amp; related components) as a raw <code>numpy</code> <code>ndarray</code>, which could be thought of as a ""2d array"" or ""2d matrix"". (It's not really a <code>list</code> of <code>list</code> unless you convert it to be that less-optimal form â€“ though of course with Pythonic polymorphism you can generally pretend it was a <code>list</code> of <code>list</code>). If your <code>gensim</code> <code>Word2Vec</code> model is in <code>w2v_model</code>, then the raw <code>numpy</code> array of learned vectors is inside the <code>w2v_model.wv.vectors</code> property, though the interpretation of which row corresponds to which word-token depends on the <code>w2v_model.wv.vocab</code> dictionary entries.</p>

<p>As far as I can tell, the Tensorflow <code>Embedding</code> class is for training your own embeddings inside TF (though perhaps it can be initialized with vectors trained elsewhere). Its 1st initialization argument should the size-of-the-vocabulary (per your conjectured case 100), its second is the size-of-the-desired-embeddings (per your conjectured case, also 100 - but as noted above, this match of vocab-size and dense-embedding-size is inappropriate, and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""nofollow noreferrer"">the example values in the TF docs of 1000 words and 64 dimensions</a> would be more appropriately balanced). </p>
",0,0,696,2019-07-30 03:56:34,https://stackoverflow.com/questions/57264086/how-to-combine-3d-token-embeddings-into-2d-vectors
How to assign dynamic variables in a for loop in python,"<p>I have five models for five years as follows.</p>

<pre><code>word2vec_files = [""word2vec_2011"", ""word2vec_2012"", ""word2vec_2013"", ""word2vec_2014"", ""word2vec_2015""]
years = [2011, 2012, 2013, 2014, 2015]
</code></pre>

<p>I want to open each model as <code>model_2011</code>, <code>model_2012</code>, <code>model_2013</code>, <code>model_2014</code>, <code>model_2015</code>.</p>

<p>Currently I am doing it one by one as follows.</p>

<pre><code>model_2011 = word2vec.Word2Vec.load(""word2vec_2011"")
model_2012 = word2vec.Word2Vec.load(""word2vec_2012"")
model_2013 = word2vec.Word2Vec.load(""word2vec_2013"")
model_2014 = word2vec.Word2Vec.load(""word2vec_2014"")
model_2015 = word2vec.Word2Vec.load(""word2vec_2015"")
</code></pre>

<p>Now I want to imitate the same process using a for loop. </p>

<pre><code>for i, word2vec_file in enumerate(word2vec_files):
    ""model_""+str(years[i]) = word2vec.Word2Vec.load(word2vec_file)
</code></pre>

<p>However, I get the following error <code>SyntaxError: can't assign to operator</code>. I am wondering how to assign dynamic variables in a for loop in python.</p>

<p>I am happy to provide more details if needed.</p>
","python, word2vec","<p>Unfortunately you can't create variable from strings like that; but you can use a dictionary, and add keys/values to it:</p>

<pre class=""lang-py prettyprint-override""><code>years = [2011, 2012, 2013, 2014, 2015]

models = {}

for year in years:
  models[year] = word2vec.Word2Vec.load(""word2vec_%s"" % year)

print(models)
</code></pre>

<p>That way, you can access the year on <code>models</code> to get what you need.</p>

<p>You could do the same thing with a dictionary comprehension:</p>

<pre class=""lang-py prettyprint-override""><code>years = [2011, 2012, 2013, 2014, 2015]

models = {
  year: ""word2vec_%s"" % year
  for year in years
}

print(models)
</code></pre>
",1,2,593,2019-07-30 08:15:35,https://stackoverflow.com/questions/57267171/how-to-assign-dynamic-variables-in-a-for-loop-in-python
gensim word2vec entry greater than 1,"<p>I'm new to NLP and gensim, currently trying to solve some NLP problems with gensim word2vec module. I my current understanding of word2vec, the result vectors/matrix should have all entries between -1 and 1. However, trying a simple one results into a vector which has entries greater than 1. I'm not sure which part is wrong, could anyone give some suggestions, please?</p>

<p>I've used gensim utils.simple_preprocess to generate a list of list of token. The list looks like: </p>

<pre><code>[['buffer', 'overflow', 'in', 'client', 'mysql', 'cc', 'in', 'oracle', 'mysql', 'and', 'mariadb', 'before', 'allows', 'remote', 'database', 'servers', 'to', 'cause', 'denial', 'of', 'service', 'crash', 'and', 'possibly', 'execute', 'arbitrary', 'code', 'via', 'long', 'server', 'version', 'string'], ['the', 'xslt', 'component', 'in', 'apache', 'camel', 'before', 'and', 'before', 'allows', 'remote', 'attackers', 'to', 'read', 'arbitrary', 'files', 'and', 'possibly', 'have', 'other', 'unspecified', 'impact', 'via', 'an', 'xml', 'document', 'containing', 'an', 'external', 'entity', 'declaration', 'in', 'conjunction', 'with', 'an', 'entity', 'reference', 'related', 'to', 'an', 'xml', 'external', 'entity', 'xxe', 'issue']]
</code></pre>

<p>I believe this is the correct input format for gensim word2vec.</p>

<pre><code>word2vec = models.word2vec.Word2Vec(sentences, size=50, window=5, min_count=1, workers=3, sg=1)
vector = word2vec['overflow']
print(vector)
</code></pre>

<p>I expect the output to be a vector containing probabilities (i.e., all between -1 and 1), but it actually turned out to be the following:</p>

<pre><code>[ 0.12800379 -0.7405527  -0.85575     0.25480416 -0.2535793   0.142656
 -0.6361196  -0.13117172  1.1251501   0.5350017   0.05962601 -0.58876884
  0.02858278  0.46106443 -0.22623934  1.6473309   0.5096218  -0.06609935
 -0.70007527  1.0663376  -0.5668168   0.96070313 -1.180383   -0.58649933
 -0.09380565 -0.22683378  0.71361005  0.01779896  0.19778453  0.74370056
 -0.62354785  0.11807996 -0.54997736  0.10106519  0.23364201 -0.11299669
 -0.28960565 -0.54400533  0.10737313  0.3354464  -0.5992898   0.57183135
 -0.67273194  0.6867607   0.2173506   0.15364875  0.7696457  -0.24330224
  0.46414775  0.98163396]
</code></pre>

<p>You can see there are <code>1.6473309</code> and <code>-1.180383</code> in the above vector.</p>
","python, nlp, gensim, word2vec","<p>It's <strong>not</strong> the case that individual word-vectors will have all their individual dimensions between <code>-1.0</code> and <code>1.0</code>. </p>

<p>Nor is it the case that the dimensions should be interpreted as ""probabilities"". </p>

<p>Rather, the word-vectors are learned such that the internal neural-network becomes as good as possible at predicting words from surrounding words. There's no constraint or normalization during that training forcing the individual dimensions into a restricted range, or making individual dimensions interpretable as nameable qualities. </p>

<p>It is sometimes the case that such vectors are converted, after training, into vectors of normalized unit-length, before comparison to each other. And further, when you request the cosine-similarity between two vectors, the result will always be in the range from <code>-1.0</code> to <code>1.0</code>. And, before doing the very-common <code>most_similar()</code> operation (or similar), the <code>Word2Vec</code> class with bulk-unit-normalize vectors &amp; cache the results internally. </p>

<p>But, directly asking for the raw word-vector, as per <code>model.wv['overflow']</code>, will return the raw vector with whatever original overall magnitude, and per-dimension values, as came from training. You can request the unit-normed vector instead with:</p>

<pre><code>model.wv.word_vec('overflow', use_norm=True)
</code></pre>

<p>(Separately be aware: testing <code>Word2Vec</code> on tiny toy-sized datasets will generally not get useful or realistic results: the algorithm really requires large, varied data to come up with balanced, useful word-vectors. For example, to train-up 50-dimensional vectors, I'd want at least 2,500 unique words in the vocabulary, with dozens of different uses of each word â€“ so a corpus of many tens of thousands of words. And I might also use more than the default <code>epochs=5</code>, because that's still a very small corpus.)</p>
",0,0,970,2019-07-31 18:49:40,https://stackoverflow.com/questions/57297194/gensim-word2vec-entry-greater-than-1
PySpark: create a vector from values in a group,"<p>I currently have a dataset of transaction histories of users in the following format:</p>

<pre><code>+---------+------------+------------+
| user_id | order_date | product_id |
+---------+------------+------------+
|       1 |   20190101 |        123 |
|       1 |   20190102 |        331 |
|       1 |   20190301 |       1029 |
+---------+------------+------------+
</code></pre>

<p>I'm trying to transform the dataset to be used for an Item2Vec model -- which I believe has to look like this:</p>

<pre><code>+---------+-------------------+
| user_id |      seq_vec      |
+---------+-------------------+
|    1    |  [123, 331, 1029] |
-------------------------------
</code></pre>

<p>I'm assuming the dataset has to be formatted this way from looking at examples of Word2Vec (<a href=""https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec</a>).</p>

<p>Is there a built-in PySpark method of creating a vector from the values in <code>product_id</code> column if I'm grouping by <code>user_id</code>? </p>
","vector, pyspark, word2vec","<p><code>collect_list</code> does the trick</p>

<pre class=""lang-py prettyprint-override""><code>import pyspark.sql.functions as F

rawData = [(1, 20190101, 123),
           (1, 20190102, 331),
           (1, 20190301, 1029)]

df = spark.createDataFrame(rawData).toDF(""user_id"", ""order_date"", ""product_id"")

df.groupBy(""user_id"").agg(F.collect_list(""product_id"").alias(""vec"")).show()

+-------+----------------+
|user_id|             vec|
+-------+----------------+
|      1|[123, 331, 1029]|
+-------+----------------+

</code></pre>
",1,0,937,2019-08-02 03:15:12,https://stackoverflow.com/questions/57319450/pyspark-create-a-vector-from-values-in-a-group
How to find where a positive word is in a set of documents after using Word2Vec?,"<p>I am testing with Word2Vec to find words that have the same meaning, so far it is going great as the list of positive words is accurate. However, I would like to know where each positive word was found, as in which document.</p>

<p>I tried to iterate each document and compare each word with the list of positive words, something like this:</p>

<pre><code>for i in documents: # iterating the documents
    for j in i: # iterating the words in the document
        for k in similar_words: # iterating the positive words
            if k[0] in j: # k[0] is the positive word, k[1] is the positive value
                print('found word')
</code></pre>

<p>This works fine. However, with this, the positive words are actually stemmed down, that is why I am using ""in"". So let's say the stemmed down positive word is 'ice', many words contain the phrase 'ice' in them, and maybe more than one are in the document and only one of them is the real positive word.</p>

<p>Is there a way to avoid stemming words when using Word2Vec? Or is there a way to find the document number of each positive word found?</p>

<p><strong>UPDATE</strong></p>

<p>Here is my code for training the model and using 'most_similar()'</p>

<pre><code>def remove_stopwords(texts):
    # Removes stopwords in a text
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]


def sent_to_words(sentences):
    # Tokenize each sentence into a list of words and remove unwanted characters
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))


df = pd.read_excel('my_file.xlsx')
df.columns = map(str.lower, df.columns)

data = df['Comment Section'].values.tolist()

# Remove the new line character and single quotes
data = [re.sub(r'\s+', ' ', str(sent)) for sent in data]
data = [re.sub(""\'"", """", str(sent)) for sent in data]

# Convert our data to a list of words. Now, data_words is a 2D array,
# each index contains a list of words
data_words = list(sent_to_words(data))

# Remove the stop words
data_words_nostops = remove_stopwords(data_words)

model = gensim.models.Word2Vec(
            data_words_nostops,
            alpha=0.1,
            min_alpha=0.001,
            size=250,
            window=1,
            min_count=2,
            workers=10)

model.train(data_words_nostops, total_examples=len(data_words_nostops), epochs=10)

print(model.wv.vocab) # At this step, the words are not stemmed

positive = ['injuries', 'fail', 'dangerous', 'oil']

negative = ['train', 'westward', 'goods', 'calgary', 'car', 'automobile', 'appliance']

similar_words_size = array_length(model.wv.most_similar(positive=positive, negative=negative, topn=0))

for i in model.wv.most_similar(positive=positive, negative=negative, topn=similar_words_size):
    if len(i[0]) &gt; 2:
        risks.append(i)
        print(risks) # At this step, the words are stemmed
</code></pre>
","python, machine-learning, word2vec","<p>A lot of published <code>Word2Vec</code> work, including the original papers from Google, doesn't bother with word-stemming. If you have a large enough corpus, with many varied examples of each form of a word, then each form will get a pretty-good vector (&amp; closely-positioned with other forms) even as the raw unstemmed words. (On the other hand, in smaller corpuses, stemming is more likely to help, by allowing all the different forms of a word to contribute their occurrences to a single good vector.)</p>

<p>During training, <code>Word2Vec</code> just watches the training texts go by for the nearby-words information it needs: it doesn't remember the contents of individual documents. If you need that info, you need to retain it outside <code>Word2Vec</code>, in your own code. </p>

<p>You could iterate over all documents to find occurrences individual words, as in your code. (And, as @alexey's answer notes, you should compare stemmed-words to stemmed-words, rather than just checking for substring-containment.) </p>

<p>The other option, used in full-text search, is to build a ""reverse index"" that remembers in which documents (and potentially where in each document) each word appears. Then, you essentially have a dictionary in which you look up ""iced"", and get back a list of documents like ""doc1, doc17, doc42"". (Or potentially, a list of docs-plus-positions, like ""doc2:pos11,pos91; doc17:pos22, doc42:pos77"".) That requires more work up-front, and storing the reverse-index (which depending on the level of detail retained, can be nearly as large as the original texts), but then finds docs-containing-words much faster than a full-iteration-search for each word.</p>
",1,0,301,2019-08-02 15:55:33,https://stackoverflow.com/questions/57329913/how-to-find-where-a-positive-word-is-in-a-set-of-documents-after-using-word2vec
Word shows up more than once in TSNE plot,"<p>When plotting word embedding TSNE results, words show up more than once.</p>

<p>I am reducing dimensionality of a Word2Vec word embedding, but when I plot the results for a subset of the most similar words (manually enter several words for which I want the most similar ones), the same words show up more than once:</p>

<pre><code>from sklearn.manifold import TSNE

words = sum([[k] + v for k, v in similar_words.items()], [])
wvs = model.wv[words]

tsne = TSNE(n_components=3, random_state=0, n_iter=10000, perplexity=29)
np.set_printoptions(suppress=True)
T = tsne.fit_transform(wvs)
labels = words

plt.figure(figsize=(16, 12))
plt.scatter(T[:, 0], T[:, 1], c='purple', edgecolors='purple')
for label, x, y in zip(labels, T[:, 0], T[:, 1]):
    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')
</code></pre>

<p>Is this a normal behavior for PCA and TSNE word similarity dimensionality reduction, or is there something off with my code? Is it possible that the plot is treating each of the similar words subsets as independent from each other?</p>
","matplotlib, scikit-learn, nlp, word2vec, word-embedding","<p>Each word has two vectors: as a center word and as a context word. <a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=2513s"" rel=""nofollow noreferrer"">Stanford University word2vec lecture</a> starting at 41:37.</p>
",0,0,56,2019-08-05 00:02:21,https://stackoverflow.com/questions/57350998/word-shows-up-more-than-once-in-tsne-plot
Getting word embeddings using XLNet?,"<p>Hello I have been trying to contextual extract word embedding using the novel XLNet but without luck.</p>

<p>Running on Google Colab with TPU</p>

<p>I would like to note that I get this error when I use TPU so thus I switch to GPU to avoid the error</p>

<pre><code>xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)
</code></pre>

<blockquote>
  <p>AttributeError: module 'xlnet' has no attribute 'XLNetConfig'</p>
</blockquote>

<p>However I get another error when I use GPU</p>

<pre><code>run_config = xlnet.create_run_config(is_training=True, is_finetune=True, FLAGS=FLAGS)
</code></pre>

<blockquote>
  <p>AttributeError: use_tpu</p>
</blockquote>

<p>I will post the whole code below: I am using a small sentence as an input till it work and I switch to big data then</p>

<p><strong>Main Code:</strong></p>

<pre><code>import sentencepiece as spm
import numpy as np
import tensorflow as tf
from prepro_utils import preprocess_text, encode_ids
import xlnet
import sentencepiece as spm

text = ""The metamorphic rocks of western Crete form a series some 9000 to 10,000 ft.""
sp_model = spm.SentencePieceProcessor()
sp_model.Load(""/content/xlnet_cased_L-24_H-1024_A-16/spiece.model"")

text = preprocess_text(text) 
ids = encode_ids(sp_model, text)

#print('ids',ids)

# some code omitted here...
# initialize FLAGS
# initialize instances of tf.Tensor, including input_ids, seg_ids, and input_mask

# XLNetConfig contains hyperparameters that are specific to a model checkpoint.
xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path) **ERROR 1 HERE**
from absl import flags
import sys

FLAGS = flags.FLAGS
# RunConfig contains hyperparameters that could be different between pretraining and finetuning.
run_config = xlnet.create_run_config(is_training=True, is_finetune=True, FLAGS=FLAGS) **ERROR 2 HERE**
xp = []
xp.append(ids)
input_ids = np.asarray(xp)
xlnet_model = xlnet.XLNetModel(
    xlnet_config=xlnet_config,
    run_config=run_config,
    input_ids=input_ids,
    seg_ids=None,
    input_mask=None)
embed1=tf.train.load_variable('../data/xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt','model/transformer/word_embedding/lookup_table:0')`
</code></pre>

<p><strong>Before the main code I'm cloning Xlnet from GitHub and so on (I will also post it)</strong></p>

<pre><code>! pip install sentencepiece
#Download the pretrained XLNet model and unzip only needs to be done once
! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip
! unzip cased_L-24_H-1024_A-16.zip
! git clone https://github.com/zihangdai/xlnet.git

SCRIPTS_DIR = 'xlnet' #@param {type:""string""}
DATA_DIR = 'aclImdb' #@param {type:""string""}
OUTPUT_DIR = 'proc_data/imdb' #@param {type:""string""}
PRETRAINED_MODEL_DIR = 'xlnet_cased_L-24_H-1024_A-16' #@param {type:""string""}
CHECKPOINT_DIR = 'exp/imdb' #@param {type:""string""}

train_command = ""python xlnet/run_classifier.py \
  --do_train=True \
  --do_eval=True \
  --eval_all_ckpt=True \
  --task_name=imdb \
  --data_dir=""+DATA_DIR+"" \
  --output_dir=""+OUTPUT_DIR+"" \
  --model_dir=""+CHECKPOINT_DIR+"" \
  --uncased=False \
  --spiece_model_file=""+PRETRAINED_MODEL_DIR+""/spiece.model \
  --model_config_path=""+PRETRAINED_MODEL_DIR+""/xlnet_config.json \
  --init_checkpoint=""+PRETRAINED_MODEL_DIR+""/xlnet_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=8 \
  --eval_batch_size=8 \
  --num_hosts=1 \
  --num_core_per_host=1 \
  --learning_rate=2e-5 \
  --train_steps=4000 \
  --warmup_steps=500 \
  --save_steps=500 \
  --iterations=500""

! {train_command}
</code></pre>
","python, nlp, data-mining, word2vec, glove","<p>Check this <a href=""https://colab.research.google.com/gist/amansrivastava17/c7b7c5e9902133174b4ef6d8587b34b1/embedding-as-service.ipynb#scrollTo=NQEqcqa5v7hg"" rel=""nofollow noreferrer"">gist</a> out.</p>

<p>We have made it really easy to get token level embeddings from <code>XLNet</code></p>

<p>Update: <a href=""https://colab.research.google.com/gist/ashutoshsingh0223/d6d673a942dd15546fc28e9fce875b51/embedding-as-service.ipynb"" rel=""nofollow noreferrer"">Updated gist</a> . </p>

<p>For detailed documentation and more examples check <a href=""https://github.com/amansrivastava17/embedding-as-service"" rel=""nofollow noreferrer"">Github</a></p>
",2,2,3000,2019-08-05 20:14:57,https://stackoverflow.com/questions/57365531/getting-word-embeddings-using-xlnet
Word embeddings for the same word from two different texts,"<p>If I calculate word2vec for the same word (say, ""monkey""), one time on the basis of one large text from the year 1800 and another time on the basis of one large text from the year 2000, then the results would not be comparable from my point of view. Am I right? And why is it so?
I have the following idea: the text from the past may have complete different vocabulary, which is the problem. But how one can then cure it (make embeddings comparable)?</p>

<p>Thanks in advance.</p>
",word2vec,"<p>There's no ""right"" position for any word in a <code>Word2Vec</code> model â€“ just a position that works fairly well, in relation to other words and the training data, after a bunch of the pushes-and-pulls of the incremental training. Indeed, every model starts with word-vectors in low-magnitude <strong>random</strong> positions, and the training itself includes both designed-in randomness (such as via random choice of which  words to use as negative contrastive examples) and execution-order randomness (as multiple threads make progress at slightly-different rates due to the operating system's somewhat-arbitrary CPU-scheduling choices). </p>

<p>So, your ""sentences-from-1800"" and ""sentences-from-2000"" models will differ because the training data is different â€“ likely from both the fact that authors' usage varied, and that each corpus is just a tiny sample of all existing usage. But also: just training  on the ""samples-from-1800"" corpus twice in a row will result in different models! Each such model should be about-as-good as the other, in terms of the relative distances/positions of words with respect to other words in the same model. But the coordinates of individual words could be very different, and non-comparable. </p>

<p>In order for words to be ""in the same coordinate space"", extra steps must be taken. The most direct way for words to be in the same space is for them to be trained together in the same model, with them appearing alternately in contrasting examples of usage, including with other common words. </p>

<p>So if for example you needed to compare 'calenture' (an old word for tropical fevers which might not appear in your 2000s corpus) to 'penicillin' (which was discovered in the 20th century), your best bet would be to shuffle together the two corpuses into a single corpus and train a single model. To the extent each word appeared near certain words that appeared in both eras, with relatively stable meaning, their word-vectors might then be comparable. </p>

<p>If you only need one combined word-vector for 'monkey', this approach may be fine your purposes, as well. Yes, a word's meaning drifts over time. But even at any single point in time, words are <em>polysemous</em>: they have multiple meanings. And word-vectors for words with many meanings tend to move to coordinates between each of their alternate meanings. So even if 'monkey' has drifted in meaning, it is still the case that using a combined-eras corpus would probably give you a single vector for 'monkey' that reasonably represents its average meaning over all eras. </p>

<p>If you specifically wanted to model words' changes-in-meaning over time, then you might need other approaches:</p>

<ul>
<li><p>You might want to build separate models for eras, but learn translations between them, based on the idea that some words may change-little while others change-lots. (There are ways to use certain ""anchor words"", assumed to have the same meaning, to learn a transformation between separate <code>Word2Vec</code> models, then apply that same transformation to other words to project their coordinates in another model.)</p></li>
<li><p>Or, make a combined model, but probabilistically replace words whose changing-meanings you'd like to track with era-specific alternate tokens. (For example, you might replace some proportion of 'monkey' occurrences with 'monkey@1800' and 'monkey@2000', as appropriate, so that in the end you get <strong>three</strong> word-vectors for 'monkey', 'monkey@1800', 'monkey@2000', allowing you to compare the different senses.)</p></li>
</ul>

<p>Some prior work on tracking meanings-over-time using word-vectors is the 'HistWords' project:</p>

<p><a href=""https://nlp.stanford.edu/projects/histwords/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/histwords/</a></p>
",2,1,1650,2019-08-07 10:16:44,https://stackoverflow.com/questions/57392103/word-embeddings-for-the-same-word-from-two-different-texts
Gensim built-in model.load function and Python Pickle.load file,"<p>I was trying to use Gensim to import GoogelNews-pretrained model on some English words (sampled 15 ones here only stored in a txt file with each per line, and there are no more context as corpus). Then I could use ""model.most_similar()"" to get their similar words/phrases for them. But actually the file loaded from Python-Pickle method couldn't be used for gensim-built-in <code>model.load()</code> and <code>model.most_similar()</code> function directly. </p>

<p>how should I do to cluster the 15 English words (and more in the future), since I couldn't train and save and load a model  from the beginning?</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_WORD2VEC_MODEL = '../GoogleNews-vectors-negative300.bin'

GOOGLE_ENGLISH_WORD_PATH = '../testwords.txt'

GOOGLE_WORD_FEATURE = '../word.google.vector'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, binary=True) 

word_vectors = {}

#load 15 words as a test to word_vectors

with open(GOOGLE_ENGLISH_WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None
try:
    import cPickle
except :
    import _pickle as cPickle

def save_model(clf,modelpath): 
    with open(modelpath, 'wb') as f: 
        cPickle.dump(clf, f) 

def load_model(modelpath): 
    try: 
        with open(modelpath, 'rb') as f: 
            rf = cPickle.load(f) 
            return rf 
    except Exception as e:        
        return None 

for word in word_vectors:
    try:
        v= model[word]
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,GOOGLE_WORD_FEATURE)

words_set = load_model(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)
</code></pre>

<blockquote>
<pre><code>---------------error message--------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-86c15e366696&gt; in &lt;module&gt;
----&gt; 1 words_set.most_similar(""knit"", topn=3)

AttributeError: 'dict' object has no attribute 'most_similar'
---------------error message--------
</code></pre>
</blockquote>
","gensim, word2vec","<p>You've defined <code>word_vectors</code> as a Python <code>dict</code>:</p>

<pre><code>word_vectors = {}
</code></pre>

<p>Then your <code>save_model()</code> function just saves that raw <code>dict</code>, and your <code>load_model()</code> loads that same raw <code>dict</code>. </p>

<p>Such dictionary objects <strong>don't</strong> implement the <code>most_similar()</code> method, which is specific to the <code>KeyedVectors</code> interface (&amp; related classes) of <code>gensim</code>. </p>

<p>So, you'll have to leave the data inside a <code>KeyedVectors</code>-like object to be able to use <code>most_similar()</code>. </p>

<p>Fortunately, you have a few options.</p>

<p>If you happened to need the just the <strong>first</strong> 15 words from inside the <code>GoogleNews</code> file (or first 15,000, etc), you could use the optional <code>limit</code> parameter to only read that many vectors:</p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, limit=15, binary=True)
</code></pre>

<p>Alternatively, if you really need to select an arbitrary subset of the words, and assemble them into a new <code>KeyedVectors</code> instance, you could re-use one of the classes inside <code>gensim</code> instead of a plain <code>dict</code>, then add your vectors in a slightly different way:</p>

<pre><code># instead of a {} dict
word_vectors = KeyedVectors(model.vector_size)  # re-use size from loaded model
</code></pre>

<p>...then later inside your loop of each <code>word</code> you want to add...</p>

<pre><code># instead of `word_vectors[word] = _SOMETHING_`
word_vectors.add(word, model[word])
</code></pre>

<p>Then you'll have a <code>word_vectors</code> that is an actual <code>KeyedVectors</code> object. While you <em>could</em> save that via plain Python-pickle, at that point you might as well use the <code>KeyedVectors</code> built-in <code>save()</code> and <code>load()</code> - they may be more efficient on large vector sets (by saving large sets of raw vectors as a separate file which should be kept alongside the main file). For example:</p>

<pre><code>word_vectors.save(GOOGLE_WORD_FEATURE)
</code></pre>

<p>...</p>

<pre><code>words_set = KeyedVectors.load(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)  # should work
</code></pre>
",0,0,1846,2019-08-08 12:23:40,https://stackoverflow.com/questions/57412511/gensim-built-in-model-load-function-and-python-pickle-load-file
How to Cluster words and phrases with pre-trained model on Gensim,"<p>What I want exactly is to cluster words and phrases, e.g.
knitting/knit loom/loom knitting/weaving loom/rainbow loom/home decoration accessories/loom knit/knitting loom/...And I don'd have corpus while I have only the words/phrases. Could I use a pre-trained model like the one from GoogleNews/Wikipedia/... to realise it?</p>

<p>I am trying now to use Gensim to load GoogleNews pre-trained model to get phrases similarity. I've been told that The GoogleNews model includes vectors of phrases and words. But I find that I could only get word-similarity while phrase-similarity fails with an error message that the phrase is not in the vocabulary. Please advise me. Thank you.</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_MODEL = '../GoogleNews-vectors-negative300.bin'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_MODEL, binary=True) 


# done well
model.most_similar(""computer"", topn=3) 

# done with error message ""computer_software"" is not in the vocabulory.
model.most_similar(""computer_software"", topn=3) 
</code></pre>
","gensim, word2vec","<p>The <code>GoogleNews</code> set does include many multi-word phrases, as created via some statistical analysis, but might not include something specific you're hoping it does, like <code>'computer_software'</code>. </p>

<p>On the other hand, I see an online word-list suggesting that a phrase like <code>'composite_fillings'</code> <strong>is</strong> in the <code>GoogleNews</code> vocabulary, so this will likely work for you:</p>

<pre><code>model.most_similar(""composite_fillings"", topn=3) 
</code></pre>

<p>With that vector-set, you're limited to what they chose to model as phrases. If you need similarly-strong vectors for other phrases, you'd likely need to train your own model, on a corpus where the phrases important to you have been combined into single tokens. (If you just need something-better-than-nothing, averaging together the constituent words' word-vectors would give you something to work with... but that's a pretty-crude stand-in for truly modeling the bigram/multigram against its unique contexts.)</p>
",0,1,727,2019-08-09 09:01:22,https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim
Get a dictionary of incorrect spelling words in a dataframe,"<p>Am working on sentiment analysis problem. Tried to use <code>autocorrect</code> but that requires a lot computing power which I don't have access to because of the size of corpus. So came up with a different approach of solving the problem by creating a dictionary of <code>{key = 'incorrect', value = 'correct'}</code> and then manually correcting all words.</p>

<p>The problem is that how should I get that dictionary of miss-spelled words in the dictionary. Is <a href=""https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/#c-removing-misspells"" rel=""nofollow noreferrer"">this link</a> same as the solution to my problem?(Rather than misspelled words should I look for OOV words?) </p>

<p>And if not, please suggest some better method.</p>

<p>Code used for <code>autocorrect</code>:</p>

<pre><code>!pip install autocorrect
from autocorrect import spell 
train['text'] = [' '.join([spell(i) for i in x.split()]) for x in train['text']]
</code></pre>
","python, machine-learning, nlp, word2vec, autocorrect","<p><em>How many times can you spell a word correctly?</em> Only 1.</p>

<p><em>Now, how many times can you spell a word incorrectly?</em> I should say infinite.</p>

<p>This answers your question: </p>

<blockquote>
  <p>Rather than misspelled words should I look for OOV words?</p>
</blockquote>

<ul>
<li>Sure, especially if your misspells are not neologisms or commonly used misspells that repeat often.</li>
</ul>

<p>Now, how then can you get the features if they are misspelled? One way is to use ""Levenstein Distance"" (or minimum edit distance), which compares a misspelled word to your word dictionary, checking whether the distance from it to any of your words is small. That is probably what is behind the autocorrect package. You can check some more information about it in this <a href=""https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/"" rel=""nofollow noreferrer"">link</a>.</p>

<p>So, in short, probably you have to either discard OOV words or employ some computational resources on them, since computers are not able to ""guess"" without doing some computation on top of it.</p>
",1,0,352,2019-08-12 08:05:05,https://stackoverflow.com/questions/57457568/get-a-dictionary-of-incorrect-spelling-words-in-a-dataframe
How to get doc2vec or sen2vec trained vectors in readable (csv or txt) format linewise?,"<p>I trained fasttext or Sen2vec, or word2vec model for my news collection in csv file, were each news have one line like that</p>

<pre><code>0 Trump is a liar.....
1 Europa going for brexit.....
2 Russia is no more world power......
</code></pre>

<p>So, I got trained model and now I can happily get vectors for any line in my csv file like that 
(fasttext)</p>

<pre><code>import csv  
import re

train = open('tweets.train3','w')  
test = open('tweets.valid3','w')  
with open(r'C:\Users\123\Desktop\data\osn-9.csv', mode='r', encoding = ""utf- 
 8"" ,errors='ignore') as csv_file:  
csv_reader = csv.DictReader(csv_file, fieldnames=['sen', 'text'])
line = 0
for row in csv_reader:
    # Clean the training data
    # First we lower case the text
    text = row[""text""].lower()
    # remove links
    text = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','',text)
    #Remove usernames
    text = re.sub('@[^\s]+','', text)
    text = ' '.join(re.sub(""[\.\,\!\?\:\*\(\)\;\-\=]"", "" "", text).split())
    # replace hashtags by just words
    text = re.sub(r'#([^\s]+)', r'\1',  text)
    #correct all multiple white spaces to a single white space
    text = re.sub('[\s]+', ' ', text)
    # Additional clean up : removing words less than 3 chars, and remove 
    space at the beginning and teh end
    text = re.sub(r'\W*\b\w{1,3}\b', '', text)
    text = text.strip()
    line = line + 1
    # Split data into train and validation
    if line &gt; 8416:
        print(f'__label__{row[""sen""]} {text}', file=test)
    else:
        print(f'__label__{row[""sen""]} {text}', file=train)
 import fasttext
 hyper_params = {""lr"": 0.1,
""epoch"": 500,
""wordNgrams"": 2,
""dim"": 100,
""loss"":""softmax""}


model = fasttext.train_supervised(input='tweets.train3',**hyper_params)
model.get_sentence_vector('Trump is a liar.....')
array([-0.20266785,  0.3407566 ,  ...,  0.03044436,  0.39055538], 
dtype=float32).
</code></pre>

<p>or like that
(gensim)</p>

<pre><code>In [10]:
model.infer_vector(['Trump', 'is', 'a ', 'liar'])
Out[10]:
array([ 0.24116205,  0.07339828, -0.27019867, -0.19452883,  0.126193  ,
 ........................,
    0.09754166,  0.12638392, -0.09281237, -0.04791372,  0.15747668],
  dtype=float32)
</code></pre>

<p>But how I can get vectors not as arrays for each line in my csv file? Like that</p>

<pre><code>0  Trump is a liar..... -0.20266785,  0.3407566 ,  ...,  0.03044436,  
1  Europa going for brexit..... 0.24116205,  0.07339828,.... -0.27019867
2  Russia is no more world power...... 0.12638392, -0.09281237 
 ...-0.04791372, 
</code></pre>

<p>Or like that</p>

<pre><code>0   -0.20266785,  0.3407566 ,  ...,  0.03044436,  
1   0.24116205,  0.07339828,.... -0.27019867
2   0.12638392, -0.09281237...-0.0479137
</code></pre>
","python, arrays, vector, word2vec","<p>The <a href=""https://docs.python.org/3/library/csv.html"" rel=""nofollow noreferrer"">CSV</a> Python library will get you started. The examples are very straight forward, all you should have to do is pass your lists as parameters and make sure it has the correct settings.</p>

<p>Loose example:</p>

<pre><code>import csv 

#This should be a list of all the lists that
#you would like to write into the csv
master_list = []

with open('mycsv.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile, delimiter=',')
    for item in master_list:
        writer.writerow(item)
</code></pre>

<p>This should at least get you started. I did light testing and it worked for me at the very least.</p>
",0,0,283,2019-08-12 12:59:41,https://stackoverflow.com/questions/57461705/how-to-get-doc2vec-or-sen2vec-trained-vectors-in-readable-csv-or-txt-format-li
Can we compare word vectors from different models using transfer learning?,"<p>I want to train two word2vec/GLoVe models on different corpora and then compare the vectors of a single word. I know that it makes no sense to do so as different models start at different random states, but what if we use pre-trained word vectors as the starting point. Can we assume that the two models will continue to build upon the pre-trained vectors by incorporating the respective domain-specific knowledge, and not go into completely different states?</p>

<p>Tried to find some research papers which discuss this problem, but couldn't find any.</p>
","nlp, word2vec, glove","<p>Simply starting your models with pre-trained bectors would eliminate some of the randomness, but with each training epoch on your new corpora:</p>

<ul>
<li>there's still randomness introduced by negative-sampling (if using that default mode), by frequent-word downsampling (if using default values of the <code>sample</code> parameter in word2vec), and by the interplay of different threads</li>
<li>each epoch with your new corpora will be pulling the word-vectors for present words to new, better positions <strong>for that corpora</strong>, but leaving original words unmoved. The net movements over many epochs could move words arbitrarily far from where they started, in response to the whole-corpus-effects on all words.</li>
</ul>

<p>So, doing so wouldn't necessarily achieve your goal in a reliable (or theoretically-defensible) way, though it might kinda-work â€“ at least better than starting from purely random initialization â€“ especially if your corpora are small and you do few training epochs. (That's usually a bad idea â€“ you want big varied training data and enough passes for extra passes to make little incremental difference. But doing those things ""wrong"" could make your results look ""better"" in this scenario, where you don't want your training to change the original coordinate-space ""too much"". I wouldn't rely on such an approach.)</p>

<p>Especially if the words you need to compare are a small subset of the total vocabulary, a couple things you could consider:</p>

<ul>
<li><p>combine the corpora into one training corpus, shuffled together, but for those words you need to compare, replace them with corpora-specific tokens. For example, replace <code>'sugar'</code> with <code>'sugar_c1'</code> and <code>'sugar_c2'</code> â€“ leaving the vast majority of surrounding words to be the same tokens (and thus learn a single vector across the  whole corpus). Then, the two variant tokens for the ""same word"" will learn different vectors, based on their differing contexts that still share many of the same tokens. </p></li>
<li><p>using some ""anchor set"" of words that you know (or confidently conjecture) either do mean the same across both contexts, or <em>should</em> mean the same, train two models but learn a transformation between the two space based on those guide words. Then, when you apply that transformation to other words, that weren't used to learn the transformation, they'll land in contrasting positions in each others' spaces, <strong>maybe</strong> achieving the comparison you need. This is a technique that's been used for language-to-language translation, and there's a <a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">helper class</a> and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">example notebook</a> included with the Python <code>gensim</code> library. </p></li>
</ul>

<p>There may be other better approaches, these are just two quick ideas that might work without much change to existing libraries. A project like '<a href=""https://nlp.stanford.edu/projects/histwords/"" rel=""nofollow noreferrer"">HistWords</a>', which used word-vector training to try to track evolving changes in word-meaning over time, might also have ideas for usable techniques. </p>
",2,0,536,2019-08-12 21:53:40,https://stackoverflow.com/questions/57468725/can-we-compare-word-vectors-from-different-models-using-transfer-learning
Could I use BERT to Cluster phrases with pre-trained model,"<p>I found it was a failure that I had used Gensim with GoogleNews pre-trained model to cluster phrases like:</p>

<ul>
<li>knitting</li>
<li>knit loom</li>
<li>loom knitting</li>
<li>weaving loom</li>
<li>rainbow loom</li>
<li>home decoration accessories</li>
<li>loom knit/knitting loom</li>
<li>...</li>
</ul>

<p>I am advised that <a href=""https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim"">GoogleNews model does't have the phrases in it</a>. The phrases I have are a little specific to GoogleNews model while I don't have corpus to train a new model. I have only the phrases. And now I am considering to turn to BERT. But could BERT do that as I expected as above? Thank you.</p>
","tensorflow, nlp, pytorch, gensim, word2vec","<p>You can feed a phrase into the pretrained BERT model and get an embedding, i.e. a fixed-dimension vector. So BERT can embed your phrases in a space. Then you can use a clustering algorithm (such as k-means) to cluster the phrases. The phrases do not need to occur in the training corpus of BERT, as long as the words they consist of are in the vocabulary. You will have to try to see if the embeddings give you relevant results.</p>
",1,1,2134,2019-08-13 10:30:07,https://stackoverflow.com/questions/57475889/could-i-use-bert-to-cluster-phrases-with-pre-trained-model
Text Classification with word2vec,"<p>I am doing text classification and plan to use word2vec word embeddings.
I have used gensim module for word2vec Training.</p>

<p>I have tried several Options. But I am getting error that word 'xyz' not in vocabulary. I am not able to find my mistake.</p>

<h1>Text processing</h1>

<pre><code>def clean_text(text):

text = text.translate(string.punctuation)

text = text.lower().split()

stops = set(stopwords.words(""english""))
text = [w for w in text if not w in stops]

text = "" "".join(text)
text = re.sub(r""[^\w\s]"", "" "",text)
text = re.sub(r""[^A-Za-z0-9^,!.\/'+-=]"", "" "",text)

text = text.split()
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(w) for w in text]
text = "" "".join(lemmatized_words)


return text

data['text'] = data['text'].map(lambda x: clean_text(x))
</code></pre>

<p>Please help me to solve my issue.</p>

<h1>Definig Corpus</h1>

<pre><code>def build_corpus(data):
""Creates a list of lists containing words from each sentence""
corpus = []
for col in ['text']:
    for sentence in data[col].iteritems():
        word_list = sentence[1].split("" "")
        corpus.append(word_list)
return corpus

corpus = build_corpus(data)
</code></pre>

<h1>Word2vec model</h1>

<pre><code>from gensim.models import word2vec
 model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=20,    workers=12, sg=1)

words = list(model.wv.vocab)

tokenizer = Tokenizer()
X = data.text
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
X = pad_sequences(sequences, maxlen=10000)

embedding_vector_size=100

vocab_size = len(words)
embedding_matrix = np.zeros((vocab_size, embedding_vector_size))
for index, word in enumerate(words):    
 embedding_vector = model.wv[word]
 if embedding_vector is not None:
    embedding_matrix[index] = embedding_vector
</code></pre>

<p>Now I am using my created word embeddings on the downstream classification task.</p>

<h1>classification model</h1>

<pre><code>labels = data['Priority']
</code></pre>

<p>where I have two priorities. I want to classify it.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)
</code></pre>

<p>I am using folllowing network for classification</p>

<pre><code>model3 = Sequential()
model3.add(Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, input_length = max_len, weights=[embedding_matrix]))
model3.add(SpatialDropout1D(0.7))
model3.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))
model3.add(Dense(2, activation='softmax'))
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model3.summary())
</code></pre>

<p>I am getting error here:</p>

<pre><code>'ValueError: ""input_length"" is 10000, but received input has shape (None, 3)'
</code></pre>

<p>Please help me to solve it out.Thank you.</p>
","python-3.x, word2vec, text-classification","<p>Not all words from <em>corpus</em> will be kept in the word2vec model. </p>

<p>Replace:</p>

<pre><code>vocab_size = len(tokenizer.word_index) + 1
</code></pre>

<p>With:</p>

<pre><code>vocab_size = len(words)
</code></pre>

<p>And replace:</p>

<pre><code>for word, i in tokenizer.word_index.items():
</code></pre>

<p>With:</p>

<pre><code>for i, word in enumerate(words):
</code></pre>

<p>Thus ensuring your embedding matrix contains only words that are in the model. </p>
",1,1,2274,2019-08-16 13:04:12,https://stackoverflow.com/questions/57525190/text-classification-with-word2vec
Cluster similar words using word2vec,"<p>I have various restaurant labels with me and i have some words that are unrelated to restaurants as well. like below:</p>

<p><code>vegan
vegetarian
pizza
burger
transportation
coffee
Bookstores
Oil and Lube</code></p>

<p>I have such mix of around 500 labels. I want to know is there a way pick the similar labels that are related to food choices and leave out words like oil and lube, transportation.</p>

<p>I tried using word2vec but, some of them have more than one word and could not figure out a right way. </p>

<p>Brute-force approach is to tag them manually. But, i want to know is there a way using NLP or Word2Vec to cluster all related labels together.</p>
","nlp, data-science, word2vec","<p>Word2Vec could help with this, but key factors to consider are:</p>

<ul>
<li><p>How are your word-vectors trained? Using off-the-shelf vectors (like say the popular <code>GoogleNews</code> vectors trained on a large corpus of news stories) are unlikely to closely match the senses of these words in your domain, or include multi-word tokens like 'oil_and_lube'. But, if you have a good training corpus from your own domain, with multi-word tokens from a controlled vocabulary (like <code>oil_and_lube</code>) that are used in context, you might get quite good vectors for exactly the tokens you need.</p></li>
<li><p>The similarity of word-vectors isn't strictly 'synonymity' but often other forms of close-relation including oppositeness and other ways words can be interchangeable or be used in similar contexts. So whether or not the word-vector similarity-values provide a good threshold cutoff for your particular desired ""related to food"" test is something you'd have to try out &amp; tinker around. (For example: whether words that are drop-in replacements for each other are closest to each other, or words that are common-in-the-same-topics are closest to each other, can be influenced by whether the <code>window</code> parameter is smaller or larger. So you could find tuning Word2Vec training parameters improve the resulting vectors for your specific needs.)</p></li>
</ul>

<p>Making more recommendations for how to proceed would require more details on the training data you have available â€“ where do these labels come from? what's the format they're in? how much do you have? â€“ and your ultimate goals â€“ why is it important to distinguish between restaurant- and non-restaurant- labels?</p>
",0,0,1806,2019-08-17 13:02:10,https://stackoverflow.com/questions/57536510/cluster-similar-words-using-word2vec
Adding vocabulary and improve word embedding with another model that was built on bigger corpus,"<p>I'm new to NLP. I'm currently building a NLP system in a specific domain. After training a word2vec and fasttext model on my documents, I found that the embedding is not really good because I didn't feed enough number of documents (e.g. the embedding can't see that ""bar"" and ""pub"" is strongly correlated to each other because ""pub"" only appears a few in the documents). Later, I found a word2vec model online built on that domain-specific corpus which definitely has a way better embedding (so ""pub"" is more related to ""bar""). Is there any way to improve my word embedding using the model I found? Thanks!</p>
","machine-learning, nlp, word2vec, fasttext","<p>Word2Vec (and similar) models really require a large volume of varied data to create strong vectors. </p>

<p>But also, a model's vectors are typically only meaningful alongside other vectors that were trained together in the same session. This is both because the process includes some randomness, and the vectors only acquire their useful positions via a tug-of-war with all other vectors and aspects of the model-in-training. </p>

<p>So, there's no standard location for a word like 'bar' - just a good position, within a certain model, given the training data and model parameters and other words co-populating the model.</p>

<p>This means mixing vectors from different models is non-trivial. There are ways to learn a 'translation' that moves vectors from the space of one model to another â€“ but that is itself a lot like a re-training. You can pre-initialize a model with vectors from elsewhere... but as soon as training starts, all the words in your training corpus will start drifting into the best alignment for that data, and gradually away from their original positions, and away from pure comparability with other words that aren't being updated. </p>

<p>In my opinion, the best approach is usually to expand your corpus with more appropriate data, so that it has ""enough"" examples of every word important to you, in sufficiently varied contexts. </p>

<p>Many people use large free text dumps like Wikipedia articles for word-vector training, but be aware that its style of writing â€“ dry, authoritative reference texts â€“ may not be optimal for all domains. If your problem-area is ""business reviews"", you'd probably do best finding other review texts. If it's fiction stories, more fictional writing. And so forth. You can shuffle these other text-soruces in with your data to expand the vocabulary coverage. </p>

<p>You can also potentially shuffle in <strong>extra</strong> repeated examples of your own local data, if you want it to effectively have relatively more influence. (Generally, merely repeating a small number of non-varied examples can't help improve word-vectors: it's the subtle contrasts of different examples that helps. But as a way to incrementally boost the influence of some examples, when there are plenty of examples overall, it can make more sense.)</p>
",1,0,1464,2019-08-24 04:31:17,https://stackoverflow.com/questions/57635101/adding-vocabulary-and-improve-word-embedding-with-another-model-that-was-built-o
Loading pre-trained word embeddings,"<p>I am trying to load the pre-trained word2Vec model using the command below but get an Unicode error. Need some help getting to the bottom of it. I googled around but could not find a working solution to this.</p>

<pre><code>python -m spacy init-model en /tmp/google_news_vectors --vectors-loc ~/Downloads/GoogleNews-vectors-negative300.bin.gz


UnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 7: invalid start byte
</code></pre>
","python-3.x, word2vec, spacy","<p>Spacy expects the vectors to be in the text format rather than the binary format:</p>

<p><a href=""https://spacy.io/api/cli#init-model"" rel=""nofollow noreferrer"">https://spacy.io/api/cli#init-model</a></p>

<p>For how to convert the binary model, see: <a href=""https://stackoverflow.com/a/33183634/461847"">https://stackoverflow.com/a/33183634/461847</a></p>
",3,1,446,2019-08-26 05:08:11,https://stackoverflow.com/questions/57652054/loading-pre-trained-word-embeddings
evaluating word2vec model using SimLex-999,"<p>i have trained my model with Gensim.now i wanna evaluate my model with simlexx-999 but it gives me error.
my code.</p>

<pre><code>model.wv.evaluate_word_analogies('SimLex-999.txt')
2019-08-25 13:43:22,766 : INFO : Evaluating word analogies for top 300000 words in the model on SimLex-999.txt
</code></pre>

<p>error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-60cb96c45579&gt; in &lt;module&gt;()
----&gt; 1 model.wv.evaluate_word_analogies('SimLex-999.txt')

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_analogies(self, analogies, restrict_vocab, case_insensitive, dummy4unknown)
   1088             else:
   1089                 if not section:
-&gt; 1090                     raise ValueError(""Missing section header before line #%i in %s"" % (line_no, analogies))
   1091                 try:
   1092                     if case_insensitive:

ValueError: Missing section header before line #0 in SimLex-999.txt
</code></pre>

<p>i have tried</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.evaluate_word_pairs(datapath('SimLex-999.txt'))

print(similarities)
</code></pre>

<p>but it gives me keyError.Please help me to solve the problem.</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-29-caeb682cb7ff&gt; in &lt;module&gt;()
      1 from gensim.test.utils import datapath
      2 
----&gt; 3 similarities = model.wv.evaluate_word_pairs(datapath('SimLex-999.txt'),dummy4unknown=True)
      4 
      5 print(similarities)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_pairs(self, pairs, delimiter, restrict_vocab, case_insensitive, dummy4unknown)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in &lt;listcomp&gt;(.0)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

KeyError: 'movie'
</code></pre>
","python-3.x, gensim, word2vec","<p><code>SimLex-999.txt</code> does not appear to be a list of word analogies appropriate as an argument for the <code>evaluate_word_analogies()</code> function.</p>

<p>Have you tried the <code>evaluate_word_pairs()</code> function? Its description is at:</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs</a></p>
",0,1,1748,2019-08-26 06:31:05,https://stackoverflow.com/questions/57652804/evaluating-word2vec-model-using-simlex-999
How to specify word vector for OOV terms in Spacy?,"<p>I have a pre-trained word2vec model that I load to <code>spacy</code> to vectorize new words. Given new text I perform <code>nlp('hi').vector</code> to obtain the vector for the word 'hi'. </p>

<p>Eventually, a new word needs to be vectorized which is not present in the vocabulary of my pre-trained model. In this scenario <code>spacy</code> defaults to a vector filled with zeros. I would like to be able to set this default vector for OOV terms. </p>

<p>Example:</p>

<pre><code>import spacy
path_model= '/home/bionlp/spacy.bio_word2vec.model'
nlp=spacy.load(path_spacy)
print(nlp('abcdef').vector, '\n',nlp('gene').vector)
</code></pre>

<p>This code outputs a dense vector for the word 'gene' and a vector full of 0s for the word 'abcdef' (since it's not present in the vocabulary): </p>

<p><a href=""https://i.sstatic.net/Qha2U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qha2U.png"" alt=""enter image description here""></a></p>

<p>My goal is to be able to specify the vector for missing words, so instead of getting a vector full of 0s for the word 'abcdef' you can get (for instance) a vector full of 1s. </p>
","python, word2vec, spacy","<p>If you simply want your plug-vector instead of the SpaCy default all-zeros vector, you could just add an extra step where you replace any all-zeros vectors with yours. For example:</p>

<pre><code>words = ['words', 'may', 'by', 'fehlt']
my_oov_vec = ...  # whatever you like
spacy_vecs = [nlp(word) for word in words]
fixed_vecs = [vec if vec.any() else my_oov_vec 
              for vec in spacy_vecs]
</code></pre>

<p>I'm not sure why you'd want to do this. Lots of work with word-vectors simply elides out-of-vocabulary words; using any plug value, including SpaCy's zero-vector, may just be adding unhelpful noise. </p>

<p>And if better handling of OOV words is important, note that some other word-vector models, like FastText, can synthesize better-than-nothing guess-vectors for OOV words, by using vectors learned for subword fragments during training. That's similar to how people can often work out the gist of a word from familiar word-roots.</p>
",2,1,1982,2019-08-26 13:28:37,https://stackoverflow.com/questions/57658888/how-to-specify-word-vector-for-oov-terms-in-spacy
Is it appropriate to train W2V model on entire corpus?,"<p>I have a corpus of free text medical narratives, for which I am going to use for a classification task, right now for about 4200 records. </p>

<p>To begin, I wish to create word embeddings using w2v, but I have a question about a train-test split for this task. </p>

<p>When I train the w2v model, is it appropriate to use all of the data for the model creation? Or should I only use the train data for creating the model? </p>

<p>Really, my question sort of comes down to: do I take the whole dataset, create the w2v model, transform the narratives with the model, and then split, or should I split, create w2v, and then transform the two sets independently?</p>

<p>Thanks!</p>

<p><strong>EDIT</strong></p>

<p>I found an internal project at my place of work which was built by a vendor; they create the split, and create the the w2v model on ONLY the train data, then transform the two sets independently in different jobs; so it's the latter of the two options that I specified above. This is what I thought would be the case, as I wouldn't want to contaminate the w2v model on any of the test data. </p>
","python, machine-learning, nlp, word2vec","<p>The answer to most questions like these in NLP is ""try both"" :-) </p>

<p>Contamination of test vs train data is not relevant or a problem in generating word vectors. That is a relevant issue in the model you use the vectors with. I found performance to be better with whole corpus vectors in my use cases.</p>

<p>Word vectors improve in quality with more data. If you don't use test corpus, you will need to have a method for initializing out-of-vocabulary vectors and understanding the impact they may have on your model performance.</p>
",2,2,1517,2019-08-26 17:30:55,https://stackoverflow.com/questions/57662405/is-it-appropriate-to-train-w2v-model-on-entire-corpus
How can I use a pretrained embedding to gensim skipgram model?,"<p>I want to train part of the corpus first and then based on the embeddings train on the whole corpus. Can I achieve this with gensim skipgram?</p>

<p>I haven't found an API that can pass initial embeddings.</p>

<p>what I want is some thing like</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""],
             [""cat2"", ""say2"", ""meow""], [""dog2"", ""say"", ""woof""]]
model = Word2Vec(sentences[:2], min_count=1)
X = #construct a new one
model = Word2Vec(sentences, min_count=1, initial_embedding=X)
</code></pre>
","python, machine-learning, gensim, word2vec","<p>I'm not sure why you'd want to do this: if you have the whole corpus, and can train on the whole corpus, you're likely to get the best results from whole-corpus training. </p>

<p>And, to the extent there's anything missing from the 2nd-corpus, the 2nd-corpus training will tend to pull vectors for words still training away from words that are no longer in the corpus â€“ causing comparability of vectors within the corpus to decay. (It's only the interleaved tug-of-war between examples including all words that nudges them into positions that are meaningfully related to each other.)</p>

<p>But, keeping that caveat in mind: you can continue to <code>train()</code> a model with new data. That is:</p>

<pre><code># initialize &amp; do all default training
model = Word2Vec(sentences[:2], min_count=1)
# now train again even more with a slightly different mix
model.train(sentences, total_examples = len(sentences), epochs=model.epochs)
</code></pre>

<p>Note in such a case the model's discovered vocabulary is only based on the original initialization. If there are words only in <code>sentences[0]</code>, when those sentences are presented to the model that didn't see those words during its initialization, they will be ignored â€“ and never get vectors. (If using your tiny example corpus in this way, the word 'cat' won't get a vector. Again, you really want to train on the largest corpus â€“ or at least use the largest corpus, with a superset of words, 1st.)</p>

<p>Also, a warning will be logged, because the 2nd training will again start the internal <code>alpha</code> learning-rate at its larger starting value, then gradually decrease it to the final <code>min_alpha</code> value. To be yo-yo'ing the value like this isn't standard SGD, and usually indicates a user error. But, it might be tolerable depending on your goals â€“ you just need to be aware when you're doing unusual training sequences like this, you're off in experimental/advanced land and have to deal with possible side-effects via your own understanding.</p>
",0,1,252,2019-08-28 14:53:10,https://stackoverflow.com/questions/57695150/how-can-i-use-a-pretrained-embedding-to-gensim-skipgram-model
I get &#39;single&#39; characters as learned vocabulary on word2vec genism as an output,"<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
","nlp, gensim, word2vec, feature-extraction, text-classification","<p><code>Word2Vec</code> needs its training corpus to be a sequence where each item (text/sentence) is a <strong>list-of-string-tokens</strong>. </p>

<p>If you instead pass texts that are raw strings, each will appear as a <strong>list-of-one-character-tokens</strong>, and that will result in the final vocabulary you're seeing, where all learned 'words' are just single-characters. </p>

<p>So, take a closer look at your <code>token_list5</code> variable. As it is a list, what is <code>token_list5[0]</code>? (Is it a list-of-strings?) What is <code>token_list5[0][0]</code>? (Is it a full word?)</p>
",1,0,348,2019-08-30 13:59:02,https://stackoverflow.com/questions/57728181/i-get-single-characters-as-learned-vocabulary-on-word2vec-genism-as-an-output
Is it possible to set the matrix weights of embeddings before or after training word2vec?,"<p>I need to change the matrix embedding in word2vec after to train this. Here is the example:</p>

<pre><code>w2v=Word2Vec(sentences,size=100,window=1,min_count=1,negative=15,iter=3)
w2v.save(""word2vec.model"")

#Getting embedding matrix
embedding_matrix=w2v.wv.vectors

for p in (""mujer"", ""hombre""):
    result=w2v.wv.similar_by_word(p)
    print(""Similar words from '"",p,""': "",result[:3])

#Trying to set wights matrix
w2v.wv.vectors=np.random.rand(w2v.wv.vectors.shape[0],w2v.wv.vectors.shape[1])

print()

for p in (""mujer"", ""hombre""):
    result=w2v.wv.similar_by_word(p)
    print(""Similar words from '"",p,""': "",result[:3])
</code></pre>

<p>And here is the output:</p>

<pre><code>Similar words from ' mujer ':  [('honra', 0.9999152421951294), ('muerte', 0.9998959302902222), ('contento', 0.999891459941864)]
Similar words from ' hombre ':  [('valor', 0.9999064207077026), ('nombre', 0.9998984336853027), ('llegar', 0.9998887181282043)]

Similar words from ' mujer ':  [('honra', 0.9999152421951294), ('muerte', 0.9998959302902222), ('contento', 0.999891459941864)]
Similar words from ' hombre ':  [('valor', 0.9999064207077026), ('nombre', 0.9998984336853027), ('llegar', 0.9998887181282043)]
</code></pre>

<p>As you can see, I get the same predictions despite having changed the embedding matrix by random numbers.</p>

<p>I don't get any method in the documentation to make this change.</p>

<p>Will it be possible?</p>
","python-3.x, word2vec","<p>I already found the solution. Just use the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.init_sims"" rel=""nofollow noreferrer"">init_sims</a>() function after setting the array.</p>

<pre><code>w2v=Word2Vec(sentences,size=100,window=1,min_count=1,negative=15,iter=3)
w2v.save(""word2vec.model"")

#Getting embedding matrix
embedding_matrix=w2v.wv.vectors

for p in (""mujer"", ""hombre""):
    result=w2v.wv.similar_by_word(p)
    print(""Similar words from '"",p,""': "",result[:3])

#Setting new values on wights matrix
w2v.wv.vectors=np.random.rand(w2v.wv.vectors.shape[0],w2v.wv.vectors.shape[1])

#This line create a l2 normalization over the embedding matrix 
word_vectors.vectors_norm=word_vectors.init_sims(replace=False)

print()

for p in (""mujer"", ""hombre""):
    result=w2v.wv.similar_by_word(p)
    print(""Similar words from '"",p,""': "",result[:3])
</code></pre>
",0,0,136,2019-09-01 10:40:58,https://stackoverflow.com/questions/57745276/is-it-possible-to-set-the-matrix-weights-of-embeddings-before-or-after-training
Perform matrix multiplication with cosine similarity function,"<p>I have two lists:</p>

<pre><code>list_1 = [['flavor', 'flavors', 'fruity_flavor', 'taste'],
          ['scent', 'scents', 'aroma', 'smell', 'odor'],
          ['mental_illness', 'mental_disorders','bipolar_disorder']
          ['romance', 'romances', 'romantic', 'budding_romance']]

list_2 = [['love', 'eating', 'spicy', 'hand', 'pulled', 'noodles'],
          ['also', 'like', 'buy', 'perfumes'],
          ['suffer', 'from', 'clinical', 'depression'],
          ['really', 'love', 'my', 'wife']]
</code></pre>

<p>I would like to compute the cosine similarity between the two lists above in such a way where the cosine similarity between the first sub-list in list1 and all sublists of list 2 are measured against each other. Then the same thing but with the second sub-list in list 1 and all sub-lists in list 2, etc.</p>

<p>The goal is to create a <strong>len(list_2) by len(list_1) matrix</strong>, and each entry in that matrix is a cosine similarity score. Currently I've done this the following way:</p>

<pre><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)
</code></pre>

<p>However, I'd like to implement this with matrix multiplication and no for loops. </p>

<p>My two questions are:</p>

<ol>
<li>Is there a way to do some sort of element-wise matrix multiplication but with <code>gensim's n_similiarity() method</code> to generate the required matrix?</li>
<li>Would it be more efficient and faster using the current method or matrix multiplication?</li>
</ol>

<p>I hope my question was clear enough, please let me know if I can clarify even further.</p>
","python-3.x, numpy, gensim, word2vec, cosine-similarity","<p>There are two problems in code, the second last and last line.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)</code></pre>
</div>
</div>
</p>

<p>Answers to you questions:<br>
1. You are already using a direct function to calculate the similarity between two sentences(L1 and L2) which are first converted to two vectors and then cosine similarity is calculated of those two vectors. Everything is already done inside the n_similarity() so you can't do any kind of matrix multiplication.<br>
If you want to do your own matrix multiplication then instead of directly using n_similarity() calculates the vectors of the sentences and then apply matrix multiplication while calculating cosine similarity.<br>
2. As I said in (1) that everything is done in n_similarity() and creators of gensim takes care of the efficiency when writing the libraries so any other multiplication method will most likely not make a difference.<br></p>
",-1,1,2964,2019-09-02 21:11:33,https://stackoverflow.com/questions/57762713/perform-matrix-multiplication-with-cosine-similarity-function
Does the &quot;iter&quot; parameter of gensim.models.Word2Vec method iterate over the whole corpus or the sentence passed to it at a time?,"<p>I am using gensim to train a word2Vec model. Here I am passing one sentence at a time to the gensim.models.Word2Vec() method from my corpus to gradually train the model on my whole corpus. But I am confused what should the value of iter parameter be as I'm not sure whether it iterates over the passed sentence n times or the whole corpus.    </p>

<p>I have tried checking the documentation of gensim. it states the definition as follows:<br></p>

<blockquote>
  <p>iter (int, optional) â€“ Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>But I am confused as I am not passing the whole corpus but only a single sentence on each iteration.</p>

<p>My line in the code that trains the model looks like this:<br>
<code>model = gensim.models.Word2Vec(data, min_count=2, window=arg.window_size, size=arg.dim_size, workers=4, sg=0, hs=0, negative=10, ns_exponent=0.75, alpha=0.025, iter=1)</code>
<br>Here ""data"" represents a single sentence passed at a time from a generator. </p>

<p>Suppose I have a corpus of 2 sentences. ""X is a variable. Y is a variable too."". The model receives data = ""X is a variable."" first and data = ""Y is a variable too."" in 2nd iteration. 
Now to clarify, my question is,<b> whether iter = 50 will train my model iterating though ""X is a variable."" 50 times &amp; ""Y is a variable too."" 50 times or will it iterating though ""X is a variable. Y is a variable too."" (my whole corpus) 50 times. 
</b></p>
","python, gensim, word2vec","<p><code>Word2Vec</code> is a class. Calling it as <code>model = Word2Vec(...)</code> returns one new model instance. </p>

<p>If you supply data to that instantiation call, it expects a full training corpus, with all examples, as the data (<code>sentences</code> parameter). It will iterate over that data once to learn the vocabulary, then again the number of times specified in the <code>epochs</code> argument for training. (This argument was previously called <code>iter</code>, which still works.)</p>

<p>So:</p>

<ul>
<li>You shouldn't be calling <code>Word2Vec(...)</code> multiple times with single texts. You should call it once, with a re-iterable sequence of all your texts as the data. </li>
<li>That full supplied corpus will be iterated over <code>epochs</code> + 1 times as part of the model's initialization &amp; training, via the single call to <code>Word2Vec(...)</code>.</li>
</ul>

<p>You should enable logging at the INFO level to get a better idea of what's happening when you try different approaches. </p>

<p>You should also look at working examples, like the <code>word2vec.ipynb</code> notebook bundled with <code>gensim</code> inside its <code>docs/notebooks</code> directory, to understand usual usage patterns. (This is best viewed, and interactively run, from your local installation â€“ but can also be browsed online at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb</a>.)</p>

<p>Note that you <strong>can</strong> avoid supplying any data to the <code>Word2Vec(...)</code> instantiation call, but then you need to call <code>model.build_vocab(full_corpus)</code> and then <code>model.train(full_corpus, epochs=desired_iterations)</code> to complete the model initialization &amp; training. (While you can then continue calling <code>train()</code> with fragments of training data, that's an advanced &amp; highly error-prone approach. Only calling it just once, with one combined full training set, will easily and automatically do the right thing with the training learning-rate decay and number of training iterations.)</p>
",3,0,5355,2019-09-08 04:47:17,https://stackoverflow.com/questions/57839264/does-the-iter-parameter-of-gensim-models-word2vec-method-iterate-over-the-whol
Gensim&#39;s word2vec returning awkward vectors,"<p>Given heavily cleaned input in the format</p>

<pre class=""lang-py prettyprint-override""><code>model_input = [['TWO people admitted fraudulently using bank cards (...)'],
               ['All tyrants believe forever',
                'But history especially People Power (...) first Bulatlat']]
</code></pre>

<p>word2vec is returning alongside the more obvious results super-specific vectors such as</p>

<pre class=""lang-py prettyprint-override""><code>{'A pilot shot dogfight Pakistani aircraft returned India Friday freed Islamabad called peace gesture following biggest standoff two countries years':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572828&gt;,
 'This story published content partnership POLITICO':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572a58&gt;,
 'Facebook says none 200 people watched live video New Zealand mosque shooting flagged moderators underlining challenge tech companies face policing violent disturbing content real time': 
    &lt;gensim.models.keyedvectors.Vocab at 0x12a93572ba8&gt;}
</code></pre>

<p>It appears to be occurring to more documents than not, and I have a hard time believing they each appear more than five times.</p>

<p>I'm using the following code to create my model:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 5 
DIMS = 250

vocab_model = gensim.models.Word2Vec(model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     min_count=MIN_COUNT)
</code></pre>

<p>What am I doing wrong that I'm getting such useless vectors?</p>
","python, python-3.x, gensim, word2vec","<p><code>Word2Vec</code> expects its training corpus â€“ its <code>sentences</code> argument â€“ to be a re-iterable Python sequence where each item is itself a list-of-words. </p>

<p>Your <code>model_input</code> list appears to be a list, where each item is itself a list, but where each item in those lists is a full sentence of many words as a string. As a result, where it's expecting individual word-tokens (as strings), you're giving it full untokenized sentences (as strings). </p>

<p>If you break your texts into lists-of-words, and feed a sequence of those lists-of-words to the model as training data, then you'll get vectors for word-tokens, rather than sentence-strings.</p>
",1,0,77,2019-09-11 20:04:04,https://stackoverflow.com/questions/57896070/gensims-word2vec-returning-awkward-vectors
Handling OOV words in GoogleNews-vectors-negative300.bin,"<p>I need to calculate the word vectors for each word of a sentence that is tokenized as follows:</p>

<pre><code>['my', 'aunt', 'give', 'me', 'a', 'teddy', 'ruxpin']. 
</code></pre>

<p>If I was using the pretrained [fastText][1] Embeddings: cc.en.300.bin.gz by facebook. I could get by OOV. However, when I use Google's word2vec from GoogleNews-vectors-negative300.bin, it returns an InvalidKey Error. My question is how to we calculate the word vectors that are OOV then? I searched online I could not find anything. Of course on way to do this is removing all the sentences that have words not listed in the google's word2vec. However, I noticed only 5550 out of 16134 have words completely in the embedding. </p>

<p>I did also </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True) 
model.train(sentences_with_OOV_words)
</code></pre>

<p>However, tensorflow 2 returns an error.</p>

<p>Any help would be greatly appreciate it. </p>
","word2vec, oov","<p>Awesome! Thank you very much.</p>

<pre><code>def get_vectorOOV(s):
  try:
    return np.array(model.wv.get_vector(s))
  except KeyError:
    return np.zeros((300,))
</code></pre>
",1,0,1765,2019-09-16 04:18:26,https://stackoverflow.com/questions/57950325/handling-oov-words-in-googlenews-vectors-negative300-bin
"Python3, word2vec, How can I get the list of similarity rank about &quot;price&quot; in my model","<p>In gensim's word2vec python, I want to get the list of cosine similarity for ""price"".</p>

<p>I read the document of gensim word2vec, but document it describes <code>most_similar</code> and <code>n_similarity</code> function)()</p>

<p>I want the whole list of similarity between price and all others.</p>
","python, gensim, word2vec, similarity, cosine-similarity","<p>If you call <code>wv.most_similar('price', topn=len(wv))</code>, with a <code>topn</code> argument of the full vocabulary count of your model, you'll get back a ranked list of every word's similarity to <code>'price'</code>. </p>

<p>If you call with <code>topn=0</code>, you'll get the raw similarities with all model words, unsorted (in the order the words appear inside <code>wv.index2entity</code>). </p>
",0,0,264,2019-09-17 07:53:15,https://stackoverflow.com/questions/57969707/python3-word2vec-how-can-i-get-the-list-of-similarity-rank-about-price-in-my
Is there a reason to not normalize the document output vectors of Doc2Vec for clustering?,"<p>I know that in Word2Vec the length of word vectors could encode properties like term frequency. In that case, we can see two word vectors, say synonyms, with a similar meaning but with a different length given their usage in our corpus.</p>

<p>However, if we normalize the word vectors, we keep their ""directions of meaning"" and we could clusterize them according that: meaning.</p>

<p>Following that train of thought, the same would be applicable to document vectors in Doc2Vec.</p>

<p>But my question is, is there a reason to <strong>NOT</strong> normalize document vectors if we want to cluster them? In Word2Vec we can say we want to keep the frequency property of the words, is there a similar thing for documents?</p>
","vector, nlp, linear-algebra, word2vec, doc2vec","<p>I'm not familiar with any reasoning or research precedent which implies that either unit-normalized or non-normalized document-vectors are better for clustering. </p>

<p>So, I'd try both to see which seems to work better for your purposes. </p>

<p>Other thoughts:</p>

<p>In <code>Word2Vec</code>, my general impression is that larger-magnitude word-vectors are associated with words that, in the training data, have more unambiguous meaning. (That is, they reliably tend to imply the same smaller set of neighboring words.) Meanwhile, words with multiple meanings (polysemy) and usage amongst many other diverse words tend to have lower-magnitude vectors. </p>

<p>Still, the common way of comparing such vectors, cosine-similarity, is oblivious to magnitudes. That's likely because most comparisons just need the best sense of a word, without any more subtle indicator of ""unity of meaning"". </p>

<p>A similar effect <em>might</em> be present in <code>Doc2Vec</code> vectors: lower-magnitude doc-vectors could be a hint that the document has more broad word-usage/subject-matter, while higher-magnitude doc-vectors suggest more focused documents. (I'd similarly have the hunch that <em>longer</em> documents may tend to have <em>lower-magnitude</em> doc-vectors, because they use a greater diversity of words, whereas small documents with a narrow set of words/topics may have <em>higher-magnitude</em> doc-vectors. But I have not specifically observed/tested this hunch, and any effect here could be heavily influenced by other training choices, like the number of training iterations.)</p>

<p>Thus, it's <em>possible</em> that the non-normalized vectors would be interesting for some clustering goals, like separating focused  documents from more general documents. So again, after this longer analysis: I'd suggest trying it both ways to see if one or the other seems to work better for your specific needs.</p>
",1,1,861,2019-09-22 02:21:57,https://stackoverflow.com/questions/58045535/is-there-a-reason-to-not-normalize-the-document-output-vectors-of-doc2vec-for-cl
"UnpicklingError: invalid load key, &#39;`&#39;","<p>Tried use pretrained model for russian lang. from 
<a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a></p>

<p>But can't load model from pkl file. 
Tried to use other encoders as cp1251, latin1, windows-1252. Unfortunately, it drops down.</p>

<pre class=""lang-py prettyprint-override""><code>model = Word2Vec.load_word2vec_format('ruwiki_20180420_100d.pkl')

UnpicklingError: invalid load key, '`'
</code></pre>
","python-3.x, load, pickle, word2vec","<p>According to the text on the page you've referenced, <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a>, the binary files there should be loaded with <code>Wikipedia2Vec.load()</code>.</p>

<p>Only the other text files there, with suffixes <code>.txt</code>, can be loaded with <code>gensim</code>'s <code>load_word2vec_format()</code> method. </p>

<p>Either use <code>Wikipedia2Vec.load()</code> with the file you've mentioned, or try the text file variants instead.</p>
",0,0,829,2019-09-22 11:57:15,https://stackoverflow.com/questions/58048977/unpicklingerror-invalid-load-key
gensim word2vec extremely big and what are the methods to make file size smaller?,"<p>I have a pre-trained word2vec bin file by using skipgram. The file is pretty big (vector dimension of 200 ), over 2GB. I am thinking some methods to make the file size smaller. This bin file contains vectors for punctuation, some stop words. So, I want to know what are the options to decrease the file size for this word2vec. Is it safe to delete those punctuation and stop words rows and what would be the most effective way ?</p>
","python, gensim, word2vec","<p>The size of a full <code>Word2Vec</code> model is chiefly determined by the chosen vector-size, and the size of the vocabulary. </p>

<p>So your main options for big savings is to train smaller vectors, or a smaller vocabulary. </p>

<p>Discarding a few hundred stop-words or punctuation-tokens won't make a noticeable dent in the model size. </p>

<p>Discarding many of the least-frequent words can make a big difference in model size â€“ and often those less-frequent words aren't as important as you might think. (While there are a lot of them in total, each only appears rarely. And because they're rare in the training data, they often tend not to have very good vectors, anyway â€“ based on few examples, and their training influence is swamped by the influence of more-frequent words.)</p>

<p>The easiest way to limit the vocabulary size is to use a higher <code>min_count</code> value during training (ignoring all words with fewer occurrences), or a  fixed <code>max_final_vocab</code> cap (which will keep only that many of the most-frequent words). </p>

<p>Note also that if you've been saving/reloading full <code>Word2Vec</code> models (via the gensim-internal <code>.save()</code>/<code>.load()</code> methods), you're retaining model internal weights that are only needed for continued training, and will nearly double the model-size on disk or re-load. </p>

<p>You may want to save just the raw word-vectors in the <code>.wv</code> property instead (via either the gensim-internal <code>.save()</code> or the <code>.save_word2vec_format()</code> methods). </p>
",2,1,3507,2019-09-23 20:00:29,https://stackoverflow.com/questions/58069421/gensim-word2vec-extremely-big-and-what-are-the-methods-to-make-file-size-smaller
Choosing loss function for lstm trained on word2vec vectors when target is a vector of same dimensions,"<p>I have an lstm I'm using as a sequence-generator trained on word2vec vectors. The previous implementation produced a probability distribution for all the different labels. There was one label for every word in the vocabulary. This implementation used Pytorch's CrossEntropyLoss. I now want to change this so the lstm outputs a vector that has the same dimensions as the vectors used for training. This way I could use the euclydian distance measure to match wit to nearby vectors in the vocabulary. The problem is that in order to do this, I have to use a different loss function, because CrossEntropyLoss is appropriate for classifiers and not for regression problems. </p>

<p>I tried changing the format of the target vector, but torch's CrossEntropyLoss function requires integer input, and I have a word vector. Having looked at a few options it seems Cosine Embedding Loss might be a good idea but I don't understand how it works and what kind of input it takes. </p>

<p>I have already changed my fully connected layer to output vectors of the same dimensions as the Word Embeddings used for training:   </p>

<pre><code>nn.Linear(in_features=self.cfg.lstm.lstm_num_hidden,out_features=self.cfg.lstm.embedding_dim,bias=True)
</code></pre>

<p>Any advice and examples would be much appreciated.</p>
","pytorch, lstm, word2vec","<p>As the documentation of <a href=""https://pytorch.org/docs/stable/nn.html?highlight=cosine%20embedding%20loss#torch.nn.CosineEmbeddingLoss"" rel=""nofollow noreferrer"">CosineEmbeddingLoss</a> says:</p>

<blockquote>
  <p>Creates a criterion that measures the loss given two input tensors and a Tensor label with values 1 or -1.</p>
</blockquote>

<p>In your scenario, you should always provide 1 as the Tensor label.</p>

<pre><code>batch_size, seq_len, w2v_dim = 32, 100, 200
x1 = torch.randn(batch_size, seq_len, w2v_dim)
x2 = torch.randn(batch_size, seq_len, w2v_dim)
y = torch.ones(batch_size, seq_len)
loss_fn = torch.nn.CosineEmbeddingLoss(reduction='none')
loss = loss_fn(x1.view(-1, w2v_dim), 
               x2.view(-1, w2v_dim),  
               y.view(-1))
loss = loss.view(batch_size, seq_len)
</code></pre>

<p>Here, I assume <code>x1</code> is the word embeddings, <code>x2</code> is the output of the LSTM followed by some transformation.</p>

<blockquote>
  <p>Why should I always provide 1 as the Tensor label?</p>
</blockquote>

<p>First, you should see the loss function.</p>

<p><a href=""https://i.sstatic.net/W63PK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/W63PK.png"" alt=""enter image description here""></a></p>

<p>In your scenario, the higher the cosine similarity is, the lower the loss should be. In other words, you want to maximize the cosine similarity. So, you need to provide 1 as the label.</p>

<p>On the other hand, if you want to minimize the cosine similarity, you need to provide -1 as the label.</p>
",1,0,2037,2019-09-25 14:45:38,https://stackoverflow.com/questions/58101091/choosing-loss-function-for-lstm-trained-on-word2vec-vectors-when-target-is-a-vec
Range for vector values in gensim model,"<p>I am extracting the word embeddings vector from a word2vec model using model.wv. What is the range of values for each element in this vector?</p>

<pre><code>import gensim

word2vec_model = gensim.models.Word2Vec.load(""testModel"")
word2vec_model.wv[""increase""] #What is range of values for each vector element?
</code></pre>

<p>Can't seem to find this information in the documentation.</p>
","gensim, word2vec","<p>Every dimension of the vector is 32-bit floating point value. </p>

<p>There's no essential or enforced limit other than that, though the training process is such that individual dimensions tend not to be ""very large"" â€“ often staying in the range between -1.0 and 1.0.</p>

<p>It's common (but not required or beneficial for all applications) to normalize word-vectors to have a magnitude of 1.0 before comparing them to other similarly-normalized word-vectors. </p>

<p>You can request such a unit-normalized version of a word-vector with the <code>word_vec()</code> method's <code>use_norm</code> parameter:</p>

<pre><code>model.wv.word_vec(word, use_norm=True)
</code></pre>

<p>In such a unit-normed vector, no single dimension will be outside the range of -1.0 to 1.0.</p>
",4,1,1755,2019-09-26 19:01:59,https://stackoverflow.com/questions/58123189/range-for-vector-values-in-gensim-model
AWS Lambda Boto gensim model module initialization error: __exit__,"<p>Hosting a word2vec model with gensim on AWS lambda </p>

<p>using python 2.7
boto==2.48.0
gensim==3.4.0</p>

<p>and I have a few lines in my function.py file where I load the model directly from s3</p>

<pre><code>print('################### connecting to s3...')
s3_conn = boto.s3.connect_to_region(
        region,
        aws_access_key_id = Aws_access_key_id,
        aws_secret_access_key = Aws_secret_access_key,
        is_secure = True,
        calling_format = OrdinaryCallingFormat()
        )
print('################### connected to s3...')
bucket = s3_conn.get_bucket(S3_BUCKET)
print('################### got bucket...')
key = bucket.get_key(S3_KEY)
print('################### got key...')
model =  KeyedVectors.load_word2vec_format(key, binary=True)
print('################### loaded model...')
</code></pre>

<p>on the model loading line</p>

<pre><code>    model =  KeyedVectors.load_word2vec_format(key, binary=True)
</code></pre>

<p>getting a mysterious error without much details:</p>

<p>on the cloud watch can see all of my print messages til '################### got key...' inclusive, 
then I get: </p>

<pre><code>START RequestId: {req_id} Version: $LATEST 
</code></pre>

<p>then right after it [no time delays between these two messages]</p>

<pre><code>module initialization error: __exit__ 
</code></pre>

<p>please, is there a way to get a detailed error or more info?</p>

<p>More background details :
I was able to download the model from s3 to /tmp/ and it did authorize and retrieve the model file, but it went out of space [file is ~2GB, /tmp/ is 512MB]</p>

<p>so, switched to directly loading the model by gensim as above and now getting that mysterious error.</p>

<p>running the function with python-lambda-local works without issues </p>

<p>so, this probably narrows it down to an issue with gensim's smart open or aws lambda, would appreciate any hints, thanks! </p>
","python, amazon-web-services, aws-lambda, gensim, word2vec","<p>instead of connecting using boto,
simply:</p>

<pre><code>model = KeyedVectors.load_word2vec_format('s3://{}:{}@{}/{}'.format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, S3_BUCKET, S3_KEY), binary=True)
</code></pre>

<p>worked!</p>

<p>but of course, unfortunately, it doesn't answer the question on why the mysterious <strong>exit</strong> error came up and how to get more info :/</p>
",1,0,376,2019-10-01 09:53:47,https://stackoverflow.com/questions/58182293/aws-lambda-boto-gensim-model-module-initialization-error-exit
Word Embedding Model,"<p>I have been searching and attempting to implement a word embedding model to predict similarity between words. I have a dataset made up 3,550 company names, the idea is that the user can provide a new word (which would not be in the vocabulary) and calculate the similarity between the new name and existing ones.</p>

<p>During preprocessing I got rid of stop words and punctuation (hyphens, dots, commas, etc). In addition, I applied stemming and separated prefixes with the hope to get more precision. Then words such as <code>BIOCHEMICAL</code> ended up as <code>BIO</code> <code>CHEMIC</code> which is the word divided in two (prefix and stem word)</p>

<p>The average company name length is made up 3 words with the following frequency:</p>

<p><a href=""https://i.sstatic.net/yKeFP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yKeFP.png"" alt=""enter image description here""></a></p>

<p>The tokens that are the result of preprocessing are sent to word2vec:</p>

<pre><code>#window: Maximum distance between the current and predicted word within a sentence
#min_count: Ignores all words with total frequency lower than this.
#workers: Use these many worker threads to train the model
#sg: The training algorithm, either CBOW(0) or skip gram(1). Default is 0s
word2vec_model = Word2Vec(prepWords,size=300, window=2, min_count=1, workers=7, sg=1)
</code></pre>

<p>After the model included all the words in the vocab , the average sentence vector is calculated for each company name:
    df['avg_vector']=df2.apply(lambda row : avg_sentence_vector(row, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word)).tolist())</p>

<p>Then, the vector is saved for further lookups:</p>

<pre><code>##Saving name and vector values in file
df.to_csv('name-submission-vectors.csv',encoding='utf-8', index=False)
</code></pre>

<p>If a new company name is not included in the vocab after preprocessing (removing stop words and punctuation), then I proceed to create the model again and calculate the average sentence vector and save it again.</p>

<p>I have found this model is not working as expected. As an example, calculating the most similar words <code>pet</code> is getting the following results:</p>

<pre><code>ms=word2vec_model.most_similar('pet')

('fastfood', 0.20879755914211273)
('hammer', 0.20450574159622192)
('allur', 0.20118337869644165)
('wright', 0.20001833140850067)
('daili', 0.1990675926208496)
('mgt', 0.1908089816570282)
('mcintosh', 0.18571510910987854)
('autopart', 0.1729743778705597)
('metamorphosi', 0.16965581476688385)
('doak', 0.16890916228294373)
</code></pre>

<p>In the dataset, I have words such as paws or petcare, but other words are creating relationships with <code>pet</code> word.</p>

<p>This is the distribution of the nearer words for <code>pet</code>:</p>

<p><a href=""https://i.sstatic.net/KpGoJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KpGoJ.png"" alt=""enter image description here""></a></p>

<p>On the other hand, when I used the <code>GoogleNews-vectors-negative300.bin.gz</code>, I could not add new words to the vocab, but the similarity between <code>pet</code> and words around was as expected:</p>

<pre><code>ms=word2vec_model.most_similar('pet')
('pets', 0.771199643611908)
('Pet', 0.723974347114563)
('dog', 0.7164785265922546)
('puppy', 0.6972636580467224)
('cat', 0.6891531348228455)
('cats', 0.6719794869422913)
('pooch', 0.6579219102859497)
('Pets', 0.636363685131073)
('animal', 0.6338439583778381)
('dogs', 0.6224827170372009)
</code></pre>

<p>This is the distribution of the nearest words:</p>

<p><a href=""https://i.sstatic.net/2jt2o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2jt2o.png"" alt=""enter image description here""></a></p>

<p>I would like to get your advice about the following:</p>

<ul>
<li>Is this dataset appropriate to proceed with this model? </li>
<li>Is the length of the dataset enough to allow <code>word2vec</code> ""learn"" the relationships between the words?</li>
<li>What can I do to improve the model to make <code>word2vec</code> create relationships of the same type as GoogleNews where for instance word <code>pet</code> is correctly set among similar words?</li>
<li>Is it feasible to implement another alternative such as <code>fasttext</code> considering the nature of the current dataset?</li>
<li>Do you know any public dataset that can be used along with the current dataset to create those relationships?</li>
</ul>

<p>Thanks</p>
","machine-learning, deep-learning, word2vec, word-embedding, fasttext","<p>3500 texts (company names) of just ~3 words each is only around 10k total training words, with a much smaller vocabulary of unique words. </p>

<p>That's very, very small for word2vec &amp; related algorithms, which rely on lots of data, and sufficiently-varied data, to train-up useful vector arrangements.</p>

<p>You may be able to squeeze some meaningful training from limited data by using <em>far more</em> training epochs than the default <code>epochs=5</code>, and <em>far smaller</em> vectors than the default <code>size=100</code>. With those sorts of adjustments, you may start to see more meaningful <code>most_similar()</code> results.</p>

<p>But, it's unclear that word2vec, and specifically word2vec in your averaging-of-a-name's-words comparisons, is matched to your end goals. </p>

<p>Word2vec needs lots of data, doesn't look at subword units, and can't say anything about word-tokens not seen during training. An average-of-many-word-vectors can often work as an easy baseline for comparing multiword texts, but might also dilute some word's influence compared to other methods.</p>

<p>Things to consider might include:</p>

<ul>
<li><p>Word2vec-related algorithms like FastText that also learn vectors for subword units, and can thus bootstrap not-so-bad guess vectors for words not seen in training. (But, these are also data hungry, and to use on a small dataset you'd again want to reduce vector size, increase epochs, and additionally shrink the number of <code>buckets</code> used for subword learning.)</p></li>
<li><p>More sophisticated comparisons of multi-word texts, like ""Word Mover's Distance"". (That can be quite expensive on longer texts, but for names/titles of just a few words may be practical.)</p></li>
<li><p>Finding more data that's compatible with your aims for a stronger model. A larger database of company names might help. If you just want your analysis to understand English words/roots, more generic training texts might work too.</p></li>
<li><p>For many purposes, a mere lexicographic comparison - edit distances, count of shared character-n-grams â€“ may be helpful too, though it won't detect all synonyms/semantically-similar words.</p></li>
</ul>
",1,1,847,2019-10-04 05:03:40,https://stackoverflow.com/questions/58230214/word-embedding-model
Gensim word2vec model outputs 1000 dimension ndarray but the maximum number of ndarray dimensions is 32 - how?,"<p>I'm trying to use <a href=""https://github.com/idio/wiki2vec/"" rel=""nofollow noreferrer"">this</a> 1000 dimension wikipedia word2vec model to analyze some documents.</p>

<p>Using introspection I found out that the vector representation of a word is a 1000 dimension numpy.ndarray, however whenever I try to create an ndarray to find the nearest words I get a value error:</p>

<pre><code>ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>and from what I can tell by looking around online 32 is indeed the maximum supported number of dimensions for an ndarray - so what gives? How is gensim able to output a 1000 dimension ndarray?</p>

<p>Here is some example code:</p>

<pre><code>doc = [model[word] for word in text if word in model.vocab]
out = []
n = len(doc[0])
print(n)
print(len(model[""hello""]))
print(type(doc[0]))
for i in range(n):
    sum = 0
    for d in doc:
        sum += d[i]
    out.append(sum/n)
out = np.ndarray(out)
</code></pre>

<p>which outputs:</p>

<pre><code>1000
1000
&lt;class 'numpy.ndarray'&gt;
ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>The goal here would be to compute the average vector of all words in the corpus in a format that can be used to find nearby words in the model so any alternative suggestions to that effect are welcome.</p>
","python, gensim, word2vec","<p>You're calling <code>numpy</code>'s <code>ndarray()</code> constructor-function with a list that has 1000 numbers in it â€“ your hand-calculated averages of each of the 1000 dimensions. </p>

<p>The <code>ndarray()</code> function expects its argument to be the <em>shape</em> of the matrix constructed, so it's trying to create a new matrix of shape <code>(d[0], d[1], ..., d[999])</code> â€“ and then every individual value inside that matrix would be addressed with a 1000-int set of coordinates. And, indeed <code>numpy</code> arrays can only have 32 independent dimensions. </p>

<p>But even if you reduced the list you're supplying to <code>ndarray()</code> to just 32 numbers, you'd still have a problem, because your 32 numbers are floating-point values, and <code>ndarray()</code> is expecting integral counts. (You'd get a <code>TypeError</code>.)</p>

<p>Along the approach you're trying to take â€“ which isn't quite optimal as we'll get to below â€“ you really want to create a <em>single vector</em> of 1000 floating-point dimensions. That is, 1000 cell-like values â€“ <strong>not</strong> <code>d[0] * d[1] * ... * d[999]</code> separate cell-like values. </p>

<p>So a crude fix along the lines of your initial approach could be replacing your last line with either:</p>

<pre><code>result = np.ndarray(len(d))
for i in range(len(d)):
    result[i] = d[i]
</code></pre>

<p>But there are many ways to incrementally make this more efficient, compact, and idiomatic â€“ a number of which I'll mention below, even though the <em>best</em> approach, at bottom, makes most of these interim steps unnecessary.</p>

<p>For one, instead of that assignment-loop in my code just above, you could use Python's bracket-indexing assignment option:</p>

<pre><code>result = np.ndarray(len(d))
result[:] = d  # same result as previous 3-lines w/ loop
</code></pre>

<p>But in fact, <code>numpy</code>'s <code>array()</code> function can essentially create the necessary <code>numpy</code>-native <code>ndarray</code> from a given list, so instead of using <code>ndarray()</code> at all, you could just use <code>array()</code>:</p>

<pre><code>result = np.array(d)  # same result as previous 2-lines
</code></pre>

<p>But further, <code>numpy</code>'s many functions for natively working with arrays (and array-like lists) already include things to do averages-of-many-vectors in a single step (where even the looping is hidden inside very-efficient compiled code or CPU bulk-vector operations). For example, there's a <code>mean()</code> function that can average lists of numbers, or multi-dimensional arrays of numbers, or aligned sets of vectors, and so forth.</p>

<p>This allows faster, clearer, one-liner approaches that can replace your entire original code with something like:</p>

<pre><code># get a list of available word-vetors
doc = [model[word] for word in text if word in model.vocab]
# average all those vectors
out = np.mean(doc, axis=0)
</code></pre>

<p>(Without the <code>axis</code> argument, it'd average together all individual dimension-values , in all slots, into just one single final average number.)</p>
",2,1,1825,2019-10-06 00:41:33,https://stackoverflow.com/questions/58253405/gensim-word2vec-model-outputs-1000-dimension-ndarray-but-the-maximum-number-of-n
How do you correctly cluster document names &amp; find similarities between documents based on Word2Vec model?,"<p>I have a set of documents (3000) which each contain a short description. I want to use Word2Vec model to see if I can cluster these documents based on the description. </p>

<p>I'm doing it the in the following way, but I am not sure if this is a ""good"" way to do it. Would love to get feedback.</p>

<p>I'm using Google's trained w2v model.</p>

<pre><code>wv = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,encoding=""ISO-8859-1"", limit = 100000)
</code></pre>

<p>Each document is split into words where stop words are removed, and I have used stemming as well.</p>

<p>My initial idea was to fetch the word vector for each word in each documents description, average it, and then cluster based on this. </p>

<pre><code>doc2vecs = []
for i in range(0, len(documents_df['Name'])):
    vec = [0 for k in range(300)] 
    for j in range(0, len(documents_df['Description'][i])):
        if documents_df['Description'][i][j] in wv:
            vec += wv[documents_df['Description'][i][j]]
    doc2vecs.append(vec/300)
</code></pre>

<p>I'm then finding similarities using</p>

<pre><code>similarities = squareform(pdist(doc2vecs, 'cosine'))
</code></pre>

<p>Which returns a matrix of the cosine between each vector in <code>doc2vec</code>.</p>

<p>I then try to cluster the documents. </p>

<pre><code>num_clusters = 2
km = cluster.KMeans(n_clusters=num_clusters)
km.fit(doc2vecs)
</code></pre>

<p>So basically what I am wondering is:</p>

<p>Is this method of clustering the average word vector for each word in the document a reasonable way to cluster the documents?</p>
","python, nlp, cluster-analysis, word2vec","<p>In 2019, unless you have serious resource constraints, you don't need to vectorize documents by averaging word embeddings. You can use Universal Sentence Encoder to vectorize documents <a href=""https://tfhub.dev/google/universal-sentence-encoder/2"" rel=""nofollow noreferrer"">in a few lines of code</a>.</p>
<p>Most clustering algorithms do better in low dimensions, so from here you want to do dimensionality reduction, then clustering. AFAIK, you'll get the best results from UMAP. <a href=""https://umap-learn.readthedocs.io/en/latest/clustering.html"" rel=""nofollow noreferrer"">Their docs</a> explain how to do this very clearly.</p>
",3,1,801,2019-10-08 18:07:27,https://stackoverflow.com/questions/58291846/how-do-you-correctly-cluster-document-names-find-similarities-between-document
How to view word2vec model,"<p>I just want to be able to see the values in my word2vec model.</p>

<p>I have a  very small corpus. I just want to see exactly what happens in each step for this particular corpus.</p>

<p>A section of my code is below.</p>

<pre class=""lang-py prettyprint-override""><code>word2vec = Word2Vec(corpus, min_count=1)
word_vectors = word2vec.wv 

termsim_index = WordEmbeddingSimilarityIndex(word_vectors)


dictionary = corpora.Dictionary(food)
bow_corpus = [dictionary.doc2bow(doc) for doc in food]


similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  
docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)

</code></pre>

<p>So I want to see what exactly is in <code>word_vectors</code>,<code>termsim_index</code>,<code>similarity_matrix</code> , <code>docsim_index</code></p>
","python, gensim, word2vec","<p>To see more of what's happening during each function, you should enable logging at the <code>INFO</code> level. </p>

<p>But then, each of your created objects have documented properties you can freely examine â€“ either by looking at the gensim docs per class, or using generic Python operations â€“ like those described in other SO questions, such as <a href=""https://stackoverflow.com/questions/192109/is-there-a-built-in-function-to-print-all-the-current-properties-and-values-of-a"">Is there a built-in function to print all the current properties and values of an object?</a>. </p>

<p>To give more specific suggestions, you'd have to explain more what exactly you ""want to see"".</p>
",1,0,259,2019-10-09 09:49:46,https://stackoverflow.com/questions/58301450/how-to-view-word2vec-model
How to save as a gensim word2vec file?,"<p>I have two lists, A is a list of words, for example [""hello"",""world"",......], Len(A) is 10000. List B contains the all pre-trained vectors corresponding to A, which is a [10000,512], 512 is the vector dimension. I want to convert two lists into gensim word2vec model format in order to load the model in later, such as <code>model = Word2Vec.load(""word2vec.model"")</code> how should I do this? </p>
","gensim, word2vec","<p>As you only have the words and their vectors, you don't quite have enough info for a full <code>Word2Vec</code> model (which includes other things like the internal neural network's hidden weights, and word frequencies). </p>

<p>But you can create a <code>gensim</code> <code>KeyedVectors</code> object, of the general kind that's in a <code>gensim</code> <code>Word2Vec</code> model <code>.wv</code> property. It has many of the helper methods (like <code>most_similar()</code>) you may be interested in using. </p>

<p>Let's assume your <em>A</em> list-of-words is in a more-helpfully named Python list called <code>words_list</code>, and your <em>B</em> list-of-vectors is in a more-helpfully named Python list called 'vectors_list`.</p>

<p>Try:</p>

<pre><code>from gensim.models import KeyedVectors
kv = new KeyedVectors(512)
kv.add(words_list, vectors_list)
kv.save(`mywordvecs.kvmodel`)
</code></pre>

<p>You could then later re-load these via:</p>

<pre><code>kv2 = KeyedVectors.load(`mywordvecs.kvmodel`)
</code></pre>

<p>(You could also use <code>save_word2vec_format()</code> and <code>load_word2vec_format()</code> instead of gensim's native <code>save()</code>/<code>load()</code>, if you wanted simpler plain-vectors formats that could also be loaded by other tools that use that format. But if you're staying within <code>gensim</code>, the plain <code>save()</code>/<code>load()</code> are just as good â€“ and would be better if saving a more complex trained <code>Word2Vec</code> model, because they'd retain the extra info those objects contain.)</p>
",6,4,9525,2019-10-15 10:58:26,https://stackoverflow.com/questions/58393090/how-to-save-as-a-gensim-word2vec-file
Is there an equivalent of word2vec for images?,"<p>I'm wondering if it would be possible to create dense vector representations for an image, similar to how you might create a word embedding with an algorithm like Word2Vec?</p>

<p>I understand that there are some big differences between text and image data â€“ specifically the fact that word2vec uses the word's context to train â€“ but I'm hoping to find a similar counterpart for images.</p>

<p>If a simplistic example of w2v (<a href=""https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469"" rel=""nofollow noreferrer"">from Allison Parrish's GitHub Gist</a>) is:</p>

<pre><code>            | cuteness (0-100) | size (0-100) |
|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|
| kitten    |        95        |     15       |
| tarantula |         8        |      3       |
| panda     |        75        |     40       |
| mosquito  |         1        |      1       |
| elephant  |        65        |     90       |
</code></pre>

<p>And another example being <code>king - man + woman = queen</code></p>

<p>Is there some analog (or way of creating some type of analog) for images where you might get something generally along these lines (with some made-up numbers):</p>

<pre><code>                             | amount of people | abstract-ness |
                             | in image (0-100) |    (0-100)    |
|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“|
| Starry Night               |         0        |       75      |
| Mona Lisa                  |         1        |        9      |
| American Gothic            |         2        |        7      |
| Garden of Earthly Delights |        80        |       50      |
| Les Demoiselles d'Avignon  |         5        |       87      |
</code></pre>

<p>(and just to clarify, know the actual vectors created by an algorithm like Word2Vec wouldn't cleanly fit into human-interpretable categories, but I just wanted to give an analogy to the Word2Vec example.)</p>

<p>or <code>(starry night) - (landscape) + (man) = (van Gogh self portrait)</code> or <code>= (abstract self portrait)</code> or something generally along those lines.</p>

<p>Those might not be the best examples but just to recap, I'm looking for some sort of algorithm for creating an abstract n-dimensional learned representation for an image that can be grouped or compared with vectors representing other images.</p>

<p>Thanks for your help!</p>
","image-processing, deep-learning, word2vec, word-embedding","<p>Absolutely! But...</p>

<p>Such models tend to need significantly <em>larger</em> and <em>deeper</em> neural-networks to learn the representations. </p>

<p>Word2vec uses a very-shallow network, and performs a simple prediction of neighboring-words, often from a tightly limited vocabulary, as the training-goal which (as a beneficial side-effect) throws off compact vectors for each word. </p>

<p>Image-centric algorithms instead try to solve labeling/classification tasks, or regenerate original images under compressed-representation (or adversarial-classifier) constraints. They use 'convolutions' or other multi-layer constructs to intepret the much-larger space of possible pixel values, and some of the interim neural-network layers can be interpretable as compact vectors for the input images.  </p>

<p>Note that even in textual word2vec, the individual ""dense embedding"" dimensions, learned in an unsupervised fashion, <strong>don't</strong> have neat human-interpretability (like ""bigness"", ""cuteness"", etc.). Often, certain directions/neighborhood of the high-dimensional space are vaguely-interpretable, but they're not precise nor aligned exactly with major dimension axes. </p>

<p>Similarly, any compact representations from deep-neural-network image-modeling won't inherently have individual dimensions with clear meanings (unless specific extra constraints with those goals were designed-in) â€“ but again, certain directions/neighborhoods of the high-dimensional space tend to be meaningful (""crowds"", ""a car"", ""smiles"", etc). </p>

<p>A nice overview of some key papers in deep-learning based image-analysis â€“ the kinds of algorithms which throw off compact &amp; meaningful vector summaries of images â€“ that I just found is at:</p>

<p><a href=""https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"" rel=""nofollow noreferrer"">https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a></p>
",2,4,2394,2019-10-15 18:54:36,https://stackoverflow.com/questions/58401016/is-there-an-equivalent-of-word2vec-for-images
how to load a vector of certrain word form word2vec saved model?,"<p>how can i find a respective words vector from previous trained word2vec model?</p>

<pre><code>data = {'one': array([-0.06590105,  0.01573388,  0.00682817,  0.53970253, -0.20303348,
   -0.24792041,  0.08682659, -0.45504045,  0.89248925,  0.0655603 ,
   ......
   -0.8175681 ,  0.27659689,  0.22305458,  0.39095637,  0.43375066,
    0.36215973,  0.4040089 , -0.72396156,  0.3385369 , -0.600869  ],
  dtype=float32),
 'two': array([ 0.04694849,  0.13303463, -0.12208422,  0.02010536,  0.05969441,
   -0.04734801, -0.08465996,  0.10344813,  0.03990637,  0.07126121,
    ......
    0.31673026,  0.22282903, -0.18084198, -0.07555179,  0.22873943,
   -0.72985399, -0.05103955, -0.10911274, -0.27275378,  0.01439812],
  dtype=float32),
 'three': array([-0.21048863,  0.4945509 , -0.15050395, -0.29089224, -0.29454648,
    0.3420335 , -0.3419629 ,  0.87303966,  0.21656844, -0.07530259,
    ......
   -0.80034876,  0.02006451,  0.5299498 , -0.6286509 , -0.6182588 ,
   -1.0569025 ,  0.4557548 ,  0.4697938 ,  0.8928275 , -0.7877308 ],
  dtype=float32),
  'four': ......
}
</code></pre>

<p>now i want to obtain like </p>

<pre><code>word = ""one""
wordvector = data.get_vector(word)
</code></pre>

<p>and returns </p>

<pre><code>[-0.06590105,  0.01573388,  0.00682817,  0.53970253, -0.20303348,
   -0.24792041,  0.08682659, -0.45504045,  0.89248925,  0.0655603 ,
   ......
   -0.8175681 ,  0.27659689,  0.22305458,  0.39095637,  0.43375066,
    0.36215973,  0.4040089 , -0.72396156,  0.3385369 , -0.600869  ]
</code></pre>
","machine-learning, nlp, word2vec","<pre><code>one_array = data['one']
</code></pre>

<p><code>data</code>is a dictionary. To get the value of a dictionary for a certain key, you call <code>value = dict[key]</code>.
With <code>one_list = data['one'].tolist()</code>, you get the wordvector of the word 'one' as a list, which seems to be your expected output.</p>
",0,0,29,2019-10-20 12:40:03,https://stackoverflow.com/questions/58473063/how-to-load-a-vector-of-certrain-word-form-word2vec-saved-model
Iterating Over Numpy Array for NLP Application,"<p>I have a Word2Vec model that I'm building where I have a vocab_list of about 30k words.  I have a list of sentences (sentence_list) about 150k large.  I am trying to remove tokens (words) from the sentences that weren't included in vocab_list.  The task seemed simple, but nesting for loops and reallocating memory is slow using the below code.  This task took approx. 1hr to run so I don't want to repeat it.  </p>

<p>Is there a cleaner way to try this? </p>

<pre><code>import numpy as np
from datetime import datetime

start=datetime.now()
timing=[]
result=[]
counter=0
for sent in sentences_list:
    counter+=1
    if counter %1000==0 or counter==1:
        print(counter, 'row of', len(sentences_list), ' Elapsed time: ', datetime.now()-start)
        timing.append([counter, datetime.now()-start])
    final_tokens=[]
    for token in sent:
        if token in vocab_list:
            final_tokens.append(token)
    #if len(final_tokens)&gt;0:
    result.append(final_tokens)
print(counter, 'row of', len(sentences_list),' Elapsed time: ', datetime.now()-start)
timing.append([counter, datetime.now()-start])
sentences=result
del result
timing=pd.DataFrame(timing, columns=['Counter', 'Elapsed_Time'])
</code></pre>
","python, numpy, nlp, word2vec","<p>Note that typical word2vec implementations (like Google's original <code>word2vec.c</code> or <code>gensim</code> <code>Word2Vec</code>) will often just ignore words in their input that aren't part of their established vocabulary (as specified by <code>vocab_list</code> or enforced via a <code>min_count</code>). So you may not need to perform this filtering at all. </p>

<p>Using a more-idiomatic Python list-comprehension <em>might</em> be noticeably faster (and would certainly be more compact). Your code could simply be:</p>

<pre><code>filtered_sentences = [ 
    [word for word in sent if word in vocab_list] 
    for sent in sentences_list
]
</code></pre>
",1,0,329,2019-10-24 14:15:43,https://stackoverflow.com/questions/58543321/iterating-over-numpy-array-for-nlp-application
How to save self-trained word2vec to a txt file with format like &#39;word2vec-google-news&#39; or &#39;glove.6b.50d&#39;,"<p>I wonder that how can I save a self-trained word2vec to txt file with the format like 'word2vec-google-news' or 'glove.6b.50d' which has the tokens followed by matched vectors.<a href=""https://i.sstatic.net/YdCbv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YdCbv.png"" alt="".""></a></p>

<p>I export my self-trained vectors to txt file which only has vectors but no tokens in the front of those vectors.
<a href=""https://i.sstatic.net/gu2yr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gu2yr.png"" alt=""enter image description here""></a></p>

<p>My code for training my own word2vec:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import math
import random
import numpy as np
from six.moves import xrange
import zipfile
import tensorflow as tf
import pandas as pd

filename = ('data/data.zip')

# Step 1: Read the data into a list of strings.
def read_data(filename):
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

words = read_data(filename)
#print('Data size', len(words))


# Step 2: Build the dictionary and replace rare words with UNK token.
vocabulary_size = 50000
def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    #print(""count"",len(count))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary

data, count, dictionary, reverse_dictionary = build_dataset(words)

#del words  # Hint to reduce memory.
#print('Most common words (+UNK)', count[:5])
#print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])



data_index = 0

# Step 3: Function to generate a training batch for the skip-gram model.
def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1  # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
#for i in range(8):
 #print(batch[i], reverse_dictionary[batch[i]],'-&gt;', labels[i, 0], reverse_dictionary[labels[i, 0]])

# Step 4: Build and train a skip-gram model.
batch_size = 128
embedding_size = 128
skip_window = 2
num_skips = 2
valid_size = 9
valid_window = 100
num_sampled = 64    # Number of negative examples to sample.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)

graph = tf.Graph()
with graph.as_default():
    # Input data.
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Ops and variables pinned to the CPU because of missing GPU implementation
    with tf.device('/cpu:0'):
        # Look up embeddings for inputs.
        embeddings = tf.Variable(
            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
        embed = tf.nn.embedding_lookup(embeddings, train_inputs)

        # Construct the variables for the NCE loss
        nce_weights = tf.Variable(
            tf.truncated_normal([vocabulary_size, embedding_size],
                                stddev=1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32)

    # Compute the average NCE loss for the batch.
    # tf.nce_loss automatically draws a new sample of the negative labels each
    # time we evaluate the loss.
    loss = tf.reduce_mean(
            tf.nn.nce_loss(weights=nce_weights,biases=nce_biases, inputs=embed, labels=train_labels,
                 num_sampled=num_sampled, num_classes=vocabulary_size))

    # Construct the SGD optimizer using a learning rate of 1.0.
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

    # Compute the cosine similarity between minibatch examples and all embeddings.
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)

    # Add variable initializer.
    init = tf.global_variables_initializer()

# Step 5: Begin training.
num_steps = 20000

with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    #print(""Initialized"")

    average_loss = 0
    for step in xrange(num_steps):
        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

        # We perform one update step by evaluating the optimizer op (including it
        # in the list of returned values for session.run()
        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
        average_loss += loss_val

        #if step % 2000 == 0:
         #   if step &gt; 0:
          #      average_loss /= 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
           # print(""Average loss at step "", step, "": "", average_loss)
            #average_loss = 0

    final_embeddings = normalized_embeddings.eval()


np.savetxt('data/w2v.txt', final_embeddings)
</code></pre>
","machine-learning, nlp, word2vec, word-embedding","<p>You may want to look at the implementation of <code>_save_word2vec_format()</code> in <code>gensim</code> for an example of Python code which writes that format:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/utils_any2vec.py#L104"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/utils_any2vec.py#L104</a></p>

<pre class=""lang-py prettyprint-override""><code>def _save_word2vec_format(fname, vocab, vectors, fvocab=None, binary=False, total_vec=None):
    """"""Store the input-hidden weight matrix in the same format used by the original
    C word2vec-tool, for compatibility.
    Parameters
    ----------
    fname : str
        The file path used to save the vectors in.
    vocab : dict
        The vocabulary of words.
    vectors : numpy.array
        The vectors to be stored.
    fvocab : str, optional
        File path used to save the vocabulary.
    binary : bool, optional
        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.
    total_vec : int, optional
        Explicitly specify total number of vectors
        (in case word vectors are appended with document vectors afterwards).
    """"""
    if not (vocab or vectors):
        raise RuntimeError(""no input"")
    if total_vec is None:
        total_vec = len(vocab)
    vector_size = vectors.shape[1]
    if fvocab is not None:
        logger.info(""storing vocabulary in %s"", fvocab)
        with utils.open(fvocab, 'wb') as vout:
            for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
                vout.write(utils.to_utf8(""%s %s\n"" % (word, vocab_.count)))
    logger.info(""storing %sx%s projection weights into %s"", total_vec, vector_size, fname)
    assert (len(vocab), vector_size) == vectors.shape
    with utils.open(fname, 'wb') as fout:
        fout.write(utils.to_utf8(""%s %s\n"" % (total_vec, vector_size)))
        # store in sorted order: most frequent words at the top
        for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
            row = vectors[vocab_.index]
            if binary:
                row = row.astype(REAL)
                fout.write(utils.to_utf8(word) + b"" "" + row.tostring())
            else:
                fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
</code></pre>
",1,0,2714,2019-10-28 08:59:27,https://stackoverflow.com/questions/58588057/how-to-save-self-trained-word2vec-to-a-txt-file-with-format-like-word2vec-googl
Finding both target and center word2vec matrices,"<p>I've read and heard(In the CS224 of Stanford) that the Word2Vec algorithm actually trains two matrices(that is, two sets of vectors.) These two are the U and the V set, one for words being a target and one for words being the context. The final output is the average of these two.
I have two questions in mind. one is that:  </p>

<ul>
<li><p>Why do we get an average of two vectors? Why it makes sense? Don't we lose some information?  </p></li>
<li><p>The second question is, using pre-trained word2vec models, how can I get access to both matrices? Is there any downloadable word2vec with both sets of vectors? I don't have enough resources to train a new one.</p></li>
</ul>

<p>Thanks</p>
","nlp, word2vec, word-embedding","<p>That relayed description isn't quite right. The word-vectors traditionally retrieved from a word2vec model come from a ""projection matrix"" which converts individual words to a right-sized input-vector for the shallow neural network. </p>

<p>(You could think of the projection matrix as turning a one-hot encoding into a dense-embedding for that word, but libraries typically implement this via a dictionary-lookup â€“ eg: ""what row of the vectors-matrix should I consult for this word-token?"")</p>

<p>There's another matrix of weights leading to the model's output nodes, whose interpretation varies based on the training mode. In the common default of negative-sampling, there's one node per known word, so you could also interpret this matrix as having a vector per word. (In hierarchical-softmax mode, the known-words aren't encoded as single output nodes, so it's harder to interpret the relationship of this matrix to individual words.)</p>

<p>However, this second vector per word is rarely made directly available by libraries. Most commonly, the word-vector is considered simply the trained-up input vector, from the projection matrix. For example, the export format from Google's original <code>word2vec.c</code> release only saves-out those vectors, and the large ""GoogleNews"" vector set they released only has those vectors. (There's no averaging with the other output-side representation.)</p>

<p>Some work, especially that of Mitra et all of Microsoft Research (in ""<a href=""https://www.microsoft.com/en-us/research/project/dual-embedding-space-model-desm/"" rel=""nofollow noreferrer"">Dual Embedding Space Models</a>"" &amp; associated writeups) has noted those output-side vectors may be of value in some applications as well â€“ but I haven't seen much other work using those vectors. (And, even in that work, they're not <em>averaged</em> with the traditional vectors, but consulted as a separate option for some purposes.)</p>

<p>You'd have to look at the code of whichever libraries you're using to see if you can fetch these from their full post-training model representation. In the Python <code>gensim</code> library, this second matrix in the negative-sampling case is a model property named <code>syn1neg</code>, following the naming of the original <code>word2vec.c</code>.</p>
",0,0,156,2019-10-30 15:13:03,https://stackoverflow.com/questions/58628480/finding-both-target-and-center-word2vec-matrices
Is it possible to trace back words to its original doc in doc2vec?,"<p>I would love to make a doc2vec/word2vec dataset that is able to traceback or remember its original placement. For now I would love to know in which row or txt file it came from but in the future even its original paragraph. For example, I would love to be able to do it with multiple txt files or an csv. </p>

<p>Searching for similar codes or ideas like these did not do any justice. So Iâ€™m curious if someone else knows how or if it even would be even possible at all to; <b>embed or let the words remember its original location(document).</b></p>

<p><b>Example Input:</b></p>

<pre><code>        Author    |   Title   |     d2v_text     
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”                  
0          Name 1 |  Title 1  | this is the first text. first text paragraph.    
1          Name 2 |  Title 2  | this is the second text. second text paragraph.
2          Name 3 |  Title 3  | this is the thirth text. thirth text paragraph.

Name1Title1.txt  (this is the first text. first text paragraph) 
Name2Title2.txt  (this is the second text. second text paragraph)
Name3Title3.txt  (this is the thirth text. thirth text paragraph)

</code></pre>

<p><b>Example output:</b></p>

<pre><code>(â€˜secondâ€™, 0.2384900293, â€˜Name2Title2â€™)
(â€˜textâ€™,0.34948302,â€™Name1Title1,Name2Title2,Name3Title3â€™) 

w1 = [â€œtextâ€]
model.wv.most_similar (positive=w1,topn=1)

[(â€˜secondâ€™, 0.2384900293, â€˜Name2Title2â€™)]
</code></pre>

<p>What I would like to achieve is that, when loading and printing a certain vector from a dataset that it would know its original document. Could someone help me with this?</p>
","python, word2vec, doc2vec","<p>These models don't store their training data â€“ they just observe it during each of the training passes, to build their vector-models for each word or doc. </p>

<p>For <code>Doc2Vec</code> doc-vectors, it's traditional to name the doc-vector with some unique key to the original document, such as an ID-number or filename. So for doc-vectors, the tag reported with results probably already provides the key you need. </p>

<p>For words, when you want a list of all documents a word appears in, the two traditional approaches are:</p>

<ul>
<li><p>brute-force scan, as with the command-line program <code>grep</code>, where you look over every word in every document and return the list of documents where the word appeared. For example, if you had a directory with your <code>Name1Title1.txt</code> etc files, the command <code>grep -l -E '(^|\W)second(\W|$)' *.txt</code> would print those files containing the word <code>second</code>. Of course, this is very slow over a large corpus.</p></li>
<li><p>building an <em><a href=""https://en.wivipedia.org/wiki/Inverted_index"" rel=""nofollow noreferrer"">inverted index</a></em> of which documents contain which words. Then, after the cost of one scan and building/storing the index, finding the list of docs for any word is very quick. This is the foundational technique enabling full-text search engines. </p></li>
</ul>

<p>A simple inverted index takes just a few lines of Python:</p>

<pre class=""lang-py prettyprint-override""><code>from collections import defaultdict
docs = (  # tuples of doc-name, words
    ('Name1Title1.txt', ""this is the first text. first text paragraph"".split()),
    ('Name2Title2.txt', ""this is the second text. second text paragraph"".split()),
    ('Name3Title3.txt', ""this is the thirth text. thirth text paragraph"".split()),
)
inv_index = defaultdict(list)
for title, words in docs:
    for word in set(words):
        inv_index[word].append(title)
</code></pre>

<p>Then, the list of any docs where a word appeared is a simple lookup:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; inv_index['second']
['Name2Title2.txt']
&gt;&gt;&gt; inv_index['this']
['Name1Title1.txt', 'Name2Title2.txt', 'Name3Title3.txt']
</code></pre>

<p>Inverted indexes can be quite large, and so often use other datastructures for compactness, including file-based indexes â€“ so if your corpus is large, you may need to research other libraries for inverted-indexing to be able to build your index and do your lookups in a practical way.</p>
",2,1,194,2019-11-01 17:44:33,https://stackoverflow.com/questions/58663594/is-it-possible-to-trace-back-words-to-its-original-doc-in-doc2vec
word2vec - KeyError: &quot;word X not in vocabulary&quot;,"<p>Using the <code>Word2Vec</code> implementation of the module <code>gensim</code> in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word <code>happy</code> is defined in the vocabulary, getting the error <code>KeyError: ""word 'happy' not in vocabulary""</code>. Tried to apply the given the answers to <a href=""https://stackoverflow.com/questions/41133844/keyerror-word-word-not-in-vocabulary-in-word2vec"">a similar question</a>, but did not work. Hence, posted my own question.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    # When I debug, both of the words 'happy' and 'birthday' exist in the variable 'data'
    word2vec = Word2Vec(data, min_count=5, size=10000, window=5, workers=4)

    # Print result
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru word2vec: {word2vec.similarity(word_1, word_2)}')
except Exception as err:
    print(f'An error happened! Detail: {str(err)}')
</code></pre>
","gensim, word2vec, word-embedding","<p>When you get a ""not in vocabulary"" error like this from <code>Word2Vec</code>, you can trust it: <code>'happy'</code> really isn't in the model. </p>

<p>Even if your visual check shows <code>'happy'</code> inside your file, a few reasons why it might not wind up inside the model include:</p>

<ul>
<li><p>it doesn't occur at least <code>min_count=5</code> times</p></li>
<li><p>the <code>data</code> format isn't correct for <code>Word2Vec</code>, so it's not seeing the words you expect it to see. </p></li>
</ul>

<p>Looking at how <code>data</code> is prepared by your code, it looks like a giant list of all words in your file. <code>Word2Vec</code> instead expects a sequence that has, as each item, a list-of-words for that one text. So: not a list-of-words, but a list where each item is a list-of-words. </p>

<p>If you've supplied...</p>

<pre class=""lang-py prettyprint-override""><code>[
  'happy',
  'birthday',
]
</code></pre>

<p>...instead of the expected...</p>

<pre class=""lang-py prettyprint-override""><code>[
  ['happy', 'birthday',],
]
</code></pre>

<p>...those single-word-strings will be seen a lists-of-characters, so <code>Word2Vec</code> will think you want to learn word-vectors for a bunch of one-character words. You can check if this has affected your model by seeing if the vocabulary size seems small (<code>len(model.wv)</code>) or if a sample of learned-words is only single-character words ('model.wv.index2entity[:10]`).</p>

<p>If you supply a word in the right format, at least <code>min_count</code> times, as part of the training-data, it will wind up with a vector in the model.</p>

<p>(Separately: <code>size=10000</code> is a choice way outside the usual range of 100-400. I've never seen a project using such high-dimensionality for word-vectors, and it would only be theoretically justifiable if you had a massively-large vocabulary and training-set. Oversized vectors with smaller vocabularies/data are likely to create uselessly overfit results.)</p>
",2,0,3011,2019-11-01 22:45:31,https://stackoverflow.com/questions/58666699/word2vec-keyerror-word-x-not-in-vocabulary
How to add words and vectors manually to Word2vec gensim?,"<p>Let's say, <strong>word2vec.model</strong> is my trained word2vec model. When a out-of-vocabulary word (<strong>oov_word</strong>) occurs, I compute a vector <strong>vec</strong> using <em>compute_vec(oov_word)</em> method. Now, I want to add/append <strong>oov_word</strong> and its corresponding vector <strong>vec</strong> to my already trained model <strong>word2vec.model</strong>.</p>

<p>I have already checked the below links. But they do not answer my question.</p>

<p><a href=""https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models"">Combining/adding vectors from different word2vec models</a></p>

<p><a href=""https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"">https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words</a></p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add</a></p>
","gensim, word2vec","<pre><code>from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors
vector_length = 100
kv = WordEmbeddingsKeyedVectors(vector_length)

# wordList - list of words
# vectorList - list of the vector corresponding to the words

kv.add(wordList, vectorList)

kv.most_similar(word1) # gives the list of words similar to word1
</code></pre>
",3,3,4785,2019-11-05 13:55:28,https://stackoverflow.com/questions/58712856/how-to-add-words-and-vectors-manually-to-word2vec-gensim
&#39;similar_by_word&#39; did not improve over iterations,"<p>I'm using Gensim to train a skip-gram word2vec model. The dataset has 1 million sentences, but the vocabulary is of size 200. I would like to see the model accuracy over iterations, so I used <code>model.wv.similar_by_word</code> in the callback function to see the scores. But the returned values were not updated over iterations.</p>

<p>The <code>iter</code> was set to be <code>100</code>.
I tried to change the values of <code>window</code> and <code>size</code>, but it has no effect.</p>

<p>The model was initialized with callbacks:</p>

<pre class=""lang-py prettyprint-override""><code>Word2Vec(self.train_corpus, workers=multiprocessing.cpu_count(), compute_loss=True, callbacks=[A_CallBack], **word2vec_params)
</code></pre>

<p>In the class <code>A_CallBack</code>, I have something like this:</p>

<pre class=""lang-py prettyprint-override""><code>def on_epoch_end(self, model):
    word, score = model.wv.similar_by_word(word='target_word', topn=1)[0]
    print(word, score)
</code></pre>

<p>The <code>word</code> and <code>score</code> were printed out for every epoch, but the values have never changed.</p>

<p>I was expecting the values of them to be updated over iterations, which should make sense?</p>

<p>I'm new to machine learning and word2vec. Thanks a lot for the help.</p>
","python, gensim, word2vec","<p>The various <code>gensim</code> similarity functions are optimized via the pre-calculation of unit-length normed vectors, and that pre-calculation is cached in a way that doesn't expect further training to happen. </p>

<p>As a result, when you first check similarities mid-training, as you've done with your callback code, the cache gets filled with the model's early state â€“ and not refreshed after later training. There's a <a href=""https://github.com/RaRe-Technologies/gensim/issues/2260"" rel=""nofollow noreferrer"">pending bug</a> (as of <code>gensim-3.8.1</code> in November 2019) to fix this behavior, in the meantime, you can either:</p>

<ul>
<li>refrain from checking similarity-operations until after training is done, or</li>
<li>manually clear some of the caches after you've done more training. For a plain <code>Word2Vec</code> model, it should be enough to do: <code>model.wv.vectors_norm = None</code>. (Some other models require extra steps, see the bug discussion for more details.)</li>
</ul>
",0,0,349,2019-11-07 17:50:21,https://stackoverflow.com/questions/58754450/similar-by-word-did-not-improve-over-iterations
How to free the memory taken by a pyspark model (JavaModel)?,"<p>As described, I load a trained word2vec model through pyspark. </p>

<pre><code>word2vec_model = Word2VecModel.load(""saving path"")
</code></pre>

<p>After using that, I want to delete it since it will take much memory space on single node (I used the findSynonyms function, and the doc says it should be local used only)
I tried to use</p>

<pre><code>del word2vec_model
gc.collect()
</code></pre>

<p>but it seems that doesn't word. And it's not an rdd file, I can't use .unpersist(). I didn't find any like unload() fuction in the doc.</p>

<p>Anyone could help me or give me some advice?</p>
","python, apache-spark, pyspark, nlp, word2vec","<p>You can ensure that the object is dereferenced by the py4j gateway by running the following statement:</p>

<p>Given <code>word2vec_model</code> a pyspark <code>Transformer</code>:</p>

<ul>
<li>Given <code>spark</code> a <code>SparkSession</code>:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>spark.sparkContext._gateway.detach(word2vec_model._java_obj)
</code></pre>

<ul>
<li>... or given <code>sc</code> a <code>SparkContext</code>:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>sc._gateway.detach(word2vec_model._java_obj)
</code></pre>

<p>Explanations:</p>

<ol>
<li><strong>Access underlying wrapper object</strong>: Your model is a pyspark <code>Transformer</code> and each transformer holds an instance of <code>JavaObject</code> in a private <code>_java_obj</code> attribute. </li>
<li><strong>Access the <code>SparkContext</code>'s py4j gateway</strong>.</li>
<li>Use the gateway's <code>detach</code> method on the wrapper object (instance of <code>JavaObject</code>)</li>
</ol>
",2,0,453,2019-11-08 03:26:01,https://stackoverflow.com/questions/58759929/how-to-free-the-memory-taken-by-a-pyspark-model-javamodel
Is there any Polish implementation for similar words in word2vec?,"<p>I found GoogleNews-vectors-negative300.bin library, but only for ENG words, Is there any Polish implementation for similar words in word2vec?</p>

<p>I have already tried using cc.pl.300.bin and NKJP-PodkorpusMilionowy libraries...</p>

<pre><code>    public  Word2Vec getWord2Vec() {
        File gModel = new File(""C:/Users/user/Desktop/GoogleNews-vectors-negative300.bin.gz"");
        return WordVectorSerializer.readWord2VecModel(gModel);
    }
</code></pre>
","java, nlp, word2vec, polish","<p>The file...</p>

<p><a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pl.vec"" rel=""nofollow noreferrer"">https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pl.vec</a></p>

<p>...as linked from...</p>

<p><a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a></p>

<p>...may work for you, if your library loads the simple 'text' format for exchanging word-vectors. (It's not in the Facebook FastText-specific binary format, as your <code>cc.pl.300.bin</code> file was.)</p>
",1,0,292,2019-11-08 13:47:04,https://stackoverflow.com/questions/58767797/is-there-any-polish-implementation-for-similar-words-in-word2vec
Question pairs (ground truth) datasets for Word2Vec model testing?,"<p>I'm looking for test datasets to optimize my Word2Vec model. I have found a good one from gensim:</p>

<p>gensim/test/test_data/questions-words.txt </p>

<p>Does anyone know other similar datasets?</p>

<p>Thank you!</p>
","machine-learning, nlp, word2vec, word-embedding","<p>It is important to note that there isn't really a ""ground truth"" for word-vectors. There are interesting tasks you can do with them, and some arrangements of word-vectors will be better on a specific tasks than others.</p>

<p>But also, the word-vectors that are best on one task â€“ such as analogy-solving in the style of the <code>questions-words.txt</code> problems â€“ might not be best on another important task â€“ like say modeling texts for classification or info-retrieval.</p>

<p>That said, you can make your own test data in the same format as <code>questions-words.txt</code>.  Google's original <code>word2vec.c</code> release, which also included a tool for statistically combining nearby words into multi-word phrases, also included a <a href=""https://github.com/tmikolov/word2vec/blob/master/questions-phrases.txt"" rel=""nofollow noreferrer""><code>questions-phrases.txt</code></a> file, in the same format, that can be used to test word-vectors that have been similarly constructed for 'words' that are actually short multiple-word phrases. </p>

<p>The Python <code>gensim</code> word-vectors support includes an extra method, <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer""><code>evaluate_word_pairs()</code></a> for checking word-vectors not on analogy-solving but on conformance to collections of human-determined word-similarity-rankings. The documentation for that method includes a link to an appropriate test-set for that method, <a href=""https://fh295.github.io//simlex.html"" rel=""nofollow noreferrer""><code>SimLex-999</code></a>, and you may be able to find other test sets of the same format elsewhere. </p>

<p>But, again, none of these should be considered the absolute test of word-vectors' overall quality. The best test, for your particular project's use of word-vectors, would be some repeatable domain-specific evaluation score you devise yourself, that's inherently correlated to your end goals.</p>
",0,-2,495,2019-11-08 17:40:47,https://stackoverflow.com/questions/58771410/question-pairs-ground-truth-datasets-for-word2vec-model-testing
Word2Vec Subsampling -- Implementation,"<p>I am implementing the <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">Skipgram</a> model, both in Pytorch and Tensorflow2. I am having doubts about the implementation of subsampling of frequent words. Verbatim from the paper, the probability of subsampling word <code>wi</code> is computed as</p>

<p><a href=""https://i.sstatic.net/Eq2u8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Eq2u8.png"" alt=""enter image description here""></a></p>

<p>where <code>t</code> is a custom threshold (usually, a small value such as <em>0.0001</em>) and <code>f</code> is the frequency of the word in the document. Although the authors implemented it in a different, but almost equivalent way, let's stick with this definition.</p>

<p>When computing the <code>P(wi)</code>, we can end up with negative values. For example, assume we have 100 words, and one of them appears extremely more often than others (as it is the case for my dataset).</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import seaborn as sns

np.random.seed(12345)

# generate counts in [1, 20]
counts = np.random.randint(low=1, high=20, size=99)

# add an extremely bigger count
counts = np.insert(counts, 0, 100000)

# compute frequencies
f = counts/counts.sum()

# define threshold as in paper
t = 0.0001

# compute probabilities as in paper
probs = 1 - np.sqrt(t/f)
sns.distplot(probs);
</code></pre>

<p><strong>Q: What is the correct way to implement subsampling using this ""probability""?</strong></p>

<p>As an additional info, I have seen that in <a href=""https://keras.io/preprocessing/sequence/"" rel=""nofollow noreferrer"">keras</a> the function <code>keras.preprocessing.sequence.make_sampling_table</code> takes a different approach: </p>

<pre class=""lang-py prettyprint-override""><code>def make_sampling_table(size, sampling_factor=1e-5):
    """"""Generates a word rank-based probabilistic sampling table.
    Used for generating the `sampling_table` argument for `skipgrams`.
    `sampling_table[i]` is the probability of sampling
    the i-th most common word in a dataset
    (more common words should be sampled less frequently, for balance).
    The sampling probabilities are generated according
    to the sampling distribution used in word2vec:
    ```
    p(word) = (min(1, sqrt(word_frequency / sampling_factor) /
        (word_frequency / sampling_factor)))
    ```
    We assume that the word frequencies follow Zipf's law (s=1) to derive
    a numerical approximation of frequency(rank):
    `frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))`
    where `gamma` is the Euler-Mascheroni constant.
    # Arguments
        size: Int, number of possible words to sample.
        sampling_factor: The sampling factor in the word2vec formula.
    # Returns
        A 1D Numpy array of length `size` where the ith entry
        is the probability that a word of rank i should be sampled.
    """"""
    gamma = 0.577
    rank = np.arange(size)
    rank[0] = 1
    inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1. / (12. * rank)
    f = sampling_factor * inv_fq

    return np.minimum(1., f / np.sqrt(f))
</code></pre>
","keras, word2vec, tf.keras, subsampling","<p>I tend to trust deployed code more than paper write-ups, especially in a case like word2vec, where the original authors' <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L407"" rel=""nofollow noreferrer""><code>word2vec.c</code> code</a> released by the paper's authors has been widely used &amp; served as the template for other implementations. If we look at its subsampling mechanism...</p>

<pre class=""lang-c prettyprint-override""><code>        if (sample &gt; 0) {
          real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;
          next_random = next_random * (unsigned long long)25214903917 + 11;
          if (ran &lt; (next_random &amp; 0xFFFF) / (real)65536) continue;
        }
</code></pre>

<p>...we see that those words with tiny counts (<code>.cn</code>) that could give negative values in the original formula instead here give values greater-than <code>1.0</code>, and thus can never be less than the <code>long</code>-random-masked-and-scaled to never be more than <code>1.0</code> (<code>(next_random &amp; 0xFFFF) / (real)65536</code>). So, it seems the authors' intent was for all negative-values of the original formula to mean ""never discard"". </p>

<p>As per the <em>keras</em> <code>make_sampling_table()</code> comment &amp; implementation, they're <strong>not</strong> consulting the actual word-frequencies at all. Instead, they're assuming a Zipf-like distribution based on word-rank order to synthesize a simulated word-frequency. </p>

<p>If their assumptions were to hold â€“ the related words are from a natural-language corpus with a Zipf-like frequency-distribution â€“ then I'd expect their sampling probabilities to be close to down-sampling probabilities that would have been calculated from true frequency information. And that's probably ""close enough"" for most purposes.</p>

<p>I'm not sure why they chose this approximation. Perhaps other aspects of their usual processes have not maintained true frequencies through to this step, and they're expecting to always be working with natural-language texts, where the assumed frequencies will be generally true. </p>

<p>(As luck would have it, and because people often want to impute frequencies to public sets of word-vectors which have dropped the true counts but are still sorted from most- to least-frequent, just a few days ago I wrote <a href=""https://stackoverflow.com/a/58737377/130288"">an answer about simulating a fake-but-plausible distribution using Zipf's law</a> â€“ similar to what this keras code is doing.)</p>

<p>But, if you're working with data that <strong>doesn't</strong> match their assumptions (as with your synthetic or described datasets), their sampling-probabilities will be quite different than what you would calculate yourself, with any form of the original formula that uses true word frequencies. </p>

<p>In particular, imagine a distribution with one token a million times, then a hundred tokens all appearing just 10 times each. Those hundred tokens' order in the ""rank"" list is arbitrary â€“ truly, they're all tied in frequency. But the simulation-based approach, by fitting a Zipfian distribution on that ordering, will in fact be sampling each of them very differently. The one 10-occurrence word lucky enough to be in the 2nd rank position will be far more downsampled, as if it were far more frequent. And the 1st-rank ""tall head"" value, by having its true frequency *under-*approximated, will be less down-sampled than otherwise. Neither of those effects seem beneficial, or in the spirit of the frequent-word-downsampling option - which should only ""thin out"" very-frequent words, and in all cases leave words of the same frequency as each other in the original corpus roughly equivalently present to each other in the down-sampled corpus. </p>

<p>So for your case, I would go with the original formula (probability-of-discarding-that-requires-special-handling-of-negative-values), or the <code>word2vec.c</code> practical/inverted implementation (probability-of-keeping-that-saturates-at-1.0), rather than the keras-style approximation. </p>

<p>(As a totally-separate note that nonetheless may be relevant for your dataset/purposes, if you're using negative-sampling: there's another parameter controlling the relative sampling of negative examples, often fixed at <code>0.75</code> in early implementations, that <a href=""https://arxiv.org/abs/1804.04212"" rel=""nofollow noreferrer"">one paper has suggested can usefully vary for non-natural-language token distributions &amp; recommendation-related end-uses</a>. This parameter is named <code>ns_exponent</code> in the Python <code>gensim</code> implementation, but simply <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L55"" rel=""nofollow noreferrer"">a fixed <code>power</code> value internal to a sampling-table pre-calculation in the original <code>word2vec.c</code> code</a>.)</p>
",4,2,3246,2019-11-08 19:35:17,https://stackoverflow.com/questions/58772768/word2vec-subsampling-implementation
how to use Merge in kerase&gt;2.0?,"<pre><code>word_model=Sequential() word_model.add(Embedding(vocab_size,embed_size,               embeddings_initializer=""glorot_uniform"",               input_length=1)) word_model.add(Reshape((embed_size,)))

context_model=Sequential() context_model.add(Embedding(vocab_size,embed_size,               embeddings_initializer=""glorot_uniform"",               input_length=1)) context_model.add(Reshape((embed_size,)))

model=Sequential()model.add(Merge([word_model,context_model],mode=""dot"")) model.add(Dense(1,init=""glorot_unifor"",activation=""sigmod"")) model.compile(loss=""mean_squared_error"",optimizer=""adam"")
</code></pre>

<p>how to change those in keras2, which has no merge methods any more</p>
","keras, word2vec, keras-layer, keras-2","<p>The <code>Merge</code> API has been changed. The new API is documented <a href=""https://keras.io/layers/merge/#dot"" rel=""nofollow noreferrer"">here</a>.</p>

<p>In your example, instead of doing this:<br>
<code>Merge([word_model,context_model],mode=""dot"")</code></p>

<p>Do this:<br>
<code>keras.layers.dot([word_model,context_model])</code></p>

<p>and it should work!</p>
",0,0,57,2019-11-11 04:50:55,https://stackoverflow.com/questions/58795715/how-to-use-merge-in-kerase2-0
How to store gensim&#39;s KeyedVectors object in a global variable inside a Redis Queue worker,"<p>I'm trying to store data in a global variable inside a Redis Queue (RQ) worker so that this data remains pre-loaded, i.e. it doesn't need to be loaded for every RQ job.</p>

<p>Specifically, I'm working with Word2Vec vectors and loading them using gensim's KeyedVectors.</p>

<p>My app is in Python Flask, running on a Linux server, containerized using Docker.</p>

<p>My goal is to reduce processing time by keeping a handful of large vectors files loaded in memory at all times. </p>

<p>I first tried storing them in global variables in Flask, but then <em>each</em> of my 8 gunicorn workers loads the vectors, which eats up a lot of RAM.</p>

<p>I only need <em>one</em> worker to store a particular vectors file.</p>

<p>I've been told that one solution is to have a set number of RQ workers holding the vectors in a global variable, so that I can control which workers get which vectors files loaded in.</p>

<p>Here is what I have so far:</p>

<p><em>RQ_worker.py</em></p>

<pre><code>from rq import Worker, Connection
from gensim.models.keyedvectors import KeyedVectors
from my_common_methods import get_redis

W2V = KeyedVectors.load_word2vec_format('some_path/vectors.bin', binary=True)

def rq_task(some_args):
    # use some_args and W2V to do some processing, e.g.:
    with open(some_args_filename, 'w') as f_out:
        f_out.write(str(W2V['word']))

if __name__ == '__main__':
    with Connection(get_redis()):
        worker = Worker(['default'])
        worker.work()
</code></pre>

<p><em>app.py</em></p>

<pre><code>from rq import Queue, Connection
from RQ_worker import rq_task

@app.route(""/someroute"", methods=['POST'])
def some_route():
    # test Redis Queue
    with Connection(get_redis()):
        q = Queue()
        task = q.enqueue(rq_task, some_args)
</code></pre>

<p><em>docker-stack.yml</em></p>

<pre><code>version: '3.7'

services:
  nginx:
    image: nginx:mainline-alpine
    deploy: ...
    configs: ...
    networks: ...

  flask:
    image: ...
    deploy: ...
    environment: ...
    networks: ...
    volumes: ...

  worker:
    image: ...
    command: python2.7 RQ_worker.py
    deploy:
      replicas: 1
    networks: ...
    volumes:
      - /some_path/data:/some_path/data

configs:
  nginx.conf:
    external: true
    name: nginx.conf

networks:
  external:
    external: true
  database:
    external: true
</code></pre>

<p>(I redacted a bunch of stuff from Docker, but can provide more details, if relevant.)</p>

<p>The above generally works, <strong>except</strong> that the RQ worker seems to load W2V <strong>from scratch</strong> each time it gets a new job, which defeats the whole purpose. It should keep the vectors stored in W2V as a global variable, so they don't need to be reloaded each time.</p>

<p>Am I missing something? Should I set it up differently? </p>

<p>I've been told that it might be possible to use mmap to load the vectors file into a global variable that the RQ worker sits on, but I'm not sure how that would work with KeyedVectors.</p>

<p>Any advice would be much appreciated!</p>
","docker, flask, redis, gensim, word2vec","<p>If you use <code>load_word2vec_format()</code>, the code will always be parsing the (not-native-to-gensim-or-Python) word-vectors format, and allocating new objects/memory to store the results. </p>

<p>You can instead use gensim's native <code>.save()</code> to store in a friendlier format for later native <code>.load()</code> operations. Large arrays of vectors will be stored in separate, memory-map ready files. Then, when you <code>.load(..., mmap='r')</code> those files, even multiple times from different threads or processes within the same container, they'll share the same RAM. </p>

<p>(Note that this doesn't even require any shared globals. The OS will notice that each process is requesting the same read-only memory-mapped file, and automatically share those RAM pages. The only duplication will be redundant Python <code>dict</code>s helping each separate <code>.load()</code> know indexes into the shared-array.)</p>

<p>There are some extra wrinkles to consider when doing similarity-operations on vectors that the model will want to repeatedly unit-norm - see this older answer for more details on how to work-around that:</p>

<p><a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>

<p>(Note that <code>syn0</code> and <code>syn0_norm</code> have been renamed <code>vectors</code> and <code>vectors_norm</code> in more-recent <code>gensim</code> versions, but the old names might still work with deprecation warnings for a while still.)</p>
",0,2,681,2019-11-11 07:24:27,https://stackoverflow.com/questions/58797101/how-to-store-gensims-keyedvectors-object-in-a-global-variable-inside-a-redis-qu
"In Fasttext skipgram training, what will happen if some sentences in the corpus have just one word?","<p>Imagine that you have a corpus in which some lines have just one word, so there is no context around some of the words. In this situation how does Fasttext perform to provide embeddings for these single words? <em>Note that the frequency of some of these words are one and there is no cut-off to get rid of them.</em>    </p>
","neural-network, word2vec, word-embedding, fasttext","<p>There's no way to train a <code>context_word -&gt; target_word</code> skip-gram pair for such words (in either 'context' or 'target' roles), so such words can't receive trained representations. Only texts with at least 2 tokens contribute anything to word2vec or FastText word-vector training. </p>

<p>(One possible exception: FastText in its 'supervised classification' mode <em>might</em> be able to make use of, and train vectors for, such words, because then even single words can be used to predict the known-label of training texts.)</p>

<p>I suspect that such corpuses will still result in the model counting the word in its initial vocabulary-discovery scan, and thus it will be allocated a vector (if it appears at least <code>min_count</code> times), and that vector will receive the usual small-random-vector initialization. But the word-vector will receive no further training â€“ so when you request the vector back after training, it will be of low-quality, with the only meaningful contributions coming from any char n-grams shared with other words that received real training. </p>

<p>You should consider any text-breaking process that results in single-word texts as buggy for the purposes of FastText. If those single-word texts come from another meaningful context where they were once surrounded by other contextual words, you should change your text-breaking process to work in larger chunks that retain that context. </p>

<p>Also note: it's rare for <code>min_count=1</code> to be a good idea for word-vector models, at least when the training text is real natural-language material where word-token frequencies roughly follow Zipf's law. There will be many, many 1-occurrence (or few-occurrence) words, but with just one to a few example usage contexts, not likely representing the true breadth and subtleties of that word's real usages, it's nearly impossible for such words to receive good vectors that generalize to other uses of those same words elsewhere. </p>

<p>Training good vectors require a variety of usage examples, and just one or a few examples will practically be ""noise"" compared to the tens-to-hundreds of examples of other words' usage. So keeping these rare words, instead of dropping them like a default <code>min_count=5</code> (or higher in larger corpuses) would do, tends to slow training, slow convergence (""settling"") of the model, and lower the quality of the <em>other</em> more-frequent word vectors at the end â€“ due to the significant-but-largely-futile efforts of the algorithm to helpfully position these many rare words.</p>
",1,0,326,2019-11-11 16:08:37,https://stackoverflow.com/questions/58804843/in-fasttext-skipgram-training-what-will-happen-if-some-sentences-in-the-corpus
Batch-train word2vec in gensim with support of multiple workers,"<p><strong>Context</strong></p>

<p>There exists severals questions about how to train <code>Word2Vec</code> using <code>gensim</code> with streamed data. Anyhow, these questions don't deal with the issue that streaming cannot use multiple workers since there is no array to split between threads.</p>

<p>Hence I wanted to create a generator providing such functionality for gensim. My results look like:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec as w2v

#The data is stored in a python-list and unsplitted.
#It's too much data to store it splitted, so I have to do the split while streaming.
data = ['this is document one', 'this is document two', ...]

#Now the generator-class
import threading

class dataGenerator:
    """"""
    Generator for batch-tokenization.
    """"""

    def __init__(self, data: list, batch_size:int = 40):
        """"""Initialize generator and pass data.""""""

        self.data = data
        self.batch_size = batch_size
        self.lock = threading.Lock()


    def __len__(self):
        """"""Get total number of batches.""""""
        return int(np.ceil(len(self.data) / float(self.batch_size)))


    def __iter__(self) -&gt; list([]):
        """"""
        Iterator-wrapper for generator-functionality (since generators cannot be used directly).
        Allows for data-streaming.
        """"""
        for idx in range(len(self)):
            yield self[idx]


    def __getitem__(self, idx):

        #Make multithreading thread-safe
        with self.lock:

            # Returns current batch by slicing data.
            return [arr.split("" "") for arr in self.data[idx * self.batch_size : (idx + 1) * self.batch_size]]


#And now do the training
model = w2v(
             sentences=dataGenerator(data),
             size=300,
             window=5,
             min_count=1,
             workers=4
            )
</code></pre>

<p>This results in the error </p>

<blockquote>
  <p>TypeError: unhashable type: 'list'</p>
</blockquote>

<p>Since <code>dataGenerator(data)</code> would work if I'd just yield a single splitted document, I assume that gensims <code>word2vec</code> wraps the generator within an extra list. In this case the <code>__iter__</code> would look like:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>Hence, my batch would also be wrapped resulting in something like <code>[[['this', '...'], ['this', '...']], [[...], [...]]]</code> (=> list of list of list) which cannot be processed by gensim.</p>

<p><br>
<br>
<br>
<strong>My question:</strong></p>

<p><em>Can I ""stream""-pass batches in order to use multiple workers?
How can I change my code accordingly?</em></p>
","python, nlp, batch-processing, gensim, word2vec","<p>It seems I was too impatient. I ran the streaming-function written above which processes only one document instead of a batch:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>After starting the <code>w2v</code>-function it took about ten minutes until all cores were working correctly. </p>

<p>It seems that building the vocabulary does not support multiple cores and, hence, only one was used for this task. Presumably, it took so long because auf the corpus-size. After gensim built the vocab, all cores were used for the training.</p>

<p>So if you are running in this issue as well, maybe some patience will already help :)</p>
",1,1,1485,2019-11-12 15:56:58,https://stackoverflow.com/questions/58822292/batch-train-word2vec-in-gensim-with-support-of-multiple-workers
Saving word2vec model results in messed up file,"<p>Saving word2vec in the word2vec text format gives a file with weird characters in it. </p>

<p><img src=""https://i.sstatic.net/dL1Vc.png"" alt=""What the saved word2vec text file looks like""></p>

<p>The contents of the file word2vec is making vectors from.</p>

<p><img src=""https://i.sstatic.net/XUUxl.png"" alt=""Cleaned and tokenized text""></p>

<p>I get no errors until I try and use the vector files in an analogy test. The text originally comes from an East African online newspaper.</p>

<p>My code:</p>

<pre><code>word2vec = gensim.models.Word2Vec(all_words, min_count=3, workers = 2)
save_as_1 = ""daily_nation_"" + str(subject) + ""_"" + str(startyr) + ""_"" + str(endyr) + ""_vectors.txt""
save_as_2 = ""daily_nation_"" + str(subject) + ""_"" + str(startyr) + ""_"" + str(endyr) + ""_vectors.bin""
word2vec.wv.save_word2vec_format(save_as_1, binary = ""FALSE"")
word2vec.wv.save_word2vec_format(save_as_2, binary = ""TRUE"")
vocabulary = word2vec.wv.vocab
print(""Vectors: "")
print(vocabulary)
sim_words = word2vec.wv.most_similar('woman')
print(""Words most similar to woman are: "" + str(sim_words))
</code></pre>

<p>I  want to  create proper text files of the embeddings.</p>
",word2vec,"<p>You have to provide a boolean to the <code>binary</code> argument:</p>

<pre><code>word2vec.wv.save_word2vec_format(save_as_1, binary = False)
</code></pre>

<p>Your code does not work since any non empty string evaluates to <code>True</code>.</p>
",1,0,185,2019-11-15 19:03:36,https://stackoverflow.com/questions/58883170/saving-word2vec-model-results-in-messed-up-file
How to find the most similar words from a set of input words by CBOW (GenSim)?,"<p>I use GenSim and CBOW for training the corpus. How can I get the most similar words from a set of input words?</p>

<p>For example:
Given a set of input words: [""David"", ""Mary"", ""married""]. Can I infer some output words like: ""wedding"", ""husband"", ""wife"", ""couple"", etc?</p>
","gensim, word2vec","<p>You can use the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">wv.most_similar</a> method of your model.</p>
",2,0,1668,2019-11-16 01:04:05,https://stackoverflow.com/questions/58886579/how-to-find-the-most-similar-words-from-a-set-of-input-words-by-cbow-gensim
How to incrementally train a word2vec model with new vocabularies,"<p>I got a dataset over 40G. The program of my tokenizer is killed due to limited memory, so I try to split my dataset. How can I train the word2vec model incrementally, that is, how can I use separate datasets to train one word2vec model?</p>

<p>My current word2vec code is:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=1, workers=10)
model.train(documents,total_examples=len(documents),epochs=epochs)
model.save(""./word2vec150d/word2vec_{}.model"".format(epochs))
</code></pre>

<p>Any help would be appreciated!</p>
","python, word2vec","<p>I have found the solution: use <code>PathLineSentences</code>. It is very fast. Incrementally training a word2vec model cannot learn new vocabularies, but <code>PathLineSentences</code> can.</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.word2vec import PathLineSentences

model = Word2Vec(PathLineSentences(input_dir), size=100, window=5, min_count=5, workers=multiprocessing.cpu_count() * 2, iter=20,sg=1)
</code></pre>
<p>For single file, use <code>LineSentences</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.word2vec import LineSentence

model = Word2Vec(LineSentence(file), size=100, window=5, min_count=5, workers=multiprocessing.cpu_count() * 2, iter=20,sg=1)
...
</code></pre>
",3,1,763,2019-11-19 02:14:35,https://stackoverflow.com/questions/58925659/how-to-incrementally-train-a-word2vec-model-with-new-vocabularies
How to delete words in self-trained word2vec model,"<p>I got a self-trained word2vec model (2G, end with "".model""). I convert the model into a text file (over 50G, end with "".txt"") because I have to use the text file in my other python codes. I am trying to reduce the size of the text file by deleting words that I do not need. I have built up a vocabulary set with all the words I need. How can I filter unnecessary words in the model?</p>

<p>I have tried to build a dictionary for the text file, but I am out of RAM.</p>

<pre class=""lang-py prettyprint-override""><code>emb_dict = dict()
with open(emb_path, ""r"", encoding=""utf-8"") as f:
    lines = f.readlines()
    for l in lines:
        word, embedding = l.strip().split(' ',1)
        emb_dict[word] = embedding
</code></pre>

<p>I am thinking if I can delete words in the "".model"" file. How can I do it? Any help would be appreciated!</p>
","python, word2vec","<p>It's hard to answer further without more precise code but you could batch your analysis of the text file</p>

<pre class=""lang-py prettyprint-override""><code>lines_to_keep = []
new_file = ""some_path.txt""
words_to_keep = set(some_words)
with open(emb_path, ""r"", encoding=""utf-8"") as f:
    for l in f:
        word, embedding = l.strip().split(' ',1)
        if word in words_to_keep:
            lines_to_keep.append(l.strip())
        if lines_to_keep and len(lines_to_keep) % 1000 == 0:
            with open(new_file, ""a"") as f:
                f.write(""\n"".join(lines_to_keep)
            lines_to_keep = []
</code></pre>
",2,4,863,2019-11-22 03:21:48,https://stackoverflow.com/questions/58987014/how-to-delete-words-in-self-trained-word2vec-model
How does Gensim implement subsampling in Word2Vec?,"<p>I am trying to reimplement wor2vec in pytorch. I implemented subsamping according to the <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L407"" rel=""nofollow noreferrer"">code</a> of the original paper. However, I am trying to understand how subsampling is implemented in Gensim. I looked at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">source code</a>, but I did not manage to grasp how it reconnects to the original paper.</p>

<p>Thanks a lot in advance.</p>
","gensim, word2vec, subsampling","<p>The key line is:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec_inner.pyx#L543"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec_inner.pyx#L543</a></p>

<pre class=""lang-py prettyprint-override""><code>    if c.sample and word.sample_int &lt; random_int32(&amp;c.next_random):
        continue
</code></pre>

<p>If <code>c.sample</code> tests if frequent-word downsampling is enabled at all (any non-zero value).</p>

<p>The <code>word.sample_int</code> is a value, per vocabulary word, that was precalculated during the vocabulary-discovery phase. It's essentially the 0.0-to-1.0 probability that a word should be kept, but scaled to the range 0-to-(2^32-1). </p>

<p>Most words, that are never down-sampled, simply have the value (2^32-1) there - so no matter what random int was just generated, that random int is less than the threshold, and the word is retained.</p>

<p>The few most-frequent words have other scaled values there, and thus sometimes the random int generated is larger than their <code>sample_int</code>. Thus, that word is, in that one training-cycle, skipped via the <code>continue</code> to the next word in the sentence. (That one word doesn't get made part of <code>effective_words</code>, this one time.)</p>

<p>You can see the original assignment &amp; precalculation of the <code>.sample_int</code> values, per unique vocabulary word, at and around:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec.py#L1544"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec.py#L1544</a></p>
",2,0,640,2019-11-23 16:19:38,https://stackoverflow.com/questions/59009670/how-does-gensim-implement-subsampling-in-word2vec
How to add numpy arrays as values in a dictionary of dictionaries?,"<p>Assume I have the following variables:</p>

<pre><code>import gensim
from gensim.models import KeyedVectors
wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
dict_dict = {
    ""abc"": ('dog', 'cat', 'bat'),
    ""def"": ('fat', 'hat', 'rat')
}
</code></pre>

<p>In this situation, <code>wv</code> is a word2vec model. </p>

<p>I want to take the values of each key in <code>dict_dict</code>, extract the value's vector (<code>e.g. wv['dog']</code>), and have the value now acts as a key to a sub dictionary:</p>

<pre><code>dict_dict = {
    ""abc"": ({'dog': array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]), {'cat':array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]), array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]):'bat')}
</code></pre>

<p>Would I have to create a new dictionary to do this?</p>
","python, numpy, dictionary, word2vec","<p>You could replace each of the existing <code>dict_dict</code> values â€“ which are currently tuples â€“ with new dicts, created via a Python dict comprehension. For example:</p>

<pre class=""lang-py prettyprint-override""><code>for key in dict_dict.keys():
    words = dict_dict[key]
    subdict = { word : wv[word] for word in words}
    dict_dict[key] = subdict
</code></pre>
",0,0,183,2019-11-25 20:07:56,https://stackoverflow.com/questions/59039366/how-to-add-numpy-arrays-as-values-in-a-dictionary-of-dictionaries
MemoryError: unable to allocate array with shape and data type float32 while using word2vec in python,"<p>I am trying to train the word2vec model from Wikipedia text data, for that I am using following code.</p>

<pre><code>import logging
import os.path
import sys
import multiprocessing

from gensim.corpora import  WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments

    if len(sys.argv) &lt; 3:
        print (globals()['__doc__'])
        sys.exit(1)
    inp, outp = sys.argv[1:3]

    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())

    # trim unneeded model memory = use (much) less RAM
    model.init_sims(replace=True)

    model.save(outp)
</code></pre>

<p>But after 20 minutes of program running, I am getting following error</p>

<p><a href=""https://i.sstatic.net/6BjOz.png"" rel=""nofollow noreferrer"">Error message</a></p>
","python, multiprocessing, python-multiprocessing, gensim, word2vec","<p>Ideally, you should paste the <em>text</em> of your error into your question, rather than a screenshot. However, I see the two key lines:</p>

<pre><code>&lt;TIMESTAMP&gt; : INFO : estimated required memory for 2372206 words and 400 dimensions: 8777162200 bytes
...
MemoryError: unable to allocate array with shape (2372206, 400) and data type float32
</code></pre>

<p>After making one pass over your corpus, the model has learned how many unique words will survive, which reports how large of a model must be allocated: one taking about <code>8777162200 bytes</code> (about 8.8GB). But, when trying to allocate the required vector array, you're getting a <code>MemoryError</code>, which indicates not enough computer addressable-memory (RAM) is available. </p>

<p>You can either:</p>

<ol>
<li>run where there's more memory, perhaps by adding RAM to your existing system; or</li>
<li>reduce the amount of memory required, chiefly by reducing either the number of unique word-vectors you'd like to train, or their dimensional size.</li>
</ol>

<p>You could reduce the number of words by increasing the default <code>min_count=5</code> parameter to something like <code>min_count=10</code> or <code>min_count=20</code> or <code>min_count=50</code>. (You probably don't need over 2 million word-vectors â€“ many interesting results are possible with just a vocabulary of a few tens-of-thousands of words.) </p>

<p>You could also set a <code>max_final_vocab</code> value, to specify an exact number of unique words to keep. For example, <code>max_final_vocab=500000</code> would keep just the 500000 most-frequent words, ignoring the rest. </p>

<p>Reducing the <code>size</code> will also save memory. A setting of <code>size=300</code> is popular for word-vectors, and would reduce the memory requirements by a quarter.</p>

<p>Together, using <code>size=300, max_final_vocab=500000</code> should trim the required memory to under 2GB.</p>
",6,4,47193,2019-11-26 12:07:13,https://stackoverflow.com/questions/59050644/memoryerror-unable-to-allocate-array-with-shape-and-data-type-float32-while-usi
how calculate distance between 2 node2vec model,"<p>I have 2 node2vec models in different timestamps. I want to calculate the distance between 2 models. Two models have the same vocab and we update the models.</p>

<p>My models are like this</p>

<pre><code>model1:
""1"":0.1,0.5,...
""2"":0.3,-0.4,...
""3"":0.2,0.5,...
.
.
.    
model2:
    ""1"":0.15,0.54,...
    ""2"":0.24,-0.35,...
    ""3"":0.24,0.47,...
    .
    .
    .
</code></pre>
","python, graph, nlp, word2vec","<p>Assuming you've used a standard <code>word2vec</code> library to train your models, each run bootstraps a wholly-separate model whose coordinates are not necessarily comparable to any other model. </p>

<p>(Due to some inherent randomness in the algorithm, or in the multi-threaded handling of training input, even running two training sessions on the exact same data will result in different models. They should each be about as useful for downstream applications, but individual tokens could be in arbitrarily-different positions.)</p>

<p>That said, you could try to synthesize some measures of how much two models are different. For example, you might:</p>

<ul>
<li><p>Pick a bunch of random (or domain-significant) word-pairs. Check the similarity between each pair, in each model individually, then compare those values between models. (That is, compare <code>model1.similarity(token_a, token_b)</code> with <code>model2.similarity(token_a, token_b)</code>.) Consider the difference-between-the-models as as some weighted combination of all the tested similarity-differences.</p></li>
<li><p>For some significant set of relevant tokens, collect the top-N most-similar tokens in each model. Compare this lists via some sort of rank-correlation measure, to see how much one model has changed the 'neighborhoods' of each token.</p></li>
</ul>

<p>For each of these, I'd suggest verifying their operation against a baseline case of the exact-same training data that's been shuffled and/or trained with a different starting random <code>seed</code>. Do they show such models as being ""nearly equivalent""? If not, you'd need to adjust the training parameters or synthetic measure until it does have the expected result - that models from the same data are judged as alike, even though tokens have very different coordinates. </p>

<p>Another option might be to train one giant combined model from a synthetic corpus where:</p>

<ul>
<li>all the original unmodified 'texts' from both eras all appear once</li>
<li>texts from each separate era appear again, but with some random-proportion of their tokens modified with an era-specific modifier. (For example, '<code>foo</code>' sometimes becomes <code>'foo_1'</code> when in first-era texts, and sometimes becomes <code>'foo_2'</code> in second-era texts. (You don't want to convert <em>all</em> tokens in any one text to era-specific tokens, because only tokens that co-appear with each other influence each other, and you thus want tokens from either era to sometimes appear with common/shared variants, but also often appear with era-specific variants.)</li>
</ul>

<p>At the end, the original token <code>'foo'</code> will get three vectors: <code>'foo'</code>, <code>'foo_1'</code>, and <code>'foo_2'</code>. They should all be quite similar, but the era-specific variants will be relatively more-influenced by the era-specific contexts. Thus the differences between those three (and relative movement in the now common coordinate space) will be an indication of the magnitude and kinds of changes that happened between the two eras' data. </p>
",2,0,366,2019-11-28 07:57:37,https://stackoverflow.com/questions/59084092/how-calculate-distance-between-2-node2vec-model
Shape of weights in the Softmax layer in Word2vec(skip-gram),"<p>I have a question about a shape of the weights for the Softmax Layer.</p>

<p>Suppose our vocabulary is 10000 words and our Embedding layer will reduce the dimensionality to 300.</p>

<p>So an Input is a one-hot-vector of length 10000 and Embedding layer has 300 neurons. It means, that weight matrix from Input layer to the Embedding layer has shape 10000*300(number of words in vocabulary* neurons in Embedding layer).</p>

<p>According to this tutorial(<a href=""https://www.kaggle.com/christofer/word2vec-skipgram-model-with-tensorflow"" rel=""nofollow noreferrer"">https://www.kaggle.com/christofer/word2vec-skipgram-model-with-tensorflow</a>) and many others the next weight matrix(that connects Embedding layer and Softmax classifier) has the same shape(number of words in vocabulary* neurons in Embedding layer or in our case 10000 * 300). I don't understand why? Shouldnt it be 300 * 10000(because we have to predict 10000 probabilities for each class)?</p>

<p>Can you explain me this?</p>
","tensorflow, deep-learning, nlp, word2vec, softmax","<p>It's because of the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a> function. The way this function is designed, it needs the weight matrix to have the shape, <code>[vocabulary size, dim]</code>.</p>

<p>From the documentation,</p>

<blockquote>
  <p>weights: A Tensor of shape [num_classes, dim], or a list of Tensor objects whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-sharded) class embeddings.</p>
</blockquote>

<h2>Why this is the case?</h2>

<p>The way <code>sampled_softmax_loss</code> works is by sampling weights belonging to a subset of output nodes that are going to be optimized every iteration (i.e. not running optimization on weights for all output nodes). The way it's done is using <a href=""https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_impl.py#L1733"" rel=""nofollow noreferrer""><code>embedding_lookup</code></a>. Therefore, having the weight in the shape <code>[vocab_size, dim]</code> makes it ideal for this purpose.</p>
",0,1,294,2019-12-03 11:05:32,https://stackoverflow.com/questions/59155692/shape-of-weights-in-the-softmax-layer-in-word2vecskip-gram
"Word2Vec - How to rid of &quot;TypeError: unhashable type: &#39;list&#39;&quot; and &quot;AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found&quot;?","<p>Getting <code>TypeError: unhashable type: 'list'</code> and <code>AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found</code> errors when I create my model based on <code>Word2Vec</code> implementation of the <code>gensim</code> module.</p>

<p><strong>Each entry has three parts</strong> which are presented within a list. And, <strong>the model contains three entries</strong> for the sake of demonstration.</p>

<p>Here is what I have tried:</p>

<pre><code>model = Word2Vec(sentences=features, size=100, sg=1, window=3, min_count=1, iter=10, workers=Pool()._processes)

model.build_vocab(features)

model.train(features)
</code></pre>

<p>The value of the <code>features</code> is: </p>

<pre><code>  [
    [
      ['permission.ACCESS_WIFI_STATE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.CHANGE_WIFI_STATE'],
       ['intent.action.MAIN', 'intent.action.BATTERY_CHANGED_ACTION', 'intent.action.SIG_STR', 'intent.action.BOOT_COMPLETED'],
       []
    ],
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.INSTALL_PACKAGES', 'permission.SEND_SMS', 'permission.DELETE_PACKAGES'],
      ['intent.action.BOOT_COMPLETED', 'intent.action.USER_PRESENT', 'intent.action.PHONE_STATE', 'intent.action.MAIN'],
      []
    ], 
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_FINE_LOCATION', 'permission.INTERNET', 'permission.READ_PHONE_STATE', 'permission.ACCESS_COARSE_LOCATION', 'permission.CALL_PHONE', 'permission.READ_CONTACTS', 'permission.READ_SMS'], 
      ['intent.action.PHONE_STATE', 'intent.action.MAIN'], 
      []
    ]
  ]
</code></pre>

<p><strong>Edit:</strong> The error stack trace after correcting the form of the feature vector according to the comment of @gojomo.</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 3 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 2 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 1 more threads
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 0 more threads
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - EPOCH - 10 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - training on a 60 raw words (2 effective words) took 0.1s, 21 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:WARNING - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
2019-12-13 12:24:34,521:gensim.utils:INFO - saving Word2Vec object under model/word2vec_model, separately None
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
    debugger.enable_tracing(apply_to_all_threads=True)  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
      File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
2019-12-13 12:24:34,521:gensim.utils:INFO - not storing attribute vectors_norm
        func = self.__getitem__(name)func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__

2019-12-13 12:24:34,522:gensim.utils:INFO - not storing attribute cum_table
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
    func = self._FuncPtr((name_or_ordinal, self))AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found

AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found
func = self._FuncPtr((name_or_ordinal, self))
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
AttributeError: dlsym(0x7fed18d2ff70, AttachDebuggerTracing): symbol not found
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,524:gensim.utils:INFO - saved model/word2vec_model
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
        pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
result = lib.AttachDebuggerTracing(  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18b09320, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
</code></pre>
","gensim, word2vec, word-embedding","<p>Gensim's <code>Word2Vec</code> expects its corpus <code>sentences</code> to be a <em>sequence</em> where each individual item is a <em>list of string tokens</em>. (That is, those string tokens are words.)</p>

<p>Instead, you have a list (which is acceptable as a sequence), where each of its items is a list (which is also acceptable), but then each of those lists instead has as each item yet another <em>list</em> â€“ when for <code>Word2Vec</code> training, each of those items should be a string token (word). </p>

<p>(I've edited your example data to be be structurally-indented, to make the levels of nesting clearer.)</p>

<p>If those innermost lists-of-strings are your real individual ""sentences"", you need to be sure they're the items in your outermost list. </p>

<p>(If, on the other hand, you really want a cluster like <code>['intent.action.PHONE_STATE', 'intent.action.MAIN']</code> to be a single ""word"" in your model, you'll want to change that list into a single string token, so it can look like a word â€“ and thus hashable key â€“ to <code>Word2Vec</code> and Python.)</p>
",2,0,1405,2019-12-12 20:08:37,https://stackoverflow.com/questions/59312001/word2vec-how-to-rid-of-typeerror-unhashable-type-list-and-attributeerro
Gensim Word2Vec or FastText build vocab from frequency,"<p>I wonder what does <code>.build_vocab_from_freq()</code> function from gensim actually do? What is the difference when I'm not using it? Thank you!</p>
","python, gensim, word2vec, fasttext","<p>It ""builds a vocabulary from a dictionary of word frequencies"". You need a vocabulary for your gensim models. Usually you build it from your corpus. This is basically an alternative option to build your vocabulary from a word frequencies dictionary. Word frequencies for example are usually used to filter low or high frequent words which are meaningless for your model.</p>
",0,0,611,2019-12-17 05:48:21,https://stackoverflow.com/questions/59368232/gensim-word2vec-or-fasttext-build-vocab-from-frequency
Processing a Corpus For a word2vec Implementation,"<p>As part of a class project, I'm trying to write a word2vec implementation in Python and train it on a corpus of ~6GB. I'm trying to code a reasonably optimized solution so I don't have to let my PC sit for days.</p>

<p>Going through the C word2vec source code, I notice that there, each thread reads words from a file, and takes the time to look up the index of every word. At the end, it stores a ""sentence"" of word indexes.</p>

<p>Wouldn't it be logical to translate the whole corpus into one containing integer indexes of the appropriate words? That way, time isn't lost during training on hash-table lookups, while the translation process is a one-time expense.</p>

<p>I understand that for extremely large corpuses, you are effectively doubling the amount it takes on disk, which you might want to avoid.</p>

<p>However, if you do have the memory, wouldn't this offer a noticeable increase in efficiency? Or am I just overestimating the impact of a table lookup?</p>
","python, machine-learning, nlp, word2vec","<p>Hashtable lookups can be very fast, and repeated lookups may not contribute much to the overall runtime.</p>

<p>But the only way to really know the potential speedup of your proposed optimization is to implement it, and profile it in comparison to the prior behavior. </p>

<p>Also, as you note, to be able to re-use a single-pass token-lookup, you'd need to store those results somewhere. Google's <code>word2vec.c</code> code, like many other implementations, seeks to work well with input corpuses that are far larger than addressable memory. Writing the interim tokenization to disk would require extra code complication, and extra working space on disk, compared to the baseline of repeated lookups. So: even if it did speed things a little, implementors might consider the extra complexity undesirable.</p>
",1,0,77,2019-12-24 22:42:34,https://stackoverflow.com/questions/59473926/processing-a-corpus-for-a-word2vec-implementation
"For a given word, Predict the cluster and get the nearest words from the cluster","<p>I have trained my corpus on w2v and k-means following the instructions given this link.</p>

<p><a href=""https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/"" rel=""nofollow noreferrer"">https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/</a></p>

<p>What I am want to do this 
a. find the cluster ID for a given word
b. get the top 20 nearest words from the cluster for the given word.</p>

<p>I have figured out how to the words in a given cluster. What I want is to find out the words that are closer to my given word in the given cluster. </p>

<p>Any help is appreciated.</p>
","python-3.x, cluster-analysis, k-means, word2vec, supervised-learning","<p>Your linked guide is, with its given data, a bit of misguided. You can't get meaningful 100-dimensional word-vectors (the gensim <code>Word2Vec</code> class default) from a mere 30-word corpus. The results from such a model will be nonsense, useless for clustering or other downstream steps â€“ so any tutorial purporting to show this process, with true results, should be using far more data. </p>

<p>If you are in fact using far more data, and have succeeded in clustering words, the <code>Word2Vec</code> model's <code>most_similar()</code> function will give you the top-N (default 10) nearest-words for any given input word. (Specifically, they will be returned as <code>(word, cosine_similarity)</code> tuples, ranked by highest <code>cosine_similarity</code>.)</p>

<p>The <code>Word2Vec</code> model is of course oblivious to the results of clustering, so you would have to filter those results to discard words outside the cluster of interest. </p>

<p>I'll assume that you have some lookup object <code>cluster</code>, that for <code>cluster[word]</code> gives you the cluster ID for a specific word. (This might be a dict, or something that does a KMeans-model <code>predict()</code> on the supplied vector, whatever.) And, that <code>total_words</code> is the total number of words in your model. (For example: <code>total_words = len(w2v_model.wv)</code>. Then your logic should be roughly like</p>

<pre class=""lang-py prettyprint-override""><code>target_cluster = cluster[target_word]
all_similars = w2v_model.wv.most_similar(target_word, topn=total_words)
in_cluster_similars = [sim for sim in all_similars 
                       if cluster[sim[0]] = target_cluster]
</code></pre>

<p>If you just want the top-20 results, clip to <code>in_cluster_similars[:20]</code>. </p>
",2,0,562,2019-12-25 09:31:20,https://stackoverflow.com/questions/59476989/for-a-given-word-predict-the-cluster-and-get-the-nearest-words-from-the-cluster
Gensim Doc2Vec infer_vector on unseen words differs based on characters in these words,"<p>Gensim Doc2Vec infer_vector on paragraphs with unseen words generates vectors that differ based on the characters in the unsween words.</p>

<pre><code>for i in range(0, 2):
    print(model.infer_vector([""zz""])[0:2])
    print(model.infer_vector([""zzz""])[0:2])
    print(model.infer_vector([""zzzz""])[0:2])
    print(""\n"")

[ 0.00152548 -0.00055992]
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]


[ 0.00152548 -0.00055992] # same as in previous iteration
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]
</code></pre>

<p>I am trying understand how unseen words affect initialization of the infer_vector. It looks like different characters will produce different vectors. Trying to understand why.</p>
","gensim, word2vec, doc2vec","<p>Unseen words are ignored for the actual process of iterative inference: tuning a vector to better-predict a text's words, according to a frozen <code>Doc2Vec</code> model. </p>

<p>However, inference starts with a pseudorandomly-initialized vector. And, the full set of tokens passed-in (including unknown words) are used as the seed for that random-initialization. </p>

<p>This seeded initialization is done as a potential small aid to those seeking fully-reproducible inference â€“ but in practice, seeking such exact-reproduction, rather than just run-to-run similarity, is usually a bad idea. See the gensim FAQs  <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11</a> &amp; <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q12-ive-used-doc2vec-infer_vector-on-a-single-text-but-the-resulting-vector-is-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-doc2vec-inference-non-determinism"" rel=""nofollow noreferrer"">Q12</a> about varying results from run-to-run for more details.</p>

<p>So what you're seeing is:</p>

<ul>
<li>your different tokenized texts each cause a pseudorandom, but deterministic with respect to the source text, vector initialization</li>
<li>since no words are known, inference afterwards is a no-op: there are no words to predict</li>
<li>the pseudorandom initialized vector is returned</li>
</ul>

<p>The <code>infer_vector()</code> method should probably log a warning, or return a flag value (like perhaps the origin vector), as a better hint that nothing meaningful is actually happening. </p>

<p>But you may wish to check any text before you supply it to <code>infer_vector()</code> â€“ if none of its words are in the <code>d2v_model.wv</code>, then inference will simply be returning a small random initialization vector. </p>
",4,2,750,2019-12-25 22:01:34,https://stackoverflow.com/questions/59482140/gensim-doc2vec-infer-vector-on-unseen-words-differs-based-on-characters-in-these
where can i download a pretrained word2vec map?,"<p>I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.</p>

<p>What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. </p>
","python, nlp, word2vec, word-embedding","<p>You can try out Google's <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">word2vec</a> model trained with about 100 billion words from various news articles.</p>
<p>An interesting fact about word vectors, <code>w2v(king) - w2v(man) + w2v(woman) â‰ˆ w2v(queen)</code></p>
",9,10,12179,2020-01-04 13:10:19,https://stackoverflow.com/questions/59590993/where-can-i-download-a-pretrained-word2vec-map
Error while implementing Word2Vec model with embedding_vector,"<p>I'm getting an <strong>AttributeError</strong> while trying to implement with embedding_vector:</p>

<pre><code>from gensim.models import KeyedVectors
embeddings_dictionary = KeyedVectors.load_word2vec_format('model', binary=True)

embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
</code></pre>

<blockquote>
  <p>AttributeError: 'Word2VecKeyedVectors' object has no attribute 'get'</p>
</blockquote>
","python, machine-learning, keras, gensim, word2vec","<p>Yes, <code>gensim</code>'s <code>KeyedVectors</code> abstraction does not offer a <code>get()</code> method. (What docs or example are you following that suggests it does?)</p>

<p>You can use standard Python <code>[]</code>-indexing, eg:</p>

<pre><code>embedding_dictionary[word]
</code></pre>

<p>Though, there isn't really a reason for your loop copying each vector into your own <code>embedding_matrix</code>. The <code>KeyedVectors</code> instance already has a raw array, with each vector in a row, in the order of the <code>KeyedVectors</code> <code>.index2entity</code> list â€“ in its <code>vectors</code> property:</p>

<pre><code>embedding_dictionary.vectors
</code></pre>
",2,0,11363,2020-01-19 19:33:28,https://stackoverflow.com/questions/59813664/error-while-implementing-word2vec-model-with-embedding-vector
word2vec window size at sentence boundaries,"<p>I am using word2vec (and doc2vec) to get embeddings for sentences, but i want to completely ignore word order.
I am currently using gensim, but can use other packages if necessary.</p>

<p>As an example, my text looks like this:</p>

<pre><code>[
['apple', 'banana','carrot','dates', 'elderberry', ..., 'zucchini'],
['aluminium', 'brass','copper', ..., 'zinc'],
...
]
</code></pre>

<p>I intentionally want 'apple' to be considered as close to 'zucchini' as it is to 'banana' so I have set the window size to a very large number, say 1000.
I am aware of 2 problems that may arise with this.</p>

<p>Problem 1:
The window might <em>roll</em> in at the start of a sentence creating the following training pairs:
<code>('apple', ('banana')), ('apple', ('banana', 'carrot')), ('apple', ('banana', 'carrot', 'date'))</code> before it eventually gets to the correct <code>('apple', ('banana','carrot', ..., 'zucchini'))</code>.
This would seem to have the effect of making 'apple' closer to 'banana' than 'zucchini',
since their are so many more pairs containing 'apple' and 'banana' than there are pairs containing 'apple' and 'zucchini'.</p>

<p>Problem 2:
I heard that pairs are sampled with inverse proportion to the distance from the target word to the context word- This also causes an issue making nearby words more seem more connected than I want them to be.</p>

<p>Is there a way around problems 1 and 2?
Should I be using cbow as opposed to sgns? Are there any other hyperparameters that I should be aware of?
What is the best way to go about removing/ignoring the order in this case?</p>

<p>Thank you</p>
","gensim, word2vec","<p>I'm not sure what you mean by ""Problem 1"" - there's no ""roll"" or ""wraparound"" in the usual interpretation of a word2vec-style algorithm's <code>window</code> parameter. So I wouldn't worry about this. </p>

<p>Regarding ""Problem 2"", this factor can be essentially made negligible by the choice of a giant <code>window</code> value â€“ say for example, a value one million times larger than your largest sentence. Then, any difference in how the algorithm treats the nearest-word and the 2nd-nearest-word is vanishingly tiny. </p>

<p>(More specifically, the way the gensim implementation â€“ which copies the original Google <code>word2vec.c</code> in this respect â€“ achieves a sort of distance-based weighting is actually via random dynamic shrinking of the actual <code>window</code> used. That is, for each visit during training to each target word, the effective <code>window</code> truly used is some random number from 1 to the user-specified <code>window</code>. By effectively using smaller windows much of the time, the nearer words have more influence â€“ just without the cost of performing other scaling on the whole window's words every time. But in your case, with a giant <code>window</code> value, it will be incredibly rare for the effective-window to ever be smaller than your actual sentences. Thus every word will be included, equally, almost every time.)</p>

<p>All these considerations would be the same using SG or CBOW mode. </p>

<p>I believe a million-times-larger <code>window</code> will be adequate for your needs, for if for some reason it wasn't, another way to essentially cancel-out any nearness effects could be to ensure your corpus's items individual word-orders are re-shuffled between each time they're accessed as training data. That ensures any nearness advantages will be mixed evenly across all words â€“ especially if each sentence is trained on many times. (In a large-enough corpus, perhaps even just a 1-time shuffle of each sentence would be enough. Then, over all examples of co-occurring words, the word co-occurrences would be sampled in the right proportions even with small windows.)</p>

<p>Other tips:</p>

<p>If your training data starts in some arranged order that clumps words/topics together, it can be beneficial to shuffle them into a random order instead. (It's better if the full variety of the data is interleaved, rather than presented in runs of many similar examples.) </p>

<p>When your data isn't true natural-language data (with its usual distributions &amp; ordering significance), it may be worth it to search further from the usual defaults to find optimal metaparameters. This goes for <code>negative</code>, <code>sample</code>, &amp; especially <code>ns_exponent</code>. (One paper has suggested the optimal <code>ns_exponent</code> for training vectors for recommendation-systems is far different from the usual 0.75 default for natural-language modeling.)</p>
",3,2,1778,2020-01-23 05:25:42,https://stackoverflow.com/questions/59872029/word2vec-window-size-at-sentence-boundaries
Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?,"<p>I need to combine Word2Vec with my <code>CNN</code> model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (<em>a.k.a.</em> target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of <code>Word2Vec</code> as I need both of them in order to train my deep neural network?</p>

<p>p.s. I'm using <code>Gensim</code> implementation of <code>Word2Vec</code>.</p>

<p>p.s. My corpus has <strong>6,925</strong> sentences, and <code>Word2Vec</code> produces <strong>5,260</strong> vectors.</p>

<p><strong>Edit:</strong> More detail regarding my corpus (as requested):</p>

<p>The structure of the corpus is as follows:</p>

<ol>
<li><p>sentences (label: <code>positive</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
<li><p>sentences (label: <code>negative</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
</ol>

<p>Then all the sentences were given as the input to <code>Word2Vec</code>.</p>

<pre><code>word2vec = Word2Vec(all_sentences, min_count=1)
</code></pre>

<p>I'll feed my CNN with the extracted features (which is the <code>vocabulary</code> in this case) and the <code>targets</code> of sentences. So, I need these labels of the sentences as well.</p>
","deep-learning, gensim, word2vec, one-hot-encoding, word-embedding","<p>Because the <code>Word2Vec</code> model doesn't retain any representation of the individual training texts, this is entirely a matter for you in your own Python code. </p>

<p>That doesn't seem like very much data. (It's rather tiny for typical <code>Word2Vec</code> purposes to have just a 5,260-word final vocabulary.) </p>

<p>Unless each text (aka 'sentence') is very long, you could even just use a Python dict where each key is the full string of a sentence, and the value is your flag. </p>

<p>But if, as is likely, your source data has some other unique identifier per text â€“ like a unique database key, or even a line/row number in the canonical representation â€“ you should use that identifier as a key instead. </p>

<p>In fact, if there's a canonical source ordering of your 6,925 texts, you could just have a list <code>flags</code> with 6,925 elements, in order, where each element is your flag. When you need to know the status of a text from position <code>n</code>, you just look at <code>flags[n]</code>. </p>

<p>(To make more specific suggestions, you'd need to add more details about the original source of the data, and exactly when/why you'd need to be checking this extra property later.)</p>
",0,2,443,2020-01-27 00:56:17,https://stackoverflow.com/questions/59924168/word2vec-how-can-i-store-and-retrieve-extra-information-regarding-each-instanc
What does each element in an embedding mean?,"<p>I've been working with facial embeddings but I think Word2Vec is a more common example.</p>

<p>Each entry in that matrix is a number that came from some prediction program/algorithm, but what are they? Are they learned features?</p>
","python, word2vec, feature-extraction, embedding, word-embedding","<p>Those numbers are learned vectors that each represents a dimension that best separates each word from each other, given some limiting number of dimensions (normally ~200). So if one group of words tends to appear in the same context, then they'd likely share a similar score on one or more dimensions. </p>

<p>For example, words like North, South, East, West are likely to be very close since they are interchangeable in many contexts. </p>

<p>The dimensions are chosen by algorithm to maximize the variance they encode, and what they mean is not necessarily something we can talk about in words. But imagine a bag of fridge-magnets each representing a letter of the alphabet - if you shine a light on them so as to cast a shadow, there will be some orientations of the letters that yield more discriminatory information in the shadows than for other orientations. </p>

<p>The dimensions in a word-embedding represent the best ""orientations"" that give light to the most discriminatory ""shadows"". Sometimes these dimensions might approximate things we recognise as having direct meaning, but very often, they wont.</p>

<p>That being said, if you collect words that do have similar functions, and find the vectors from those words to other words that are the endpoint of some kind of fixed relationship - say England, France, Germany as one set of words consisting of Countries, and London, Paris, Berlin as another set of words consisting of the respective Capital-Cities, you will find that the <em>relative</em> vectors between each country and its capital are often very, very similar in both direction and magnitude. </p>

<p>This has an application for search because you can start with a new word location, say ""Argentina"" and by looking in the location arrived at by applying the relative ""has_capital_city"" vector, you <em>should</em> arrive at the word ""Buenos Aires"".</p>

<p>So the raw dimensions probably have little meaning of their own, but by performing these A is to B as X is to Y comparisons, it is possible to derive relative vectors that do have a meaning of sorts. </p>
",4,0,509,2020-02-05 13:05:55,https://stackoverflow.com/questions/60076497/what-does-each-element-in-an-embedding-mean
Error in loading NLTK resources: &quot;Please use the NLTK Downloader to obtain the resource:\n\n&quot;,"<p>I adapted the following code from Susan Li's <a href=""https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"" rel=""nofollow noreferrer"">post</a>, but incurred an error when the code tries to tokenize text using <code>NLTK</code>'s resources (or, there could be something wrong with ""keyed vectors"" loaded from the web). The error occurred on the 5th code block (see below, might take a while to load from the web):</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>## 1. load packages and data

import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk import sent_tokenize
STOPWORDS = set(stopwords.words('english'))
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import re
from bs4 import BeautifulSoup
%matplotlib inline

df = pd.read_csv('https://www.dropbox.com/s/b2w7iqi7c92uztt/stack-overflow-data.csv?dl=1')
df = df[pd.notnull(df['tags'])]

my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']

## 2. cleaning

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):

    text = BeautifulSoup(text, ""lxml"").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text
    
df['post'] = df['post'].apply(clean_text)

## 3. train test split

X = df.post
y = df.tags
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)

## 4. load keyed vectors from the web: will take a while to load

import gensim
word2vec_path = ""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz""
wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
wv.init_sims(replace=True)


## 5. this is where it goes wrong

def w2v_tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text, language='english'):
        for word in nltk.word_tokenize(sent, language='english'):
            if len(word) &lt; 2:
                continue
            tokens.append(word)
    return tokens
    
train, test = train_test_split(df, test_size=0.3, random_state = 42)

test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values
train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values

X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)


## 6. perform logistic regression test

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg = logreg.fit(X_train_word_average, train['tags'])
y_pred = logreg.predict(X_test_word_average)
print('accuracy %s' % accuracy_score(y_pred, test.tags))
print(classification_report(test.tags, y_pred,target_names=my_tags))</code></pre>
</div>
</div>
</p>

<p><strong>Update on part 5</strong> (per <code>@luigigi</code>'s comments)</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>## 5. download nltk and use apply() function without using lambda

import nltk
nltk.download()
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk import sent_tokenize

    def w2v_tokenize_text(text):
        tokens = []
        for sent in nltk.sent_tokenize(text, language='english'):
            for word in nltk.word_tokenize(sent, language='english'):
                if len(word) &lt; 2:
                    continue
                tokens.append(word)
        return tokens
        
    train, test = train_test_split(df, test_size=0.3, random_state = 42)

    def w2v_tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text, language='english'):
        for word in nltk.word_tokenize(sent, language='english'):
            if len(word) &lt; 2:
                continue
            tokens.append(word)
    return tokens
    
train, test = train_test_split(df, test_size=0.3, random_state = 42)

test_tokenized = test['post'].apply(w2v_tokenize_text).values

train_tokenized = train['post'].apply(w2v_tokenize_text).values

    X_train_word_average = word_averaging_list(wv,train_tokenized)
    X_test_word_average = word_averaging_list(wv,test_tokenized)

## now run the test

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg = logreg.fit(X_train_word_average, train['tags'])
y_pred = logreg.predict(X_test_word_average)
print('accuracy %s' % accuracy_score(y_pred, test.tags))
print(classification_report(test.tags, y_pred,target_names=my_tags))</code></pre>
</div>
</div>
</p>

<p>This should work.</p>
","python, nltk, tokenize, word2vec","<p>Then <a href=""https://www.nltk.org/api/nltk.tokenize.html"" rel=""nofollow noreferrer"">nltk tokenizer</a> expects the <a href=""http://www.nltk.org/api/nltk.tokenize.html?highlight=punkt#module-nltk.tokenize.punkt"" rel=""nofollow noreferrer"">punkt</a> resource so you have to download it first:</p>

<pre><code>nltk.download('punkt')
</code></pre>

<p>Also, you dont need a <code>lambda</code> expression to apply your tokenizer function. You can simply use: </p>

<pre><code>test_tokenized = test['post'].apply(w2v_tokenize_text).values
train_tokenized = train['post'].apply(w2v_tokenize_text).values
</code></pre>
",1,1,5099,2020-02-06 13:32:30,https://stackoverflow.com/questions/60096180/error-in-loading-nltk-resources-please-use-the-nltk-downloader-to-obtain-the-r
Extracting skills from a job description using TF-IDF or Word2Vec,"<p>I have a situation where I need to extract the skills of a particular applicant who is applying for a job from the job description avaialble and store it as a new column altogether.
The dataframe X looks like following:</p>

<pre><code>Job_ID        Job_Desc 
1             Applicant should posses technical capabilities including proficient knowledge of python and SQL
2             Applicant should posses technical capabilities including proficient knowledge of python and SQL and R
</code></pre>

<p>The resultant output should look like following:</p>

<pre><code>Job_ID       Skills
1            Python,SQL
2            Python,SQL,R
</code></pre>

<p>I have used tf-idf count vectorizer to get the most important words within the Job_Desc column but still I am not able to get the desired skills data in the output. Could this be achieved somehow with Word2Vec using skip gram or CBOW model? </p>

<p>My code looks like this :</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_df=0.50)
word_count_vector=cv.fit_transform(X)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)
tfidf_transformer.fit(word_count_vector)

def sort_coo(coo_matrix):
tuples = zip(coo_matrix.col, coo_matrix.data)
return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)

def extract_topn_from_vector(feature_names, sorted_items, topn=10):
""""""get the feature names and tf-idf score of top n items""""""

#use only topn items from vector
sorted_items = sorted_items[:topn]

score_vals = []
feature_vals = []

for idx, score in sorted_items:
    fname = feature_names[idx]

    #keep track of feature name and its corresponding score
    score_vals.append(round(score, 3))
    feature_vals.append(feature_names[idx])

#create a tuples of feature,score
#results = zip(feature_vals,score_vals)
results= {}
for idx in range(len(feature_vals)):
    results[feature_vals[idx]]=score_vals[idx]

return results

feature_names=cv.get_feature_names()
doc=X[0]

tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))
sorted_items=sort_coo(tf_idf_vector.tocoo())
keywords=extract_topn_from_vector(feature_names,sorted_items,10)
print(""\n=====Title====="")
print(X[0])
print(""\n===Keywords==="")
for k in keywords:
   print(k,keywords[k])
</code></pre>
","python-3.x, machine-learning, word2vec, pos-tagger, tfidfvectorizer","<p>I can't think of a way that TF-IDF, Word2Vec, or other simple/unsupervised algorithms could, alone, identify the kinds of 'skills' you need. </p>

<p>You'll likely need a large hand-curated list of skills â€“ at the very least, as a way to automate the evaluation of methods that purport to extract skills. </p>

<p>With a curated list, then something like Word2Vec might help suggest synonyms, alternate-forms, or related-skills. (For known skill X, and a large Word2Vec model on your text, terms similar-to X are likely to be similar skills â€“ but not guaranteed, so you'd likely still need human review/curation.)</p>

<p>With a large-enough dataset mapping texts to <em>outcomes</em> â€“ like, a candidate-description text (resume) mapped-to whether a human reviewer chose them for an interview, or hired them, or they succeeded in a job, you might be able to identify terms that are highly predictive of fit in a certain job role. Those terms might often be de facto 'skills'. But discovering those correlations could be a much larger learning project.</p>
",2,1,5207,2020-02-10 16:31:37,https://stackoverflow.com/questions/60154561/extracting-skills-from-a-job-description-using-tf-idf-or-word2vec
Using autoencoder for pairwise text similarity,"<p>I have a dataset containing only 500 samples. The dataset has three columns </p>

<ol>
<li>Sentence1 </li>
<li>Sentence2 </li>
<li>0 or 1 (to indicate similarity). </li>
</ol>

<p>My task was to train an encoder that takes two sentences as input and returns 1 if sentences are similar and 0 otherwise.</p>

<p>I use pre-trained word2vec embeddings to extract features. My model has achieved only 50% accuracy.</p>

<pre><code>sent_in = Input(shape=(150, ))
sent_emb = Embedding(input_dim=vocab_size, output_dim=300, weights=[E],)(sent_in)
conv1 = Conv1D(32, 5, activation='relu', padding='same')(sent_emb)
pool1 = MaxPooling1D(2)(conv1)
conv2 = Conv1D(64, 5, activation='relu', padding='same')(pool1)
pool2 = MaxPooling1D(2)(conv2)
conv3 = Conv1D(128, 5, activation='relu', padding='same')(pool2)
flat1 = Flatten()(conv3)

sent_in2 = Input(shape=(150, ))
sent_emb2 = Embedding(input_dim=vocab_size, output_dim=300, weights=[E],)(sent_in2)
conv4 = Conv1D(32, 5, activation='relu', padding='same')(sent_emb2)
pool3 = MaxPooling1D(2)(conv4)
conv5 = Conv1D(64, 5, activation='relu', padding='same')(pool3)
pool4 = MaxPooling1D(2)(conv5)
conv6 = Conv1D(128, 5, activation='relu', padding='same')(pool4)
flat2 = Flatten()(conv6)

concatenated = concatenate([flat1, flat2])

dense1 = Dense(32, activation='relu')(concatenated)
out = Dense(1, activation='sigmoid')(dense1)
model = Model(inputs=[sent_in,sent_in2], outputs=out)
model.summary()
</code></pre>

<p>My network is illustrated in the image below</p>

<p><a href=""https://i.sstatic.net/1miXV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1miXV.jpg"" alt=""My Network""></a></p>

<p>Questions:</p>

<p>1) Must every autoencoder have an encoder and a decoder?</p>

<p>2) How can I improve my accuracy?</p>
","python, tensorflow, keras, word2vec, autoencoder","<blockquote>
  <p><strong><em>1) Must every autoencoder have an encoder and a decoder?</em></strong></p>
</blockquote>

<p>Yes, it's a must. If you are going to train the encoder and decoder as separate neural networks you may soon face the problem that we do not know the ground truth of what the encoding (compressed set of features) ought to be.
There is no label to say that the input features should correspond to this particular encoding.
Thus, itâ€™s impossible to train our encoder! Without the encoder, we will not have the encoding and thus we have no input features to the decoder!
This makes it impossible to train our decoder as well!</p>

<blockquote>
  <p><strong><em>2) How can I improve my accuracy?</em></strong></p>
</blockquote>

<p>Well I think this has been answered here: <a href=""https://stackoverflow.com/questions/54643064/how-to-improve-the-accuracy-of-autoencoder"">How to improve the accuracy of autoencoder?</a></p>

<p>The following explanation below can also help:</p>

<p><strong>The effect of an auto-encoder on the accuracy of a convolutional neural network classification task</strong></p>

<p>It is reasonable to say that a lower encoded size leads to lower accuracy and the accuracy becomes stable when the
encoded size reaches a certain number because <strong><em>a lower encoded size means more loss</em></strong> in the image data, which means
the restructured image would be more different from the original data. Moreover, even though the encoded size is large
enough, there is still a loss in the image data. Hence, the accuracy is still less than the original accuracy and the accuracy
in the box plot stops increasing. When the <strong><em>auto-encoders</em></strong> are trained with the encoded size which is large enough, the
accuracy for both cases is about 92%. Compared to CNN which takes original data as input and achieves the
accuracy of 99%, the accuracy loss due to auto-encoders is not too much.</p>

<p>This is based on the research paper. </p>
",1,1,772,2020-02-21 16:25:59,https://stackoverflow.com/questions/60342713/using-autoencoder-for-pairwise-text-similarity
Why I have a different number of terms in word2vec and TFIDF? How I can fix it?,"<p>I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. <strong>I am using the same corpus for get both matrix, I don't know why each matrix have a different number of terms
.</strong></p>

<p>My problem is that I have a matrix TFIDF with the shape <code>(56096, 15500)</code> (corresponding to: number of terms, number of documents) and matrix Word2vec with the shape <code>(300, 56184)</code> (corresponding to : number of word-embeddings, number of terms).<br>
And I need the same numbers of terms in both matrix. </p>

<p>I use this code for get the matrix of word-embeddings Word2vec: </p>

<pre><code>def w2vec_gensim(norm_corpus):
    wpt = nltk.WordPunctTokenizer()
    tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]
    # Set values for various parameters
    feature_size = 300
    # Word vector dimensionality
    window_context = 10
    # Context window size
    min_word_count = 1
    # Minimum word count
    sample = 1e-3
    # Downsample setting for frequent words
    w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=window_context, min_count =  min_word_count, sample=sample, iter=100)
    words = list(w2v_model.wv.vocab)
    vectors=[]
    for w in words:
        vectors.append(w2v_model[w].tolist())
    embedding_matrix= np.array(vectors)
    embedding_matrix= embedding_matrix.T
    print(embedding_matrix.shape)

    return embedding_matrix
</code></pre>

<p>And this code for get the TFIDF matrix: </p>

<pre><code>tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)


def matriz_tf_idf(datos, tv):
    tv_matrix = tv.fit_transform(datos)
    tv_matrix = tv_matrix.toarray()
    tv_matrix = tv_matrix.T
    return tv_matrix
</code></pre>

<p>And I need the same number of terms in each matrix. For example, if I have 56096 terms in TFIDF, I need the same number in embeddings matrix, I mean matrix TFIDF with the shape <code>(56096, 1550)</code> and matrix of embeddings Word2vec with the shape <code>(300, 56096)</code>. How I can get the same number of terms in both matrix? 
Because I can't delete without more data, due to I need the multiplication to make sense because my goal is to get the embeddings from the documents. </p>

<p>Thank you very much in advance.</p>
","python, matrix, word2vec, embedding","<p>The problem is that TFIDF is cutting out around 90 terms. This is because tokenize is neccesary. 
This is the solution: </p>

<pre><code>wpt = nltk.WordPunctTokenizer()
tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True,
                     tokenizer=wpt.tokenize)
</code></pre>
",0,-3,103,2020-03-02 18:41:22,https://stackoverflow.com/questions/60494795/why-i-have-a-different-number-of-terms-in-word2vec-and-tfidf-how-i-can-fix-it
word2vec best library,"<p>Hey I want to use word2vec algorithm without implementing it (I saw a lot of places that teaches how to implement one).
<br/>
<br/>
does anyone can tell me what is the best lib to use?
I saw there is Genesim, and Deeplearning4j. also TensorFlow but I can't find a place where they have the function that I need (only how to implement with this lib).
<br/>
<br/>
can someone give some comparison about efficiency? how easy to use? the word2vec algorithm for each lib?
any helpful tip or resources would be great.</p>
","machine-learning, deep-learning, word2vec, deeplearning4j","<p><a href=""https://spacy.io/"" rel=""nofollow noreferrer"">SpaCy</a> comes with pretrained word embeddings that you can use - it's very easy to use, you can find examples on how to download the embedding and use it <a href=""https://spacy.io/usage/vectors-similarity#basics"" rel=""nofollow noreferrer"">here</a>.</p>
",1,1,3171,2020-03-03 15:05:20,https://stackoverflow.com/questions/60510121/word2vec-best-library
Can word2vec deal with sequence of number?,"<p>I am very new to network embedding, especially for the attributed network embedding. Currently, I am studying the node2vec algorithm. I think the process is </p>

<pre><code>RandomWalk with p and q
Fed the walks to Word2Vec
</code></pre>

<p>For the second step, I see the algorithm takes every node as a string.</p>

<p>But my problem is that the nodes of my network are values. Maybe some nodes have the same value. I think this strategy will take the nodes with the same value as 'one' node. </p>

<p>Then what should I do if I want to embed such a network? My network is an attributed graph, each node has n dimensional attributes.</p>

<p>Thanks so much!</p>
","nlp, networkx, word2vec, grapheme-cluster","<p>I believe most applications of word2vec to graphs give each node a unique ID, which is then used as the 'word' token fed to the algorithm. If your nodes have other values, that repeat, those values aren't ideal as the node-IDs.</p>

<p>(While word2vec doesn't natively handle continuous-magnitudes, there has been some research extending it that way â€“ for example, I think <a href=""https://research.fb.com/downloads/starspace/"" rel=""nofollow noreferrer"">Facebook's 'StarSpace'</a> allows mixing scalar features with the discrete tokens of traditional word2vec. I suppose you could also consider banding <em>ranges</em> of your nodes' scalar dimensions into discrete tokens, which could sometimes be used instead of IDs, to learn embeddings for what a range-of-values might be related to.)</p>
",0,0,268,2020-03-14 19:21:26,https://stackoverflow.com/questions/60686492/can-word2vec-deal-with-sequence-of-number
[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count,"<p>Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.</p>

<p>I'm using the function <code>Word2Vec</code> from the python library <code>gensim</code>.</p>

<p>My problem is that I <strong>can't run my model on every word of my corpus as long as I set the parameter <code>min_count</code> greater than one</strong>. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an <strong>error saying <em>word 'blabla' is not in the vocabulary</em></strong>, whereas this is exactly what I want ( I want this word to be out of the vocabulary).</p>

<p>I can imagine this is not very clear, then find below a reproducible example:</p>

<pre><code>import gensim
from gensim.models import Word2Vec

# My corpus
corpus=[[""paris"",""not"",""great"",""city""],
       [""praha"",""better"",""great"",""than"",""paris""],
       [""praha"",""not"",""country""]]

# Load a pre-trained model - The orignal one based on google news 
model_google = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)

# Initializing our model and upgrading it with Google's 
my_model = Word2Vec(size=300, min_count=2)#with min_count=1, everything works fine
my_model.build_vocab(corpus)
total_examples = my_model.corpus_count
my_model.build_vocab([list(model_google.vocab.keys())], update=True)
my_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)
my_model.train(corpus, total_examples=total_examples, epochs=my_model.iter)

# Show examples
print(my_model['paris'][0:10])#works cause 'paris' is present twice
print(my_model['country'][0:10])#does not work cause 'country' appears only once
</code></pre>

<p>You can find Google's model <a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">there</a> for example, but feel free to use any model or just do without, this is not the point of my post.</p>

<p>As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter <code>min_count</code> to 1, everything works fine.</p>

<p>I hope it is clear enough.</p>

<p>Thanks.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>It is supposed to throw an error if you ask for a word that's not present because you chose not to learn vectors for rare words, like <code>'country'</code> in your example. (And: such words with few examples usually don't get good vectors, and retaining them can worsen the vectors for remaining words, so a <code>min_count</code> as large as you can manage, and perhaps much larger than <code>1</code>, is usually a good idea.)</p>

<p>The fix is to do one of the following:</p>

<ol>
<li>Don't ask for words that aren't present. Check first, via something like Python's <code>in</code> operator. For example:</li>
</ol>

<pre><code>if 'country' in my_model:
    print(my_model['country'][0:10])
else: 
    pass  # do nothing, since `min_count=2` means there's no 'country' vector
</code></pre>

<ol start=""2"">
<li>Catch the error, falling back to whatever you want to happen for absent words:</li>
</ol>

<pre><code>try:
    print(my_model['country'][0:10])
except:
    pass  # do nothing, or perhaps print an error, whatever
</code></pre>

<ol start=""3"">
<li>Change to using a model that always returns something for any word, like <code>FastText</code> â€“ which will try to synthesize a vector for unknown words, using subwords learned during training. (It might be garbage, it might be pretty good if the unknown word is highly similar to known words in characters &amp; meaning, but for some uses it's better than nothing.) </li>
</ol>
",2,2,4752,2020-03-17 17:05:46,https://stackoverflow.com/questions/60727025/word2vecgensim-handling-missing-words-in-vocabulary-with-the-parameter-min-c
How does word2vec predicts the word correctly but the actual dataset does not contain it?,"<p>I'm trying to understand how word2vec predicts a word, given a list of words.
Specifically, I trained my skip-gram model on twitter data of 500k tweets with the following parameters:</p>

<pre><code>model = gensim.models.Word2Vec(data, window=5, workers=7, sg=1, min_count=10, size=200)
</code></pre>

<p>Given the words <code>discrimination</code> and <code>uberx</code>, I get the following output:</p>

<pre><code>model.wv.most_similar(positive=[PorterStemmer().stem(WordNetLemmatizer().lemmatize(""discrimination"", pos='v')), WordNetLemmatizer().lemmatize(""uberx"", pos='v')], topn=30)
[('discret', 0.7425585985183716),
 ('fold_wheelchair', 0.7286415696144104),
 ('illeg_deni', 0.7280288338661194),
 ('tradit_cab', 0.7262350916862488),
 ('mobil_aid', 0.7252357602119446),
 ('accommod_disabl', 0.724936842918396),
 ('uberwav', 0.720955491065979),
 ('discrimin_disabl', 0.7206833958625793),
 ('deni_access', 0.7202375531196594),...]
</code></pre>

<p>However, when I search the dataset <code>data</code> which I dumped on my hard drive, for the words ""discrimination"", ""uberx"", and any other word from the output list, I never find a single instance of a datapoint which contained all 3 words. So my question is, how does the model know that, say, word ""accommodation disabled"" is the right word for the context ""discrimination"" and ""uberx"" if it has never seen those 3 words together in a single tweet?</p>
","python, nlp, word2vec","<p>The skip-gram model is working like the question of filling the blank. For example, there are two twitter data:</p>

<p>1) </p>

<p>It's summer now. Today is ___.</p>

<p>It's ______ now. Today is hot.</p>

<p>2) </p>

<p>It's winter now. Today is ____.</p>

<p>It's ______ now. Today is cold.</p>

<p>By training a model to predict the blank, the model learns that the representations of these two words, either (cold and winter) or (hot and summer), should be closer.</p>

<p>At the same time, it also learns that the distance between ""cold"" and ""summer"" should be increased, because when the context contains ""cold"", the blank is more likely to be ""winter"", which in turn suppresses the possibility of being ""summer"".</p>

<p>Thus, even though there is no one data containing ""cold"" and ""summer"", the model still can learn the relationship between these two words.</p>

<p>This is my humble opinion on skip-gram. Please feel free to discuss :)</p>
",2,0,64,2020-03-21 21:24:52,https://stackoverflow.com/questions/60793449/how-does-word2vec-predicts-the-word-correctly-but-the-actual-dataset-does-not-co
Fasttext aligned word vectors for translating homographs,"<p><a href=""https://en.wikipedia.org/wiki/Homograph"" rel=""nofollow noreferrer"">Homograph</a> is a word that shares the same written form as another word but has a different meaning, like <strong>right</strong> in the sentences below:</p>

<ul>
<li>success is about making the <strong>right</strong> decisions.</li>
<li>Turn <strong>right</strong> after the traffic light</li>
</ul>

<p>The English word ""right"", in the first case is translated to Swedish as ""rÃ¤tt"" and to ""hÃ¶ger"" in the second case. The correct translation is possible by looking at the context (surrounding words).</p>

<p><strong>Question 1.</strong> I wonder if fasttext aligned word embedding can come to help for translating these homograph words or words with several possible translations into another language?</p>

<p><strong>[EDIT]</strong> The goal is <strong>not</strong> to query the model for the right translation. The goal is to <strong>pick</strong> the right translation when the following information is given: </p>

<ul>
<li>the two (or several) possible translations options in the target language like ""rÃ¤tt"" and ""hÃ¶ger""</li>
<li>the surrounding words in the source language </li>
</ul>

<p><strong>Question 2.</strong> I loaded the <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">english pre-trained vectors model</a> and the <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">English <strong>aligned</strong> vector model</a>. While both were trained on Wikipedia articles, I noticed that the distances between two words were sort of preserved but the size of the dataset files (wiki.en.vec vs wiki.en.align.vec) are noticeably different (1GB). Wouldn't it make sense if we only use the aligned version? What information is not captured by the aligned dataset?</p>
","machine-learning, nlp, word2vec, fasttext, machine-translation","<p>For question 1, I suppose it's possible that these 'aligned' vectors could help translate homographs, but still face the problem that any token only has a single vector â€“ even if that one token has multiple meanings. </p>

<p>Are you assuming that you already know that <code>right[en]</code> could be translated into either <code>rÃ¤tt[se]</code> or <code>hÃ¶ger[se]</code>, from some external table? (That is, you're not using the aligned word-vectors as the primary means of translation, just an adjunct to other methods?)</p>

<p>If so, one technique that might help would be to see which of <code>rÃ¤tt[se]</code> or <code>hÃ¶ger[se]</code> is closer to other words that surround your particular instance of <code>right[en]</code>. (You might tally each's rank-closeness to every word within <em>n</em> spots of <code>right[en]</code>, or calculate their cosine-similarity to the average of the <em>n</em> words around <code>right[en]</code>, for example.)</p>

<p>(You could potentially even do this with <em>non-aligned</em> word vectors, if your more-precise words have multiple, alternate, non-homograph/non-polysemous translations in English. For example, to determine which sense of <code>right[en]</code> is more likely, you could use the non-aligned English word vectors for <code>correct[en]</code> and <code>rightward[en]</code> â€“ less polysemous correlates of <code>rÃ¤tt[se]</code> &amp; <code>hÃ¶ger[se]</code> â€“ to check for similarity-to-surrounding words.)</p>

<p>A write-up that might create other ideas is ""<a href=""http://www.offconvex.org/2016/07/10/embeddingspolysemy/"" rel=""nofollow noreferrer"">Linear algebraic structure of word meanings</a>"" which, quite surprisingly, is able to tease-out alternate meanings of homograph tokens even when the original word-vectors training was <em>not</em> word-sense-aware. (Might the 'atoms of discourse' in their model be equally findable across merged/aligned multi-language vector spaces, and then the closeness-of-context-words to different atoms a good guide to word-sense-disambiguation?)</p>

<p>For question 2, you imply the aligned word set is smaller in size. Have you checked if that's just because it includes fewer words? That seems the simplest explanation, and just checking which words are left out would let you know what you're losing. </p>
",1,0,1151,2020-03-25 11:16:29,https://stackoverflow.com/questions/60847705/fasttext-aligned-word-vectors-for-translating-homographs
Training time of gensim word2vec,"<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

class Corpus():
    """"""Iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = Corpus()

model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")

</code></pre>

<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>

<p><a href=""https://i.sstatic.net/QrJRM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QrJRM.png"" alt=""enter image description here""></a></p>

<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. 
Should I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>
","python, nlp, gensim, word2vec","<p>Completing a model requires one pass over all the data to discover the vocabulary, then multiple passes, with a default of 5, to perform vector training. So, you should expect to see about 6x your data size in disk-reads, just from the model training.</p>

<p>(If your machine winds up needing to use virtual-memory swapping during the process, there could be more disk activity â€“ but you absolutely do not want that to happen, as the random-access pattern of word2vec training is nearly a worst-case for virtual memory usage, which will slow training immensely.)</p>

<p>If you'd like to understand the code's progress, and be able to estimate its completion time, you should enable Python logging to at least the <code>INFO</code> level. Various steps of the process will report interim results (such as the discovered and surviving vocabulary size) and estimated progress. You can often tell if something is going wrong before the end of a run by studying the logging outputs for sensible values, and once the 'training' phase has begun the completion time will be a simple projection from the training completed so far. </p>

<p>I believe most laptops should throttle their own CPU if it's becoming so hot as to become unsafe or risk extreme wear on the CPU/components, but whether yours does, I can't say, and definitely make sure its fans work &amp; vents are unobstructed. </p>

<p>I'd suggest you choose some small random subset of your data â€“ maybe 1GB? â€“ to be able to run all your steps to completion, becoming familiar with the <code>Word2Vec</code> logging output, resource usage, and results, and tinkering with settings to observe changes, before trying to run on your full dataset, which might require days of training time. </p>

<p>Some of your shown parameters aren't optimal for speedy training. In particular:</p>

<ul>
<li><p><code>min_count=1</code> retains every word seen in the corpus-survey, including those with only a single occurrence. This results in a much, much larger model - potentially risking a model that doesn't fit into RAM, forcing disastrous swapping. But also, words with just a few usage examples can't possibly get good word vectors, as the process requires seeing many subtly-varied alternate uses. Still, via typical 'Zipfian' word-frequencies, the number of such words with just a few uses may be very large in total, so retaining all those words takes a lot of training time/effort, and even serves a bit like 'noise' making the training of other words, with plenty of usage examples, less effective. So for model size, training speed, <strong>and</strong> quality of remaining vectors, a larger <code>min_count</code> is desirable. The default of <code>min_count=5</code> is better for more projects than <code>min_count=1</code> â€“ this is a parameter that should only really be changed if you're sure you know the effects. And, when you have plentiful data â€“ as with your 34GB â€“ the <code>min_count</code> can go much higher to keep the model size manageable. </p></li>
<li><p><code>hs=1</code> should only be enabled if you want to use the 'hierarchical-softmax' training mode instead of 'negative-sampling' â€“ and in that case, <code>negative=0</code> should also be set to disable 'negative-sampling'. You probably don't want to use hierarchical-softmax: it's not the default for a reason, and it doesn't scale as well to larger datasets. But here you've enabled in in addition to negative-sampling, likely more-than-doubling the required training time. </p></li>
<li><p>Did you choose <code>negative=10</code> because you had problems with the default <code>negative=5</code>? Because this non-default choice, again, would slow training noticeably. (But also, again, a non-default choice here would be more common with smaller datasets, while larger datasets like yours are more likely to experiment with a smaller <code>negative</code> value.)</p></li>
</ul>

<p>The theme of the above observations is: ""only change the defaults if you've already got something working, and you have a good theory (or way of testing) how that change might help"". </p>

<p>With a large-enough dataset, there's another default parameter to consider changing to speed up training (&amp; often improve word-vector quality, as well): <code>sample</code>, which controls how-aggressively highly-frequent words (with many redundant usage-examples) may be downsampled (randomly skipped). </p>

<p>The default value, <code>sample=0.001</code> (aka <code>1e-03</code>), is very conservative. A smaller value, such as <code>sample=1e-05</code>, will discard many-more of the most-frequent-words' redundant usage examples, speeding overall training considerably. (And, for a corpus of your size, you could eventually experiment with even smaller, more-aggressive values.)</p>

<p>Finally, to the extent all your data (for either a full run, or a subset run) can be in an already-space-delimited text file, you can use the <code>corpus_file</code> alternate method of specifying the corpus. Then, the <code>Word2Vec</code> class will use an optimized multithreaded IO approach to assign sections of the file to alternate worker threads â€“ which, if you weren't previously seeing full saturation of all threads/CPU-cores, could increase our throughput. (I'd put this off until after trying other things, then check if your best setup still leaves some of your 8 threads often idle.)</p>
",14,7,5907,2020-03-25 16:22:38,https://stackoverflow.com/questions/60852962/training-time-of-gensim-word2vec
Document classification: Preprocessing and multiple labels,"<p>I have a question about the word representation algorithms:
Which one of the algorithms word2Vec, doc2Vec and Tf-IDF is more suitable for handling text classification tasks ?
The corpus used in my supervised learning classification is composed of a list of multiple sentences, with both short length sentences and long length ones. As discussed in <a href=""https://stackoverflow.com/questions/42643074/document-similarity-vector-embedding-versus-tf-idf-performance"">this thread</a>, doc2vec vs word2vec choice is a matter of document length. As for Tf-Idf vs. word embedding, it's more a matter of text representation.</p>

<p>My other question is, what if for the same corpus I had more than one label to link to the sentences in it ? If I create multiple entries/labels for the same sentence, it affects the decision of the final classification algorithm. How can I tell the model that every label counts equal for every sentence of the document ? </p>

<p>Thank you in advance,</p>
","word2vec, text-classification, tf-idf, doc2vec","<p>You should try multiple methods of turning your sentences into 'feature vectors'. There are no hard-and-fast rules; what works best for your project will depend a lot on your specific data, problem-domains, &amp; classification goals.</p>

<p>(Don't extrapolate guidelines from other answers â€“ such as the one you've linked that's about document-similarity rather than classification â€“ as best practices for your project.)</p>

<p>To get initially underway, you may want to focus on some simple 'binary classification' aspect of your data, first. For example, pick a single label. Train on all the texts, merely trying to predict if that one label applies or not.  </p>

<p>When you have that working, so you have a understanding of each step â€“ corpus prep, text processing, feature-vectorization, classification-training, classification-evaluation â€“ then you can try extending/adapting those steps to either single-label classification (where each text should have exactly one unique label) or multi-label classification (where each text might have any number of combined labels). </p>
",1,1,312,2020-03-27 11:53:07,https://stackoverflow.com/questions/60885461/document-classification-preprocessing-and-multiple-labels
How to set PYTHONHASHSEED environment variable in PyCharm for testing Word2Vec model?,"<p>I need to write a fully reproducible Word2Vec test, and need to set PYTHONHASHSEED to a fixed value. This is my current set-yp</p>

<pre><code># conftest.py
@pytest.fixture(autouse=True)
def env_setup(monkeypatch):
    monkeypatch.setenv(""PYTHONHASHSEED"", ""123"")

</code></pre>

<pre><code># test_w2v.py

def test_w2v():
    assert os.getenv(""PYTHONHASHSEED"") == ""123""
    expected_words_embeddings = np.array(...)
    w2v = Word2Vec(my_tokenized_sentences, workers=1, seed=42, hashfxn=hash)
    words_embeddings = np.array([w2v.wv.get_vector(word) for word in sentence for sentence in my_tokenized_sentences)])
    np.testing.assert_array_equal(expected_words_embeddings, words_embeddings)
</code></pre>

<p>Here is the curious thing.  </p>

<p>If I run the test from the terminal by doing <code>PYTHONHASHSEED=123 python3 -m pytest test_w2v.py</code> the test passes without any issues. However, if I run the test from PyCharm (using pytest, set up from Edit Configurations -> Templates -> Python tests -> pytest) then it fails. Most interestingly, it doesn't fail at <code>assert os.getenv(""PYTHONHASHSEED"") == ""123""</code>, but it fails at <code>np.testing.assert_array_equal(expected_words_embeddings, words_embeddings)</code></p>

<p>Why could this be the case, and is there a way to fix this issue?</p>
","python, unit-testing, testing, pytest, word2vec","<p>You can't set <code>PYTHONHASHSEED</code> in Python code; it needs to be set before the Python interpreter starts, because that's the only time it's consulted by the interpreter. You could possibly set it globally, before launching PyCharm, or there may be a PyCharm option to set environment variables for whatever execution environment you're triggering from PyCharm. (See for example: <a href=""https://stackoverflow.com/questions/42708389/how-to-set-environment-variables-in-pycharm"">How to set environment variables in PyCharm?</a> )</p>

<p>But more generally, you generally shouldn't be trying to make your <code>gensim</code> <code>Word2Vec</code> tests this deterministic. </p>

<p>If whatever you're testing is <em>that</em> sensitive to exact parameters â€“ because only an exact seeding &amp; (much slower) single-threaded training gets within your chosen tolerances, or gets an exact answer you copied from an earlier run â€“ then you're not really verifying the algorithm's contributions under the sorts of real randomness that it is typically subject-to. See more discussion in the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">gensim FAQ</a>. </p>
",3,1,3598,2020-03-29 11:16:53,https://stackoverflow.com/questions/60913405/how-to-set-pythonhashseed-environment-variable-in-pycharm-for-testing-word2vec-m
significance of periods in sentences while training documents with Doc2Vec,"<p>Doubt - 1</p>
<p>I am training Doc2Vec with 150000 documents. Since these documents are from legal domain they are really hard to clean and get it ready for further training. Hence I decided to remove all the periods from a document. Having said that, I am confused on how the parameter of <code>Window_size</code> in doc2vec recognize the sentences now. There are two views presented in the question :<a href=""https://stackoverflow.com/questions/42242521/doc2vec-differentiate-sentence-and-document"">Doc2Vec: Differentiate Sentence and Document</a></p>
<ol>
<li>The algorithm just works on chunks of text, without any idea of what a sentence/paragraph/document etc might be.</li>
<li>It's even common for the tokenization to retain punctuation, such as the periods between sentences, as standalone tokens.</li>
</ol>
<p>Therefore I am in confusion if my adopted approach of eliminating the punctuation (periods) is right. Kindly provide me with some supportive answers.</p>
<p>Doubt-2</p>
<p>The documents that I <strong>scraped</strong> range from 500 - 5500 tokens hence my approach to have a pretty even sized documents for training doc2vec and even to reduce the vocabulary is :
Consider a document of size greater than 1500 tokens in this case I make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens. The motivation for this kind of approach is from a paper related to Classification of documents using BERT where the sequence of 512 tokens were generated like this.</p>
<p>So I want to know if this idea is somewhat good to proceed or it's not recommended to do this?</p>
<p><strong>Update</strong> - I just saw the common_text corpus used by gensim in the tutorial link <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> and found that the documents in that corpus are simply tokens of words and do not contain any punctuation.
eg:</p>
<p><code>from gensim.test.utils import common_texts, common_dictionary, common_corpus</code></p>
<p><code>print(common_texts[0:10])</code></p>
<p>Output:</p>
<p><code>[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]</code></p>
<p>Same has been followed in the tutorial <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>.
So is my approach of removing periods in the document valid, if so then how will the window parameter work because in the documentation it is defined as follows:
window (int, optional) â€“ The maximum distance between the current and predicted word within a sentence.</p>
","python, gensim, word2vec, doc2vec","<p>Some people keep periods and other punctuation as standalone tokens, some eliminate them. </p>

<p>There's no definitively 'right' approach, and depending on your end goals, one or the other might make a slight difference in the doc-vector quality. So for now just do what's easiest for you, and then later if you have time, you can evaluate the alternate approach to see if it helps. </p>

<p>Despite any reference to 'sentences' in the docs, the <code>Word2Vec</code>/<code>Doc2Vec</code>/etc classes in <code>gensim</code> don't have any understanding of sentences, or special sensitivity to punctuation. They just see the lists-of-tokens you pass in as individual items in the corpus. So if you were to leave periods in, as in a short text like...</p>

<pre><code>['the', 'cat', 'was', 'orange', '.', 'it', 'meowed', '.']
</code></pre>

<p>...then the <code>'.'</code> string is just another pseudo-word, which will get a vector, and the training windows will reach through it just like any other word. (And, <code>'meowed'</code> will be 5 tokens away from <code>'cat'</code>, and thus have some influence if <code>window=5</code>.)</p>

<p>I don't quite understand what you mean about ""make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens"". <code>Doc2Vec</code> works fine up to texts of 10000 tokens. (More tokens than that will be silently ignored, due to an internal implementation limit of <code>gensim</code>.) It's not necessary or typical to break docs into smaller chunks, unless you have some other need to model smaller chunks of text. </p>

<p>The tiny <code>common_texts</code> set of word-lists is a contrived, toy-sized bit of data to demonstrate some basic code usage - it's not an example of recommended practices. The demos based on the 'Lee' corpus are similarly a quick intro to a tiny and simple approach that's just barely sufficient to show basic usage and results. It's text tokenization â€“Â via the <code>simple_preprocess()</code> utility method â€“ is an OK thing to try but not 'right' or 'best' compared to all the other possibilities.  </p>
",2,0,431,2020-04-05 10:19:12,https://stackoverflow.com/questions/61041080/significance-of-periods-in-sentences-while-training-documents-with-doc2vec
saving word2vec in text format,"<p>I tried to save word2vec vector as text, but it didnt work out, I got an error, that I dont really understand, what duplicates appear here and what is this ""wv"", that is proposed. Maybe somone can explain is to me. Thank you in advance </p>

<pre><code>model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
model.save_word2vec_format('test_w2v.txt', binary=False)
</code></pre>

<pre><code>WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
Word2Vec(vocab=20, size=300, alpha=0.025)
Traceback (most recent call last):
  File ""/word2vec.py"", line 26, in &lt;module&gt;
    model.save_word2vec_format('test_w2v.txt', binary=False)
  File ""/word2vec.py"", line 1307, in save_word2vec_format
    raise DeprecationWarning(""Deprecated. Use model.wv.save_word2vec_format instead."")
DeprecationWarning: Deprecated. Use model.wv.save_word2vec_format instead.

</code></pre>
","python, gensim, word2vec","<p>Because <code>.save_word2vec_format()</code> only saves the vectors â€“ not the full model â€“ it should only be used on the sub-property <code>.wv</code> of the model. (That's an object that just contains the vectors.)</p>

<p>So, if you run <code>model.wv.save_word2vec_format('test_w2v.txt', binary=False)</code>, as recommended by the error message, you'll save the vectors in text format.</p>

<p>(If you need to save the full model, use <code>model.save()</code> - it will save more information, and possibly use multiple additional files, but it will be in a Python- and gensim-specific format, unlike the plain text format that other tools can read.)</p>
",3,0,4528,2020-04-07 18:58:56,https://stackoverflow.com/questions/61087427/saving-word2vec-in-text-format
How does spaCy generate vectors for phrases?,"<p>Medium and large vocabularies of spaCy can generate vectors for words and phrases.  Let's consider the following example:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
    
nlp = spacy.load(&quot;en_core_web_md&quot;)
tokens = nlp(&quot;apple cat sky&quot;)
    
print(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector 
    
for token in tokens:
    print(token.text, token.vector[:3], token.vector_norm)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>apple cat sky [-0.06734333  0.03672066 -0.13952099] 4.845729844425328
apple [-0.36391  0.43771 -0.20447] 7.1346846
cat [-0.15067  -0.024468 -0.23368 ] 6.6808186
sky [ 0.31255  -0.30308   0.019587] 6.617719
</code></pre>
<p>It is clear that the vocabulary contains vectors for each word, but how are the vectors for the entire phase generated? As one can see it is not just simple sum of vectors.</p>
","nlp, spacy, word2vec","<p>By default, the vector of a <code>Doc</code> is the average of the vectors of the tokens, cf <a href=""https://spacy.io/usage/vectors-similarity"" rel=""noreferrer"">https://spacy.io/usage/vectors-similarity</a>:</p>

<blockquote>
  <p>Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.</p>
</blockquote>
",5,5,6895,2020-04-10 03:19:23,https://stackoverflow.com/questions/61133531/how-does-spacy-generate-vectors-for-phrases
How to initialize second glove model with solution from first?,"<p>I am trying to implement one of the solutions to the question about <a href=""https://stackoverflow.com/q/40697463/1036500"">How to align two GloVe models in text2vec?</a>. I don't understand what are the proper values for input at <code>GlobalVectors$new(..., init = list(w_i, w_j)</code>. How do I ensure the values for <code>w_i</code> and <code>w_j</code> are correct?</p>

<p>Here's a minimal reproducible example. First, prepare some corpora to compare, taken from the quanteda tutorial. I am using <code>dfm_match(all_words)</code> to try and ensure all words are present in each set, but this doesn't seem to have the desired effect. </p>

<pre><code>library(quanteda)

# from https://quanteda.io/articles/pkgdown/replication/text2vec.html

# get a list of all words in all documents
all_words &lt;-
  data_corpus_inaugural %&gt;% 
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %&gt;% 
  types()

# should expect this mean features in each set
length(all_words)

# these are our three sets that we want to compare, we want to project the
# change in a few key words on a fixed background of other words
corpus_1 &lt;- data_corpus_inaugural[1:19]
corpus_2 &lt;- data_corpus_inaugural[20:39]
corpus_3 &lt;- data_corpus_inaugural[40:58]

my_tokens1 &lt;- texts(corpus_1) %&gt;%
  char_tolower() %&gt;%
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) 

my_tokens2 &lt;- texts(corpus_2) %&gt;%
  char_tolower() %&gt;%
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) 

my_tokens3 &lt;- texts(corpus_3) %&gt;%
  char_tolower() %&gt;%
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) 

my_feats1 &lt;- 
  dfm(my_tokens1, verbose = TRUE) %&gt;%
  dfm_trim(min_termfreq = 5) %&gt;% 
  dfm_match(all_words) %&gt;% 
  featnames()

my_feats2 &lt;- 
  dfm(my_tokens2, verbose = TRUE) %&gt;%
  dfm_trim(min_termfreq = 5) %&gt;%
  dfm_match(all_words) %&gt;% 
  featnames()

my_feats3 &lt;- 
  dfm(my_tokens3, verbose = TRUE) %&gt;%
  dfm_trim(min_termfreq = 5) %&gt;%
  dfm_match(all_words) %&gt;% 
  featnames()

# leave the pads so that non-adjacent words will not become adjacent
my_toks1_2 &lt;- tokens_select(my_tokens1, my_feats1, padding = TRUE)
my_toks2_2 &lt;- tokens_select(my_tokens2, my_feats2, padding = TRUE)
my_toks3_2 &lt;- tokens_select(my_tokens3, my_feats3, padding = TRUE)

# Construct the feature co-occurrence matrix
my_fcm1 &lt;- fcm(my_toks1_2, context = ""window"", tri = TRUE)
my_fcm2 &lt;- fcm(my_toks2_2, context = ""window"", tri = TRUE)
my_fcm3 &lt;- fcm(my_toks3_2, context = ""window"", tri = TRUE)
</code></pre>

<p>Somewhere in the above steps I believe I need to ensure that the <code>fcm</code> for each set has all the words of all sets to get the matrix dimensions the same, but I'm not sure how to accomplish that. </p>

<p>Now fit the word embedding model for the first set:</p>

<pre><code>
library(""text2vec"")

glove1 &lt;- GlobalVectors$new(rank = 50, 
                            x_max = 10)

my_main1 &lt;- glove1$fit_transform(my_fcm1, 
                               n_iter = 10,
                               convergence_tol = 0.01, 
                               n_threads = 8)

my_context1 &lt;- glove1$components
word_vectors1 &lt;- my_main1 + t(my_context1)
</code></pre>

<p>And here is where I get stuck, I want to initialise the second model with the first, so that the coordinate system will be comparable between the first and second models. I <a href=""https://www.r-bloggers.com/text2vec-glove-implementation-details/"" rel=""nofollow noreferrer"">read</a> that <code>w_i</code> and <code>w_j</code> are main and context words, and <code>b_i</code> and <code>b_j</code> are biases. I've found output for those in my first model object, but I get an error:</p>

<pre><code>glove2 &lt;- GlobalVectors$new(rank = 50, 
                            x_max = 10,
                            init = list(w_i = glove1$.__enclos_env__$private$w_i, 
                                        b_i = glove1$.__enclos_env__$private$b_i, 
                                        w_j = glove1$.__enclos_env__$private$w_j, 
                                        b_j = glove1$.__enclos_env__$private$b_j))

my_main2 &lt;- glove2$fit_transform(my_fcm2, 
                                 n_iter = 10,
                                 convergence_tol = 0.01, 
                                 n_threads = 8)
</code></pre>

<p>The error is <code>Error in glove2$fit_transform(my_fcm2, n_iter = 10, convergence_tol = 0.01,  : 
  init values provided in the constructor don't match expected dimensions from the input matrix</code></p>

<p>Assuming I have identified <code>w_i</code>, etc., correctly in the first model, how can I get ensure they are the correct size?</p>

<p>Here's my session info: </p>

<pre><code>sessionInfo()
R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS  10.15.2

Matrix products: default
BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] text2vec_0.6   quanteda_2.0.0

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.4            pillar_1.4.3          compiler_3.6.0        tools_3.6.0           stopwords_1.0        
 [6] digest_0.6.25         packrat_0.5.0         lifecycle_0.2.0       tibble_3.0.0          gtable_0.3.0         
[11] lattice_0.20-40       pkgconfig_2.0.3       rlang_0.4.5           Matrix_1.2-18         fastmatch_1.1-0      
[16] cli_2.0.2             rstudioapi_0.11       mlapi_0.1.0           parallel_3.6.0        RhpcBLASctl_0.20-17  
[21] dplyr_0.8.5           vctrs_0.2.4           grid_3.6.0            tidyselect_1.0.0.9000 glue_1.3.2           
[26] data.table_1.12.8     R6_2.4.1              fansi_0.4.1           lgr_0.3.4             ggplot2_3.3.0        
[31] purrr_0.3.3           magrittr_1.5          scales_1.1.0          ellipsis_0.3.0        assertthat_0.2.1     
[36] float_0.2-3           rsparse_0.4.0         colorspace_1.4-1      stringi_1.4.6         RcppParallel_5.0.0   
[41] munsell_0.5.0         crayon_1.3.4.9000 

</code></pre>
","r, matrix, nlp, word2vec, quanteda","<p>Here is a working example. See <code>?rsparse::GloVe</code> documentation for details.</p>

<pre class=""lang-r prettyprint-override""><code>library(rsparse)
data(""movielens100k"")
x = crossprod(sign(movielens100k))

model = GloVe$new(rank = 10, x_max = 5)

w_i = model$fit_transform(x = x, n_iter = 5, n_threads = 1)
w_j = model$components
init = list(w_i = t(w_i), model$bias_i, w_j = w_j, b_j = model$bias_j)

model2 = GloVe$new(rank = 10, x_max = 10, init = init)
w_i2 = model2$fit_transform(x)
</code></pre>
",1,3,357,2020-04-10 18:24:52,https://stackoverflow.com/questions/61146392/how-to-initialize-second-glove-model-with-solution-from-first
How to do Topic Detection in Unsupervised Aspect Based Sentiment Analysis,"<p>I want to make an ABSA using Python where the sentiment of pre-defined aspects (e.g. delivery, quality, service) is analyzed from online reviews. I want to do it unsupervised because this will save me from manually labeling reviews and I can analyze a lot more review data (looking at around 100k reviews). Therefore, my datasets consists of only reviews and no ratings. I would like to have a model that can first detect the aspect category and then assign the sentiment polarity. E.g. when the review says ""The shipment went smoothly, but the product is broken"" I want the model to assign the word ""shipment"" to the aspect category ""delivery"" and ""smoothly"" relates to a positive sentiment. </p>

<p>I have searched for approaches to take and I would like to know if anyone has experience with this and could guide me into a direction that could help me. It will be highly appreciated!</p>
","python, nlp, word2vec, sentiment-analysis","<blockquote>
  <p>Aspect Based Sentiment Analysis (ABSA), where the task is first to
  extract aspects or features of an entity (i.e. Aspect Term Extraction
  or ATE1 ) from a given text, and second to determine the sentiment
  polarity (SP), if any, towards each aspect of that entity. The
  importance of ABSA led to the creation of the ABSA task </p>
  
  <p>B-LSTM &amp; CRF classifier will be used for feature extraction and aspect
  term detection for both supervised and unsupervised ATE.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/F4V7D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F4V7D.png"" alt=""enter image description here""></a></p>

<p><a href=""https://www.researchgate.net/profile/Andreea_Hossmann/publication/319875533_Unsupervised_Aspect_Term_Extraction_with_B-LSTM_and_CRF_using_Automatically_Labelled_Datasets/links/5a3436a70f7e9b10d842b0eb/Unsupervised-Aspect-Term-Extraction-with-B-LSTM-and-CRF-using-Automatically-Labelled-Datasets.pdf"" rel=""nofollow noreferrer"">https://www.researchgate.net/profile/Andreea_Hossmann/publication/319875533_Unsupervised_Aspect_Term_Extraction_with_B-LSTM_and_CRF_using_Automatically_Labelled_Datasets/links/5a3436a70f7e9b10d842b0eb/Unsupervised-Aspect-Term-Extraction-with-B-LSTM-and-CRF-using-Automatically-Labelled-Datasets.pdf</a></p>

<p><a href=""https://github.com/songyouwei/ABSA-PyTorch/blob/master/infer_example.py"" rel=""nofollow noreferrer"">https://github.com/songyouwei/ABSA-PyTorch/blob/master/infer_example.py</a></p>

<pre><code># -*- coding: utf-8 -*-
# file: infer.py
# author: songyouwei &lt;youwei0314@gmail.com&gt;
# Copyright (C) 2019. All Rights Reserved.

import torch
import torch.nn.functional as F
import argparse

from data_utils import build_tokenizer, build_embedding_matrix
from models import IAN, MemNet, ATAE_LSTM, AOA


class Inferer:
    """"""A simple inference example""""""
    def __init__(self, opt):
        self.opt = opt
        self.tokenizer = build_tokenizer(
            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],
            max_seq_len=opt.max_seq_len,
            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))
        embedding_matrix = build_embedding_matrix(
            word2idx=self.tokenizer.word2idx,
            embed_dim=opt.embed_dim,
            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))
        self.model = opt.model_class(embedding_matrix, opt)
        print('loading model {0} ...'.format(opt.model_name))
        self.model.load_state_dict(torch.load(opt.state_dict_path))
        self.model = self.model.to(opt.device)
        # switch model to evaluation mode
        self.model.eval()
        torch.autograd.set_grad_enabled(False)

    def evaluate(self, raw_texts):
        context_seqs = [self.tokenizer.text_to_sequence(raw_text.lower().strip()) for raw_text in raw_texts]
        aspect_seqs = [self.tokenizer.text_to_sequence('null')] * len(raw_texts)
        context_indices = torch.tensor(context_seqs, dtype=torch.int64).to(self.opt.device)
        aspect_indices = torch.tensor(aspect_seqs, dtype=torch.int64).to(self.opt.device)

        t_inputs = [context_indices, aspect_indices]
        t_outputs = self.model(t_inputs)

        t_probs = F.softmax(t_outputs, dim=-1).cpu().numpy()
        return t_probs


if __name__ == '__main__':
    model_classes = {
        'atae_lstm': ATAE_LSTM,
        'ian': IAN,
        'memnet': MemNet,
        'aoa': AOA,
    }
    # set your trained models here
    model_state_dict_paths = {
        'atae_lstm': 'state_dict/atae_lstm_restaurant_acc0.7786',
        'ian': 'state_dict/ian_restaurant_acc0.7911',
        'memnet': 'state_dict/memnet_restaurant_acc0.7911',
        'aoa': 'state_dict/aoa_restaurant_acc0.8063',
    }
    class Option(object): pass
    opt = Option()
    opt.model_name = 'ian'
    opt.model_class = model_classes[opt.model_name]
    opt.dataset = 'restaurant'
    opt.dataset_file = {
        'train': './datasets/semeval14/Restaurants_Train.xml.seg',
        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'
    }
    opt.state_dict_path = model_state_dict_paths[opt.model_name]
    opt.embed_dim = 300
    opt.hidden_dim = 300
    opt.max_seq_len = 80
    opt.polarities_dim = 3
    opt.hops = 3
    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    inf = Inferer(opt)
    t_probs = inf.evaluate(['happy memory', 'the service is terrible', 'just normal food'])
    print(t_probs.argmax(axis=-1) - 1)
</code></pre>
",2,1,1796,2020-04-17 17:22:20,https://stackoverflow.com/questions/61277201/how-to-do-topic-detection-in-unsupervised-aspect-based-sentiment-analysis
Word2vec is a Generalization or memorization algorithm?,"<p>I need to know that word2vec is a Generalization algorithm like all ML algorithms or its Memorization algorithm like KNN ?
Because we have 2 types of algorithms model based and memory based , word2vec is coming in which category when it's using for most_similar_items</p>
","machine-learning, word2vec, knn","<p>Let me define generalization as the ability of a model which has completed training to be effective in prediction across a whole range of inputs, including include inputs that is not part of training. From that perspective, Word2Vec cannot predict words that are not part of the training dataset because it simply would not have trained on the context of it to create an embedding. To qualify as a generalization method, it needs to be able to predict on an input which was not part of the training dataset. </p>

<p>Word2Vec model maintains a dictionary of words to the corresponding embedding/vector. In summary, cannot predict on unknown words. This was one of the important differences between fastText model and Word2Vec. </p>
",0,-1,95,2020-04-19 00:56:04,https://stackoverflow.com/questions/61298251/word2vec-is-a-generalization-or-memorization-algorithm
Is the Gensim word2vec model same as the standard model by Mikolov?,"<p>I am implementing a paper to compare our performance. In the paper, the uathor says </p>

<blockquote>
  <p>300-dimensional pre-trained word2vec vectors (Mikolov et al., 2013)</p>
</blockquote>

<p>I am wondering whether the pretrained word2vec Gensim model <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"" rel=""nofollow noreferrer"">here</a> is same as the pretrained embeddings on the official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google site</a> (the GoogleNews-vectors-negative300.bin.gz file)</p>

<p><br>
My source of doubt arises from this line in Gensim documentation (in Word2Vec Demo section) </p>

<blockquote>
  <p>We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases</p>
</blockquote>

<p>Does this mean the model on gensim is not fully trained? Is it different from the official embeddings by Mikolov? </p>
","python, nlp, gensim, word2vec","<p>That demo code for reading word-vectors is downloading the exact same Google-trained <code>GoogleNews-vectors-negative300</code> set of vectors. (No one else can try re-training that dataset, because the original corpus of news articles user, over 100B words of training data from around 2013 if I recall correctly, is internal to Google.)</p>

<p>Algorithmically, the <code>gensim</code> <code>Word2Vec</code> implementation was closely modeled after the <code>word2vec.c</code> code released by Google/Mikolov, so should match its results in measurable respects with regard to any newly-trained vectors. (Slight differences in threading approaches may have a slight difference.)</p>
",1,0,259,2020-04-19 11:28:03,https://stackoverflow.com/questions/61303579/is-the-gensim-word2vec-model-same-as-the-standard-model-by-mikolov
How to embed user names in word2vec model in gensim,"<p>I have some volunteer essay writings in the format of:</p>

<pre><code>volunteer_names, essay
[""emi"", ""jenne"", ""john""], [[""lets"", ""protect"", ""nature""], [""what"", ""is"", ""nature""], [""nature"", ""humans"", ""earth""]]
[""jenne"", ""li""], [[""lets"", ""manage"", ""waste""]]
[""emi"", ""li"", ""jim""], [[""python"", ""is"", ""cool""]]
...
...
...
</code></pre>

<p>I want to identify the similar users based on their essay writings. I feel like word2vec is more suitable in problems like this. However, since I want to embed user names too in the model I am not sure how to do it. The examples I found in the internet only uses the words (See example code).</p>

<pre><code>import gensim 
sentences = [['first', 'sentence'], ['second', 'sentence']]
# train word2vec on the two sentences
model = gensim.models.Word2Vec(sentences, min_count=1)
</code></pre>

<p>In that case, I am wondering if there is special way of doing this in word2vec or can I simply consider user names as just words to input to the model. please let me know your thoughts on this.</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec","<p>Word2vec infers the word representation from surrounding words: words similarly often appear in a similar company end up with similar vectors. Usually, a window of 5 words is considered. So, if you want to hack Word2vec, you would need to make sure that the student names will appear frequently enough (perhaps at a beginning and at the end of a sentence or something like that).</p>

<p>Alternatively, you can have a look at Doc2vec. During training, each document gets an ID and learns an embedding for the ID, they are in a lookup table as if they were word embeddings. If you use student names as document IDs, you would get student embeddings. If you have multiple essays from one student, I suppose you would need to hack Gensim a little bit not to have a unique ID for each essay.</p>
",2,0,997,2020-04-21 07:03:49,https://stackoverflow.com/questions/61337725/how-to-embed-user-names-in-word2vec-model-in-gensim
How to filter a model with respect to text and then use most_similar?,"<p>I have the text and I want to filter the model with respect to the text. It is OK?</p>

<pre><code>import pandas as pd
import gensim
import nltk
from nltk import word_tokenize
from nltk.collocations import *
from nltk.stem.wordnet import WordNetLemmatizer
import re

text = ""though quite simple room solid choice allocated room already used summer holiday apartment bel endroit nice place place winter""
from gensim.models import Word2Vec,  KeyedVectors

model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz')
model_filter = [w for w in list(model.wv.vocab) if w in text]
</code></pre>

<p>If it is OK how to filter in the results (model_filter ) of the most similar function (modelo_filtrado.most_similar_cosmul????), those that belong to the text?
Thx.</p>
","python, model, nltk, word2vec","<p>Your <code>text</code> is a plain string. The words inside the <code>model</code> are individual word strings. So, your existing check is looking to see if individual words appear <em>anywhere</em> as substrings inside your <code>text</code>. </p>

<p>For example, even though <code>'ice'</code> does not appear in your <code>text</code> as a word, this will evaluate as <code>True</code>:</p>

<pre><code>'ice' in ""though quite simple room solid choice allocated room already used summer holiday apartment bel endroit nice place place winter""
</code></pre>

<p>You probably want to turn your <code>text</code> into a list of words, first:</p>

<pre><code>text_words = text.split()
</code></pre>

<p>Otherwise, yes, your code will fille <code>model_filter</code> with only those words that are both in the <code>model</code> and <code>in</code> your <code>text</code> (or <code>text_words</code>). </p>
",2,0,36,2020-04-21 22:33:30,https://stackoverflow.com/questions/61354352/how-to-filter-a-model-with-respect-to-text-and-then-use-most-similar
"word2vec, using document body or keywords as training corpus","<p>I would like to train a <code>word2vec</code> model using what is an unordered list of keywords and categories for each document. Therefore my vocabulary is quite small around 2.5k tokens.</p>

<p>Would the performance be improved if at the training step, I used actual sentences from the document?</p>

<p>From example:</p>

<pre><code>doc_keywords = ['beach', 'holiday', 'warm']
doc_body = 'Going on a beach holiday it can be very warm'
</code></pre>

<p>If there is a benefit to using the full documents, could someone also explain why this is the case?</p>

<p>Since the model predicts the next word in a document, what would be the benefit to it learning <code>very -&gt; warm</code> as two words which often come together, given that <code>very</code> is not in my vocabulary.</p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>Your dataset seems quite small â€“ perhaps too small to expect good word2vec vectors. But, a small dataset at least means it shouldn't take too much time to try things in many different ways. </p>

<p>So, the best answer (and the only one that truly takes into account whatever uniqueness might be in your data &amp; project goals): do you get better final word-vectors, for your project-specific needs, when training on just the keywords, or the longer documents?</p>

<p>Two <em>potential</em> sources of advantage from using the full texts:</p>

<ul>
<li><p>Those less-interesting words might still help tease-out subtleties of meaning in the full vector space. For example, a contrast between <code>'warm'</code> and <code>'hot'</code> might become clearer when those words are forced to predict other related words that co-occur with each in different proportions. (But, such qualities of word2vec vectors require lots of subtly-varied real usage examples â€“ so such a benefit might not be possible in a small dataset.)</p></li>
<li><p>Using the real texts preserves the original proximity-influences â€“ words nearer each other have more influence. The keywords-only approach might be scrambling those original proximities, depending on how you're turning raw full texts into your reduced keywords. (In particular, you definitely do <em>not</em> want to always report keywords in some database-sort order â€“ as that would tend to create a spurious influence between keywords that happen to sort next-to each other, as opposed to appear next-to each other in natural language.)</p></li>
</ul>

<p>On the other hand, including more words makes the model larger &amp; the training slower, which might limit the amount of training or experiments you can run. And, keeping very-rare words â€“ that don't have enough varied usage examples to get good word-vectors themselves â€“ tends to act like 'noise' that dilutes the quality of other word-vectors. (That's why dropping rare words, with a <code>min_count</code> similiar to its default of <code>5</code> â€“ or larger in larger corpuses â€“ is almost always a good idea.)</p>

<p>So, there's no sure answer for which will be better: different factors, and other data/parameter/goals choices, will pull different ways. You'll want to try it in multiple ways. </p>
",1,0,385,2020-04-27 14:03:20,https://stackoverflow.com/questions/61460683/word2vec-using-document-body-or-keywords-as-training-corpus
The center of a word vector,"<p>I'm studying NLP and wrapping my head around the steps of passing through a Multi-Layer Perceptron. Since vectors are magnitude and direction in a space, I'm curious what the center of a word vector represents. In a very simple vector, my word might be 21, -5. Does 0,0 represent anything? If not, could it represent something after training a model? </p>

<p>If I understand correctly, a word that has never been seen before will be given a numerical identity and a vector of M dimensions. This vector then passes into the first layer, which has as many nodes as there are dimensions, so in this case M nodes. Through backpropagation the weights are changed so that similar words ""group"" together in vector space. (So that means the word vectors themselves are never modified from their initial random value, right?) Please correct me if I've made wrong assumptions here. I would just appreciate some insight. </p>
","tensorflow, nlp, vectorization, word2vec, dl4j","<p>You can think of word 'vectors', numerically, as just points. It's not really significant that they all 'start' at the origin (<code>[0.0, 0.0, 0.0, ..., 0.0]</code>). </p>

<p>The 'center' of any such vector is just its midpoint, which is also a vector of the same 'directionality' with half the magnitude. <strong>Often but not always</strong>, word-vectors are only compared in terms of raw-direction, <strong>not</strong> magnitude, via 'cosine similarity', which is essentially an angle-of-difference calculation that's oblvious to length/magnitude. (So, <code>cosine_similarity(a, b)</code> will be the same as <code>cosine_similarity(a/2, b)</code> or <code>cosine_similarity(a, b*4)</code>, etc.) So this 'center'/half-length instance you've asked about is usually less meaningful, with word-vectors, than in other vector models. And in general, as long as you're using cosine-similarity as your main method of comparing vectors, moving them closer to the origin-point is irrelevant. So, in that framing, the origin point doesn't really have a distinct meaning.</p>

<p>Caveat with regard to magnitudes: the actual raw vectors created by word2vec training do in fact have a variety of magnitudes. Some have observed that these magnitudes sometimes correlate with interesting word differences â€“ for example, highly polysemous words (with many alternate meanings) can often be lower-magnitude than words with one dominant meaning â€“ as the need to ""do something useful"" in alternate contexts tugs the vector between extremes during training, leaving it more ""in the middle"". And while word-to-word comparisons usually ignore these magnitudes for the purely angular cosine-similarity, sometimes downstream uses, such as text classification, may do incrementally better keeping the raw magnitudes. </p>

<p>Caveat with regard to the origin point: At least one paper, ""<a href=""https://arxiv.org/abs/1702.01417v2"" rel=""nofollow noreferrer"">All-but-the-Top: Simple and Effective Postprocessing for Word Representations</a>"" by Mu, Bhat, &amp; Viswanath, has observed that often the 'average' of all word-vectors isn't the origin-point, but significantly biased in one direction â€“ which (in my stylized understanding) sort-of leaves the whole space imbalanced, in terms of whether it's using 'all angles' to represent contrasts-in-meaning. (Also, in my experiments, the extent of this imbalance seems a function of how many <code>negative</code> examples are used in negative-sampling.) They found that postprocessing the vectors to recenter them improved performance on some tasks, but I've not seen many other projects adopt this as a standard step. (They also suggest some other postprocessing transformations to essentially 'increase contrast in the most valuable dimensions'.)</p>

<p>Regarding your ""IIUC"", yes, words are given starting vectors - <strong>but</strong> these are random, and then constantly adjusted via backprop-nudges, repeatedly after trying every training example in turn, to make those 'input word' vectors ever-so-slightly better as inputs to the neural network that's trying to predict nearby 'target/center/output' words. Both the networks 'internal'/'hidden' weights are adjusted, and the <code>input vectors</code> themselves, which are essentially 'projection weights' â€“ from a one-hot representation of a single vocabulary word, 'to' the M different internal hidden-layer nodes. That is, each 'word vector' is essentially a word-specific subset of the neural-networks' internal weights. </p>
",1,0,538,2020-04-28 13:44:05,https://stackoverflow.com/questions/61481721/the-center-of-a-word-vector
How does gensim manage to find the most similar words so fast?,"<p>Let's say we train a model with more than 1 million words. In order to find the most similar words we need to calculate the distance between the embedding of the test word and embeddings of all the 1 million words words, and then find the nearest words. It seems that Gensim calculate the results very fast. Although when I want to calculate the most similar, my function is extremely slow:</p>

<pre><code>def euclidean_most_similars (model, word, topn = 10):
  distances = {}
  vec1 = model[word]
  for item in model.wv.vocab:
    if item!= node:
      vec2 = model[item]
      dist = np.linalg.norm(vec1 - vec2)
      distances[(node, item)] = dist
  sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))
</code></pre>

<p>I would like to know how Gensim manages to calculate the most nearest words so fast and what is an efficient way to calculate the most similares.</p>
","python, time-complexity, gensim, word2vec, similarity","<p>As @g-anderson commented, the <code>gensim</code> source can be reviewed to see exactly what it does. However, <code>gensim</code> is not actually using any of its own optimized Cython or compiled-C code as part of its <code>most_similar()</code> method â€“ which can be reviewed at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/b287fd841c31d0dfa899d784da0bd5b3669e104d/gensim/models/keyedvectors.py#L689"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/b287fd841c31d0dfa899d784da0bd5b3669e104d/gensim/models/keyedvectors.py#L689</a></p>
<p>Instead, by using <code>numpy</code>/<code>scipy</code> bulk array operations, those libraries' highly optimized code will take advantage of both CPU primitives and multithreading to calculate <em>all</em> the relevant similarities far faster than an interpreted Python loop.</p>
<p>(The key workhorse is the <code>numpy</code> <code>dot</code> operation: one call which creates an ordered array of all the similarities â€“ skipping the loop &amp; your interim-results <code>dict</code> entirely. But the <code>argsort</code>, passing through to <code>numpy</code> implementations as well, likely also outperforms the idiomatic <code>sorted()</code>.)</p>
",1,3,1523,2020-04-29 20:29:46,https://stackoverflow.com/questions/61511101/how-does-gensim-manage-to-find-the-most-similar-words-so-fast
How to set PYTHONHASHSEED in python file,"<p>I'm trying to set PYTHONHASHSEED=0 in my python file and my python version is 3.6.
I'm using <strong>Word2Vec</strong> model ""(<code>Word2Vec(description, min_count=1, size= 100, workers=3, window =3, sg = 1, seed=0)</code>)"", i am not getting the consistent result.</p>

<p>Is there any way I can set the PYTHONHASHSEED for my python filename.py?</p>
","python, word2vec","<p>You need to set the environment variable <code>PYTHONHASHSEED</code> to 0 <em>before</em> Python is even running. If you start Python from the terminal you can do something like:</p>

<pre><code>export PYTHONHASHSEED=0
python ...
</code></pre>

<p>We can test it works by looking at the hash of a string and how it changes between runs of Python:</p>

<pre><code>$ python -c 'print(hash(""hi""))'
-6850579690611595074
$ python -c 'print(hash(""hi""))'
-5185907786673828222

$ export PYTHONHASHSEED=0

$ python -c 'print(hash(""hi""))'
-8951030814243160003
$ python -c 'print(hash(""hi""))'
-8951030814243160003
</code></pre>

<p>Note that this is likely not the only cause of possible indeterminism in a deep learning model. For example, for PyTorch, see <a href=""https://pytorch.org/docs/stable/notes/randomness.html"" rel=""nofollow noreferrer"">Reproducibility</a>.</p>
",1,1,4372,2020-05-11 09:08:50,https://stackoverflow.com/questions/61726053/how-to-set-pythonhashseed-in-python-file
How to compare cosine similarities across three pretrained models?,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
","nlp, gensim, word2vec, word-embedding, glove","<p>It's certainly <em>possible</em>. Whether it's meaningful, given a certain amount of data, is harder to answer. </p>

<p>Note that in separate training sessions, a given word <em>A</em> won't necessarily wind up in the same coordinates, due to inherent randomness used by the algorithm. That's even the case when training on the <em>exact same data</em>. </p>

<p>It's just the case that in general, the distances/directions to other words <em>B</em>, <em>C</em>, etc should be of similar overall usefulness, when there's sufficient data/training and well-chosen parameters. So <em>A</em>, <em>B</em>, <em>C</em>, etc may be in different places, with slightly-different distances/directions â€“ but the relative relationships are still similar, in terms of neighborhoods-of-words, or the <em>(A-B)</em> direction still be predictive of certain human-perceptible meaning-differences if applied to other words <em>C</em> etc. </p>

<p>So, you should avoid making direct cosine-similarity comparisons between words from different training-runs or corpuses, but you may find meaning in differences in similarities ( <em>A-B</em> vs <em>A' - B'</em> ) or top-N lists or relative-rankings. (This could also be how to compare against 3rd corpora: to what extent is there variance or correlation in certain pairwise-similarities, or top-N lists, or ordinal ranks of relevant words in each other words' 'most similar' results.)</p>

<p>You might want to perform a sanity check on your measures, by seeing to what extent they imply meaningful differences in comparisons where they logically ""shouldn't"". For example, multiple runs against the exact same corpus that's just bee reshuffled, or against random subsets of the exact same corpus. (I'm not aware of anything as formal as a 't-test' in checking the significance of differences between word2vec models, but checking whether some differences are enough to distinguish a truly-different corpus, from just a 1/Nth random subset of the same corpus, to a certain confidence level might be a grounded way to assert meaningful differences.)</p>

<p>To the extent such ""oughtta be very similar"" runs instead show end vector results that are tangibly different, it could be suggestive that either:</p>

<ul>
<li><p>the corpus is too small, with too few varied usage examples per word - word2vec benefits from lots of data, and political speech collections may be quite small compared to the sometimes hundreds-of-billions training words used for large word2vec models</p></li>
<li><p>the model is mis-parameterized - an oversized (and thus prone to overfitting) model, or insufficient training passes, or other suboptimal parameters may yield models that vary more for the same training data</p></li>
</ul>

<p>You'd also want to watch out for mismatches in training-corpus size. A corpus that's 10x as large means many more words would pass a fixed <code>min_count</code> threshold, and any chosen <em>N</em> <code>epochs</code> of training will involve 10x as many examples of common-words, and support stable results in a larger (vector-size) model - whereas the same model parameters with a smaller corpus would give more volatile results. </p>

<p>Another technique you could consider would be combining corpuses into one training set, but munging the tokens of key words-of-interest to be different depending on the relevant speaker. For example, you'd replace the word <code>'family'</code> with <code>'f-family'</code> or <code>'m-family'</code>, depending on the gender of the speaker. (You might do this for every occurrence, or some fraction of the occurrences. You might also enter each speech into your corpus more than once, sometimes with the actual words and sometimes with some-or-all replaced with the context-labeled alternates.)</p>

<p>In that case, you'd wind up with one final model, and all words/context-tokens in the 'same' coordinate space for direct comparison. But, the pseudowords <code>'f-family'</code> and <code>'m-family'</code> would have been more influenced by their context-specific usages - and thus their vectors might vary from each other, and from the original <code>'family'</code> (if you've also retained unmunged instances of its use) in interestingly suggestive ways.</p>

<p>Also note: if using the 'analogy-solving' methods of the original Google word2vec code release, or other libraries that have followed its example (like <code>gensim</code>), note that it specifically <em>won't</em> return as an answer any of the words supplied as input. So when solving the gender-fraught analogy <code>'man' : 'doctor' :: 'woman' : _?_</code>, via the call <code>model.most_similar(positive=['doctor', 'woman'], negative=['man'])</code>, even if the underlying model <em>still</em> has <code>'doctor'</code> as the closest word to the target-coordinates, it is automatically skipped as one of the input words, yielding the second-closest word instead. </p>

<p>Some early ""bias-in-word-vectors"" write-ups ignored this detail and thus tended to imply larger biases, due to this implementation artifact, even where such biases small-to-nonexistent. (You can supply raw vectors, instead of string-tokens, to <code>most_similar()</code> - and then get full results, without any filtering of input-tokens.)</p>
",4,1,1401,2020-05-11 18:38:20,https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models
"KeyError: &quot;word &#39;restrictions&#39; not in vocabulary&quot; while generating word embedding vectors for text, read from a textfile","<p>I have got this error: ""KeyError: word 'restriction' not in vocabulary"", when I read a text file to generate word embedding vectors, while the word 'restrictions' is in the text file. I wonder if my code for reading a textfile (a simple paragraph) is erroneous?</p>

<p>MY CODE IS WRITTN BELOW:</p>

<pre><code>from gensim.models import Word2Vec
# define training data
with open('D:\\test.txt', 'r') as file:
sentences = """"
#read from textfile
for line in file:
    for word in line.split(' '):
        sentences += word + ' '
# train model
model = Word2Vec(sentences, min_count=1)
# summarize the loaded model
print(model)
# summarize vocabulary
words = list(model.wv.vocab)
# save model
model.save('model.bin')
# load model
new_model = Word2Vec.load('model.bin')
print(new_model)
print(str(model['restriction']))
</code></pre>

<p><strong>This error does not happen when I use pre-written sentences inside the code as follows:</strong></p>

<pre><code>from gensim.models import Word2Vec
# define training data
sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],  
                ['this', 'is', 'the', 'second', 'sentence'],  
                ['yet', 'another', 'sentence'],  
                ['one', 'more', 'sentence', 'with', 'restriction'],
                ['and', 'the', 'final', 'sentence']]
# train model
model = Word2Vec(sentences, min_count=1)
# summarize the loaded model
print(model)
# summarize vocabulary
words = list(model.wv.vocab)
print(words)
# access vector for one word
print(model['sentence'])
# save model
model.save('model.bin')
# load model
new_model = Word2Vec.load('model.bin')
print(new_model)
print('the model prints: ')
print(model['restriction'])
</code></pre>
","python, deep-learning, text-files, word2vec","<p>In your code showing the problem, examine <code>sentences</code> closely, after you've constructed it, to see if it is the format you expect (or anything at all like the <code>sentences</code> of the working case). I suspect it isn't. </p>

<p>Also, take a look at the disappointing model's list of words learned â€“ the <code>words</code> variable you've created should suffice. It also may not look like the words you expect. </p>

<p>Specifically, this section of your code...</p>

<pre><code>sentences = """"
for line in file:
    for word in line.split(' '):
        sentences += word + ' '
</code></pre>

<p>...makes <code>sentences</code> one long string, with lots of space-separated words. If you did that to the <code>sentences</code> in your working code, you'd no longer have a list, where each item is a list of tokens. (That's a good input format for <code>Word2Vec</code>.) Instead, you'd have one giant run-on string:</p>

<pre><code>sentences = 'this is the first sentence for word2vec this is the second sentence yet another sentence one more sentence with restriction and the final sentence'
</code></pre>

<p>Try instead:</p>

<pre><code>sentences = []  # empty list
# OOPS, DON'T DO: sentences = """"
for line in file:
    sentences.append(line.split(' '))
</code></pre>

<p>...then your <code>sentences</code> will be a list-of-lists-of-strings (like the working case), instead of just a string (like the broken case). </p>
",0,0,161,2020-05-17 16:02:04,https://stackoverflow.com/questions/61854776/keyerror-word-restrictions-not-in-vocabulary-while-generating-word-embeddin
Gensim saving word vectors in txt format error,"<p>My issue is the following. I have some pretrained vectors saved in txt format, I load them in a dict. But when I try to save them after training them again in gensim it gives me an error, like the following:</p>

<pre><code>UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>

<p>I'm using this code to create the gensim word2vec:</p>

<pre><code>w2vObject = gensim.models.Word2Vec(min_count=1, sample=threshold, sg=1,size=dimension, negative=15, iter=epochsNum, window=3) # create only the shell

print('Starting vocab build')
# t = time()
w2vObject.build_vocab(sentences, progress_per=10000) #here is the vocab being built as told in google groups gensim

print(w2vObject.wv['the'], 'before train')
</code></pre>

<p>Then I'm replacing the current untrained vectors with:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    # vector = ''.join(num for num in values[1:])
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')

    else:
        coefs = np.asarray(vector)

f.close()
</code></pre>

<p>This code replaces the untrained random vectors wit my own pretrained:</p>

<pre><code>i = 0
for elem in w2vObject.wv.vocab:
    if elem in embeddings_index.keys():
        w2vObject.wv[elem] = embeddings_index[elem]
        i += 1
        print('Found one', i)

print(i)
</code></pre>

<p>Next I train them again with gensim:</p>

<pre><code>w2vObject.train(sentences, total_examples=w2vObject.corpus_count, epochs=epochsNum)#w2vObject.iter)
</code></pre>

<p>Finally save them:</p>

<pre><code>print(w2vObject.wv, 'after train')
w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False)
print('saved')
</code></pre>

<p>If I don't replace the vectors with my own saving works, but I need to replace them and save them as txt, any help?</p>

<p>EDIT:</p>

<p>Here is my_split() function:</p>

<pre><code>def my_split(s):
    return list(re.split(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))[0] ,list(re.findall(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))
</code></pre>

<p>And here is a bit of data 300 dimensions for the embedding_index:</p>

<pre><code>'hood -0.013093032778433955 -0.004199660490964164 -0.013285915004532987 0.004154925177649314 -0.004331536946207293 -0.013220217973950956 -0.004774150107654365 0.004774714449991327 0.0040749706101727646...
's gravenhage 0.01400977963089465 -0.0047073654478706935 -0.004326147699308312 0.01323622314514233 -0.004702524319745591 0.004695915697719624 0.00497792763673179 -0.004391661500805715 0.0046651111592470...
'tween 0.008467020793348493 -0.008027116343722267 0.007882368315816719 0.00754852526967863 0.008563484027417608 0.00812782576892597 0.008192394872536986 0.0075759585496093206...
</code></pre>

<p>Added code here:
<a href=""https://pastebin.com/GKPnENxv"" rel=""nofollow noreferrer"">Python code runs fine without my vectors, crashes with them</a></p>

<p>Populate embedding_index I go through all the words and vectors in the txt and if for some reason a vector is not 300 dim, skip it:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt', encoding='utf-8')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')
    else:
        coefs = np.asarray(vector)
        embeddings_index[word] = coefs

f.close()
</code></pre>

<p>EDIT2:
Here is the stack trace of the full error:
Traceback (most recent call last):</p>

<pre><code>  File ""GensimTestSave.py"", line 136, in &lt;module&gt;
    w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False) #encoding='utf-8' )
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/keyedvectors.py"", line 1453, in save_word2vec_format
    fname, self.vocab, self.vectors, fvocab=fvocab, binary=binary, total_vec=total_vec)
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in _save_word2vec_format
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in &lt;genexpr&gt;
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>
","python, gensim, word2vec","<p>I fixed it now. Apparently I was trying to use a nd.array() but of strings as coefficients and gensim uses an nd.array(floats) that was the problem where my own vectors when switching  to the .wv[] were of type [str]. So It ended up being empty.</p>

<p>The switching of vectors now is done like:</p>

<pre><code>for elem in setIntersection:
    if len(embeddings_index[elem]) != 300:
        print('here', elem) #cast it to the fire
    w2vObject.wv[elem] = np.asarray(embeddings_index[elem], dtype=np.float32)
print('Done!!!')
</code></pre>

<p>Thanks for your comments they helped me figure it out.</p>
",0,0,195,2020-05-18 16:01:41,https://stackoverflow.com/questions/61873864/gensim-saving-word-vectors-in-txt-format-error
Cannot load Doc2vec object using gensim,"<p>I am trying to load a pre-trained Doc2vec model using gensim and use it to map a paragraph to a vector. I am referring to <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> and the pre-trained model I downloaded is the English Wikipedia DBOW, which is also in the same link. However, when I load the Doc2vec model on wikipedia and infer vectors using the following code:</p>

<pre><code>import gensim.models as g
import codecs

model=""wiki_sg/word2vec.bin""
test_docs=""test_docs.txt""
output_file=""test_vectors.txt""

#inference hyper-parameters
start_alpha=0.01
infer_epoch=1000

#load model
test_docs = [x.strip().split() for x in codecs.open(test_docs, ""r"", ""utf-8"").readlines()]
m = g.Doc2Vec.load(model)

#infer test vectors
output = open(output_file, ""w"")
for d in test_docs:
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
output.flush()
output.close()
</code></pre>

<p>I get an error:</p>

<pre><code>/Users/zhangji/Desktop/CSE547/Project/NLP/venv/lib/python2.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
Traceback (most recent call last):
  File ""/Users/zhangji/Desktop/CSE547/Project/NLP/AbstractMapping.py"", line 19, in &lt;module&gt;
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>

<p>I know there are couple of threads regarding the infer_vector issue on stack overflow, but none of them resolved my problem. I downloaded the gensim package using</p>

<pre><code>pip install git+https://github.com/jhlau/gensim
</code></pre>

<p>In addition, after I looked at the source code in gensim package, I found that when I use Doc2vec.load(), the Doc2vec class doesn't really have a load() function by itself, but since it is a subclass of Word2vec, it calls the super method of load() in Word2vec and then make the model m a Word2vec object. However, the infer_vector() function is unique to Doc2vec and does not exist in Word2vec, and that's why it is causing the error. I also tried casting the model m to a Doc2vec, but I got this error:</p>

<pre><code>&gt;&gt;&gt; g.Doc2Vec(m)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 599, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 513, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 635, in scan_vocab
    for document_no, document in enumerate(documents):
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 1367, in __getitem__
    return vstack([self.syn0[self.vocab[word].index] for word in words])
TypeError: 'int' object is not iterable
</code></pre>

<p>In fact, all I want with gensim for now is to convert a paragraph to a vector using a pre-trained model that works well on academic articles. For some reasons I don't want to train the models on my own. I would be really grateful if someone can help me resolve the issue.</p>

<p>Btw, I am using python2.7, and the current gensim version is 0.12.4.</p>

<p>Thanks!</p>
","python, gensim, word2vec, doc2vec","<p>I would avoid using either the 4-year-old nonstandard gensim fork at <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a>, or any 4-year-old saved models that only load with such code.</p>
<p>The Wikipedia DBOW model there is also suspiciously small at 1.4GB. Wikipedia had well over 4 million articles even 4 years ago, and a 300-dimensional <code>Doc2Vec</code> model trained to have doc-vectors for the 4 million articles would be at least <code>4000000 articles * 300 dimensions * 4 bytes/dimension</code> = 4.8GB in size, not even counting other parts of the model. (So, that download is clearly <em>not</em> the 4.3M doc, 300-dimensional model mentioned in the associated paper â€“ but something that's been truncated in other unclear ways.)</p>
<p>The current gensim version is 3.8.3, released a few weeks ago.</p>
<p>It'd likely take a bit of tinkering, and an overnight or more runtime, to build your own <code>Doc2Vec</code> model using current code and a current Wikipedia dump - but then you'd be on modern supported code, with a modern model that better understands words coming into use in the last 4 years. (And, if you trained a model on a corpus of the exact kind of documents of interest to you â€“ such as academic articles â€“ the vocabulary, word-senses, and match to your own text-preprocessing to be used on later inferred documents will all be better.)</p>
<p>There's a Jupyter notebook example of building a <code>Doc2Vec</code> model from Wikipedia that either functional or very-close-to-functional inside the <code>gensim</code> source tree at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>
",0,2,954,2020-05-20 19:43:49,https://stackoverflow.com/questions/61921588/cannot-load-doc2vec-object-using-gensim
Choice of loss function,"<p>Here is a word2vec implementation:</p>

<pre><code>%reset -f

import torch
from torch.autograd import Variable
import numpy as np
import torch.functional as F
import torch.nn.functional as F

corpus = [
    'this test',
    'this separate test'
]

def get_input_layer(word_idx):
    x = torch.zeros(vocabulary_size).float()
    x[word_idx] = 1.0
    return x

def tokenize_corpus(corpus):
    tokens = [x.split() for x in corpus]
    return tokens

tokenized_corpus = tokenize_corpus(corpus)

vocabulary = []
for sentence in tokenized_corpus:
    for token in sentence:
        if token not in vocabulary:
            vocabulary.append(token)

word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}
idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}

window_size = 2
idx_pairs = []
# for each sentence
for sentence in tokenized_corpus:
    indices = [word2idx[word] for word in sentence]
    # for each word, threated as center word
    for center_word_pos in range(len(indices)):
        # for each window position
        for w in range(-window_size, window_size + 1):
            context_word_pos = center_word_pos + w
            # make soure not jump out sentence
            if context_word_pos &lt; 0 or context_word_pos &gt;= len(indices) or center_word_pos == context_word_pos:
                continue
            context_word_idx = indices[context_word_pos]
            idx_pairs.append((indices[center_word_pos], context_word_idx))

idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array

vocabulary_size = len(vocabulary)

embedding_dims = 4
W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)
W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)
num_epochs = 1
learning_rate = 0.001

for epo in range(num_epochs):
    loss_val = 0
    for data, target in idx_pairs:
        x = Variable(get_input_layer(data)).float()
        y_true = Variable(torch.from_numpy(np.array([target])).long())

        z1 = torch.matmul(W1, x)
        z2 = torch.matmul(W2, z1)

        log_softmax = F.log_softmax(z2, dim=0)

        loss = F.nll_loss(log_softmax.view(1,-1), y_true)
        print(float(loss))

        loss_val += loss.data.item()
        loss.backward()
        W1.data -= learning_rate * W1.grad.data
        W2.data -= learning_rate * W2.grad.data

        W1.grad.data.zero_()
        W2.grad.data.zero_()

        print(W1.shape)
        print(W2.shape)

        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')
</code></pre>

<p>This prints:</p>

<pre><code>0.33185482025146484
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.041481852531433105
3.302438735961914
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.45428669452667236
2.3144636154174805
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.7435946464538574
0.33418864011764526
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.7853682264685631
1.0644199848175049
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.9184207245707512
0.4970806837081909
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 0.980555810034275
3.2861199378967285
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 1.3913208022713661
6.170125961303711
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 2.16258654743433
</code></pre>

<p>Modifying the code to use mse_loss change y_true to float :</p>

<pre><code>y_true = Variable(torch.from_numpy(np.array([target])).float())
</code></pre>

<p>Use mse_loss :</p>

<pre><code>loss = F.mse_loss(log_softmax.view(1,-1), y_true)
</code></pre>

<p>Incorporated updates:</p>

<pre><code>for epo in range(num_epochs):
    loss_val = 0
    for data, target in idx_pairs:
        x = Variable(get_input_layer(data)).float()
        y_true = Variable(torch.from_numpy(np.array([target])).float())

        z1 = torch.matmul(W1, x)
        z2 = torch.matmul(W2, z1)

        log_softmax = F.log_softmax(z2, dim=0)

        loss = F.mse_loss(log_softmax.view(1,-1), y_true)
        print(float(loss))

        loss_val += loss.data.item()
        loss.backward()
        W1.data -= learning_rate * W1.grad.data
        W2.data -= learning_rate * W2.grad.data

        W1.grad.data.zero_()
        W2.grad.data.zero_()

        print(W1.shape)
        print(W2.shape)

        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')
</code></pre>

<p>Output is now:</p>

<pre><code>41.75048828125
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 5.21881103515625
16.929386138916016
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 7.334984302520752
50.63690948486328
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 13.664597988128662
36.21110534667969
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 18.190986156463623
5.304859638214111
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 18.854093611240387
9.802173614501953
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 20.07936531305313
15.515325546264648
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 22.018781006336212
30.408292770385742
torch.Size([4, 3])
torch.Size([3, 4])
Loss at epo 0: 25.81981760263443

-c:12: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
</code></pre>

<p>Why does mse loss not work as well as nll loss ? Is it related to the PyTorch warning:</p>

<pre><code>Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
</code></pre>

<p>?</p>
","python, machine-learning, pytorch, word2vec","<p>The input and target must have the same size for the <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss"" rel=""nofollow noreferrer""><code>nn.MSELoss</code></a>, because it is calculated by squaring the difference of the <em>i-th</em> element of the input and the <em>i-th</em> element of the target, i.e. <code>mse_i = (input_i - target_i) ** 2</code>.</p>

<p>Furthermore, your targets are non-negative integers in the range <em>[0, vocabulary_size]</em>, but you use log-softmax, which has values in range <em>[-âˆž, 0]</em>. With MSE the idea is to get the prediction value to the same value as the target, but the only overlap of the two intervals is 0. That means that every class besides 0 is unreachable.</p>

<p>MSE is a regression loss function and is simply not appropriate in this situation, since you are dealing with categorical data.</p>
",1,2,218,2020-05-28 10:22:02,https://stackoverflow.com/questions/62062380/choice-of-loss-function
How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.sstatic.net/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
","nlp, text-mining, gensim, word2vec, word-embedding","<p>No word2vec training is possible from a 1-word sentence, because there's no neighbor words to use as input to predict a center/target word. Essentially, that sentence is skipped.</p>

<p>If that was the only appearance of the word in the corpus, and you're seeing a vector for that word, it's just the starting random-initialization of the word, with no further training. (And, you should probably use a higher <code>min_count</code>, as keeping such rare words is usually a mistake in word2vec: they won't get good vectors, and other nearby words' vectors will improve if the 'noise' from all such insufficiently model-able rare words is removed.)</p>

<p>If that 1-word sentence actually appeared next-to other real sentences in your corpus, it could make sense to combine it with surrounding texts. There's nothing magic about actual sentences for this kind word-from-surroundings modeling - the algorithm is just working on 'neighbors', and it's common to use multi-sentence chunks as the texts for training, and sometimes even punctuation (like sentence-ending periods) is also retained as 'words'. Then words from an actually-separate sentence â€“ but still related by having appeared in the same document â€“ will appear in each other's contexts.</p>
",1,0,728,2020-06-05 08:42:07,https://stackoverflow.com/questions/62211396/how-does-gensim-word2vec-word-embedding-extract-training-word-pair-for-1-word-se
word2vec recommendation system KeyError: &quot;word &#39;21883&#39; not in vocabulary&quot;,"<p>The code works absolutely fine for the data set containing 500000+ instances but whenever I reduce the data set to 5000/10000/15000 it throws a key error : word ""***"" not in vocabulary.Not for every data point but for most them it throws the error.The data set is in excel format.  [1]: <a href=""https://i.sstatic.net/YCBiQ.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/YCBiQ.png</a>
I don't know how to fix this problem since i have very little knowledge about it,,I am still learning.Please help me fix this problem! </p>

<pre><code>    purchases_train = []
    for i in tqdm(customers_train):
        temp = train_df[train_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_train.append(temp)

    purchases_val = []
    for i in tqdm(validation_df['CustomerID'].unique()):
        temp = validation_df[validation_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_val.append(temp)


    model = Word2Vec(window = 10, sg = 1, hs = 0,
                     negative = 10, # for negative sampling
                     alpha=0.03, min_alpha=0.0007,
                     seed = 14)

    model.build_vocab(purchases_train, progress_per=200)

    model.train(purchases_train, total_examples = model.corpus_count, 
                epochs=10, report_delay=1)


    model.save(""word2vec_2.model"")
    model.init_sims(replace=True)

    # extract all vectors
    X = model[model.wv.vocab]

    X.shape

    products = train_df[[""StockCode"", ""Description""]]

    products.drop_duplicates(inplace=True, subset='StockCode', keep=""last"")


 products_dict=products.groupby('StockCode'['Description'].apply(list).to_dict()

    def similar_products(v, n = 6):
        ms = model.similar_by_vector(v, topn= n+1)[1:]
        new_ms = []
        for j in ms:
            pair = (products_dict[j[0]][0], j[1])
            new_ms.append(pair)

        return new_ms

        similar_products(model['21883'])
</code></pre>
","gensim, word2vec","<p>If you get a <code>KeyError</code> saying a word is not in the vocabulary, that's a reliable indicator that the word you're looking-up was not in the training data fed to <code>Word2Vec</code>, or did not appear enough (default <code>min_count=5</code>) times. </p>

<p>So, your error indicates the word-token <code>'21883'</code> did not appear at least 5 times in the texts (<code>purchases_train</code>) supplied to <code>Word2Vec</code>. You should do either or both of:</p>

<ul>
<li><p>Ensure all words you're going to look-up appear enough times, either with more training data or a lower <code>min_count</code>. (However, words with only one or a few occurrences tend <strong>not</strong> to get good vectors &amp; instead just drag the quaality of surrounding-words' vectors down - so keeping this value above <code>1</code>, or even raising it above the default of <code>5</code> to discard <strong>more</strong> rare words, is a better path whenever you have sufficient data.)</p></li>
<li><p>If your later code will be looking up words that might not be present, either check for their presence first (<code>word in model.wv.vocab</code>) or set up a <code>try: ... except: ...</code> to catch &amp; handle the case where they're not present.</p></li>
</ul>
",0,0,319,2020-06-12 16:58:33,https://stackoverflow.com/questions/62348981/word2vec-recommendation-system-keyerror-word-21883-not-in-vocabulary
Improving DOC2VEC Gensim efficiency,"<p>I am trying to train Gensim Doc2Vec model on tagged documents. I have around 4000000 documents. Following is my code:</p>

<pre><code>import pandas as pd
import multiprocessing
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from tqdm import tqdm
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import os
import re



def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'plurality', 'one', 'more', 'least', 'at', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))

    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

chunk_patent = pd.DataFrame()
chunksize = 10 ** 5
cores = multiprocessing.cpu_count()
directory = os.getcwd()
for root,dirs,files in os.walk(directory):
    for file in files:
       if file.startswith(""patent_cpc -""):
           print(file)
           #f=open(file, 'r')
           #f.close()
           for chunk_patent_temp in pd.read_csv(file, chunksize=chunksize):
                #chunk_patent.sort_values(by=['cpc'], inplace=True)
                #chunk_patent_temp = chunk_patent_temp[chunk_patent_temp['cpc'] == ""G06K7""]
                if chunk_patent.empty:
                    chunk_patent = chunk_patent_temp
                else:
                    chunk_patent = chunk_patent.append(chunk_patent_temp)
train_tagged = chunk_patent.apply(lambda r: TaggedDocument(words=text_process(r['text']), tags=[r.cpc]), axis=1)
print(train_tagged.values)

if os.path.exists(""cpcpredict_doc2vec.model""):
    doc2vec_model = Doc2Vec.load(""cpcpredict_doc2vec.model"")
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)), update=True)
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
else:
    doc2vec_model = Doc2Vec(dm=0, vector_size=300, min_count=100, workers=cores-1)
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)))
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
</code></pre>

<p>I have tried modifying the Doc2vec parameters but without any luck.</p>

<p>On the same data I have trained Word2vec model, which is much accurate in comparison to the doc2vec model. Further, ""most_similar"" results for word2vec model is very different from the doc2vec model. </p>

<p>Following is the code for searching most similar results:</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from gensim.models import Doc2Vec
import re

def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))
    #for index, row in df.iterrows():
    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

model = Word2Vec.load(""cpc.model"")
print(model.most_similar(positive=['barcode'], topn=30))

model1 = Doc2Vec.load(""cpcpredict_doc2vec.model"")

pred_tags = model1.most_similar('barcode',topn=10)
print(pred_tags)
</code></pre>

<p>Further, the output of the aforementioned is cited below:</p>

<pre><code>[('indicium', 0.36468246579170227), ('symbology', 0.31725651025772095), ('G06K17', 0.29797130823135376), ('dataform', 0.29535001516342163), ('rogue', 0.29372256994247437), ('certification', 0.29178398847579956), ('reading', 0.27675414085388184), ('indicia', 0.27346929907798767), ('Contra', 0.2700084149837494), ('redemption', 0.26682156324386597)]

[('searched', 0.4693435728549957), ('automated', 0.4469209909439087), ('production', 0.4364866018295288), ('hardcopy', 0.42193126678466797), ('UWB', 0.4197841286659241), ('technique', 0.4149003326892853), ('authorized', 0.4134449362754822), ('issued', 0.4129987359046936), ('installing', 0.4093806743621826), ('thin', 0.4016669690608978)]
</code></pre>
","python, nltk, gensim, word2vec, doc2vec","<p>The <code>Doc2Vec</code> mode you've chosen, <code>dm=0</code> (aka plain ""PV-DBOW""), does not train word-vectors at all. Word vectors will still be randomly-initialized, due to shared code-paths of the different models, but never trained and thus meaingless.</p>

<p>So the results of your <code>most_similar()</code>, using a word as the query, will be essentially random. (Using <code>most_similar()</code> on the model itself, rather than its <code>.wv</code> word-vectors or <code>.docvecs</code> doc-vectors, should also be generating a deprecation warning.)</p>

<p>If you need your <code>Doc2Vec</code> model to train word-vectors in addition to the doc-vectors, use either the <code>dm=1</code> mode (""PV-DM"") or <code>dm=0, dbow_words=1</code> (adding optional interleaved skip-gram word training to plain DBOW training). In both cases, words will be trained very similarly to a <code>Word2Vec</code> model (of the 'CBOW' or 'skip-gram' modes, respectively) â€“ so your word-based <code>most_similar()</code> results should then be very comparable. </p>

<p>Separately:</p>

<ul>
<li>if you have enough data to train 300-dimensional vectors, &amp; discard all words with fewer than 100 occurrences, then 50 training epochs <em>may</em> be more than needed. </li>
<li>those <code>most_similar()</code> results don't particularly look like they're result of any lemmatization, as seems intended by your <code>text_process()</code> method, but maybe that's not an issue, or some other issue entirely. Note, though, that with sufficient data, lemmatization may be a superfluous step - all variants of the same word tend to wind up usefully near each other, when there are plenty of varied examples of al the word variants in real contexts. </li>
</ul>
",2,1,836,2020-06-13 10:38:34,https://stackoverflow.com/questions/62358583/improving-doc2vec-gensim-efficiency
Inconsistent results when training gensim model with gensim.downloader vs manual loading,"<p>I am trying to understand what is going wrong in the following example.</p>
<p>To train on the 'text8' dataset as described in the docs, one only has to do the following:</p>
<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)
</code></pre>
<p>doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.</p>
<p>However, when loading the same textfile which is used above manually, as in</p>
<pre><code>text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)
</code></pre>
<p>The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.</p>
<p>What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?</p>
","python, gensim, word2vec","<p>In your second example, you've created a training dataset with just a single text with the entire contents of the file. That's about 1.1 million word tokens, in a single list.</p>
<p><code>Word2Vec</code> (&amp; other related algorithms) in gensim have an internal implementation limitation, in their optimized paths, of 10,000 tokens per text item. All additional tokens are ignored.</p>
<p>So, in your 2nd case, 99% of your data is being discarded. Training may seem instant, but very little actual training will have occurred. (Word-vectors for words that only appear past the 1st 10,000 tokens won't have been trained at all, having only their initial randomly-set values.) If you enable logging at the INFO level, you'll see more details about each step of the process, and discrepancies like this may be easier to identify.</p>
<p>Yes, the <code>api.load()</code> variant takes extra steps to break the single-line-file into 10,000-token chunks. I believe it's using the <code>LineSentence</code> utility class for this purpose, whose source can be examined here:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209</a></p>
<p>However, I recommend avoiding the <code>api.load()</code> functionality entirely. It doesn't just download data; it also downloads a shim of additional outside-of-version-control Python code for prepping that data for extra operations. Such code is harder to browse &amp; less well-reviewed than official gensim release code as packaged for PyPI/etc, which also presents a security risk. Each load target (by name like 'text8') might do something different, leaving you with a different object type as the return value.</p>
<p>It's much better for understanding to directly download precisely the data files you need, to known local paths, and do the IO/prep yourself, from those paths, so you know what steps have been applied, and the only code you're running is the officially versioned &amp; released code.</p>
",1,0,353,2020-06-23 20:47:35,https://stackoverflow.com/questions/62543491/inconsistent-results-when-training-gensim-model-with-gensim-downloader-vs-manual
What is negative sampling method use- sigmoid or softmax?,"<p>I am recently reading this paper: Word2Vec explained(<a href=""https://arxiv.org/pdf/1402.3722.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722.pdf</a>)</p>
<p>And there's something I can't understand..</p>
<p>In page 3, they say that p is defined using softmax</p>
<p>$p(D=1|w, c, \theta) = \frac{1}{1+e^{-v_c\dotv_w}}$</p>
<p><a href=""https://i.sstatic.net/l1m5Y.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>but i am confused because i have seen that formula in sigmoid function, not softmax function.</p>
<p>How you derive that definition from softmax?</p>
","machine-learning, nlp, word2vec","<p>It can be called a small abuse of notation on the author's part but it's totally fine. Sometimes people use the softmax and sigmoid interchangeably. However, in this case it is indeed a sigmoid function because of binary class problem.</p>
",0,0,579,2020-07-01 04:46:02,https://stackoverflow.com/questions/62669885/what-is-negative-sampling-method-use-sigmoid-or-softmax
"My Doc2Vec code, after many loops/epochs of training, isn&#39;t giving good results. What might be wrong?","<p>I'm training a <code>Doc2Vec</code> model using the below code, where <code>tagged_data</code> is a list of <code>TaggedDocument</code> instances I set up before:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40

model = Doc2Vec(alpha=0.025, 
                min_alpha=0.001)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.001
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>When I later check the model results, they're not good. What might have gone wrong?</p>
","gensim, word2vec, doc2vec","<p><strong>Do not call <code>.train()</code> multiple times in your own loop that tries to do <code>alpha</code> arithmetic.</strong></p>
<p>It's unnecessary, and it's error-prone.</p>
<p>Specifically, in the above code, decrementing the original <code>0.025</code> alpha by <code>0.001</code> forty times results in (<code>0.025 - 40*0.001</code>) <code>-0.015</code> final <code>alpha</code>, which would also have been negative for many of the training epochs. But a negative <code>alpha</code> <em>learning-rate</em> is nonsensical: it essentially asks the model to nudge its predictions a little bit in the <em>wrong</em> direction, rather than a little bit in the <em>right</em> direction, on every bulk training update. (Further, since <code>model.iter</code> is by default 5, the above code actually performs <code>40 * 5</code> training passes â€“ <code>200</code> â€“ which probably isn't the conscious intent. But that will just confuse readers of the code &amp; slow training, not totally sabotage results, like the <code>alpha</code> mishandling.)</p>
<p>There are other variants of error that are common here, as well. If the <code>alpha</code> were instead decremented by <code>0.0001</code>, the 40 decrements would only reduce the final <code>alpha</code> to <code>0.021</code> â€“ whereas the proper practice for this style of SGD (Stochastic Gradient Descent) with linear learning-rate decay is for the value to end &quot;very close to <code>0.000</code>&quot;). If users start tinkering with <code>max_epochs</code> â€“ it is, after all, a parameter pulled out on top! â€“ but don't also adjust the decrement every time, they are likely to far-undershoot or far-overshoot <code>0.000</code>.</p>
<p>So don't use this pattern.</p>
<p>Unfortunately, many bad online examples have copied this anti-pattern from each other, <em>and</em> make serious errors in their own <code>epochs</code> and <code>alpha</code> handling. Please don't copy their error, and please let their authors know they're misleading people wherever this problem appears.</p>
<p>The above code can be improved with the much-simpler replacement:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40
model = Doc2Vec()  # of course, if non-default parameters needed, use them here
                   # most users won't need to change alpha/min_alpha at all
                   # but many will want to use more than default `epochs=5`

model.build_vocab(tagged_data)
model.train(tagged_data, total_examples=model.corpus_count, epochs=max_epochs)

model.save(&quot;d2v.model&quot;)
</code></pre>
<p>Here, the <code>.train()</code> method will do exactly the requested number of <code>epochs</code>, smoothly reducing the internal effective <code>alpha</code> from its default starting value to near-zero. (It's rare to need to change the starting <code>alpha</code>, but even if you wanted to, just setting a new non-default value at initial model-creation is enough.)</p>
<p>Also: note that later calls to <code>infer_vector()</code> will reuse the <code>epochs</code> specified at the time of model-creation. If nothing is specified, the default <code>epochs=5</code> will be used - which is often smaller than is best for training or inference. So if you find a larger number of <code>epochs</code> (such as 10, 20 or more) is better for training, remember to also use at least the same number of <code>epochs</code> for inference. (<code>.infer_vector()</code> takes an optional <code>epochs</code> parameter whihc can override any value set at model-contruction.</p>
",9,3,1963,2020-07-08 18:10:33,https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-epochs-of-training-isnt-giving-good-results
Word2Vec - Model with high cross validation score performs incredibly bad for test data,"<p>While working on sentiment analysis of twitter data, I encountered a problem that I just can't solve. I wanted to train a RandomForest Classifier to detect hate speech. I, therefore, used a labeled dataset with tweets that are labeled as 1 for hate speech and 0 for normal tweets. For vectorization, I am using Word2Vec. I first performed a hyperparametrization to find good parameters for the classifier.
During hyperparametrization I used a repeated stratified KFold cross-validation (scoring = accuracy)
Mean accuracy is about 99.6% here. However, once I apply the model to a test dataset and plot a confusion matrix, the accuracy is merely above 50%, which is of course awful for a binary classifier.
I successfully use the exact same approach with Bag of Words and had no problems at all here.
Could someone maybe have a quick look at my code? That would be so helpful. I just cannot find what is wrong. Thank you so much!</p>
<p>(I also uploaded the code to google collab in case that is easier for you: <a href=""https://i.sstatic.net/UY5iT.png"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/15BzElijL3vwa_6DnLicxRvcs4SPDZbpe?usp=sharing</a> )</p>
<p>First I preprocessed my data:</p>
<pre><code>train_csv = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/MLDA_project/data2/train.csv')
train = train_csv     
#check for missing values (result shows that there are no missing values)
train.isna().sum()    
# remove the tweet IDs
train.drop(train.columns[0], axis = &quot;columns&quot;, inplace = True)    
# create a new column to save the cleansed tweets
train['training_tweet'] = np.nan

# remove special/unknown characters
train.replace('[^a-zA-Z#]', ' ', inplace = True, regex = True)    
# generate stopword list and add the twitter handles &quot;user&quot; to the stopword list
stopwords = sw.words('english')
stopwords.append('user')    
# convert to lowercase
train = train.applymap(lambda i:i.lower() if type(i) == str else i)    
# execute tokenization and lemmatization
lemmatizer = WordNetLemmatizer()

for i in range(len(train.index)):
    #tokenize the tweets from the column &quot;tweet&quot;
    words = nltk.word_tokenize(train.iloc[i, 1])
    #consider words with more than 3 characters
    words = [word for word in words if len(word) &gt; 3] 
    #exclude words in stopword list
    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)] 
    #Join words again
    train.iloc[i, 2]  = ' '.join(words)  
    words = nltk.word_tokenize(train.iloc[i, 2])
train.drop(train.columns[1], axis = &quot;columns&quot;, inplace = True)

majority = train[train.label == 0]
minority = train[train.label == 1]
# upsample minority class
minority_upsampled = resample(minority, replace = True, n_samples = len(majority))      
# combine majority class with upsampled minority class
train_upsampled = pd.concat([majority, minority_upsampled])
train = train_upsampled
np.random.seed(10)
train = train.sample(frac = 1)
train = train.reset_index(drop = True)
</code></pre>
<p>Now <code>train</code> has the labels in column 0 and the preprocessed tweets in column 1.</p>
<p>Next I defined the Word2Vec Vectorizer:</p>
<pre><code>def W2Vvectorize(X_train):
tokenize=X_train.apply(lambda x: x.split())
w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(X_train), epochs=20)
w2v_words = list(w2vec_model.wv.vocab)
vector=[]
from tqdm import tqdm
for sent in tqdm(tokenize):
    sent_vec=np.zeros(100)
    count =0
    for word in sent: 
        if word in w2v_words:
            vec = w2vec_model.wv[word]
            sent_vec += vec 
            count += 1
    if count != 0:
        sent_vec /= count #normalize
    vector.append(sent_vec)
return vector
</code></pre>
<p>I split the dataset into test and training set and vectorized both subsets using W2V as defined above:</p>
<pre><code>x = train[&quot;training_tweet&quot;]
y = train[&quot;label&quot;]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=train['label'])

print('X Train Shape = total * 0,8 =', X_train.shape)
print('y Train Shape = total * 0,8 =', y_train.shape)
print('X Test Shape = total * 0,2 =', X_test.shape)
print('y Test Shape = total * 0,2 =', y_test.shape) # change 0,4 &amp; 0,6

train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>Now I carry out the hyperparametrization:</p>
<pre><code># define models and parameters
model = RandomForestClassifier()
n_estimators = [10, 100, 1000]
max_features = ['sqrt', 'log2']
# define grid search
grid = dict(n_estimators=n_estimators,max_features=max_features)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf_w2v, y_train)
# summarize results
print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))
</code></pre>
<p>This results in the following output:</p>
<pre><code>Best: 0.996628 using {'max_features': 'log2', 'n_estimators': 1000}
0.995261 (0.000990) with: {'max_features': 'sqrt', 'n_estimators': 10}
0.996110 (0.000754) with: {'max_features': 'sqrt', 'n_estimators': 100}
0.996081 (0.000853) with: {'max_features': 'sqrt', 'n_estimators': 1000}
0.995885 (0.000872) with: {'max_features': 'log2', 'n_estimators': 10}
0.996481 (0.000691) with: {'max_features': 'log2', 'n_estimators': 100}
0.996628 (0.000782) with: {'max_features': 'log2', 'n_estimators': 1000}
</code></pre>
<p>Next, I wanted to draw a confusion matrix with the test data using the Model:</p>
<pre><code>clf = RandomForestClassifier(max_features = 'log2', n_estimators=1000) 
   
clf.fit(train_tf_w2v, y_train)
name = clf.__class__.__name__
        
expectation = y_test
test_prediction = clf.predict(test_tf_w2v)
acc = accuracy_score(expectation, test_prediction)   
pre = precision_score(expectation, test_prediction)
rec = recall_score(expectation, test_prediction)
f1 = f1_score(expectation, test_prediction)

fig, ax = plt.subplots(1,2, figsize=(14,4))
plt.suptitle(f'{name} \n', fontsize = 18)
plt.subplots_adjust(top = 0.8)
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, ax=ax[0])
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, normalize=True, ax = ax[1])
plt.show()
    
print(f&quot;for the {name} we receive the following values:&quot;)
print(&quot;Accuracy: {:.3%}&quot;.format(acc))
print('Precision score: {:.3%}'.format(pre))
print('Recall score: {:.3%}'.format(rec))
print('F1 score: {:.3%}'.format(f1))
</code></pre>
<p>This outputs:</p>
<p><a href=""https://i.sstatic.net/UY5iT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UY5iT.png"" alt=""confusion matrix"" /></a></p>
<p>for the RandomForestClassifier we receive the following values:
Accuracy: 57.974%
Precision score: 99.790%
Recall score: 15.983%
F1 score: 27.552%</p>
","machine-learning, cross-validation, word2vec, sentiment-analysis","<p>Ouuh... Now I feel stupid. I found what was wrong.</p>
<p>After the train/test-split, I sent both subsets independently to the <code>W2Vvectorize()</code> function.</p>
<pre><code>train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>From there the <code>W2Vvectorize()</code> function trains two independent Word2Vec models, based on the two independent subsets. Hence when I pass the vectorized test data <code>test_tf_w2v</code> to my trained RandomForest classifier, to check if the accuracy is correct for a test set as well, it appears to the trained RandomForest classifier, as if the test set would be in a different language. The two separate word2vec models just vectorize in a different way.</p>
<p>I solved that as follows:</p>
<pre><code>def W2Vvectorize(X_train):
    tokenize=X_train.apply(lambda x: x.split())
    vector=[]
    for sent in tqdm(tokenize):
        sent_vec=np.zeros(100)
        count =0
        for word in sent: 
            if word in w2v_words:
                vec = w2vec_model.wv[word]
                sent_vec += vec 
                count += 1
        if count != 0:
            sent_vec /= count #normalize
        vector.append(sent_vec)
    return vector
</code></pre>
<p>And the Word2Vec training is separate from that :</p>
<pre><code>x = train[&quot;training_tweet&quot;]
y = train[&quot;label&quot;]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=train['label'])

print('X Train Shape = total * 0,8 =', X_train.shape)
print('y Train Shape = total * 0,8 =', y_train.shape)
print('X Test Shape = total * 0,2 =', X_test.shape)
print('y Test Shape = total * 0,2 =', y_test.shape) #

tokenize=X_train.apply(lambda x: x.split())
w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(X_train), epochs=20)
w2v_words = list(w2vec_model.wv.vocab)

train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>So the Word2Vec models training is performed only on the training data. The vectorization of test data, however, has to be carried out with that exact same Word2Vec model.</p>
",2,2,1416,2020-07-13 16:56:50,https://stackoverflow.com/questions/62880636/word2vec-model-with-high-cross-validation-score-performs-incredibly-bad-for-te
How do i get word2vec similarity from the mean vector?,"<p>For example, there are words for 'apple', 'banana', and 'orange'.</p>
<p>We will execute the code below to save the distance between apple and banana.</p>
<pre><code>model.similarity('apple', 'banana')
</code></pre>
<p>But what I want to know is the similarity between 'apple' and 'whole fruits'.
How do i get the similarity of apples and whole fruits?</p>
<p>I already got vectors for the whole fruit.
e.g. <code>whole fruits=[0, 0.4, 0.2, 0.2, 0.5, .....]</code></p>
","python, machine-learning, nlp, word2vec","<p>model.similarity calculates cosine similarity behind the scenes between the embedding vectors for the words. If you have already have the vectors for &quot;apple&quot; and &quot;whole fruits&quot; then you can get the cosine similarity using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"" rel=""nofollow noreferrer"">sklearn's pairwise cosine similarity function.</a></p>
",2,0,401,2020-07-15 07:40:04,https://stackoverflow.com/questions/62909843/how-do-i-get-word2vec-similarity-from-the-mean-vector
Will the document vectors generated by Doc2Vec be similar to document vectors obtained through Word2Vec?,"<p>I came across few blog posts stating that, Document vectors can be generated not only by Doc2Vec, but also by averaging the word vectors obtained by running Word2vec algorithm.
In that case, would the vectors generated through both the Algorithms be the same?
Which would be the most efficient way to generate the Document vectors and Why?</p>
<p>Any reference links in this regard would be of great help!!</p>
<p>Thanks in Advance</p>
","nlp, word2vec, word-embedding, doc2vec","<p>Those are two different methods of creating a vector for a set-of-words.</p>
<p>The vectors will be in different positions, and of different quality.</p>
<p>Averaging is quite fast, especially if you've already got word-vectors. But it's a very simple approach that won't capture many shades of meaning â€“ indeed it is completely oblivious to word ordering/relative proximities, and the act of averaging can tend to 'cancel out' contrasting meanings in the text.</p>
<p><code>Doc2Vec</code> instead trains vectors for full texts in a manner very similar to word-vectors (and often, alongside word-vectors). Essentially, a pretend-word that's assigned to the text 'floats' alonside the word-vector training, as if it were 'near' all the other word-training (for that one text). It's a slightly more sophisticated approach, but as it uses a very-similar algorithm (&amp; model-complexity) on the same data, results on many downstream evaluations are often similar.</p>
<p>To obtain summary text-vectors capturing more subtle shades of meaning, as implied by grammatical rules and more advanced language usage, can require yet-more-sophisticated methods, such as those employing larger deep networks.</p>
<p>There's no single most efficient approach, as all real uses depend a lot on the type, quantity, and quality of your texts, and your intended uses of the vectors.</p>
",0,1,191,2020-07-15 15:31:37,https://stackoverflow.com/questions/62918448/will-the-document-vectors-generated-by-doc2vec-be-similar-to-document-vectors-ob
Word/Phrase classification,"<p>I have a column containing 5000 string records. These records are individual words or phrases (not a sentence or paragraph). Most of these records are similar or contain similar elements(e.g. &quot;Office&quot;, &quot;offise&quot; &quot;ground floor office&quot;). Also, someone manually classified 300 of these records into five categories (i.e. Residential, Industrial, Office, Retail, Other) which means I can use it to develop a supervised machine learning model. I did a bit of study on word2vec, but it seems they work on texts, not individual words and phrases. Please advise me on how I can do the classification. Please note that the number of the records in the column is growing and new records will be added in the future, so the solution must be able to classify new records.</p>
<p>The sample input and the desired output is as below:</p>
<pre><code>'industrial' -&gt; 'Industrial'
'Warehouse' -&gt; 'Industrial'
'Workshop' -&gt; 'Industrial'
'rear warehouse' -&gt; 'Industrial'
'office suite' -&gt; 'office'
'office/warehouse' -&gt; 'office'
'office(b1)' -&gt; 'office'
'house' -&gt; 'Residential'
'suite' -&gt; 'Residential'
'restaurant' -&gt; 'Retail'
'retail unit with 3 bedroom dwelling above' -&gt; 'Retail'
'shoe shop' -&gt; 'Retail'
'unit 56' -&gt; 'Other'
'24 Hastings street' -&gt; 'Other'
</code></pre>
<p><a href=""https://i.sstatic.net/jsT8s.png"" rel=""nofollow noreferrer"">Input &amp; Output</a></p>
","python, machine-learning, nlp, word2vec","<p>You have a very typical text classification task.</p>
<p>There are many classification algorithms you could use, but the main areas for choice/improvement in your task are likely to be:</p>
<ul>
<li>feature-extraction &amp; feature-engineering: how do you turn those short texts into numerical data against which rules/thresholds can be learned?</li>
<li>overall process issues: for whatever &quot;tough cases&quot; exist that can't be learned from existing data, either initially or over time, how are necessary corrections fed back into an improved system</li>
</ul>
<p>Initially, you should try 'bag of words' and 'character n-grams' (either alone or together) as ways to turn your short texts into feature vectors. That, alone, with sufficient training data should handle most of the kinds of cases you've shown so far, since it will help any classification algorithm discover certain 'slam-dunk' rules.</p>
<p>For example, that will effectively learn that 'shop' may always imply 'retail', or 'house' always implies 'residential', or 'office' implies commercial. And using character n-grams will also give the model clues as to how to handle other typos or variant forms of the same words.</p>
<p>There will be cases it can't handle well. I'd guess that you'd want `3 bedroom dwelling', alone, to be 'residential' â€“ but in your examples, you've binned 'retail unit with 3 bedroom dwelling above' as 'retail'. With enough examples of desired behavior, a classifier might get that right, because it either sees 'retail' as a category with more precedence, or other words (like 'above') implying mixed-use that usually should be binned one way or another.</p>
<p>When you look at the cases that doesn't handle well, you'll then be able to consider more advanced approaches, like perhaps using word-vectors to represent words that weren't necessarily in your (small) training set, but could be considered near-synonyms to known words. (For example, one possible policy for handling words unknown to your training set, that arrive later, would be to use some external, larger word2vec model to replace any unknown word with the known word that's closest.)</p>
<p>But, you should really start with the most-simple feature approaches, see how far those get you and thus set a baseline for later improvements. Then, consider more advanced &amp; custom techniques.</p>
",2,3,1471,2020-07-22 02:16:08,https://stackoverflow.com/questions/63025784/word-phrase-classification
How to normalize word embeddings (word2vec),"<p>I have a pre trained Word2Vec model with embeddings. I need to normalize some embeddings to do analyses with the words. Is there a simple line (or block) of code to do this? I've been searching online but can't find a simple answer.</p>
","python, nlp, normalization, word2vec, word-embedding","<p>This will work fine with embeddings</p>
<pre><code>model.init_sims(replace=True)
</code></pre>
",1,1,4250,2020-07-24 20:46:59,https://stackoverflow.com/questions/63081245/how-to-normalize-word-embeddings-word2vec
How to interpret output from gensim&#39;s Word2vec most similar method and understand how it&#39;s coming up with the output values,"<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Word2vec is going to give you n-dimensional vectors that represent each of the diseases based on their co-occurrence. This means that you are representing each of the symptoms as a vector.</p>
<p>One row -</p>
<pre><code>X = ['fever', 'loss of appetite']

X_onehot= [[0,0,0,1,0,0,0,0,0,0,0],
           [0,0,0,0,0,0,0,0,1,0,0]]

X_word2vec= [[0.002,0.25,-0.1,0.335,0.7264],
             [0.746,0.6463,0.0032,0.6301,0.223]]

Y = #flu
</code></pre>
<p>Now, you can represent each row in the data by taking the average of the word2vec such as -</p>
<pre><code>X_avg = [[0.374 ,0.44815, -0.0484, 0.48255, 0.4747]]
</code></pre>
<p>Now you have a 5 length feature vector and a class for each row in your dataset. Next, you can treat it like any other machine learning problem.</p>
<p>If you want to predict the disease then just use a classification model after train-test split. That way you can validate the data.</p>
<p><strong>Using cosine similarity to the word2vec vectors only yields similar symptoms. It will not let you build a disease recommendation model, because then you will be recommending a symptom based on other similar symptoms.</strong></p>
",1,0,217,2020-07-26 06:36:54,https://stackoverflow.com/questions/63096909/how-to-interpret-output-from-gensims-word2vec-most-similar-method-and-understan
Build a multiclass text classifier which takes vectors generated from word2vec as independent variables to predict a class,"<p>I am dealing with patient data. I want to predict the top N diseases given a set of symptoms.</p>
<p>This is a sample of my dataset: In total I have around 1200 unique Symptoms and around 200 unique Diagnosis</p>
<pre><code>     ID         Symptom combination                              Diagnosis
    Patient1: fever, loss of appetite, cold                        Flu
    Patient2: hair loss, blood pressure                           Thyroid
    Patient3: hair loss, blood pressure                            Flu
    Patient4: throat pain, joint pain                           Viral Fever

    ..
    ..
Patient30000: vomiting, nausea                                   Diarrohea
</code></pre>
<p>What I am planning to do with this dataset is to use the Symptoms column to generate word vectors using Word2vec for each row of patient data. After generating the vectors I want to build a classifier, with the vectors in each row being my independent variable and the Diagnosis being the target categorical variable.</p>
<p>Shall I take the average of the vectors to generate feature vectors generated from word2vec? If so, any clarifications on the same?</p>
","python, machine-learning, nlp, word2vec, text-classification","<p>You can average a bunch of word-vectors for symptoms together to get a single feature-vector of the same dimensionality. (If your word-vectors are 100d each, averaging them together gets a single 100d summary vector.)</p>
<p>But such averaging is fairly crude, and has some risk of diluting the information of each symptom in the averaging.</p>
<p>(As a simplified, stylized example, imagine a nurse took a patients' temperature at 9pm, and found it to be 102.6Â°F. Then again, at 7am, and found it to be 94.6Â°F. A doctor asks, &quot;how's our patient's temperature?&quot;, and the nurse says the average, &quot;98.6Â°F&quot;. &quot;Wow,&quot; says the doctor, &quot;it's rare for someone to be so on-the-dot for the normal healthy temperature. Next patient!&quot; Averaging hid the important information: that the patient had both a fever and dangerous hypothermia.)</p>
<p>It sounds like you have a controlled-vocabulary of symptoms, with just some known, capped, and not-very-large number of symptom tokens: about 1200.</p>
<p>In such a case, turning those into a categorical vector for the presence/absence of each symptom may work far better than word2vec-based approaches. Maybe you have 100 different symptoms or 10,000 different symptoms. Either way, you can turn them into a large vector of 1s and 0s representing each possible symptom in order, and lots of classifiers will do pretty well with that input.</p>
<p>If treating the list-of-symptoms like a text-of-words, a simple &quot;bag of words&quot; representation of the text will essentially be this categorical representation: a 1200-dimensional 'one-hot' vector.</p>
<p>And unless this is some academic exercise where you've been required to use word2vec, it's not a good place to start, and may not be a part of the best solution. To train good word-vectors, you need more data than you have. (To re-use word-vectors from elsewhere, they should be well-matched to your domain.)</p>
<p>Word-vectors are most likely to help if you've gots tens-of-thousands to hundreds-of-thousands of terms, and many contextual examples of each of their uses, to plot their subtle variations-of-meaning in a dense shared space. Only 30,000 'texts', of ~3-5 tokens each, and only ~1200 unique tokens, is fairly small for word2vec.</p>
<p>(I made similar points in my <a href=""https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">comments on one of your earlier questions</a>.)</p>
<p>Once you've turned each row into a feature vector â€“ whether it's by averaging symptom word-vectors, or probably better creating a bag-of-words representation â€“ you can and should try many different classifiers to see which works best.</p>
<p>Many are drop-in replacements for each other, and with the size of your data, testing many against each other in a loop may take less than an hour or few.</p>
<p>If at a total loss where to start, anything listed in the 'classifiers' upper-left area of <a href=""https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">this <code>scikit-learn</code> graphical guide</a> is worth trying:</p>
<p><a href=""https://i.sstatic.net/t9nHP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t9nHP.jpg"" alt=""scikit-learn &quot;choosing the right estimator&quot;"" /></a></p>
<p>If you want to consider an even wider range of possibilities, and get a vaguely-intuitive idea of which ones can best discover certain kinds of &quot;shapes&quot; in the underlying high-dimensional data, you can look at all those demonstrated in <a href=""https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"" rel=""nofollow noreferrer"">this <code>scikit-learn</code> &quot;classifier comparison&quot; page</a>, with these graphical representations of how well they handle a noisy 2d classification challenge (instead of your 1200d challenge):</p>
<p><a href=""https://i.sstatic.net/RRJqs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RRJqs.jpg"" alt=""enter image description here"" /></a></p>
",3,0,787,2020-07-26 20:01:10,https://stackoverflow.com/questions/63105091/build-a-multiclass-text-classifier-which-takes-vectors-generated-from-word2vec-a
"How to find accuracy, precision, recall, f1 score for my word2vec model?","<p>I am working on a project to find similarity among products. The model splits the excel data sheet into 90% training / 10% validation. When I check manually for validation the model works pretty well. But I am having trouble with the evaluation process. How should I find accuracy, precision, recall and F1 score to understand how well my model works?</p>
<p>I am very new to machine learning, still learning, please give me some clues where to start.</p>
","python, cross-validation, word2vec, evaluation","<p>Word2vec is an algorithm that's considered 'unsupervised' â€“ it's not trained using specified 'correct' answers, but rather learns from the patterns in any data. As a result, there's no native-to-word2vec idea of 'accuracy', 'precision', etcetera â€“ those concepts only have meaning in relation to a set of desired answers.</p>
<p>So to calculate those values, you have to use those word-vectors in some other downstream task, and devise your own evaluation for that downstream task. Then you can calculate accuracy &amp; other values for that whole system (including the word2vec step). This may include applying your judgement, or that of other reviewers, about what the result &quot;should&quot; be in certain cases.</p>
<p>Without any examples of your data, it's not yet clear what your Word2Vec model is doing, and how products are represented in it. (What's the individual items in the <code>customers_train</code> list you've created? Where do product names/identifiers come in? What kinds of similarity-questions or end-user operations do you need to be performing?)</p>
",4,0,1024,2020-07-28 13:08:48,https://stackoverflow.com/questions/63134913/how-to-find-accuracy-precision-recall-f1-score-for-my-word2vec-model
word2vec logging missing values,"<p>Im using gensim version '3.8.3' <br></p>
<p>when im running for model Word2Vec and FastText <code>build_vocab</code> and <code>train</code> <br>
the logs from those functions are missing the values</p>
<p>for example part of the logs of  <code>build_vocab</code> of FastText</p>
<pre><code>08/09/2020 08:19:18 AM [INFO] collecting all words and their counts
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
</code></pre>
<p>the index is missing and printed as <code>i</code></p>
<p>is there a way to solve it? is it a version bug?</p>
","python-3.x, gensim, word2vec","<p>As <a href=""https://github.com/RaRe-Technologies/gensim/issues/2914"" rel=""nofollow noreferrer"">per the discussion on the <code>gensim</code> project issue you opened for the same problem</a>, this appears to be some problem with your Python installation's logging functionality that is unrelated to <code>gensim</code> or the word2vec algorithm. And in some respects, the problem is more foundational &amp; concerning, as it indicates some replacement of core functionality with a sloppy alternative.</p>
<p>For example, if you see a similar problem with the test code...</p>
<pre><code>import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s')

logging.info(
    &quot;TEST A %i B %.2f C %.0f D %i F %i&quot;,
    1, 2, 3, 4, 5
)
</code></pre>
<p>...then the problem is in the core <code>logging</code> module.</p>
<p>I would suggest starting from a fresh development environment â€“ at the very least, a fresh separate Python environment (using either the core <code>venv</code> functionality or an environment-manager like <code>conda</code>), and if practical even a fresh machine/OS install.</p>
<p>If the problem with the above simple test code goes away in a fresh environment, then you can incrementally reproduce the original environment by adding libraries/tools, checking for working logging after each major step, and if the problem recurs you'll have a better idea of which step introduced it.</p>
",1,1,202,2020-08-09 08:24:40,https://stackoverflow.com/questions/63324116/word2vec-logging-missing-values
Resolving &#39;Resource stopwords not found&#39; in non-downloadable environment,"<p>I work at an environment where I cannot download modules. I need to work with existing modules on my computer.</p>
<p>I can use <code>nltk</code> module, but the installed version does not contain <code>stopwords</code>, so I cannot process the  corpus in my skip-gram. So, I invariably encounter this error message:</p>
<pre><code>Resource stopwords not found.
</code></pre>
<p>Is there any way to clone, copy &amp; paste, or create a <code>stopwords</code> myself  on jupyter notebook?</p>
","python, nlp, nltk, stanford-nlp, word2vec","<p>You can make a file or create a <code>stopwords</code> variable by copying the contents of <code>nltk.corpus.stopwords</code> which is simply a set of words.</p>
<pre><code>from nltk.corpus import stopwords

words = set(stopwords.words('english'))
print(words)
</code></pre>
<p>Output:</p>
<pre><code>{'does', &quot;it's&quot;, 'having', 've', &quot;hadn't&quot;, &quot;isn't&quot;, 'this', 'him', 'ours', &quot;mustn't&quot;, 'are', 'if', 'we', 'myself', 'these', 'a', 'not', 'what', 'weren', 'down', 'have', &quot;couldn't&quot;, 'after', 'again', 'most', 'at', 'such', 'by', 'just', 'd', 'i', &quot;you'll&quot;, 'should', &quot;haven't&quot;, 'as', 'do', 'from', 'other', 'than', 'which', 'were', 'mustn', 'yours', &quot;aren't&quot;, 'now', 'didn', &quot;won't&quot;, 'be', 'itself', 'in', 'all', 'once', 'few', 'through', 'its', &quot;didn't&quot;, 'needn', 'being', 'them', &quot;you'd&quot;, 'or', 'it', 'to', 'your', &quot;shan't&quot;, 'too', 'our', 'ourselves', 'his', 'am', 'below', 'isn', 'ma', 'further', 'yourself', 'out', 'up', &quot;don't&quot;, 'with', 'but', 'where', 'then', 'whom', 'each', 'hasn', 'very', 'more', 'he', 'won', 't', 'doing', 'until', 'doesn', 'herself', 'who', 'own', &quot;wasn't&quot;, 'those', 'nor', &quot;you're&quot;, 'shan', 'himself', 'll', 'that', 'both', 'shouldn', &quot;you've&quot;, 'over', 'an', 'when', 'because', 'ain', 'had', 'haven', 'themselves', 'same', 'under', 'no', &quot;mightn't&quot;, 'couldn', 'you', 'while', 'and', 'during', 'yourselves', 'my', &quot;shouldn't&quot;, &quot;wouldn't&quot;, 'off', 'she', 'me', 'wasn', 'above', 'y', 'will', 'been', 'mightn', 'was', 'before', &quot;needn't&quot;, 'so', 'on', 's', 'some', 'their', 'can', 'how', 'is', 'hadn', 'wouldn', 'here', 'why', 'her', 'only', 'the', 'o', &quot;should've&quot;, 'hers', &quot;doesn't&quot;, 'don', 'against', &quot;weren't&quot;, 'about', 'm', &quot;she's&quot;, 'of', 'into', 're', 'they', &quot;that'll&quot;, 'aren', &quot;hasn't&quot;, 'theirs', 'between', 'there', 'did', 'has', 'for', 'any'}
</code></pre>
",1,1,363,2020-08-12 05:05:28,https://stackoverflow.com/questions/63370108/resolving-resource-stopwords-not-found-in-non-downloadable-environment
Why word embedding technique works,"<p>I have look into some word embedding techniques, such as</p>
<ol>
<li>CBOW: from context to single word. Weight matrix produced used as embedding vector</li>
<li>Skip gram: from word to context (from what I see, its acutally word to word, assingle prediction is enough). Again Weight matrix produced used as embedding</li>
</ol>
<p>Introduction to these tools would always quote &quot;cosine similarity&quot;, which says words of similar meanning would convert to similar vector.</p>
<p>But these methods all based on the 'context', account only for words around a target word. I should say they are 'syntagmatic' rather than 'paradigmatic'. So why the close in distance in a sentence indicate close in meaning? I can think of many counter example that frequently occurs</p>
<ol>
<li>&quot;Have a good day&quot;. (good and day are vastly different, though close in distance).</li>
<li>&quot;toilet&quot; &quot;washroom&quot; (two words of similar meaning, but a sentence contains one would unlikely to contain another)</li>
</ol>
<p>Any possible explanation?</p>
","nlp, word2vec, word-embedding","<p>This sort of &quot;why&quot; isn't a great fit for StackOverflow, but some thoughts:</p>
<p>The essence of word2vec &amp; similar embedding models may be <strong>compression</strong>: the model is forced to predict neighbors using <strong>far less internal state</strong> than would be required to remember the entire training set. So it has to force similar words together, in similar areas of the parameter space, and force groups of words into various useful relative-relationships.</p>
<p>So, in your second example of 'toilet' and 'washroom', even though they rarely appear together, they do tend to appear around the same neighboring words. (They're synonyms in many usages.) The model tries to predict them both, to similar levels, when typical words surround them. And vice-versa: when they appear, the model should generally predict the same sorts of words nearby.</p>
<p>To achieve that, their vectors must be nudged quite close by the iterative training. The only way to get 'toilet' and 'washroom' to predict the same neighbors, through the shallow feed-forward network, is to corral their word-vectors to nearby places. (And further, to the extent they have slightly different shades of meaning â€“ with 'toilet' more the device &amp; 'washroom' more the room â€“Â they'll still skew slightly apart from each other towards neighbors that are more 'objects' vs 'places'.)</p>
<p>Similarly, words that are formally antonyms, but easily stand-in for each-other in similar contexts, like 'hot' and 'cold', will be somewhat close to each other at the end of training. (And, their various nearer-synonyms will be clustered around them, as they tend to be used to describe similar nearby paradigmatically-warmer or -colder words.)</p>
<p>On the other hand, your example &quot;have a good day&quot; probably doesn't have a giant influence on either 'good' or 'day'. Both words' more unique (and thus <strong>predictively-useful</strong>) senses are more associated with other words. The word 'good' alone can appear everywhere, so has weak relationships everywhere, but still a strong relationship to other synonyms/antonyms on an evaluative (&quot;good or bad&quot;, &quot;likable or unlikable&quot;, &quot;preferred or disliked&quot;, etc) scale.</p>
<p>All those random/non-predictive instances tend to cancel-out as noise; the relationships that have <strong>some</strong> ability to predict nearby words, even slightly, eventually find <strong>some</strong> relative/nearby arrangement in the high-dimensional space, so as to help the model for some training examples.</p>
<p>Note that a word2vec model isn't necessarily an <strong>effective</strong> way to predict nearby words. It might never be good at that task. But the <strong>attempt</strong> to become good at neighboring-word prediction, with fewer free parameters than would allow a perfect-lookup against training data, forces the model to reflect underlying semantic or syntactic patterns in the data.</p>
<p>(Note also that some research shows that a larger <code>window</code> influences word-vectors to reflect more topical/domain similarity â€“ &quot;these words are used about the same things, in the broad discourse about X&quot; â€“ while a tiny <code>window</code> makes the word-vectors reflect a more syntactic/typical similarity - &quot;these words are drop-in replacements for each other, fitting the same role in a sentence&quot;. See for example Levy/Goldberg &quot;Dependency-Based Word Embeddings&quot;, around its Table 1.)</p>
",4,1,711,2020-08-12 15:03:41,https://stackoverflow.com/questions/63379360/why-word-embedding-technique-works
How to fix unpickling key error when loading word2vec (gensim)?,"<p>I am trying to load a pre-trained word2vec model in pkl format taken from <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">here</a></p>
<p>The line of code I use to load it:</p>
<pre><code>model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl') 
</code></pre>
<p>However, i keep getting the following error (full traceback):</p>
<pre><code>UnpicklingError                           Traceback (most recent call last)
&lt;ipython-input-15-ebd5780b6636&gt; in &lt;module&gt;
     55 
     56 #Load pretrained word2vec
---&gt; 57 model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl',mmap='r')
     58 

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
   1551     @classmethod
   1552     def load(cls, fname_or_handle, **kwargs):
-&gt; 1553         model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)
   1554         if isinstance(model, FastTextKeyedVectors):
   1555             if not hasattr(model, 'compatible_hash'):

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    226     @classmethod
    227     def load(cls, fname_or_handle, **kwargs):
--&gt; 228         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    229 
    230     def similarity(self, entity1, entity2):

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    433         compress, subname = SaveLoad._adapt_by_suffix(fname)
    434 
--&gt; 435         obj = unpickle(fname)
    436         obj._load_specials(fname, mmap, compress, subname)
    437         logger.info(&quot;loaded %s&quot;, fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1396         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1397         if sys.version_info &gt; (3, 0):
-&gt; 1398             return _pickle.load(f, encoding='latin1')
   1399         else:
   1400             return _pickle.loads(f.read())

UnpicklingError: invalid load key, ':'.
</code></pre>
<p>I tried loading it with load_word2vec_format, but no luck. Any ideas what might be wrong with it?</p>
","python, gensim, word2vec","<p>Per your link <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a> these are to be loaded using that library's <code>Wikipedia2Vec.load()</code> method.</p>
<p>Gensim's <code>.load()</code> methods should only be used with files saved directly from Gensim model objects.</p>
<p>The Wikipedia2Vec project does say that their <code>.txt</code> file formats would load with <code>.load_word2vec_format()</code>, so you could also try that - but with one of their <code>.txt</code> format files.</p>
<p>Their full model <code>.pkl</code> files are only going to work with their class's own loading function.</p>
",1,0,1520,2020-08-12 22:01:03,https://stackoverflow.com/questions/63385272/how-to-fix-unpickling-key-error-when-loading-word2vec-gensim
Array reshape error when loading word2vec model,"<p>I have the following piece of code:</p>
<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('model2')
X = model[model.wv.vocab]
</code></pre>
<p>This piece of code works on one of my machines but not another. The model file is the same. What's going on? The error message I get is the following:</p>
<pre><code>  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1330, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 427, in load
    obj._load_specials(fname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/npyio.py&quot;, line 440, in load
    pickle_kwargs=pickle_kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/format.py&quot;, line 771, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 16777184 into shape (134441,128)
</code></pre>
<p>To install gensim, I used <code>conda install -c anaconda gensim</code></p>
","python, amazon-ec2, gensim, word2vec","<p>I checked what @gojomo referred to in the comments and he was correct, my file sizes were wrong. Something must have happened during upload. For large models, word2vec saves the model in 3 files. Assuming your model name is &quot;model2&quot; you will have:</p>
<ol>
<li>model2</li>
<li>model2.trainables.syn1neg.npy</li>
<li>model2.wv.vectors.npy</li>
</ol>
<p>My <code>.wv.vectors.npy</code> was a few kilo bytes too small than the version in my other machine.</p>
",0,0,404,2020-08-14 19:59:19,https://stackoverflow.com/questions/63419318/array-reshape-error-when-loading-word2vec-model
Gensim&#39;s word2vec has a loss of 0 from epoch 1?,"<p>I am using the Word2vec module of Gensim library to train a word embedding, the dataset is 400k sentences with 100k unique words (its not english)</p>
<p>I'm using this code to monitor and calculate the loss :</p>
<pre class=""lang-py prettyprint-override""><code>class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(&quot;Model loss:&quot;, model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))


monitor = MonitorCallback([&quot;MyWord&quot;])  # monitor with demo words

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT  , callbacks=[monitor])

w2v_model.build_vocab(tokenized_corpus)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(&quot;Vocab size&quot;, vocab_size)

print(&quot;[*] Training...&quot;)

# Train Word Embeddings
w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=W2V_EPOCH)
</code></pre>
<p>The problem is from epoch 1 the loss is 0 and the vector of the monitored words dont change at all!</p>
<pre><code>[*] Training...
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
</code></pre>
<p>so what is the problem here? is this normal?  the tokenized corpus is a list of lists that are something like tokenized_corpus[0] = [ &quot;word1&quot; , &quot;word2&quot; , ...]</p>
<p>I googled and seems like some of the old versions of gensim had problem with calculating loss function, but they are from almost a year ago and it seems like it should be fixed right now?</p>
<p>I tried the code provided in the answer of this question as well but still the loss is 0 :</p>
<p><a href=""https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim"">Loss does not decrease during training (Word2Vec, Gensim)</a></p>
<p>EDIT1 : after adding compute_loss=True, the loss shows up, but it keeps going higher and higher, and the top similar words and their similarity doesn't change at all :</p>
<pre><code>Model loss: 2187903.5
Model loss: 3245492.0
Model loss: 4103624.5
Model loss: 4798541.0
Model loss: 5413940.0
Model loss: 5993822.5
Model loss: 6532631.0
Model loss: 7048384.5
Model loss: 7547147.0
</code></pre>
","nlp, pytorch, gensim, word2vec","<p>The top issue with your code is that you haven't used the <code>Word2Vec</code> initialization parameter necessary to toggle loss-tracking on: <code>compute_loss=True</code></p>
<p>(See 'parameters' section of <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a> )</p>
<p>Even with that fix, the loss-reporting is still quite buggy (as of <code>gensim-3.8.3</code> &amp; this writing in August 2020):</p>
<ul>
<li>it's not the per-epoch total, or per-example average, one might expect. (So if you need that, as a workaround, your callback should be remembering the last value and computing the delta, or resetting the internal counter to <code>0.0</code>, each epoch's end.)</li>
<li>it definitely loses precision in larger training runs, eventually becoming useless. (This may not be an issue for you.)</li>
<li>it might lose some tallies due to multithreaded value-overwriting. (This may not be a practical issue for you, depending on why you're consulting the loss value.)</li>
</ul>
",2,2,1748,2020-08-20 16:58:37,https://stackoverflow.com/questions/63509864/gensims-word2vec-has-a-loss-of-0-from-epoch-1
Graph to connect sentences,"<p>I have a list of sentences of a few topics (two) like the below:</p>
<pre><code>Sentences
Trump says that it is useful to win the next presidential election. 
The Prime Minister suggests the name of the winner of the next presidential election.
In yesterday's conference, the Prime Minister said that it is very important to win the next presidential election. 
The Chinese Minister is in London to discuss about climate change.
The president Donald Trump states that he wants to win the presidential election. This will require a strong media engagement.
The president Donald Trump states that he wants to win the presidential election. The UK has proposed collaboration. 
The president Donald Trump states that he wants to win the presidential election. He has the support of his electors. 
</code></pre>
<p>As you can see there is similarity in sentences.</p>
<p><a href=""https://i.sstatic.net/TVJ8f.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/TVJ8f.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to relate multiple sentences and visualise the characteristics of them by using a graph (directed). The graph is built from a similarity matrix, by applying row ordering of sentences as shown above.
I created a new column, Time, to show the order of sentences, so first row (Trump says that....) is at time 1; second row (The Prime Minister suggests...) is at time 2, and so on.
Something like this</p>
<pre><code>Time    Sentences
1           Trump said that it is useful to win the next presidential election. 
2           The Prime Minister suggests the name of the winner of the next presidential election.

3           In today's conference, the Prime Minister said that it is very important to win the next presidential election. 

...
</code></pre>
<p>I would like then to find the relationships in order to have a clear overview of the topic.
Multiple paths for a sentence would show that there are multiple information associated with it.
To determine similarity between two sentences, I tried to extract nouns and verbs as follows:</p>
<pre><code>noun=[]
verb=[]
for  index, row in df.iterrows():
      nouns.append([word for word,pos in pos_tag(row[0]) if pos == 'NN'])
      verb.append([word for word,pos in pos_tag(row[0]) if pos == 'VB'])
</code></pre>
<p>as they are keywords in whatever sentence.
So when a keyword (noun or verb) appears in sentence x but not in the other sentences, it represents a difference between these two sentences.
I think a better approach, however, could be using word2vec or gensim (WMD).</p>
<p>This similarity has to be calculated for each sentence.
I would like to build a graph which shows the content of the sentence in my example above.
Since there are two topics (Trump and Chinese Minister), for each of them I need to look for sub-topics. Trump has sub-topic presidential election, for example. A node in my graph should represent a sentence. Words in each node represent differences for the sentences, showing new info in the sentence. For example, the word <code>states</code> in sentence at time 5 is in adjacent sentences at time 6 and 7.
I would like just to find a way to have similar results as shown in picture below. I have tried using mainly nouns and verbs extraction, but probably it is not the right way to proceed.
What I tried to do has been to consider sentence at time 1 and compare it with other sentences, assigning a similarity score (with noun and verbs extraction but also with word2vec), and repeat it for all the other sentences.
But my problem is now on how to extract difference to create a graph that can make sense.</p>
<p>For the part of the graph, I would consider to use networkx (DiGraph):</p>
<pre><code>G = nx.DiGraph()
N = Network(directed=True) 
</code></pre>
<p>to show direction of relationships.</p>
<p>I provided a different example to make it be clearer (but if you worked with the previous example, it would be fine as well. Apologies for the inconvenience, but since my first question was not so clear, I had to provide also a better, probably easier, example).</p>
","python, nlp, nltk, networkx, word2vec","<p>Didn't implement NLP for verb / noun separation, just added a list of good words.
They can be extracted and normalized with <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spacy</a> relatively easy.
Please note that <code>walk</code> occurs in 1,2,5 sentences and forms a triad.</p>
<pre class=""lang-py prettyprint-override""><code>import re
import networkx as nx
import matplotlib.pyplot as plt

plt.style.use(&quot;ggplot&quot;)

sentences = [
    &quot;I went out for a walk or walking.&quot;,
    &quot;When I was walking, I saw a cat. &quot;,
    &quot;The cat was injured. &quot;,
    &quot;My mum's name is Marylin.&quot;,
    &quot;While I was walking, I met John. &quot;,
    &quot;Nothing has happened.&quot;,
]

G = nx.Graph()
# set of possible good words
good_words = {&quot;went&quot;, &quot;walk&quot;, &quot;cat&quot;, &quot;walking&quot;}

# remove punctuation and keep only good words inside sentences
words = list(
    map(
        lambda x: set(re.sub(r&quot;[^\w\s]&quot;, &quot;&quot;, x).lower().split()).intersection(
            good_words
        ),
        sentences,
    )
)

# convert sentences to dict for furtehr labeling
sentences = {k: v for k, v in enumerate(sentences)}

# add nodes
for i, sentence in sentences.items():
    G.add_node(i)

# add edges if two nodes have the same word inside
for i in range(len(words)):
    for j in range(i + 1, len(words)):
        for edge_label in words[i].intersection(words[j]):
            G.add_edge(i, j, r=edge_label)

# compute layout coords
coord = nx.spring_layout(G)

plt.figure(figsize=(20, 14))

# set label coords a bit upper the nodes
node_label_coords = {}
for node, coords in coord.items():
    node_label_coords[node] = (coords[0], coords[1] + 0.04)

# draw the network
nodes = nx.draw_networkx_nodes(G, pos=coord)
edges = nx.draw_networkx_edges(G, pos=coord)
edge_labels = nx.draw_networkx_edge_labels(G, pos=coord)
node_labels = nx.draw_networkx_labels(G, pos=node_label_coords, labels=sentences)
plt.title(&quot;Sentences network&quot;)
plt.axis(&quot;off&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/pLbLx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pLbLx.png"" alt=""enter image description here"" /></a></p>
<p><strong>Update</strong><br>
If you want to measure the similarity between different sentences, you may want to calculate the difference between sentence embedding. <br>
This gives you an opportunity to find semantic similarity between sentences with different words like &quot;A soccer game with multiple males playing&quot; and &quot;Some men are playing a sport&quot;. Almost SoTA approach using BERT can be found <a href=""https://keras.io/examples/nlp/semantic_similarity_with_bert/"" rel=""nofollow noreferrer"">here</a>, more simple approaches are <a href=""https://stackoverflow.com/questions/45869881/finding-similarity-between-2-sentences-using-word2vec-of-sentence-with-python"">here</a>.<br>
Since you have similarity measure, just replace add_edge block to add new edge only if similarity measure is greater than some threshold. Resulting add edges code will look like this:</p>
<pre><code># add edges if two nodes have the same word inside
tresold = 0.90
for i in range(len(words)):
    for j in range(i + 1, len(words)):
        # suppose you have some similarity function using BERT or PCA
        similarity = check_similarity(sentences[i], sentences[j])
        if similarity &gt; tresold:
            G.add_edge(i, j, r=similarity)
</code></pre>
",4,9,1460,2020-08-20 23:28:52,https://stackoverflow.com/questions/63514464/graph-to-connect-sentences
Find most similar words to randomy initialized array,"<p>Using the Gensim package, I have trained a word2vec model on the corpus that I am working with as follows:</p>
<pre><code>word2vec = Word2Vec(all_words, min_count = 3, size = 512, sg = 1)
</code></pre>
<p>Using Numpy, I have initialized a random array with the same dimensions:</p>
<pre><code>vector = (rand(512)-0.5) *20
</code></pre>
<p>Now, I would like to find the words from the word2vec that are most similar to the random vector that I initialized.</p>
<p>For words in the word2vec, you can run:</p>
<pre><code>word2vec.most_similar('word')
</code></pre>
<p>And the output is a list with most similar words and their according distance.</p>
<p>I would like to get a similar output for my initialized array.</p>
<p>However, when I run:</p>
<pre><code>word2vec.most_similar(vector)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-297-3815cf183d05&gt;&quot;, line 1, in &lt;module&gt;
    word2vec.most_similar(vector)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\utils.py&quot;, line 1461, in new_func1
    return func(*args, **kwargs)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 1383, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 549, in most_similar
    for word, weight in positive + negative:

TypeError: cannot unpack non-iterable numpy.float64 object
</code></pre>
<p>What can I do to overcome this error and find the most similar words to my arrays?</p>
<p>I've checked <a href=""https://stackoverflow.com/questions/54273077/cannot-unpack-non-iterable-numpy-float64-object-python3-opencv"">this</a> and <a href=""https://stackoverflow.com/questions/59357940/typeerror-cannot-unpack-non-iterable-numpy-float64-object"">this</a> page. However, it is unclear to me how I could solve my problem with these suggestions.</p>
","python, numpy, typeerror, gensim, word2vec","<p>Gensim's <code>KeyedVectors</code> interface <code>.most_similar()</code> method <strong>can</strong> take raw vectors as its target, but in order for its current (at least through <code>gensim-3.8.3</code>) argument-type-detection to not mistake a single vector for a list-of-keys, you would need to provide it explicitly as one member of a list of items for the named <code>positive</code> parameter.</p>
<p>Specifically, this should work:</p>
<pre class=""lang-py prettyprint-override""><code>similars = word2vec.wv.most_similar(positive=[vector,])
</code></pre>
",1,1,233,2020-08-21 14:11:48,https://stackoverflow.com/questions/63524493/find-most-similar-words-to-randomy-initialized-array
Is a gensim vocab index the index in the corresponding 1-hot-vector?,"<p>I am doing research that requires direct manipulation &amp; embedding of one-hot vectors and I am trying to use gensim to load a pretrained word2vec model for this.</p>
<p>The problem is they don't seem to have a direct api for working with 1-hot-vectors. And I am looking for work arounds.</p>
<p>So I wanted to know if anyone knows of a way to do this? Or more specifically if these vocab indices (which are defined quite ambiguously). Could be indices into corresponding 1-hot-vectors?</p>
<p>Context I have found:</p>
<ul>
<li>Seems <a href=""https://stackoverflow.com/questions/40458742/gensim-word2vec-accessing-in-out-vectors"">this question</a> is related but I tried accessing the 'input embeddings' (assuming they were one-hot representations), via model.syn0 (from link in answer), but I got a non-sparse matrix...</li>
<li>Also appears <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">they refer to word indices as 'doctags'</a> (search for Doctag/index).</li>
<li><a href=""https://stackoverflow.com/questions/47117569/how-to-get-word2index-from-gensim"">Here</a> is another question giving some context to the indices (although not quite answering my question).</li>
<li><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">Here</a> is the official documentation:</li>
</ul>
<p>################################################</p>
<p>class gensim.models.keyedvectors.Vocab(**kwargs)
Bases: object</p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info, and for constructing binary trees (incl. both word leaves and inner nodes).</p>
<p>################################################</p>
","gensim, word2vec, one-hot-encoding","<p>Yes, you can think of the <code>index</code> (position) of gensim's <code>Word2Vec</code> word-vectors as being the one dimension that would be <code>1.0</code> â€“ with all other V dimensions, where V is the count of unique words, being <code>0.0</code>.</p>
<p>The implementation doesn't actually ever create one-hot vectors, as a sparse or explicit representation. It's just using the word's index as a look-up for its dense vector â€“ following in the path of the <code>word2vec.c</code> code from Google on which the gensim implementation was originally based.</p>
<p>(The term 'doctags' is only relevant in the <code>Doc2Vec</code> â€“ aka 'Paragraph Vector' â€“ implementation. There it is the name for the distinct tokens/ints that are used for looking up document-vectors, using a different namespace from in-document words. That is, in <code>Doc2Vec</code> you could use <code>'doc_007'</code> as a doc-vector name, aka a 'doctag', and even if the string-token <code>'doc_007'</code> also appears as a word inside documents, the doc-vector referenced by doctag-key <code>'doc_007'</code> and the word-vector referenced by word-key <code>'doc_007'</code> wouldn't be the same internal vector.)</p>
",1,0,414,2020-08-23 17:24:27,https://stackoverflow.com/questions/63549977/is-a-gensim-vocab-index-the-index-in-the-corresponding-1-hot-vector
Does the gensim `Word2Vec()` constructor make a completely independent model?,"<p>I'm testing feeding gensim's Word2Vec different sentences with the same overall vocabulary to see if some sentences carry &quot;better&quot; information than others. My method to train Word2Vec looks like this</p>
<pre><code>def encode_sentences(self, w2v_params, sentences):
    model = Word2Vec(sentences, **w2v_params)
    
    idx_order = torch.tensor([int(i) for i in model.wv.index2entity], dtype=torch.long)
    X = torch.zeros((idx_order.max()+1, w2v_params['size']), dtype=torch.float)
    
    # Put embeddings back in order
    X[idx_order] = torch.tensor(model.wv.vectors)    
    return X, y
</code></pre>
<p>What I'm hoping for here, is each time w2v runs, it starts with a fresh model and trains from scratch. However, I'm testing 3 kinds of sentences, so my test code looks like this:</p>
<pre><code>def test(sentence):
    w2v = {'size': 128, 'sg': 1}
    X = encode_sentences(w2v, sentence)
    evaluate(X) # Basic cluster analysis stuff here

# s1, s2 and s3 are the 3 sets of sentences with the same vocabulary in different order/frequency
[print(test(s) for s in [s1, s2, s3]]
</code></pre>
<p>However, I noticed if I remove one of the test sets, and only test <code>s1</code> and <code>s2</code> (or any combination of 2 sets of the three), the overall quality of the clusterings decreases. If I go back into <code>encode_sentences</code> and add <code>del model</code> before the <code>return</code> call, the overall cluster quality also goes down but remains consistent no matter how many datasets are tested.</p>
<p>What gives? Is the constructor not actually building a fresh model each time with new weights? The docs and source code give no indication of this. I'm quite sure it isn't my evaluation method, as everything was fixed after the <code>del model</code> was added. I'm at a loss here... Are these runs actually independent, or is each call to <code>Word2Vec(foo, ...)</code> equivalent to retraining the previous model with <code>foo</code> as new data?</p>
<p>And before you ask, no <code>model</code> is nowhere outside of the scope of the <code>encode_sentence</code> variable; that's the only time that variable name is used in the whole program. Very odd.</p>
<h3>Edit with more details</h3>
<hr />
<p>If it's important, I'm using Word2Vec to build node embeddings on a graph the way Node2Vec does with different walk strategies. These embeddings are then fed to a Logistic Regression model (<code>evaluate(X)</code>) and which calculates area under the roc.</p>
<p>Here is some sample output of the model before adding the <code>del model</code> call to the <code>encode_sentences</code> method averaged over 5 trials:</p>
<pre><code>Random walks:   0.9153 (+/-) 0.002
Policy walks:   0.9125 (+/-) 0.005
E-greedy walks: 0.8489 (+/-) 0.011
</code></pre>
<p>Here is the same output with the only difference being <code>del model</code> in the encoding method:</p>
<pre><code>Random walks:   0.8627 (+/-) 0.005
Policy walks:   0.8527 (+/-) 0.002
E-greedy walks: 0.8385 (+/-) 0.009
</code></pre>
<p>As you can see, in each case, the variance is very low (the +/- value is the standard error) but the difference between the two runs is almost a whole standard deviation. It seems odd that if each call to <code>Word2Vec</code> was truly independent that manually freeing the data structure would have such a large effect.</p>
","python, gensim, word2vec","<p>Each call to the <code>Word2Vec()</code> constructor creates an all-new model.</p>
<p>However, runs are <em>not</em> completely deterministic under normal conditions, for a <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">variety of reasons</a>, so results quality for downstream evaluations (like your unshown clustering) will jitter from run-to-run.</p>
<p>If the variance in repeated runs with the same data is very large, there are probably other problems, such an oversized model prone to overfitting. (Stability from run-to-run can be one indicator that your process is sufficiently specified that the data and model choices are driving results, not the randomness used by the algorithm.)</p>
<p>If this explanation isn't satisfying, try adding more info to your question - such as the actual magnitude of your evaluation scores, in repeated runs, both with and without the changes that you conjecture are affecting results. (I suspect the variations from the steps you think are having effect will be no larger than variations from re-runs or different <code>seed</code> values.)</p>
<p>(More generally, <code>Word2Vec</code> is generally hungry for as much varies training data as possible; only if texts are <em>non-representative of the relevant domain</em> are they likely to result in a worse model. So I generally wouldn't expect being choosier about which subset of sentences is best to be an important technique, unless some of the sentences are total junk/noise, but of course there's always a change you'll find some effects in your particular data/goals.)</p>
",1,0,188,2020-08-23 20:03:59,https://stackoverflow.com/questions/63551484/does-the-gensim-word2vec-constructor-make-a-completely-independent-model
IndexError: index is out of bounds - word2vec,"<p>I have trained a word2vec model called <code>word_vectors</code>, using the Gensim package with size = 512.</p>
<pre><code>fname = get_tmpfile('word2vec.model')
word_vectors = KeyedVectors.load(fname, mmap='r')
</code></pre>
<p>Now, I have created a new Numpy array (also of size 512) which I have added to the word2vec as follows:</p>
<pre><code>vector = (rand(512)-0.5) *20
word_vectors.add('koffie', vector)
</code></pre>
<p>Doing this seems to go fine and even when I call</p>
<pre><code>word_vectors['koffie']
</code></pre>
<p>I get the array as output, as expected.</p>
<p>However, when I want to look for the most similar words in my model and run the following code:</p>
<pre><code>word_vectors.most_similar('koffie')
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-283-ce992786ce89&gt;&quot;, line 1, in &lt;module&gt;
    word_vectors.most_similar('koffie')

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 553, in most_similar
    mean.append(weight * self.word_vec(word, use_norm=True))

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 461, in word_vec
    result = self.vectors_norm[self.vocab[word].index]

IndexError: index 146139 is out of bounds for axis 0 with size 146138


word_vector.size()
Traceback (most recent call last):

  File &quot;&lt;ipython-input-284-2606aca38446&gt;&quot;, line 1, in &lt;module&gt;
    word_vector.size()

NameError: name 'word_vector' is not defined
</code></pre>
<p>The error seems to indicate that my indexing isn't correct here. But since I am only indexing indirectly (with a key rather than an actual numeric index), I don't see what I need to change here.</p>
<p>Who knows what goes wrong here? And what can I do to overcome this error?</p>
","python-3.x, numpy, gensim, word2vec, index-error","<p>The 1st time you do a <code>.most_similar()</code>, a <code>KeyedVectors</code> instance (in gensim versions through 3.8.3) will create a cache of unit-normalized vectors to assist in all subsequent bulk-similarity operations, and place it in <code>.vectors_norm</code>.</p>
<p>It looks like your addition of a new vector didn't flush/recalculate/expand that cached <code>.vectors_norm</code> - originally the <code>KeyedVectors</code> class and <code>.most_similar()</code> operation were not designed with constantly-growing or constantly-changing sets-of-vectors in mind, but rather as utilities for a post-training, frozen set of vectors.</p>
<p>So that's the cause of your <code>IndexError</code>.</p>
<p>You should be able to work-around this by explicitly clearing the <code>.vectors_norm</code> any time you perform modifications/additions to the <code>KeyedVectors</code>, eg:</p>
<pre class=""lang-py prettyprint-override""><code>word_vectors.vectors_norm = None
</code></pre>
<p>(This shouldn't be necessary in the next 4.0.0 release of gensim, but I'll double-check there's not a similar problem there.)</p>
<p>Separately:</p>
<ul>
<li><p>Your <code>'word_vector' is not defined</code> error is simply because you seem to have left the 's' off your chosen variable name <code>word_vectors</code></p>
</li>
<li><p>You probably don't need to be using the gensim-testing-utility-method <code>get_tmpfile()</code> - just use your own explicit, intentional filesystem paths for saving and loading</p>
</li>
<li><p>Whether it's proper to use <code>KeyedVectors.load()</code> depends on what was saved. If you are in fact saving a full <code>Word2Vec</code> class instance (more than just the vectors), using <code>Word2Vec.load()</code> would be more appropriate.</p>
</li>
</ul>
",1,0,1111,2020-08-24 19:40:43,https://stackoverflow.com/questions/63567713/indexerror-index-is-out-of-bounds-word2vec
Loading pretrained glove on production with flask and Gunicorn,"<p>I have a model that requires some preprocessing using Glove from Stanford. From my experience it takes the at least 20-30 seconds until the Glove is loaded by this code:</p>
<pre><code>glove_pd = pd.read_csv(embed_path+'/glove.6B.300d.txt', sep=&quot; &quot;, quoting=3, header=None, index_col=0)
glove = {key: val.values for key, val in glove_pd.T.items()}
</code></pre>
<p>My question is what is the best practice to handle this in a production app? As far as I can understand is that everytime that I restart the server I need to wait 30 seconds until the endpoint is ready.</p>
<p>Also, <a href=""https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a"" rel=""nofollow noreferrer"">I have read</a> that when using Gunicorn, it is recommended to run with <code>workers&gt;1</code>, something like this:</p>
<pre><code>ExecStart=/path/to/gunicorn --workers 3 --bind unix:app.sock -m 007 wsgi:app
</code></pre>
<p>Does it mean that each instance of gunicorn requires to load the same glove to memory? This means that the server resources will be quite large, let me know if I am correct here.</p>
<p>Bottom line my question is what are the recommended methods for hosting a model that requires an pretrained embedding (glove/word2vec/fasttext) on a production server</p>
","python, stanford-nlp, word2vec, fasttext","<p>At one level, if you need it in memory, and that's how long it takes to read the gigabyte-plus from disk into useful RAM structures, then yes - that's how long it takes before a process is ready to use that data. But there's room for optimizations!</p>
<p>For example, reading this as 1st a Pandas dataframe, then converting it to a Python dict, involves both more steps &amp; more RAM than other options. (At the momentary peak, when both <code>glove_pd</code> and <code>glove</code> are fully constructed &amp; referenced, you'll have two full copies in memory â€“ and neither is as compact as would be ideal, which could trigger other slowdowns, especially if the bloat triggers using any virtual-memory at all.)</p>
<p>And as you fear, if 3 <code>gunicorn</code> workers each run the same loading code, 3 separate copies of the same data will be loaded â€“ but there's a way to avoid this, below.</p>
<p>I'd suggest 1st loading the vectors into a utility class for accessing word-vectors, like the <code>KeyedVectors</code> interface in the Gensim library. It'll store all the vectors in one compact <code>numpy</code> matrix, with a dict-like interface that still returns one <code>numpy</code> <code>ndarray</code> for as each individual vector.</p>
<p>For example, you can convert GLoVe text-format vectors to a slightly-different interchange format (with an extra header line, that Gensim calls <code>word2vec_format</code> after its use by the original Google <code>word2vec.c</code> code). In <code>gensim-3.8.3</code> (current release as of August 2020) you can do:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec('glove.6B.300d.txt', 'glove.6B.300d.w2vtxt')
</code></pre>
<p>Then, the utility-class <code>KeyedVectors</code> can load them like so:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors
glove_kv = KeyedVectors.load_word2vec_format('glove.6B.300d.w2vtxt', binary=False)
</code></pre>
<p>(Starting in the future <code>gensim-4.0.0</code> release, it should be possible to skip conversion &amp; just use the new <code>no_header</code> argument to read a GLoVe text file directly: <code>glove_kv = KeyedVectors.load_word2vec_format('glove.6B.300d.w2vtxt', binary=False, no_header=True)</code>. But this headerless-format will be a little slower, as it requires two passes over the file - the 1st to learn the full size.)</p>
<p>Loading just once into <code>KeyedVectors</code> should already be faster &amp; more-compact than your original generic two-step process. And, lookups that are analogous to what you were doing on the prior dict will be available on the <code>glove_kv</code> instance. (Also, there are many other convenience operations, like ranked <code>.most_similar()</code> lookup, that utilize efficient array library functions for speed.)</p>
<p>You can take another step, though, to minimize the parsing-on-load, and to defer loading unneeded ranges of the full set of vectors, and automatically reuse raw array data between processes.</p>
<p>That extra step is to re-save the vectors using the Gensim instance's <code>.save()</code> function, which will dump the raw vectors into a separate dense file that's suitable for memory-mapping upon the next load. So first:</p>
<pre class=""lang-py prettyprint-override""><code>glove_kv.save('glove.6B.300d.gs')
</code></pre>
<p>This will create <strong>more than one file</strong> which must be kept together if relocated â€“ but the <code>.npy</code> file(s) saved will be the exact minimal format ready for memory-mapping .</p>
<p>Then, when needed later, load as:</p>
<pre class=""lang-py prettyprint-override""><code>glove_kv = KeyedVectors.load('glove.6B.300d.gs', mmap='r')
</code></pre>
<p>The <code>mmap</code> argument uses underlying OS mechanisms to simply map the relevant matrix address-space to the (read-only) file(s) on disk, so that the initial 'load' is effectively instant, but any attempt to access ranges of the matrix will use virtual-memory to page-in the right ranges of the file. It thus eliminates any scanning-for-delimiters &amp; defers IO until absolutely needed. (And if there are any ranges you never access? They'll never be loaded.)</p>
<p>The other big benefit of memory-mapping is that if multiple processes each read-only memory-map the same on-disk files, the OS is smart enough to let them share any common paged-in ranges. So with, say, 3 totally-separate OS processes that each mmap the same file, you get 3X RAM savings.</p>
<p>(If after all these changes, the lag upon restarting server processes is still an issue â€“ perhaps because the server processes crash or otherwise need restarting often â€“ you could even consider using some <em>other</em> long-lived, stable process to initially mmap the vectors. Then, even the crash of all server processes wouldn't cause the OS to lose any paged-in ranges of the file, and the restart of the server processes might find some or all of the relevant data already in RAM. But the complication of this extra role may be superfluous once the other optimizations are in place.)</p>
<p>One extra caveat: if you start using <code>KeyedVectors</code> methods like <code>.most_similar()</code> that can (up through <code>gensim-3.8.3</code>) trigger the creation of a full-size cache of the unit-length-normalized word-vectors, you could lose the mmap benefits unless you take some extra steps to short-circuit that process. See more details in prior answer: <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",1,1,673,2020-08-31 07:50:56,https://stackoverflow.com/questions/63666823/loading-pretrained-glove-on-production-with-flask-and-gunicorn
Merge related words in NLP,"<p>I'd like to define a new word which includes count values from two (or more) different words. For example:</p>
<pre><code>Words Frequency
0   mom 250
1   2020    151
2   the 124
3   19  82
4   mother  81
... ... ...
10  London  6
11  life    6
12  something   6
</code></pre>
<p>I would like to define mother as <code>mom + mother</code>:</p>
<pre><code>Words Frequency
0   mother  331
1   2020    151
2   the 124
3   19  82
... ... ...
9   London  6
10  life    6
11  something   6
</code></pre>
<p>This is a way to alternative define group of words having some meaning (at least for my purpose).</p>
<p>Any suggestion would be appreciated.</p>
","python, nlp, cluster-analysis, word2vec, wordnet","<p><strong>UPDATE 10-21-2020</strong></p>
<p><strong>I decided to build a Python module to handle the tasks that I outlined in this answer. The module is called <em>wordhoard</em> and can be downloaded from <a href=""https://pypi.org/project/wordhoard/"" rel=""noreferrer"">pypi</a></strong></p>
<hr />
<p>I have attempted to use Word2vec and WordNet in projects where I needed to determine the frequency of a keyword (e.g. healthcare) and the keyword's synonyms (e.g., wellness program, preventive medicine).  I found that most NLP libraries didn't produce the results that I needed, so I decided to build my own dictionary with custom keywords and synonyms.  This approached has worked for both analyzing and classification text in multiple projects.</p>
<p>I'm sure that someone that is versed in NLP technology might have a more robust solution, but the one below is similar ones that have worked for me time and time again.</p>
<p>I coded my answer to match the Words Frequency data you had in your question, but it can be modified to use any keyword and synonyms dataset.</p>
<pre><code>import string

# Python Dictionary
# I manually created these word relationship - primary_word:synonyms
word_relationship = {&quot;father&quot;: ['dad', 'daddy', 'old man', 'pa', 'pappy', 'papa', 'pop'],
          &quot;mother&quot;: [&quot;mamma&quot;, &quot;momma&quot;, &quot;mama&quot;, &quot;mammy&quot;, &quot;mummy&quot;, &quot;mommy&quot;, &quot;mom&quot;, &quot;mum&quot;]}

# This input text is from various poems about mothers and fathers
input_text = 'The hand that rocks the cradle also makes the house a home. It is the prayers of the mother ' \
         'that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of ' \
         'her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She ' \
         'has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the ' \
         'greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend, ' \
         'This to me you have always been. Through the good times and the bad, Your understanding I have had.'

# converts the input text to lowercase and splits the words based on empty space.
wordlist = input_text.lower().split()

# remove all punctuation from the wordlist
remove_punctuation = [''.join(ch for ch in s if ch not in string.punctuation) 
for s in wordlist]

# list for word frequencies
wordfreq = []

# count the frequencies of a word
for w in remove_punctuation:
wordfreq.append(remove_punctuation.count(w))

word_frequencies = (dict(zip(remove_punctuation, wordfreq)))

word_matches = []

# loop through the dictionaries
for word, frequency in word_frequencies.items():
   for keyword, synonym in word_relationship.items():
      match = [x for x in synonym if word == x]
      if word == keyword or match:
        match = ' '.join(map(str, match))
        # append the keywords (mother), synonyms(mom) and frequencies to a list
        word_matches.append([keyword, match, frequency])

# used to hold the final keyword and frequencies
final_results = {}

# list comprehension to obtain the primary keyword and its frequencies
synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

# iterate synonym_matches and output total frequency count for a specific keyword
for item in synonym_matches:
  if item[0] not in final_results.keys():
    frequency_count = 0
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count
  else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

 
print(final_results)
# output
{'mother': 3, 'father': 2}
</code></pre>
<h2>Other Methods</h2>
<p>Below are some other methods and their out-of-box output.</p>
<hr />
<p><strong>NLTK WORDNET</strong></p>
<p>In this example, I looked up the synonyms for the word 'mother.' Note that WordNet does not have the synonyms 'mom' or 'mum' linked to the word mother.  These two words are within my sample text above.  Also note that the word 'father' is listed as a synonym for 'mother.'</p>
<pre><code>from nltk.corpus import wordnet

synonyms = []
word = 'mother'
for synonym in wordnet.synsets(word):
   for item in synonym.lemmas():
      if word != synonym.name() and len(synonym.lemma_names()) &gt; 1:
        synonyms.append(item.name())

print(synonyms)
['mother', 'female_parent', 'mother', 'fuss', 'overprotect', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']
</code></pre>
<p><strong>PyDictionary</strong></p>
<p>In this example, I looked up the synonyms for the word 'mother' using PyDictionary, which queries <a href=""https://synonym.com"" rel=""noreferrer"">synonym.com</a>. The synonyms in this example include the words 'mom' and 'mum.' This example also includes additional synonyms that WordNet did not generate.</p>
<p>BUT, PyDictionary also produced a synonym list for 'mum.' Which has nothing to do with the word 'mother.'  It seems that PyDictionary pulled this list from the <a href=""https://www.synonym.com/synonyms/mum"" rel=""noreferrer"">adjective section</a> of the page instead of the noun section.  It's hard for a computer to distinguish between the adjective mum and the noun mum.</p>
<pre><code>from PyDictionary import PyDictionary
dictionary_mother = PyDictionary('mother')

print(dictionary_mother.getSynonyms())
# output 
[{'mother': ['mother-in-law', 'female parent', 'supermom', 'mum', 'parent', 'mom', 'momma', 'para I', 'mama', 'mummy', 'quadripara', 'mommy', 'quintipara', 'ma', 'puerpera', 'surrogate mother', 'mater', 'primipara', 'mammy', 'mamma']}]

dictionary_mum = PyDictionary('mum')

print(dictionary_mum.getSynonyms())
# output 
[{'mum': ['incommunicative', 'silent', 'uncommunicative']}]
</code></pre>
<p>Some of the other possible approaches are using the Oxford Dictionary API or querying thesaurus.com. Both these methods also have pitfalls. For instance the Oxford Dictionary API requires an API key and a paid subscription based on query numbers. And thesaurus.com is missing potential synonyms that could be useful in grouping words.</p>
<pre><code>https://www.thesaurus.com/browse/mother
synonyms: mom, parent, ancestor, creator, mommy, origin, predecessor, progenitor, source, child-bearer, forebearer, procreator
</code></pre>
<h2>UPDATE</h2>
<p>Producing a precise synonym lists for each potential word in your corpus is hard and will require a multiple prong approach.  The code below using
WordNet and PyDictionary to create a superset of synonyms.  Like all the other answers, this combine methods also leads to some over counting of word frequencies. I've been trying to reduce this over-counting by combining key and value pairs within my final dictionary of synonyms.  The latter problem is much harder than I anticipated and might require me to open my own question to solve.  In the end, I think that based on your use case you need to determine, which approach works best and will likely need to combine several approaches.</p>
<p>Thanks for posting this question, because it allowed me to look at other methods for solving a complex problem.</p>
<pre><code>from string import punctuation
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from PyDictionary import PyDictionary

input_text = &quot;&quot;&quot;The hand that rocks the cradle also makes the house a home. It is the prayers of the mother
         that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of
         her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She
         has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the
         greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend,
         This to me you have always been. Through the good times and the bad, Your understanding I have had.&quot;&quot;&quot;


def normalize_textual_information(text):
   # split text into tokens by white space
   token = text.split()

   # remove punctuation from each token
   table = str.maketrans('', '', punctuation)
   token = [word.translate(table) for word in token]

   # remove any tokens that are not alphabetic
   token = [word.lower() for word in token if word.isalpha()]

   # filter out English stop words
   stop_words = set(stopwords.words('english'))

   # you could add additional stops like this
   stop_words.add('cannot')
   stop_words.add('could')
   stop_words.add('would')

   token = [word for word in token if word not in stop_words]

   # filter out any short tokens
   token = [word for word in token if len(word) &gt; 1]
   return token


def generate_word_frequencies(words):
   # list to hold word frequencies
   word_frequencies = []

   # loop through the tokens and generate a word count for each token
   for word in words:
      word_frequencies.append(words.count(word))

   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary
   word_frequencies = (dict(zip(words, word_frequencies)))

   # sort the frequency of the words from low to high
   sorted_frequencies = {key: value for key, value in 
   sorted(word_frequencies.items(), key=lambda item: item[1])}

 return sorted_frequencies


def get_synonyms_internet(word):
   dictionary = PyDictionary(word)
   synonym = dictionary.getSynonyms()
   return synonym

 
words = normalize_textual_information(input_text)

all_synsets_1 = {}
for word in words:
  for synonym in wordnet.synsets(word):
    if word != synonym.name() and len(synonym.lemma_names()) &gt; 1:
      for item in synonym.lemmas():
        if word != item.name():
          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())

all_synsets_2 = {}
for word in words:
  word_synonyms = get_synonyms_internet(word)
  for synonym in word_synonyms:
    if word != synonym and synonym is not None:
      all_synsets_2.update(synonym)

 word_relationship = {**all_synsets_1, **all_synsets_2}

 frequencies = generate_word_frequencies(words)
 word_matches = []
 word_set = {}
 duplication_check = set()

 for word, frequency in frequencies.items():
    for keyword, synonym in word_relationship.items():
       match = [x for x in synonym if word == x]
       if word == keyword or match:
         match = ' '.join(map(str, match))
         if match not in word_set or match not in duplication_check or word not in duplication_check:
            duplication_check.add(word)
            duplication_check.add(match)
            word_matches.append([keyword, match, frequency])

 # used to hold the final keyword and frequencies
 final_results = {}

 # list comprehension to obtain the primary keyword and its frequencies
 synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

 # iterate synonym_matches and output total frequency count for a specific keyword
 for item in synonym_matches:
    if item[0] not in final_results.keys():
      frequency_count = 0
      frequency_count = frequency_count + item[1]
      final_results[item[0]] = frequency_count
 else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

# do something with the final results
</code></pre>
",15,23,10080,2020-09-02 12:44:05,https://stackoverflow.com/questions/63705803/merge-related-words-in-nlp
How to retrieve array in Word2Vec,"<p>I am trying to retrieve the array/vector of a word in my trained word2vec model.  In SpaCy this is possible with model.vocab.get_vector(&quot;word&quot;), but I can't find a way to do it in word2Vec</p>
","machine-learning, nlp, word2vec","<p>From <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim documentation</a>:
Initialize a model with e.g.:</p>
<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec
    
path = get_tmpfile(&quot;word2vec.model&quot;)
    
model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(&quot;word2vec.model&quot;)
</code></pre>
<p>Now, you can get word vector of for example <code>word</code> by:</p>
<pre><code>model.wv['word']  # numpy vector of a word (OR: model.word_vec(&quot;word&quot;))
</code></pre>
",0,-2,388,2020-09-02 19:58:25,https://stackoverflow.com/questions/63712756/how-to-retrieve-array-in-word2vec
Construct word2vec (CBOW) training data from beginning of sentence,"<p>When constructing training data for CBOW, <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">Mikolov et al.</a> suggest using the word from the center of a context window. What is the &quot;best&quot; approach to capturing words at the beginning/end of a sentence (I put best in quotes because I'm sure this depends on the task). Implementations I see online do something like the this:</p>
<pre><code>for i in range(2, len(raw_text) - 2):
    context = [raw_text[i - 2], raw_text[i - 1],
               raw_text[i + 1], raw_text[i + 2]]
</code></pre>
<p>I see two issues arising from this approach.</p>
<ul>
<li><strong>Issue 1:</strong> The approach gives imbalanced focus to the middle of the sentence. For example, the first word of the sentence can only appear in 1 context window and will never appear as the target word. Compare this to the 4th word in the sentence which will appear in 4 context windows and will also be a target word. This will be an issue as some words appear frequently at the beginning of sentences (i.e. however, thus, etc.). Wouldn't this approach minimize their use?</li>
<li><strong>Issue 2:</strong> Sentences with 4 or fewer words are completely ignored, and the importance of short sentences is minimized. For example, a sentence with 5 words can only contribute one training sample while a sentence of length 8 will contribute 4 training samples.</li>
</ul>
<p>Can anyone offer insight as to how much these issues affect the results or any alternative approaches for constructing the training data? (I considered letting the first word be the target word and using the next N words as the context, but this creates issues of it's own).</p>
<p>Related question on Stack Exchange:
<a href=""https://datascience.stackexchange.com/questions/81249/construct-word2vec-cbow-training-data-from-beginning-of-sentence"">Construct word2vec (CBOW) training data from beginning of sentence</a></p>
","neural-network, nlp, text-mining, word2vec, word-embedding","<p>All actual implementations I've seen, going back to the original <code>word2vec.c</code> by Mikolov, tend to let every word take turns being the 'center target word', but truncate the context-window to whatever is available.</p>
<p>So for example, with a <code>window=5</code> (on both sides), and the 'center word' as the 1st word of a text, only the 5 following words are used. If the center word is the 2nd word, 1 word preceding, and 5 words following, will be used.</p>
<p>This is easy to implement and works fine in practice.</p>
<p>In CBOW mode, every center word is still part of the same same number of neural-network forward-propagations (roughly, prediction attempts), though words 'near the ends' participate as inputs slightly less often. But even then, they're subject to an incrementally larger update - such as when they're 1 of just 5 words, instead of 1 of just 10.</p>
<p>(In SG mode, words near-the-ends will both inputs and target-words slightly less often.)</p>
<p>Your example code â€“ showing words without full context windows never being the center target â€“ is not something I've seen, and I'd only expect that choice in a buggy/unsophisticated implementation.</p>
<p>So neither of your issues arise in common implementations, where texts are longer than 1 word long. (In even a text of 2 words, the 1st word will be predicted using a window of just the 2nd, and the 2nd will be predicted with a window of just the 1st.)</p>
<p>While the actual word-sampling does result in slightly-different treatment of words at either end, it's hard for me to imagine these slight differences in word-treatment making any difference in results, in appropriate training corpuses for word2vec â€“ large &amp; varied with plentiful contrasting examples for all relevant words.</p>
<p>(Maybe it'd be an issue in some small or synthetic corpus, where some rare-but-important tokens only appear in leading- or ending-positions. But that's far from the usual use of word2vec.)</p>
<p>Note also that while some descriptions &amp; APIs describe the units of word2vec training as 'sentences', the algorithm really just works on 'lists of tokens'. Often each list-of-tokens will span paragraphs or documents. Sometimes they retain things like punctuation, including sentence-ending periods, as pseudo-words. Bleeding the windows across sentence-boundaries rarely hurts, and often help, as the cooccurrences of words leading out of one sentence and into the next may be just as instructive as the cooccurrences of words inside one sentence. So in common practice of many-sentence training text, even fewer 'near-the-ends' words have even a slightly-different sampling treatment that you may have thought.</p>
",2,0,585,2020-09-04 21:02:41,https://stackoverflow.com/questions/63747999/construct-word2vec-cbow-training-data-from-beginning-of-sentence
Gensim Word2vec model is not updating the previous word&#39;s embedding weights during increased training,"<p>I want to train a previous-trained word2vec model in a increased way that is update the word's weights if the word has been seen in the previous training process and create and update the weights of the new words that has not been seen in the previous training process. For example:</p>
<pre><code>from gensim.models import Word2Vec
# old corpus
corpus = [[&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;2&quot;, &quot;3&quot;, &quot;1&quot;]]
# first train on old corpus
model = Word2Vec(sentences=corpus, size=2, min_count=0, window=2)
# checkout the embedding weights for word &quot;1&quot;
print(model[&quot;1&quot;])

# here comes a new corpus with new word &quot;4&quot; and &quot;5&quot;
newCorpus = [[&quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;1&quot;, &quot;5&quot;, &quot;2&quot;]]

# update the previous trained model
model.build_vocab(newCorpus, update=True)
model.train(newCorpus, total_examples=model.corpus_count, epochs=1)

# check if new word has embedding weights:
print(model[&quot;4&quot;])  # yes

# check if previous word's embedding weights are updated
print(model[&quot;1&quot;])  # output the same as before
</code></pre>
<p>It seems that the previous word's embedding is not updated even though the previous word's context has benn changed in the new corpus. Could someone tell me how to make the previous embedding weights updated?</p>
","python, nlp, gensim, word2vec","<p><strong>Answer for original question</strong></p>
<p>Try printing them out (or even just a few leading dimensions, eg <code>print(model['1'][:5])</code>) before &amp; after to see if they've changed.</p>
<p>Or, at the beginning, make <code>preEmbed</code> a proper <em>copy</em> of the values (eg: <code>preEmbed = model['1'].copy()</code>).</p>
<p>I think you'll see the values have really changed.</p>
<p>Your current <code>preEmbed</code> variable will only be a <em>view</em> into the array that changes along with the underlying array, so will always return <code>True</code>s for your later check.</p>
<p>Reviewing a writeup on <a href=""https://www.tutorialspoint.com/numpy/numpy_copies_and_views.htm"" rel=""nofollow noreferrer"">Numpy Copies &amp; Views</a> will help explain what's happening with further examples.</p>
<p><strong>Answer for updated code</strong></p>
<p>It's likely that in your subsequent single-epoch training, all examples of <code>'1'</code> are being skipped via the <code>sample</code> downsampling feature, because <code>'1'</code> is a very-frequent word in your tiny corpus: 28.6% of all words. (In realistic natural-language corpora, the most-frequent word won't be more than a few percent of all words.)</p>
<p>I suspect if you disable this downsampling feature with <code>sample=0</code>, you'll see the changes you expect.</p>
<p>(Note that this feature is really helpful with adequate training data, and more generally, lots of things about <code>Word2Vec</code> &amp; related algorithms, and especially their core benefits, require lots of diverse data â€“ and won't work well, or behave in expected ways, with toy-sized datasets.)</p>
<p>Also note: your second <code>.train()</code> should use an explicitly accurate count for the <code>newCorpus</code>. (Using <code>total_examples=model.corpus_count</code> to re-use the cached corpus count may not always be appropriate when you're supplying extra data, even if it works OK here.)</p>
<p>Another thing to watch out for: once you start using a model for more-sophisticated operations like <code>.most_similar()</code>, it will have cached some calculated data for vector-to-vector comparisons, and this data won't always (at least through <code>gensim-3.8.3</code>) be refreshed with more training. So, you may have to discard that data (in <code>gensim-3.8.3</code> by <code>model.wv.vectors_norm = None</code>) to be sure to have fresh unit-normed vectors, or fresh <code>most_similar()</code> (&amp; related method) results.</p>
",2,0,429,2020-09-05 08:40:41,https://stackoverflow.com/questions/63752033/gensim-word2vec-model-is-not-updating-the-previous-words-embedding-weights-duri
Sentences embedding using word2vec,"<p>I'd like to compare the difference among the same word mentioned in different sentences, for example &quot;travel&quot;.
What I would like to do is:</p>
<ul>
<li>Take the sentences mentioning the term &quot;travel&quot; as plain text;</li>
<li>In each sentence, replace 'travel' with travel_sent_x.</li>
<li>Train a word2vec model on these sentences.</li>
<li>Calculate the distance between travel_sent1, travel_sent2, and other relabelled mentions of &quot;travel&quot;
So each sentence's &quot;travel&quot; gets its own vector, which is used for comparison.</li>
</ul>
<p>I know that word2vec requires much more than several sentences to train reliable vectors. The official page recommends datasets including billions of words, but I have not a such number in my dataset(I have thousands of words).</p>
<p>I was trying to test the model with the following few sentences:</p>
<pre><code>    Sentences
    Hawaii makes a move to boost domestic travel and support local tourism
    Honolulu makes a move to boost travel and support local tourism
    Hawaii wants tourists to return so much it's offering to pay for half of their travel expenses
</code></pre>
<p>My approach to build the vectors has been:</p>
<pre><code>from gensim.models import Word2Vec

vocab = df['Sentences']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
df['Sentences'].apply(model.vectorize)
</code></pre>
<p>However I do not know how to visualise the results to see their similarity and get some useful insight.
Any help and advice will be welcome.</p>
<p>Update: I would use Principal Component Analysis algorithm to visualise embeddings in 3-dimensional space. I know how to do for each individual word, but I do not know how to do it in case of sentences.</p>
","python, gensim, word2vec, embedding","<p>If you are interested in comparing sentences, Word2Vec is not the best choice. It was shown that using it to create sentence embedding produces inferior results than a dedicated sentence embedding algorithm. If your dataset is not huge, you can't create (train a new) embedding space using your own data. This forces you to use a pre trained embedding for the sentences. Luckily, there are enough of those nowadays. I believe that Universal Sentence Encoder (by Google) will suit your needs best.</p>
<p>Once you get vector representation for you sentences you can go 2 ways:</p>
<ol>
<li>create a matrix of pairwise comparisons and visualize it as a heatmap. This representation is useful when you have some prior knowledge about how close are the sentences and you want to check you hypothesis. You can even <a href=""https://jinglescode.github.io/textual-similarity-universal-sentence-encoder/"" rel=""nofollow noreferrer"">try it online</a>.
<a href=""https://i.sstatic.net/z0CqK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z0CqK.png"" alt=""enter image description here"" /></a></li>
<li>run t-SNE on the vector representations. This will create a 2D projection of the sentences that will preserve relative distances between them. It presents data much better than PCA. Than you can easily find neighbors of the certain sentence:
<a href=""https://i.sstatic.net/zRieX.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zRieX.gif"" alt=""enter image description here"" /></a></li>
</ol>
<p>You can learn more from <a href=""https://jinglescode.github.io/2020/02/10/build-textual-similarity-analysis-web-app/"" rel=""nofollow noreferrer"">this</a> and <a href=""https://medium.com/oneassist-tech-blog/visualizing-context-with-googles-universal-sentence-encoder-and-graphdb-c5f92b2f3db3"" rel=""nofollow noreferrer"">this</a></p>
",3,3,7172,2020-09-07 14:48:02,https://stackoverflow.com/questions/63779875/sentences-embedding-using-word2vec
generating completely new vector based on other vectors,"<p>Assume I have four-vectors (v1,v2,v3,v4), and I want to create a new vector (vec_new) that is not close to any of those four-vectors. I was thinking about interpolation and extrapolation. Do you think they are suitable? Are they also apply for vector and generate a vector of let's say 300 dimensions? Another possible option would be the transformation matrix. But I am not sure if it fit my concern. I think averaging and concatenation are not the good ones as I might be close to some of those four-vectors.</p>
<p>based on my problem, Imagine I divided my vectors into two categories. I need to find a vector which belongs to non-of those categories.</p>
<p>Any other ideas?</p>
","vector, linear-algebra, word2vec","<p>Per my comment, I wouldn't expect the creation of synthetic &quot;far away&quot; examples to be useful for realistic goals.</p>
<p>Even things like word antonyms <strong>are not</strong> maximally cosine-dissimilar from each other, because among the realm of all word-meaning-possibilities, antonyms are quite similar to each other. For example, 'hot' and 'cold' are considered opposites, but are the same kind of word, describing the same temperature-property, and can often be drop-in replacements for each other in the same sentences. So while they may show an interesting contrast in word-vector space, the &quot;direction of difference&quot; isn't going to be through the origin -- as would create maximal cosine-dissimilarity.</p>
<p>And in classification contexts, even a simple 2-category classifier will need actual 'negative' examples. With only positive examples, the 'vector space' won't necessarily model anything about hypothesized-but-not-actually-present negative examples. (It's nearly impossible to divide the space into two categories without training examples showing the real &quot;boundaries&quot;.)</p>
<p>Still, there's an easy way to make a vector that is maximally dissimilar to another single vector: negate it. That creates a vector that's in the exact opposite direction from the original, and thus will have a cosine-similarity of <code>-1.0</code>.</p>
<p>If you have a number of vectors against which you want to find a maximally-dissimilar vector, I suspect you can't do much better than <em>negating the average of all the vectors</em>. That is, average the vectors, then negate that average-vector, to find the vector that's pointing exactly-opposite the average.</p>
<p>Good luck!</p>
",1,0,173,2020-09-10 15:43:28,https://stackoverflow.com/questions/63833232/generating-completely-new-vector-based-on-other-vectors
Looking for an effective NLP Phrase Embedding model,"<p>The goal I want to achieve is to find a good word_and_phrase embedding model that can do:
(1) For the words and phrases that I am interested in, they have embeddings.
(2) I can use embeddings to compare similarity between two things(could be word or phrase)</p>
<p>So far I have tried two paths:</p>
<p>1: Some Gensim-loaded pre-trained models, for instance:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api
# download the model and return as object ready for use
model_glove_twitter = api.load(&quot;fasttext-wiki-news-subwords-300&quot;)
model_glove_twitter.similarity('computer-science', 'machine-learning')
</code></pre>
<p>The problem with this path is that I do not know if a phrase has embedding. For this example, I got this error:</p>
<pre><code>KeyError: &quot;word 'computer-science' not in vocabulary&quot;
</code></pre>
<p>I will have to try different pre-trained models, such as word2vec-google-news-300, glove-wiki-gigaword-300, glove-twitter-200, etc. Results are similar, there are always phrases of interests not having embeddings.</p>
<ol start=""2"">
<li>Then I tried to use some BERT-based sentence embedding method: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>.</li>
</ol>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

from scipy.spatial.distance import cosine

def cosine_similarity(embedding_1, embedding_2):
    # Calculate the cosine similarity of the two embeddings.
    sim = 1 - cosine(embedding_1, embedding_2)
    print('Cosine similarity: {:.2}'.format(sim))

phrase_1 = 'baby girl'
phrase_2 = 'annual report'
embedding_1 = model.encode(phrase_1)
embedding_2 = model.encode(phrase_2)
cosine_similarity(embedding_1[0], embedding_2[0])
</code></pre>
<p>Using this method I was able to get embeddings for my phrases, but the similarity score was 0.93, which did not seem to be reasonable.</p>
<p>So what can I try else to achieve the two goals mentioned above?</p>
","nlp, gensim, word2vec, fasttext","<p>The problem with the first path is that <strong>you are loading fastText embeddings like word2vec embeddings and word2vec can't cope with Out Of Vocabulary words</strong>.</p>
<p>The good thing is that <strong>fastText can manage OOV words</strong>.
You can use Facebook original implementation (<code>pip install fasttext</code>) or Gensim implementation.</p>
<p>For example, using Facebook implementation, you can do:</p>
<pre><code>import fasttext
import fasttext.util

# download an english model
fasttext.util.download_model('en', if_exists='ignore')  # English
model = fasttext.load_model('cc.en.300.bin')

# get word embeddings
# (if instead you want sentence embeddings, use get_sentence_vector method)
word_1='computer-science'
word_2='machine-learning'
embedding_1=model.get_word_vector(word_1)
embedding_2=model.get_word_vector(word_2)

# compare the embeddings
cosine_similarity(embedding_1, embedding_2)
</code></pre>
",3,3,1655,2020-09-11 08:53:05,https://stackoverflow.com/questions/63843793/looking-for-an-effective-nlp-phrase-embedding-model
What is the best way to drop old &quot;words&quot; from gensim word2vec model?,"<p>I have a &quot;corpus&quot; built from an item-item graph, which means each sentence is a graph walk path and each word is an item. I want to train a word2vec model upon the corpus to obtain items' embedding vectors. The graph is updated everyday so the word2vec model is trained in an increased way (using <code>Word2Vec.save()</code> and <code>Word2Vec.load()</code>) to keep updating the items' vectors.</p>
<p>Unlike words, the items in my corpus have their lifetime and there will be new items added in everyday. In order to prevent the constant growth of the model size, I need to drop items that reached their lifetime while keep the model trainable. I've read the similar question
<a href=""https://stackoverflow.com/questions/48941648/how-to-remove-a-word-completely-from-a-word2vec-model-in-gensim"">here</a>, but this question's answer doesn't related to increased-training and is based on <code>KeyedVectors</code>. I come up with the below code, but I'm not sure if it is correct and proper:</p>
<pre><code>from gensim.models import Word2Vec
import numpy as np

texts = [[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], [&quot;a&quot;, &quot;h&quot;, &quot;b&quot;]]
m = Word2Vec(texts, size=5, window=5, min_count=1, workers=1)

print(m.wv.index2word)
print(m.wv.vectors)

# drop old words
wordsToDrop = [&quot;b&quot;, &quot;c&quot;]
for w in wordsToDrop:
    i = m.wv.index2word.index(w)
    m.wv.index2word.pop(i)
    m.wv.vectors = np.delete(m.wv.vectors, i, axis=0)
    del m.wv.vocab[w]

print(m.wv.index2word)
print(m.wv.vectors)
m.save(&quot;m.model&quot;)
del m

# increased training
new = [[&quot;a&quot;, &quot;e&quot;, &quot;n&quot;], [&quot;r&quot;, &quot;s&quot;]]
m = Word2Vec.load(&quot;m.model&quot;)
m.build_vocab(new, update=True)
m.train(new, total_examples=m.corpus_count, epochs=2)
print(m.wv.index2word)
print(m.wv.vectors)
</code></pre>
<p>After deleting and increased training, is the <code>m.wv.index2word</code> and <code>m.wv.vectors</code> still element-wise corresponding? Is there any side-effect of above code? If my way is not good, could someone give me an example to show how to drop the old &quot;words&quot; properly and keep the model trainable?</p>
","python, gensim, word2vec","<p>There's no official support for removing words from a Gensim <code>Word2Vec</code> model, once they've ever &quot;made the cut&quot; for inclusion.</p>
<p>Even the ability to <em>add</em> words isn't on a great footing, as the feature isn't based on any proven/published method of updating a <code>Word2Vec</code> model, and glosses over difficult tradeoffs in how update-batches affect the model, via choice of learning-rate or whether the batches fully represent the existing vocabulary. The safest course is to regularly re-train the model from scratch, with a full corpus with sufficient examples of all relevant words.</p>
<p>So, my main suggestion would be to regularly replace your model with a new one trained with all still-relevant data. That would ensure it's no longer wasting model state on obsolete terms, and that all still-live terms have received coequal, interleaved training.</p>
<p>After such a reset, word-vectors won't be comparable to word-vectors from a prior 'model era'. (The same word, even if its tangible meaning hasn't changed, could be an arbitrarily different place - but the relative relationships with other vectors should remain as good or better.) But, that same sort of drift-out-of-comparison is <em>also</em> happening with any set of small-batch updates that don't 'touch' every existing word equally, just at some unquantifiable rate.</p>
<p>OTOH, if you think you need to stay with such incremental updates, even knowing the caveats, it's plausible that you could patch-up the model structures to retain as much as is sensible from the old model &amp; continue training.</p>
<p>Your code so far is a reasonable start, missing a few important considerations for proper functionality:</p>
<ul>
<li>because deleting earlier-words changes the index location of later-words, you'd need to update the <code>vocab[word].index</code> values for every surviving word, to match the new <code>index2word</code> ordering. For example, after doing all deletions, you might do:</li>
</ul>
<pre><code>for i, word in enumerate(m.wv.index2word):
    m.wv.vocab[word].index = i
</code></pre>
<ul>
<li><p>because in your (default negative-sampling) <code>Word2Vec</code> model, there is <em>also</em> another array of per-word weights related to the model's output layer, that should also be updated in sync, so that the right output-values are being checked per word. Roughly, wheneever you delete a row from <code>m.wv.vectors</code>, you should delete the same row from <code>m.traininables.syn1neg</code>.</p>
</li>
<li><p>because the surviving vocabulary has different relative word-frequencies, both the negative-sampling and downsampling (controlled by the <code>sample</code> parameter) functions should work off different pre-calculated structures to assist their choices. For the cumulative-distribution table used by negative-sampling, this is pretty easy:</p>
</li>
</ul>
<pre><code>m.make_cum_table(m.wv)
</code></pre>
<p>For the downsampling, you'd want to update the <code>.sample_int</code> values similar to the logic you can view around the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/models/word2vec.py#L1534"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/models/word2vec.py#L1534</a>. (But, looking at that code now, I think it may be <a href=""https://github.com/RaRe-Technologies/gensim/issues/2951"" rel=""nofollow noreferrer"">buggy</a> in that it's updating all words with just the frequency info in the new dict, so probably fouling the usual downsampling of truly-frequent words, and possibly erroneously downsampling words that are only frequent in the new update.)</p>
<p>If those internal structures are updated properly in sync with your existing actions, the model is probably in a consistent state for further training. (But note: these structures change a lot in the forthcoming <code>gensim-4.0.0</code> release, so any custom tampering will need to be updated when upgrading then.)</p>
<p>One other efficiency note: the <code>np.delete()</code> operation will create a new array, the full size of the surviving array, and copy the old values over, each time it is called. So using it to remove many rows, one at a time, from a very-large original array is likely to require a lot of redundant allocation/copying/garbage-collection. You may be able to call it once, at the end, with a list of all indexes to remove.</p>
<p>But really: the simpler &amp; better-grounded approach, which may also yield significantly better continually-comparable vectors, would be to retrain with all current data whenever possible or a large amount of change has occurred.</p>
",1,1,772,2020-09-16 08:43:17,https://stackoverflow.com/questions/63916338/what-is-the-best-way-to-drop-old-words-from-gensim-word2vec-model
How to find a &#39;connection&#39; between words for clustering sentences,"<p>I would need to connect the word <code>4G</code> and <code>mobile phones</code> or <code>Internet</code> in order to cluster sentences about technology all together.
I have the following sentences:</p>
<pre><code>4G is the fourth generation of broadband network.
4G is slow. 
4G is defined as the fourth generation of mobile technology
I bought a new mobile phone. 
</code></pre>
<p>I need to consider in the same cluster the above sentences. Currently it does not, probably because it does not find a relation between 4G and mobile.
I tried to use first <code>wordnet.synsets</code> to find synonyms for connecting 4G to Internet or mobile phone, but unfortunately it did not find any connection.
To cluster the sentences I am doing as follows:</p>
<pre><code>rom sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import numpy

texts = [&quot;4G is the fourth generation of broadband network.&quot;,
    &quot;4G is slow.&quot;,
    &quot;4G is defined as the fourth generation of mobile technology&quot;,
    &quot;I bought a new mobile phone.&quot;]

# vectorization of the sentences
vectorizer = TfidfVectorizer(stop_words=&quot;english&quot;)
X = vectorizer.fit_transform(texts)
words = vectorizer.get_feature_names()
print(&quot;words&quot;, words)


n_clusters=3
number_of_seeds_to_try=10
max_iter = 300
number_of_process=2 # seads are distributed
model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)

labels = model.labels_
# indices of preferible words in each cluster
ordered_words = model.cluster_centers_.argsort()[:, ::-1]

print(&quot;centers:&quot;, model.cluster_centers_)
print(&quot;labels&quot;, labels)
print(&quot;intertia:&quot;, model.inertia_)

texts_per_cluster = numpy.zeros(n_clusters)
for i_cluster in range(n_clusters):
    for label in labels:
        if label==i_cluster:
            texts_per_cluster[i_cluster] +=1 

print(&quot;Top words per cluster:&quot;)
for i_cluster in range(n_clusters):
    print(&quot;Cluster:&quot;, i_cluster, &quot;texts:&quot;, int(texts_per_cluster[i_cluster])),
    for term in ordered_words[i_cluster, :10]:
        print(&quot;\t&quot;+words[term])

print(&quot;\n&quot;)
print(&quot;Prediction&quot;)

text_to_predict = &quot;Why 5G is dangerous?&quot;
Y = vectorizer.transform([text_to_predict])
predicted_cluster = model.predict(Y)[0]
texts_per_cluster[predicted_cluster]+=1

print(text_to_predict)
print(&quot;Cluster:&quot;, predicted_cluster, &quot;texts:&quot;, int(texts_per_cluster[predicted_cluster])),
for term in ordered_words[predicted_cluster, :10]:
print(&quot;\t&quot;+words[term])
</code></pre>
<p>Any help on this would be greatly appreciated it.</p>
","python, scikit-learn, nlp, k-means, word2vec","<p>As @sergey-bushmanov's comment notes, dense word embeddings (as from word2vec or similar algorithms) may help.</p>
<p>They will convert words to dense high-dimensional vectors, where words with similar meanings/usages are close to each other. And even: certain directions in space will often be roughly associated with the <em>kinds</em> of relationships between words.</p>
<p>So, word-vectors trained on sufficiently-representative (large and varied) text will place the vectors for <code>'4G'</code> and <code>'mobile'</code> somewhat near each other, and then if your sentence-representations are bootstrapped from word-vectors, that may help your clustering.</p>
<p>One quick way to use individual word-vectors to model sentences is to use the average of all a sentence's word-vectors as the sentence vector. That's too simple to model many shades of meaning (especially those that come from grammar and word-order), but often works as a good baseline, especially for matters of broad topicality.</p>
<p>Another calculation, &quot;Word Mover's Distance&quot;, treats sentences as sets-of-word-vectors (without averaging them), and can do sentence-to-sentence distance calculations that work better than simple averages â€“ but become very expensive to calculate for longer sentences.</p>
",1,1,66,2020-09-17 19:49:07,https://stackoverflow.com/questions/63945281/how-to-find-a-connection-between-words-for-clustering-sentences
How is the output of glove2word2vec() different from keyed_vectors.save(),"<p>I am new to NLP and I am running into this issue that I do not understand at all:</p>
<p>I have a text file with gloVe vectors.
I converted it to Word2Vec using</p>
<pre><code>glove2word2vec(TXT_FILE_PATH, KV_FILE_PATH)
</code></pre>
<p>this creates a KV file in my path which can then be loaded using</p>
<pre><code>word_vectors = KeyedVectors.load_word2vec_format(KV_FILE_PATH, binary=False)
</code></pre>
<p>I then save it using</p>
<pre><code>word_vectors.save(KV_FILE_PATH)
</code></pre>
<p>But when I now try to use the new KV file in intersect_word2vec_format it gives me an encoding error</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-11-d975bb14af37&gt; in &lt;module&gt;
      6 
      7 print(&quot;Intersect with pre-trained model...&quot;)
----&gt; 8 model.intersect_word2vec_format(KV_FILE_PATH, binary=False)
      9 
     10 print(&quot;Train custom word2vec model...&quot;)

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py in intersect_word2vec_format(self, fname, lockf, binary, encoding, unicode_errors)
    890         logger.info(&quot;loading projection weights from %s&quot;, fname)
    891         with utils.open(fname, 'rb') as fin:
--&gt; 892             header = utils.to_unicode(fin.readline(), encoding=encoding)
    893             vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    894             if not vector_size == self.wv.vector_size:

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    366     if isinstance(text, unicode):
    367         return text
--&gt; 368     return unicode(text, encoding, errors=errors)
    369 
    370 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","python, nlp, stanford-nlp, gensim, word2vec","<p>The <code>.save()</code> method saves a model in Gensim's native format - which is primarily Python pickling, with large arrays as separate files (which must be kept alongside the main save file).</p>
<p>That format is not the same as the <code>word2vec_format</code> that can be loaded by <code>load_word2vec_format()</code> or <code>intersect_word2vec_format()</code>.</p>
<p>If you want to save a set of vectors into the <code>word2vec_format</code>, use the method <code>.save_word2vec_format()</code>, not plain <code>.save()</code>.</p>
",1,0,290,2020-09-24 04:15:04,https://stackoverflow.com/questions/64039454/how-is-the-output-of-glove2word2vec-different-from-keyed-vectors-save
How to deal with large amount of sentences with gensim word2vec?,"<p>I have a very large amount of sentences, the problem is i cannot load them all at once in memory, specially when i tokenize the sentences and split them into list of words my RAM goes full really fast.</p>
<p>but i couldn't find any example of how can i train the gensim word2vec with batches, meaning in each epoch i guess i have to somehow load batches of data from disk, tokenize them and give it to the model then unload it and load the next batch.</p>
<p>how can i overcome this problem and train a word2vec model when i don't have enough ram to load all the sentences (not even 20% of them).</p>
<p>my sentences are basically in a text file, each line representing a sentence.</p>
","nlp, gensim, word2vec","<p>You can define your own corpus as suggested in <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model"" rel=""nofollow noreferrer"">docs</a> and basically size of corpus doesn't matter in this case:</p>
<pre><code>from gensim.test.utils import datapath
from gensim import utils

class MyCorpus(object):
    &quot;&quot;&quot;An interator that yields sentences (lists of str).&quot;&quot;&quot;

    def __iter__(self):
        corpus_path = datapath('lee_background.cor')
        for line in open(corpus_path):
            # assume there's one document per line, tokens separated by whitespace
            yield utils.simple_preprocess(line)
</code></pre>
<p>Then train it as follow:</p>
<pre><code>import gensim.models

sentences = MyCorpus()
model = gensim.models.Word2Vec(sentences=sentences)
</code></pre>
",2,0,908,2020-09-24 07:14:27,https://stackoverflow.com/questions/64041275/how-to-deal-with-large-amount-of-sentences-with-gensim-word2vec
"In CBOW model, do we need to take Average at Hidden layer?","<p>I search and read some articles about CBOW. But seem to have difference between these articles.</p>
<p>As I understand:</p>
<ul>
<li>Input is a batch vector. And we will feed it to Hidden layer. So that we will get another <strong>batch vector H</strong> at Hidden layer.</li>
<li>In an <a href=""https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"" rel=""nofollow noreferrer"">article (part 2.2.1)</a>, they say that we will not use any Activation Function at Hidden layer, but we will <strong>take average on <em>batch vector H</em></strong> to get a single vector (not a batch anymore). Then we will feed this average vector to Output layer and apply Softmax on it.</li>
</ul>
<p><a href=""https://i.sstatic.net/x1Gsb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x1Gsb.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>However, in this <a href=""https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-forward-propagation-Vphwi"" rel=""nofollow noreferrer"">Coursera's video</a>, they <em>don't</em> take average on batch vector H. They just feed this batch vector H to Output layer and apply Softmax on batch Output vector. And then calculate Cost function on it.</li>
<li>And, in <a href=""https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-forward-propagation-Vphwi"" rel=""nofollow noreferrer"">Coursera's video</a>, they say that we can use RelU as Activation function at Hidden layer. Is this a new method? Because I read many articles, but they always say that there is no Activation function at Hidden layer.</li>
</ul>
<p>Can you please help me to answer it?</p>
","machine-learning, nlp, word2vec, word-embedding","<p>In actual implementations â€“ whose source code you can review â€“ the set of context-word vectors are averaged together <em>before</em> being fed as the &quot;input&quot; to the neural-network.</p>
<p>Then, any back-propagated adjustments to the input are also applied to all the vectors contributing to that average.</p>
<p>(For example, in the original <code>word2vec.c</code> released with Google's original word2vec paper, you can see the tallying of vectors into <code>neu1</code>, then averaging via division by the context-window count <code>cw</code>, at:</p>
<p><a href=""https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L444-L448"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L444-L448</a>
)</p>
",1,0,710,2020-09-24 20:31:18,https://stackoverflow.com/questions/64053985/in-cbow-model-do-we-need-to-take-average-at-hidden-layer
Can doc2vec work on an artificial &quot;text&quot;?,"<p>I've created an artificial corpus (with 52624 documents). Each document is a list of objects (there are 461 of them).</p>
<p>So one possibility could be: <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code></p>
<p>Here's a bar plot (log-scale) of the vocabulary.</p>
<p><a href=""https://i.sstatic.net/B1CP3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B1CP3.png"" alt=""enter image description here"" /></a></p>
<p>And this is how I defined the model:</p>
<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=8, workers=4, min_count=1,epochs=40, dm=0)
</code></pre>
<p>Observing at:
<code>model.wv.most_similar_cosmul(positive = [&quot;chair&quot;])</code></p>
<p>I see non related words</p>
<p>And it seems to me that the following works poorly as well:</p>
<pre><code>inferred_vector = model.infer_vector([&quot;chair&quot;])
model.docvecs.most_similar([inferred_vector])
</code></pre>
<p>Where has my model failed?</p>
<p><strong>UPDATE</strong></p>
<p>There is the data (JSON file):</p>
<p><a href=""https://gofile.io/d/bZDcPX"" rel=""nofollow noreferrer"">https://gofile.io/d/bZDcPX</a></p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>Yes, <code>Doc2Vec</code> &amp; <code>Word2Vec</code> are often tried, and useful, on synthetic data. But whether they work may require a lot more tinkering, and atypical parameters, when the data doesn't reflect the same sort of correlations/distributions as the natural-language on which these algorithms were 1st developed.</p>
<p>First and foremost with your setup, you're using the <code>dm=0</code> mode. That's the <code>PV-DBOW</code> mode of the original &quot;Paragraph Vector&quot; paper, which specifically <strong>does not</strong> train word-vectors <strong>at all</strong>, only the doc-vectors. So if you're testing such a model by looking at word-vectors, your results will only reflect the random, untrained initialization values of any word-vectors.</p>
<p>Check the <code>model.docvecs</code> instead, for similarities between any doc-tags you specified in your data, and their may be more useful relationships.</p>
<p>(If you want your <code>Doc2Vec</code> model to learn words too â€“ which isn't necessarily important, especially with a small dataset or where the doc-vectors are your main interest â€“ you'd have to use either <code>dm=1</code> mode, or add <code>dbow_words=1</code> to <code>dm=0</code> so that the model adds interleaved skip-gram training. But note word-vector training may be weak/meaningless/harmful with data that's looks like it's just sorted runs of repeating tokens, as with your <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code> example item.)</p>
<p>Separately, using a very-low <code>min_count=1</code> is often a bad idea in such models - as such tokens with arbitrary idiosyncractic non-representative appearances do more damage to the coherence of surrounding more-common tokens than they help.</p>
",2,1,56,2020-09-28 12:15:48,https://stackoverflow.com/questions/64102023/can-doc2vec-work-on-an-artificial-text
Scala / Java word2vec reader,"<p>I've got several word2vec text files with the following standard layout:</p>
<pre><code>numWords vecSize
word1 vec1 vec2 ...
word2 vec1 vec2 ...
...
</code></pre>
<p>Is there any Scala or Java library to read these and calculate simple stuff like:</p>
<ul>
<li>isWordInVocab</li>
<li>getWordVectors</li>
<li>nearestNeighbours</li>
<li>cosDistance</li>
<li>...</li>
</ul>
<p>I could find some only for binary formats, is there a way to use these (apart from writing my own one)?</p>
","java, scala, word2vec","<p>I've just ended up writing my own class to avoid the whole dl4j/nd4j import/setup/run procedure.</p>
",0,0,52,2020-09-30 14:34:21,https://stackoverflow.com/questions/64139849/scala-java-word2vec-reader
Doc2Vec most similar vectors don&#39;t match an input vector,"<p>I've got a dataset of job postings with about 40 000 records. I extracted skills from descriptions using NER with about 30 000 skills in the dictionary. Every skill is represented as an unique identificator.</p>
<p>The distribution of skills number for a posting looks like that:</p>
<p>mean        15.12 |
std         11.22 |
min          1.00 |
25%          7.00 |
50%         13.00 |
75%         20.00 |</p>
<p>I've trained a word2vec model using only skill ids and it works more or less fine. I can find most similar skills to a given one and the result looks okay.</p>
<p>But when it comes to a doc2vec model I'm not satisfied with the result.</p>
<p>I have about 3200 unique job titles, most of them have only few entries and there are quite a few of them being from the same field ('front end developer', 'senior javascript developer', 'front end engineer'). I delibirately went for a variety of job titles which I use as tags in doc2vec.TaggedDocument(). My goal is to see a number of relevant job titles when I input a vector of skills into docvecs.most_similar().</p>
<p>After training a model (I've tried different number of epochs (100,500,1000) and vector sizes (40 and 100)) sometimes it works correctly, but most of the time it doens't. For example for a skills set like [numpy, postgresql, pandas, xgboost, python, pytorch] I get the most similar job title with a skill set like [family court, acting, advising, social work].</p>
<p>Can it be a problem with the size of my dataset? Or the size of docs (I consider that I have short texts)? I also think that I misunderstand something about doc2vec mechanism and just ignore it. I'd also like to ask if you know any other, maybe more advanced, ideas how I can get relevant job titles from a skill set and compare two skill set vectors if they are close or far.</p>
<p>UPD:</p>
<p>Job titles from my data are 'tags' and skills are 'words'. Each text has a single tag. There are 40 000 documents with 3200 repeating tags. 7881 unique skill ids appear in the documents. The average number of skill words per document is 15.</p>
<p>My data example:</p>
<pre><code>         job_titles                                             skills
1  business manager                 12 13 873 4811 482 2384 48 293 48
2    java developer      48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist      383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>The example of my code:</p>
<pre><code>def tagged_document(df):
    #tagging documents
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [row['job_title']])


data_for_training = list(tagged_document(job_data[['job_titles', 'skills']])

model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)

#the skill set contains close skills which represent a front end developer
skillset_ids = '12 34 556 453 1934'.split()                                                  
new_vector = model_d2v.infer_vector(skillset_ids, epochs=100)
model_d2v.docvecs.most_similar(positive=[new_vector], topn=30)
</code></pre>
<p>I've been experimenting recently and noticed that it performs a little better if I filter out documents with less than 10 skills. Still, there are some irrelevant job titles coming out.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Without seeing your code (or at least a sketch of its major choices), it's hard to tell if you might be making shooting-self-in-foot mistakes, like perhaps the common &quot;managing <code>alpha</code> myself by following crummy online examples&quot; issue: <a href=""https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-of-training-isnt-giving-good-results-what-m"">My Doc2Vec code, after many loops of training, isn&#39;t giving good results. What might be wrong?</a></p>
<p>(That your smallest number of tested <code>epochs</code> is 100 seems suspicious; 10-20 epochs are common values in published work, when both the size of the dataset and size of each doc are plentiful, though more passes can sometimes help with thinner data.)</p>
<p>Similarly, it's not completely clear from your description what your training docs are like. For example:</p>
<ul>
<li>Are the <code>tags</code> titles and the <code>words</code> skills?</li>
<li>Does each text have a single <code>tag</code>?</li>
<li>If there are 3,200 unique <code>tags</code> and 30,000 unique <code>words</code>, is that just 3,200 <code>TaggedDocuments</code>, or more with repeating titles?</li>
<li>What's the average number of skill-words per <code>TaggedDocument</code>?</li>
</ul>
<p>Also, if you are using word-vectors (for skills) as query vectors, you have to be sure to use a training mode that actually trains those. Some <code>Doc2Vec</code> modes, such as plain PV-DBOW (<code>dm=0</code>) don't train word-vectors at all, but they will exist as randomly-initialized junk. (Either adding non-default <code>dbow_words=1</code> to add skip-gram word-training, or switching to PV-DM <code>dm=1</code> mode, will ensure word-vectors are co-trained and in a comparable coordinate space.)</p>
",1,1,524,2020-10-02 15:15:43,https://stackoverflow.com/questions/64174071/doc2vec-most-similar-vectors-dont-match-an-input-vector
Word2Vec- does the word embedding change?,"<p>just wanted to know if there are 2 sentences-</p>
<ol>
<li>The <em><strong>bank</strong></em> remains closed on public holidays</li>
<li>Don't go near the river <em><strong>bank</strong></em></li>
</ol>
<p>The word 'bank' will have different word embeddings or same? If we use word2vec or glove?</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>You can't meaningfully train a dense word embedding on just 2 texts. You'd need these, and dozens (or ideally hundreds) more examples of the use of <code>'bank'</code> in subtly-varying contexts to get a good word-vector for <code>'bank'</code>. (And that word-vector would only have meaning in comparison to other word-vectors for other well-sampled words in the same trained model.)</p>
<p>Let's assume you do have a large, diverse training corpus with many examples of <code>'bank'</code> in contexts. And you've trained a model, either word2vec or GLoVe on that corpus.</p>
<p>Then, imagine that corpus was changed so that there were relatively more contexts that included the 'river' sense. (Perhaps, a bunch of new texts are added that talk about nature, parks, boating, &amp; irrigation.) Then, you retrain your model, from scratch, on the new corpus.</p>
<p>In the new model, <code>'bank'</code> (and related words) will typically have been nudged to have more 'river bank'-like neighbors.</p>
<p>These words may be in totally different coordinates, overall, as each run includes enough randomness to change words' ending positions a lot. But their <em>relative neighborhoods</em> &amp; <em>relative directions</em> will tend to be of similar value from subsequent runs, and changes in the mix of examples will tend to nudge results in one direction or another.</p>
<p>This is the case for both GLoVe and word2vec: their end results will both be influenced by the relative preponderance of alternate word senses.</p>
<p>(That words have multiple contrasting meanings is generally referred to in the relevant literature as 'polysemy', so searches like [polysemy word-vectors] should turn up a lot more work related to your question.)</p>
",1,0,681,2020-10-08 11:25:20,https://stackoverflow.com/questions/64261521/word2vec-does-the-word-embedding-change
Word2vec in pandas dataframe,"<p>I am trying to apply word2vec to check similarity of two columns per each row of my dataset.</p>
<p>For instance:</p>
<pre><code>Sent1                                     Sent2
It is a sunny day                         Today the weather is good. It is warm outside
What people think about democracy         In ancient times, Greeks were the first to propose democracy  
I have never played tennis                I do not know who Roger Feder is 
</code></pre>
<p>To apply word2vec, I consider the following:</p>
<pre><code>import numpy as np

words1 = sentence1.split(' ')
words2 = sentence2.split(' ')
#The meaning of the sentence can be interpreted as the average of its words
sentence1_meaning = word2vec(words1[0])
count = 1
for w in words1[1:]:

    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words1[0])
count = 1

for w in words1[1:]:
    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words2[0])
count = 1
sentence2_meaning = word2vec(words2[0])
count = 1
for w in words2[1:]:
    sentence2_meaning = np.add(sentence2_meaning, word2vec(w))
    count += 1
sentence2_meaning /= count

#Similarity is the cosine between the vectors
similarity = np.dot(sentence1_meaning, sentence2_meaning)/(np.linalg.norm(sentence1_meaning)*np.linalg.norm(sentence2_meaning))
</code></pre>
<p>However, this should work for two sentences not in a pandas dataframe.</p>
<p>Can you please tell me what I need to do for applying word2vec in case of a pandas dataframe to check similarity between sent1 and sent2? I would like a new column for the results.</p>
","python, pandas, nlp, word2vec","<p>I don't have <code>word2vec</code> trained and available, so I will show how to do what you want with a bogus <code>word2vec</code>, with words converted to sentences by <code>tfidf</code> weights.</p>
<p><strong>Step 1</strong>. Prepare data</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
df = pd.DataFrame({&quot;sentences&quot;: [&quot;this is a sentence&quot;, &quot;this is another sentence&quot;]})

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df.sentences).todense()
vocab = tfidf.vocabulary_
vocab
{'this': 3, 'is': 1, 'sentence': 2, 'another': 0}
</code></pre>
<p><strong>Step 2</strong>. Have bogus <code>word2vec</code> (of the size of our vocab)</p>
<pre><code>word2vec = np.random.randn(len(vocab),300)
</code></pre>
<p><strong>Step 3.</strong> Calculate a column containing word2vec for sentences:</p>
<pre><code>sent2vec_matrix = np.dot(tfidf_matrix, word2vec) # word2vec here contains vectors in the same order as in vocab
df[&quot;sent2vec&quot;] = sent2vec_matrix.tolist()
df

sentences   sent2vec
0   this is a sentence  [-2.098592110459085, 1.4292324332403232, -1.10...
1   this is another sentence    [-1.7879436822159966, 1.680865619703155, -2.00...
</code></pre>
<p><strong>Step 4.</strong> Calcualte similarity matrix</p>
<pre><code>from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(df[&quot;sent2vec&quot;].tolist())
similarity
array([[1.        , 0.76557098],
       [0.76557098, 1.        ]])
</code></pre>
<p>For your <code>word2vec</code> to work you will need slightly adjust Step 2, so that <code>word2vec</code> contains all the words in <code>vocab</code> in the same order (as specified by value, or alphabetically).</p>
<p>For your case it should be:</p>
<pre><code>sorted_vocab = sorted([word for word,key in vocab.items()])
sorted_word2vec = []
for word in sorted_vocab:
    sorted_word2vec.append(word2vec[word])
</code></pre>
",-1,1,8200,2020-10-11 10:53:41,https://stackoverflow.com/questions/64303203/word2vec-in-pandas-dataframe
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte while reading a text file,"<p>I am training a word2vec model, using about 700 text files as my corpus. But, when I start reading the files after the preprocessing step, I get the mentioned error. The code is as follows</p>
<pre><code>class MyCorpus(object):
    def __iter__(self):
        for i in ceo_path:                              /// ceo_path contains abs path of all text files
            file = open(i, 'r', encoding='utf-8')
            text = file.read()

            ###########                                        
            ###########                                 /// text preprocessing steps
            ###########
            
            yield final_text                            /// returns preprocessed text


sentences = MyCorpus()
logging.basicConfig(format=&quot;%(levelname)s - %(asctime)s: %(message)s&quot;, datefmt= '%H:%M:%S', level=logging.INFO)

# training the model
cores = multiprocessing.cpu_count()
w2v_model = Word2Vec(min_count=5,
                     iter=30,
                     window=3,
                     size=200,
                     sample=6e-5,
                     alpha=0.025,
                     min_alpha=0.0001,
                     negative=20,
                     workers=cores-1,
                     sg=1)
w2v_model.build_vocab(sentences)
w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
w2v_model.save('ceo1.model')
</code></pre>
<p>The error that I am getting is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 131, in &lt;module&gt;
    w2v_model.build_vocab(sentences)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 921, in build_vocab
    total_words, corpus_count = self.vocabulary.scan_vocab(
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1403, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1372, in _scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 65, in __iter__
    text = file.read()
  File &quot;C:\Users\name\AppData\Local\Programs\Python\Python38-32\lib\codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I am not able to understand the error as I am new to this. I was not getting the error in reading the text files when I wasn't using the <strong>iter</strong> function and sending the data in chunks as I am doing currently.</p>
","python-3.x, gensim, word2vec","<p>It looks like one of your files doesn't have proper <code>utf-8</code>-encoded text.</p>
<p>(Your <code>Word2Vec</code>-related code probably isn't necessary for hitting the error, at all. You could probably trigger the same error with just: <code>sentences_list = list(MyCorpus())</code>.)</p>
<p>To find which file, two different possibilities might be:</p>
<ol>
<li>Change your <code>MyCorpus</code> class so that it prints the path of each file before it tries to read it.</li>
<li>Add a Python <code>try: ... except UnicodeDecodeError: ...</code> statement around the <code>read</code>, and when the exception is caught, print the offending filename.</li>
</ol>
<p>Once you know the file involved, you may want to fix the file, or change the code to be able to handle the files you have.</p>
<p>Maybe they're not really in <code>utf-8</code> encoding, in which case you'd specify a different <code>encoding</code>.</p>
<p>Maybe just one or a few have problems, and it's be OK to just print their names for later investigation, and skip them. (You could use the exception-handling approach above to do that.)</p>
<p>Maybe, those that aren't <code>utf-8</code> are always in some other platform-specific encoding, so when <code>utf-8</code> fails, you could try a 2nd encoding.</p>
<p>Separately, when you solve the encoding issue, your iterable <code>MyCorpus</code> is not yet returning whet the <code>Word2Vec</code> class expects.</p>
<p>It doesn't want full text plain strings. It needs those texts to already be broken up into individual word-tokens.</p>
<p>(Often, simply performing a <code>.split()</code> on a string is close-enough-to-real-tokenization to try as a starting point, but usually, projects use some more-sophisticated punctuation-aware tokenization.)</p>
",0,1,1177,2020-10-26 15:48:02,https://stackoverflow.com/questions/64540488/unicodedecodeerror-utf-8-codec-cant-decode-byte-0x80-in-position-0-invalid
Text similarity using WMD within the same time period,"<p>I have a dataset</p>
<pre><code>       Title                                                Year
0   Sport, there will be a match between United and Tottenham ...   2020
1   Forecasting says that it will be cold next week                 2019
2   Sport, Mourinho is approaching the anniversary at Tottenham     2020
3   Sport, Tottenham are sixth favourites for the title behind Arsenal. 2020
4   Pochettino says clear-out of fringe players at Tottenham is inevitable.     2018
... ... ...
</code></pre>
<p>I would like to study the text similarity within the same year, rather in the whole dataset. To find most similar texts, I am using the WM distance similarity.
For two text would be:</p>
<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
distance = word2vec_model.wmdistance(&quot;string 1&quot;.split(), &quot;string 2&quot;.split())
</code></pre>
<p>However I would need to iterate the distance through sentences in the same year to get the similarity of each text with others, creating a list of similar text per row in the dataframe.
Could you please tell me how to iterate the wmdistance function across text published in the same year, in order to get for each text the most similar ones within the same period?</p>
","python, pandas, gensim, word2vec, similarity","<p>Generating a distance matrix for every group and then picking min value should work. This will get you a single nearest document index in a given year. You should be able to modify this code if you want n documents, or something else like that, quite easily.</p>
<pre><code>from scipy.spatial.distance import pdist, squareform

def nearest_doc(group):
    sq = squareform(pdist(group.to_numpy()[:,None], metric=lambda x, y:word2vec_model.wmdistance(x[0], y[0])))

    return group.index.to_numpy()[np.argmin(np.where(sq==0, np.inf, sq), axis=1)]

df['nearest_doc'] = df.groupby('Year')['Title'].transform(nearest_doc)
</code></pre>
<p>result:</p>
<pre><code>Title   Year    nearest_doc
0   Sport, there will be a match between United an...   2020    3
1   Forecasting says that it will be cold next week     2019    1
2   Sport, Mourinho is approaching the anniversary...   2020    3
3   Sport, Tottenham are sixth favourites for the ...   2020    2
4   Pochettino says clear-out of fringe players at...   2018    4
</code></pre>
",0,1,86,2020-11-01 02:25:08,https://stackoverflow.com/questions/64628163/text-similarity-using-wmd-within-the-same-time-period
How to get vectors for each document using Google News Word2Vec,"<p>I am trying out Google's word2vec pre-trained model to get word embeddings. I am able to load the model in my code and I can see that I get a 300-dimensional representation of a word. Here is the code -</p>
<pre><code>import gensim
from gensim import models
from gensim.models import Word2Vec
model = gensim.models.KeyedVectors.load_word2vec_format('/Downloads/GoogleNews-vectors-negative300.bin', binary=True)
dog = model['dog']
print(dog.shape)
</code></pre>
<p>which gives me below output -</p>
<pre><code>&gt;&gt;&gt; print(dog.shape)
(300,)
</code></pre>
<p>This works but I am interested in obtaining a vector representation for entire document and not just one word. How can I do it using word2vec model ?</p>
<pre><code>dog_sentence = model['it is a cute little dog']
KeyError: &quot;word 'it is a cute little dog' not in vocabulary&quot;
</code></pre>
<p>I plan to apply these on many documents and then train a clustering model on topic of it to do unsupervised learning and topic modeling.</p>
","python, word2vec, word-embedding","<p>That's a set of word-vectors. There's no single canonical way to turn word-vectors into vectors for longer runs of text, like sentences or documents.</p>
<p>You can try simply averaging the word-vectors for each word in the text. (To do this, you wouldn't pass the whole string text, but break it into words, look up each word-vector, then average all those vectors.)</p>
<p>That's quick and simple to calculate, and works OK as a baseline for some tasks, especially topical-analyses on very-short texts. But as it takes no account of grammar/word-order, and dilutes all words with all others, it's often outperformed by more sophisticated analyses.</p>
<p>Note also: that set of word-vectors was calculated by Google around 2013, from news articles. It will miss words and word-senses that have arisen since then, and its vectors will be flavored by the way news-articles are written - very different from other domains of language. If you have enough data, training your own word-vectors, on your own domain's texts, may outperform them in both word-coverage and vector-relevance.</p>
",0,1,2335,2020-11-02 18:01:14,https://stackoverflow.com/questions/64650845/how-to-get-vectors-for-each-document-using-google-news-word2vec
R package &#39;word2vec&#39; doc2vec function,"<p>I am a student (computer science). This is my first question in stackoverflow. I really would appreciate your help! (The package I am referring to is called 'word2vec', thats why the tags/title are a bit confusing to choose.)</p>
<p>In the description of the doc2vec function (here <a href=""https://cran.r-project.org/web/packages/word2vec/word2vec.pdf"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/word2vec/word2vec.pdf</a>) it says:</p>
<blockquote>
<p>Document vectors are the sum of the vectors of the words which are part of the document standardised by the scale of the vector space. This scale is the sqrt of the average inner product of the vector
elements.</p>
</blockquote>
<p>From what I understood, doc2vec takes one additional vector for every paragraph. Which, in my eyes, seems to be different than the above description.</p>
<p>Is my understanding of doc2vec correct, or close enough?
And: Does the cited implementation work like the doc2vec-algorithm?</p>
","r, word2vec, doc2vec","<p>Many people use &quot;Doc2Vec&quot; to refer to the word2vec-like algorithm introduced by a paper titled <a href=""https://arxiv.org/abs/1405.4053"" rel=""nofollow noreferrer"">Distributed Representation of Sentences and Documents</a> (by Le &amp; Mikolov). That paper calls the algorithm 'Paragraph Vector', without using the name 'Doc2Vec', and indeed introduces an extra  vector per document, like you describe. (That is, the doc-vector is trained a bit like a 'floating' pseudoword-vector, that contributes to to the input 'context' for every training prediction in that document.)</p>
<p>I'm not familiar with R or that R <code>word2vec</code> package, but from the docs you forwarded, it does <strong>not</strong> sound like that <code>doc2vec</code> function implements the 'Paragraph Vector' algorithm that others call 'Doc2Vec'. In particular:</p>
<ul>
<li><p>'Paragraph Vector' doc-vectors are <strong>not</strong> a simple sum-of-word-vectors</p>
</li>
<li><p>'Paragraph Vector' doc-vectors are created by a separate word2vec-like training process that co-creates any necessary word-vectors simultaneous with that training. Specifically: that process does <strong>not</strong> normally use as input some other pre-trained word-vectors, nor create word-vectors as a 1st step. (And further: the PV-DBOW option of the 'Paragraph Vector' paper doesn't create traditional word-vectors at all.)</p>
</li>
</ul>
<p>It appears that function is poorly-named, and if you need to use the actual 'Paragraph Vector' algorithm, you will need to look elsewhere.</p>
",0,0,679,2020-11-10 15:52:26,https://stackoverflow.com/questions/64772221/r-package-word2vec-doc2vec-function
Difference between Text Embedding and Word Embedding,"<p>I am working on a dataset of amazon alexa reviews and wish to cluster them in positive and negative clusters. I am using Word2Vec for vectorization so wanted to know the difference between <strong>Text Embedding</strong> and <strong>Word Embedding</strong>. Also, which one of them will be useful for my clustering of reviews (Please consider that I want to predict the cluster of any reviews that I enter.)
Thanks in advance!</p>
","python-3.x, nlp, k-means, gensim, word2vec","<p>Text Embeddings are typically a way to aggregate a number of Word Embeddings for a sentence or a paragraph of text. There are various ways this can be done. The easiest way is to average word embeddings but not necessarily yielding best results.</p>
<p>Application-wise:</p>
<ul>
<li><a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2vec</a> from <code>gensim</code></li>
<li><a href=""https://datascience.stackexchange.com/questions/17140/difference-between-paragraph2vec-and-doc2vec"">par2vec vs. doc2vec</a></li>
</ul>
",0,1,319,2020-11-19 20:19:47,https://stackoverflow.com/questions/64919359/difference-between-text-embedding-and-word-embedding
Force gensim&#39;s word2vec vectors to be positive?,"<p>Is there any way in gensim that i can force the learned vectors in word2vec to be all positive? (all the elements of vector be positive). i am working on a different task that needs these vectors to be positive ( the reason is really complicated so don't ask why )</p>
<p>so what is the easiest way for me to force gensim to learn positive vectors?</p>
","gensim, word2vec","<p>There is no built-in feature of Gensim that would allow this extra constraint/regularization to be applied during training.</p>
<p>You should probably try to explain your 'really complicated' reason for this idosyncratic request. There might be a better way to achieve the real end-goal, rather than shoehorning vectors that are typically bushy-and-balanced around the origin into a non-negative representation.</p>
<p>Notably, a paper called '<a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">All-but-the-Top: Simple and Effective Postprocessing for Word Representations</a>' has suggested word-vectors can be improved by postprocessing to ensure they are <em>more</em> balanced around the origin, rather than less (as seems a reliable side-effect of typical negative-sampling configurations).</p>
<p>If you're still interested to experiment in the opposite direction â€“ transforming usual word2vec word-vectors into a representation where all dimensions are positive â€“ I can think of a number of trivial, superficial ways to achieve that. I have no idea whether they'd actually preserve, or ruin, beneficial properties in the vectors â€“ but you could try them, and see. For example:</p>
<ul>
<li>You could try simply setting all negative dimensions to 0.0 - truncation. (Loses lots of info but might give a quick indication if a dirt-simple experiment gives you any of the benefits you seek.)</li>
<li>You could find the largest negative dimension that appears anywhere in any of the vectors, then add its absolute value to all other dimensions. Voila! No vector dimension is now lower than 0.0. (You could also try this in a per-dimension manner - only correct dimension #0 with the lowest dimension #0 value. Or, try other re-scalings of each dimension such that the previously-highly-negative values are 0.0, and the previous-highly-positive values stay where they are or only shift a little.)</li>
<li>You could try turning every dimension in the original word-vectors into two dimensions in a transformed set: one that's the original positive value, or 0.0 if it was negative, and a 2nd dimension that's the absolute value of the original negative value, or 0.0 if it was positive. (Or similarly: one dimension that's the absolute-value of the original value, and one dimension that's 0.0 or 1.0 depending on whether original value was negative or positive.)</li>
</ul>
<p>There are probably other more-sophisticated factorization/decompositions for re-representing the full set of word-vectors in a transformed array with only non-negative individual values, but I don't know them offhand, other than to think it might be worth searching for them.</p>
<p>And, whether any of these transformations work for your next steps, who knows? But it might be worth trying. (And if any of these offer surprisingly good results, it'd be great to hear in a followup comment!)</p>
",3,0,570,2020-11-22 14:28:03,https://stackoverflow.com/questions/64955331/force-gensims-word2vec-vectors-to-be-positive
Gensim most similar word to vector,"<p>I am using the pretrained word vectors from Wikipedia, <code>&quot;glove-wiki-gigaword-100&quot;</code>, in Gensim. As <a href=""https://github.com/kavgan/nlp-in-practice/blob/master/pre-trained-embeddings/Pre-trained%20embeddings.ipynb"" rel=""nofollow noreferrer"">this example documentation</a> shows, you can query the most similar words for a given word or set of words using</p>
<pre><code>model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)
</code></pre>
<p>However, I would like to query the most similar words to a <strong>given vector</strong>, specified as an array (of the same format as a word-vector from the pretrained model). For example, the result from adding or subtracting two word-vectors in the pretrained model, like</p>
<pre><code>vec = model_gigaword['king']-model_gigaword['man']
</code></pre>
<p>Output: (for <code>vec</code>)</p>
<pre><code>array([-0.696     , -1.26119   , -0.49109   ,  0.91179   ,  0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002],
      dtype=float32)
</code></pre>
<p>How do I get the most similar words to <code>vec</code>?</p>
","python, nlp, gensim, word2vec","<p>You can directly use this with <code>model_gigaword.wv.most_similar</code></p>
<pre><code>your_word_vector = np.array([-0.696, -1.26119, -0.49109, 0.91179, 0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002])

model_gigaword.wv.most_similar(positive=[your_word_vector], topn=10)
</code></pre>
<pre><code>[('vajiravudh', 0.7130449414253235),
 ('prajadhipok', 0.6764554381370544),
 ('andrianampoinimerina', 0.6474215984344482),
 ('jeongjo', 0.6449092626571655),
 ('taejong', 0.6352322697639465),
 ('rehoboam', 0.6319528818130493),
 ('injo', 0.6317901611328125),
 ('gojong', 0.6302404999732971),
 ('seonjo', 0.6272163391113281),
 ('elessar', 0.6250109672546387)]

</code></pre>
<p><strong>These results will be almost garbage, as expected. Read the reason below.</strong></p>
<hr />
<p>One important point though. I see you are trying to find the words that are similar to the difference vector in the euclidean space of the word vectors. The difference between <code>king</code> and <code>man</code> results in a vector that is similar to the difference between <code>queen</code> and <code>woman</code> means that the length and direction of the difference vector encode the contextual difference between the 2 respective pairs of words.</p>
<p><strong>The literal position of that vector maybe garbage because by checking it in the euclidean space, you will anchor it on the origin. Both the difference vectors (King-&gt;Man and Queen-&gt;Woman) above are anchored on 'King' and 'Queen' respectively.</strong></p>
<p>The intuition you should have is that A-&gt;B and C-&gt;D may have similar vectors connecting them even though A, B and C, D may line in completely separate parts of the euclidean space, IF they have a similar contextual difference between them. This is what the vector space in a properly trained word2vec is encoding.</p>
<p><a href=""https://i.sstatic.net/XgKwq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XgKwq.png"" alt=""enter image description here"" /></a></p>
",4,2,3522,2020-11-23 18:48:18,https://stackoverflow.com/questions/64974507/gensim-most-similar-word-to-vector
Importing a gensim doc2vec model in deeplearning4j,"<p>I have trained a <code>doc2vec</code> model with <code>gensim</code> and like to import it into <code>Deeplearning4j</code> in order to deploy that model.</p>
<p>For <code>word2vec</code> models, I know that this is possible by saving the model with</p>
<pre><code>model.wv.save_word2vec_format(&quot;word2vec.bin&quot;, binary=True)
</code></pre>
<p>and importing if in Java with</p>
<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(&quot;word2vec.bin&quot;);
</code></pre>
<p>Is there a similar way to import a <code>doc2vec</code> model?</p>
","java, gensim, word2vec, doc2vec, deeplearning4j","<p>The <code>save_word2vec_format()</code> method saves just the word-vectors, not the full model.</p>
<p>If you were to use Gensim's <code>.save()</code> to save the full model, it'd use Python's native serialization - so any Java code to read it would have to understand that format before rearranging relevant properties into the DL4J objects.</p>
<p>I don't see anything in the docs for <a href=""https://deeplearning4j.org/api/latest/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.html"" rel=""nofollow noreferrer"">DL4J's <code>ParagraphVectors</code> class docs</a> suggesting it can read Gensim-formatted models, so I doubt there's any built-in support.</p>
<p>It's theoretically possible that some Python code could be written to dump all the relevant subparts of the model in forms amenable to reading in Java, then patching into a Dl4J model, or for Java code to be written to understand the Python serialized objects â€“ but that'd require some familiarity with both the Gensim &amp; DL4J source code.</p>
<p>(If the <code>toJson()</code> &amp; <code>fromJson()</code> methods in DL4J work with full model representations â€“ which isn't clear from the docs, and would be an extremely bloated format for the bulk of the model state â€“ that'd likely make the model-translation a little easier, as it'd provide a straightforward template for what some new Python code would need to write-out.)</p>
",1,0,446,2020-11-26 09:39:11,https://stackoverflow.com/questions/65019412/importing-a-gensim-doc2vec-model-in-deeplearning4j
How do I get the word-embedding matrix from ft_word2vec (sparklyr-package)?,"<p>I have another question in the word2vec universe.
I am using the 'sparklyr'-package. Within this package I call the ft_word2vec() function. I have some trouble understanding the output:
For each number of sentences/paragraphs I am providing to the ft_word2vec() function, I always get the same amount of vectors. Even, if I have more sentences/paragraphs than words. For me, that looks like I get the paragraph-vectors. Maybe a Code-example helps to understand my problem?</p>
<pre><code># add your spark_connection here as 'spark_connection = '

# create example data frame
FK_data = data.frame(sentences = c(&quot;This is my first sentence&quot;,
  &quot;It is followed by the second sentence&quot;,
  &quot;At the end there is the last sentence&quot;))

# move the data to spark
sc_FK_data &lt;- copy_to(spark_connection, FK_data, name = &quot;FK_data&quot;, overwrite = TRUE)

# prepare data for ft_word2vec (sentences have to be tokenized [=list of words instead of one string in each row])
sc_FK_data &lt;- ft_tokenizer(sc_FK_data, input_col = &quot;icd_long&quot;, output_col = &quot;tokens&quot;)

# split data into test and trainings sets
partitions &lt;- sc_FK_data %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 123456) 
FK_train &lt;- partitions$training
FK_test &lt;- partitions$test

# given a trainings data set (FK_train) with a column &quot;tokens&quot; (for each row = a list of strings)
mymodel = ft_word2vec(
  FK_train,
  input_col = &quot;tokens&quot;,
  output_col = &quot;word2vec&quot;,
  vector_size = 15,
  min_count = 1,
  max_sentence_length = 4444,
  num_partitions = 1,
  step_size = 0.1,
  max_iter = 10,
  seed = 123456,
  uid = random_string(&quot;word2vec_&quot;))

# I tried to get the data from spark with:
myemb = mymodel %&gt;% sparklyr::collect()
</code></pre>
<p>Has somebody had similar experiences? Can someone explain what exactly the ft_word2vec() function returns? Do you have an example on how to get the word embedding vectors with this function? Or does the returned column indeed contain the paragraph vectors?</p>
","word2vec, sparklyr","<p>my colleague found a solution! If you know how to do it, the instructions really begin to make sense!</p>
<pre><code># add your spark_connection here as 'spark_connection = '

# create example data frame
FK_data = data.frame(sentences = c(&quot;This is my first sentence&quot;,
  &quot;It is followed by the second sentence&quot;,
  &quot;At the end there is the last sentence&quot;))

# move the data to spark
sc_FK_data &lt;- copy_to(spark_connection, FK_data, name = &quot;FK_data&quot;, overwrite = TRUE)

# prepare data for ft_word2vec (sentences have to be tokenized [=list of words instead of one string in each row])
sc_FK_data &lt;- ft_tokenizer(sc_FK_data, input_col = &quot;icd_long&quot;, output_col = &quot;tokens&quot;)

# split data into test and trainings sets
partitions &lt;- sc_FK_data %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 123456) 
FK_train &lt;- partitions$training
FK_test &lt;- partitions$test

# CHANGES FOLLOW HERE:
# We have to use the spark connection instead of the data. For me this was the confusing part, since i thought no data -&gt; no model.
# maybe we can think of this step as an initialization
mymodel = ft_word2vec(
  spark_connection,
  input_col = &quot;tokens&quot;,
  output_col = &quot;word2vec&quot;,
  vector_size = 15,
  min_count = 1,
  max_sentence_length = 4444,
  num_partitions = 1,
  step_size = 0.1,
  max_iter = 10,
  seed = 123456,
  uid = random_string(&quot;word2vec_&quot;))

# now that we have our model initialized, we add the word-embeddings to the model
w2v_model = ml_fit(w2v_model, sc_FK_EMB)

# now we can collect the embedding vectors
emb = word2vecmodel$vectors %&gt;% collect()

</code></pre>
",2,3,137,2020-11-27 15:42:50,https://stackoverflow.com/questions/65040039/how-do-i-get-the-word-embedding-matrix-from-ft-word2vec-sparklyr-package
word2vec is not defined (Julia),"<p>I'm getting an error that prevents me from using the <code>word2vec</code> function in Julia on a corpus.</p>
<p><code>Install.pkg(&quot;Word2Vec&quot;)</code></p>
<p>Code:</p>
<pre><code>using Word2Vec
word2vec(&quot;text8&quot;,&quot;vec.txt&quot;,verbose=true)
</code></pre>
<p>Error Message:</p>
<pre><code>ERROR: UndefVarError: word2vec not defined
Stacktrace:
 [1] word2vec(::String, ::String; size::Int64, window::Int64, sample::Float64, hs::Int64, negative::Int64, threads::Int64, iter::Int64, min_count::Int64, alpha::Float64, debug::Int644, binary::Int64, cbow::Int64, save_vocab::Nothing, read_vocab::Nothing, verbose::Bool) at C:\Users\15714\.julia\packages\Word2Vec\knfyL\src\interface.jl:73
 [2] top-level scope at none:1
</code></pre>
<p>Is anyone else having this problem?</p>
","nlp, julia, word2vec","<p>The function signature is correct and works for me on macOS with Julia 1.5.2. I am guessing that perhaps you are a Windows user and it looks like Windows is not supported. See here for details: <a href=""https://github.com/JuliaText/Word2Vec.jl#installation"" rel=""nofollow noreferrer"">https://github.com/JuliaText/Word2Vec.jl#installation</a></p>
",1,2,396,2020-11-27 18:02:57,https://stackoverflow.com/questions/65041885/word2vec-is-not-defined-julia
"gensim most_similar with positive and negative, how does it work?","<p>I was reading <a href=""https://stackoverflow.com/a/54581599/7339624"">this answer</a> That says about Gensim <code>most_similar</code>:</p>
<blockquote>
<p>it performs vector arithmetic: adding the positive vectors,
subtracting the negative, then from that resulting position, listing
the known-vectors closest to that angle.</p>
</blockquote>
<p>But when I tested it, that is not the case. I trained a Word2Vec with Gensim <code>&quot;text8&quot;</code> dataset and tested these two:</p>
<pre><code>model.most_similar(positive=['woman', 'king'], negative=['man'])

&gt;&gt;&gt; [('queen', 0.7131118178367615), ('prince', 0.6359186768531799),...]
</code></pre>
<hr />
<pre><code>model.wv.most_similar([model[&quot;king&quot;] + model[&quot;woman&quot;] - model[&quot;man&quot;]])

&gt;&gt;&gt; [('king', 0.84305739402771), ('queen', 0.7326322793960571),...]
</code></pre>
<p>They are clearly not the same. even the queen score in the first is <code>0.713</code> and on the second <code>0.732</code> which are not the same.</p>
<p><strong>So</strong> I ask the question again, How does Gensim <code>most_similar</code> work? why the result of the two above are different?</p>
","python, nlp, gensim, word2vec","<p>The adding and subtracting isn't <em>all</em> that it does; for an exact description, you should look at the source code:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:%7E:text=def%20most_similar,self%2C"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:~:text=def%20most_similar,self%2C</a></p>
<p>You'll see there that the addition and subtraction is on the <em>unit-normed</em> version of each vector, via the <code>get_vector(key, use_norm=True)</code> accessor.</p>
<p>If you change your use of <code>model[key]</code> to <code>model.get_vector(key, use_norm=True)</code>, you should see your outside-the-method calculation of the target vector give the same results as letting the method combine the <code>positive</code> and <code>negative</code> vectors.</p>
",6,4,3434,2020-11-29 12:16:18,https://stackoverflow.com/questions/65059959/gensim-most-similar-with-positive-and-negative-how-does-it-work
loop over pandas column for wmd similarity,"<p>I have two dataframe. both have two columns. I want to use wmd to find closest match for each entity in column <code>source_label</code> to entities in column <code>target_label</code> However, at the end I would like to have a DataFrame with all the 4 columns with respect to the entities.</p>
<h3>df1</h3>
<pre><code>,source_Label,source_uri
'neuronal ceroid lipofuscinosis 8',&quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;
'autosomal dominant distal hereditary motor neuronopathy',&quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;
</code></pre>
<h3>df2</h3>
<pre><code>,target_label,target_uri
'neuronal ceroid ',&quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;
'autosomal dominanthereditary',&quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;
</code></pre>
<h3>Expected result</h3>
<pre><code>,source_label, target_label, source_uri, target_uri, wmd score
'neuronal ceroid lipofuscinosis 8', 'neuronal ceroid ', &quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;, 0.98
'autosomal dominant distal hereditary motor neuronopathy', 'autosomal dominanthereditary', &quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;, 0.65
</code></pre>
<p>The dataframe is so big that I am looking for some faster way to iterate over both label columns. So far I tried this:</p>
<pre><code>list_distances = []
temp = []

def preprocess(sentence):
    return [w for w in sentence.lower().split()]

entity = df1['source_label']
target = df2['target_label']

 for i in tqdm(entity):
    for j in target:
        wmd_distance = model.wmdistance(preprocess(i), preprocess(j))
        temp.append(wmd_distance)
    list_distances.append(min(temp))
# print(&quot;list_distances&quot;, list_distances)
WMD_Dataframe = pd.DataFrame({'source_label': pd.Series(entity),
                              'target_label': pd.Series(target),
                              'source_uri': df1['source_uri'],
                              'target_uri': df2['target_uri'],
                              'wmd_Score': pd.Series(list_distances)}).sort_values(by=['wmd_Score'])
WMD_Dataframe = WMD_Dataframe.reset_index()

</code></pre>
<p>First of all this code is not working well as the other two columns are coming directly from the dfs' and do not take entities relation with the uri into consideration.
How one can make it faster as the entities are in millions. Thanks in advance.</p>
","python, pandas, numpy, gensim, word2vec","<p>A quick fix :</p>
<pre><code>closest_neighbour_index_df2 = []


def preprocess(sentence):
    return [w for w in sentence.lower().split()]



 
for i in tqdm(entity):
    temp = []
    for j in target:
        wmd_distance = model.wmdistance(preprocess(i), preprocess(j))
        temp.append(wmd_distance)
    # maybe assert to make sure its always right
    closest_neighbour_index_df2.append(np.argmin(np.array(temp))) 
    # return argmin to return index rather than the value. 
    
# Add the indices from df2 to df1

df1['closest_neighbour'] = closest_neighbour_index_df2 
# add information to respective row from df2 using the closest_neighbour column
</code></pre>
",1,0,111,2020-11-30 08:57:09,https://stackoverflow.com/questions/65070534/loop-over-pandas-column-for-wmd-similarity
Is there an advantage in using a word2vec model as a feature extractor for text clustering?,"<p>I am doing text classification using scikit-learn following <a href=""https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html"" rel=""nofollow noreferrer"">the example in the documentation</a>.</p>
<p>In order to extract features, that is, to convert the text in a set of vectors, the example uses a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer"" rel=""nofollow noreferrer"">HashingVectorizer</a> and a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"" rel=""nofollow noreferrer"">TfidfVectorizer</a> vectorizer.</p>
<p>I am doing a stemmatization before the vectorizer in order to handle different stems of the same word. That is, I would like &quot;running&quot; and &quot;run&quot; to be mapped to the same vectors.</p>
<p>I wonder if there is an advantage in using as a vectorizer a <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">word2vec model</a> instead. I thought that this would allow me to handle synonyms, that is, to map different words that have the same meaning to vectors very near between each other in the vector space.</p>
<p>Is my reasoning correct, or the following <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"" rel=""nofollow noreferrer"">KMeans</a> alorithm for clusterization will handle synonyms for me?</p>
","cluster-analysis, word2vec, feature-extraction, tf-idf, tfidfvectorizer","<p>Yes, word2vec-based-features sometimes offer an advantage.</p>
<p>But whether &amp; how it can help will depend on your exact data/goals, and the baseline results you've achieved before trying word2vec-enhanced approaches. And those aren't described or shown in your question.</p>
<p>The <a href=""https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html"" rel=""nofollow noreferrer""><code>scikit-learn</code> example you report as your model</a> doesn't integrate any word2vec features. What happens if you add such features? (As one very clumsy but simple example, what if you either replace, or concatenate into, the <code>HashingVectorizer</code> features a vector that's the average of all a text's word-vectors.)</p>
<p>Do the results improve, by either some quantitative score or a rough eyeballed review?</p>
",1,0,235,2020-11-30 16:15:08,https://stackoverflow.com/questions/65077158/is-there-an-advantage-in-using-a-word2vec-model-as-a-feature-extractor-for-text
Using Word2Vec in scikit-learn pipeline,"<p>I am trying to run the w2v on this sample of data</p>
<pre><code>Statement              Label
Says the Annies List political group supports third-trimester abortions on demand.       FALSE
When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.         TRUE
&quot;Hillary Clinton agrees with John McCain &quot;&quot;by voting to give George Bush the benefit of the doubt on Iran.&quot;&quot;&quot;     TRUE
Health care reform legislation is likely to mandate free sex change surgeries.    FALSE
The economic turnaround started at the end of my term.     TRUE
The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.    TRUE
Jim Dunnam has not lived in the district he represents for years now.    FALSE
</code></pre>
<p>using the code provided in this GitHub folder (FeatureSelection.py):</p>
<p><a href=""https://github.com/nishitpatel01/Fake_News_Detection"" rel=""nofollow noreferrer"">https://github.com/nishitpatel01/Fake_News_Detection</a></p>
<p>I would like to include word2vec features in my Naive Bayes model.
First I considered X and y and used train_test_split:</p>
<pre><code>X = df['Statement']
y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

dataset = pd.concat([X_train, y_train], axis=1)
</code></pre>
<p>This is the code I am currently using:</p>
<pre><code>#Using Word2Vec 
with open(&quot;glove.6B.50d.txt&quot;, &quot;rb&quot;) as lines:
    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))
           for line in lines}

training_sentences = DataPrep.train_news['Statement']

model = gensim.models.Word2Vec(training_sentences, size=100) # x be tokenized text
w2v = dict(zip(model.wv.index2word, model.wv.syn0))


class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X, y): # what are X and y?
        return self

    def transform(self, X): # should it be training_sentences?
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])


&quot;&quot;&quot;
class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())
    def fit(self, X, y):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of 
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])
        return self
    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])
&quot;&quot;&quot;
</code></pre>
<p>and in classifier.py, I am running</p>
<pre><code>nb_pipeline = Pipeline([
        ('NBCV',FeaturesSelection.w2v),
        ('nb_clf',MultinomialNB())])
</code></pre>
<p>However this is not working and I am getting this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-07045943a69c&gt; in &lt;module&gt;
      2 nb_pipeline = Pipeline([
      3         ('NBCV',FeaturesSelection.w2v),
----&gt; 4         ('nb_clf',MultinomialNB())])

/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     71                           FutureWarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 73         return f(**kwargs)
     74     return inner_f
     75 

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in __init__(self, steps, memory, verbose)
    112         self.memory = memory
    113         self.verbose = verbose
--&gt; 114         self._validate_steps()
    115 
    116     def get_params(self, deep=True):

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in _validate_steps(self)
    160                                 &quot;transformers and implement fit and transform &quot;
    161                                 &quot;or be the string 'passthrough' &quot;
--&gt; 162                                 &quot;'%s' (type %s) doesn't&quot; % (t, type(t)))
    163 
    164         # We allow last estimator to be None as an identity transformation

TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '{' ': array([-0.17019527,  0.32363772, -0.0770281 , -0.0278154 , -0.05182227, ....
</code></pre>
<p>I am using all the programs in that folder, so the code can be reproducible if you use them.</p>
<p>If you could explain me how to fix it and what other changes in the code would be necessary, it would be great. My goal is to compare models (naive bayes, random forest,...) with BoW, TF-IDF and Word2Vec.</p>
<p>Update:</p>
<p>After the answer below (from Ismail), I updated the code as follows:</p>
<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec, size=100):
        self.word2vec = word2vec
        self.dim = size
</code></pre>
<p>and</p>
<pre><code>#building Linear SVM classfier
svm_pipeline = Pipeline([
        ('svmCV',FeaturesSelection_W2V.MeanEmbeddingVectorizer(FeaturesSelection_W2V.w2v)),
        ('svm_clf',svm.LinearSVC())
        ])

svm_pipeline.fit(DataPrep.train_news['Statement'], DataPrep.train_news['Label'])
predicted_svm = svm_pipeline.predict(DataPrep.test_news['Statement'])
np.mean(predicted_svm == DataPrep.test_news['Label'])
</code></pre>
<p>However, I am still getting errors.</p>
","python, scikit-learn, gensim, word2vec","<p>Step 1. MultinomialNB <code>FeaturesSelection.w2v</code> is a <code>dict</code> and it does not have <code>fit</code> or <code>fit_transform</code> functions. Also <code>MultinomialNB</code> needs non-negative values, so it doesn't work. So I decided to add a pre-processing stage to normalize negative values.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import MinMaxScaler

nb_pipeline = Pipeline([
        ('NBCV',MeanEmbeddingVectorizer(FeatureSelection.w2v)),
        ('nb_norm', MinMaxScaler()),
        ('nb_clf',MultinomialNB())
    ])
</code></pre>
<p>... instead of</p>
<pre class=""lang-py prettyprint-override""><code>nb_pipeline = Pipeline([
        ('NBCV',FeatureSelection.w2v),
        ('nb_clf',MultinomialNB())
    ])
</code></pre>
<p>Step 2. I have got an error on <code>word2vec.itervalues().next()</code>. So I have decided to change dimension shape with predefined that is the same value of <code>Word2Vec</code>'s size.</p>
<pre class=""lang-py prettyprint-override""><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec, size=100):
        self.word2vec = word2vec
        self.dim = size
</code></pre>
<p>... instead of</p>
<pre class=""lang-py prettyprint-override""><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(word2vec.itervalues().next())
</code></pre>
",1,3,5732,2020-12-06 01:45:44,https://stackoverflow.com/questions/65163881/using-word2vec-in-scikit-learn-pipeline
How can I Convert a dataset to glove or word2vec format?,"<p>I have my twitter archive downloaded and wanted to run word2vec to experiment most similar words, analogies etc on it.</p>
<p>But I am stuck at first step - how to convert a given dataset / csv / document so that it can be input to word2vec? i.e. what is the process to convert data to glove/word2vec format?</p>
","python, nlp, stanford-nlp, word2vec","<p>Typically implementations of the word2vec &amp; GLoVe algorithms do one or both of:</p>
<ul>
<li><p>accept a plain text file, where tokens are delimited by (one or more) spaces, and text is considered each newline-delimited line at a time (with lines that aren't &quot;too long&quot; - usually, short-article or paragraph or sentence per line)</p>
</li>
<li><p>have some language/library-specific interface for feeding texts (lists-of-tokens) to the algorithm as a stream/iterable</p>
</li>
</ul>
<p>The Python Gensim library offers both options for its <code>Word2Vec</code> class.</p>
<p>You should generally try working through one or more tutorials to get a working overview of the steps involved, from raw data to interesting results, before applying such libraries to your own data. And, by examining the formats used by those tutorials â€“ and the extra steps they perform to massage the data into the formats needed by exactly the libraries you're using â€“ you'll also see ideas for how your data needs to be prepared.</p>
",1,0,176,2020-12-11 16:24:54,https://stackoverflow.com/questions/65255029/how-can-i-convert-a-dataset-to-glove-or-word2vec-format
How to get a dump of all vectors from a gensim W2V model?,"<p>Using a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVectors</a> object, I can get the W2V vector, given a word, like so.</p>
<pre><code>from gensim.models import KeyedVectors

model = KeyedVectors.load('vectors.kv')
model.get_vector('example')  # output =&gt; [0.12, 0.41, ..., 0.92]
</code></pre>
<p>How can I do the same, for <em>every</em> term (key) contained in the model?</p>
<p><em>Note that this doesn't <em>have</em> to be a KeyedVectors object, it could alternatively be a <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> object.</em></p>
<p><strong>EDIT</strong> - thanks to gojomo:</p>
<pre><code>vector_dct = {}
for word in kv_model.index2word: 
    vector_dct[word] = kv_model.get_vector(word)

df = pd.DataFrame(vector_dct).T
</code></pre>
","python, vector, gensim, word2vec","<pre class=""lang-py prettyprint-override""><code>for word in kv_model.index_to_key:  # was kv_model.index2word pre-gensim-4.0.0, when Q 1st asked
    kv_model.get_vector(word)
</code></pre>
",2,1,1475,2020-12-12 14:56:30,https://stackoverflow.com/questions/65266342/how-to-get-a-dump-of-all-vectors-from-a-gensim-w2v-model
How to load and use word2vec model properly in a web-application via Flask RESTful APIs?,"<p>I built a small code to find analogies using word2vec and it runs fine as stand alone application. Here is the working code</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np

# Get the interactive Tools for Matplotlib
%matplotlib notebook


from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os
glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
glove2word2vec(glove_file, word2vec_glove_file)
model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
def analogy(x1, x2, y1):
    result = model.most_similar(positive=[y1, x2], negative=[x1])
    return result[0][0]
analogy('woman', 'queen', 'man')    </code></pre>
</div>
</div>
</p>
<p>Now, I plan to use flask to create a small web application, so that users can find analogies via the webpage. For this I have a basic question</p>
<ol>
<li>I assume I need to save the model and then load it when I start the server. Please correct me  I am I am wrong.</li>
</ol>
<p>Here is the code that using Flask, it is working, but can you please suggest if saving model is required here?
2. Any suggestions to improve this code are welcome!</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np



from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os 

from flask import Flask, request


app = Flask(__name__)
@app.route(""/"", methods=['GET'])
def welcome():
    return ""Welcome to our Machine Learning REST API!""
@app.route(""/analogy"", methods=['GET'])
def analogy_route():
    word1 = request.args.get(""word1"")
    word2 = request.args.get(""word2"")
    word3 = request.args.get(""word3"")
    result = model.most_similar(positive=[word3, word2], negative=[word1])
    return str(result[0][0])
if __name__ == ""__main__"":
    glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
    word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
    glove2word2vec(glove_file, word2vec_glove_file)

    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
    app.run(host='0.0.0.0', port=5000, debug=True)</code></pre>
</div>
</div>
</p>
<p><a href=""https://i.sstatic.net/H5g4j.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H5g4j.png"" alt=""enter image description here"" /></a></p>
","flask, gensim, word2vec, flask-restful","<p>You probably don't want to be doing the GLoVe-to-word2vec format conversion, into a temporary file, every time you start your service. (It probably takes a noticeable amount of time, and may be filling a temp directory with redundant copies of the same data.)</p>
<p>Instead, perform the conversion only once, into a non-temporary location. Then, ignore the original <code>glove.6B.100d.txt</code> file entirely â€“ it's no longer needed. Instead, just ensure the converted file is available to your web service in a stable location.</p>
<p>Very roughly, that means:</p>
<ol>
<li>Run once, anywhere:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>glove2word2vec('glove.6B/glove.6B.100d.txt', `glove.6B.100d.word2vec.txt`)
</code></pre>
<p>(Note that neither the use of <code>absfile()</code> for <code>get_tmpfile()</code> are strictly necessary â€“ you can supply string paths directly to the <code>glove2word2vec()</code> function.)</p>
<ol start=""2"">
<li><p>Ensure that the new file <code>glove.6B.100d.word2vec.txt</code> is available in the working directory of your web service.</p>
</li>
<li><p>Have your web service's <code>__main__</code> branch just load the already-converted file, avoiding redundant repeated conversion work:</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    model = KeyedVectors.load_word2vec_format('glove.6B.100d.word2vec.txt')
    app.run(host='0.0.0.0', port=5000, debug=True)
</code></pre>
<p>(The exact path <code>'glove.6B.100d.word2vec.txt'</code> might be slightly different depending on where you choose to place the full file.)</p>
",2,0,607,2020-12-13 16:53:10,https://stackoverflow.com/questions/65278189/how-to-load-and-use-word2vec-model-properly-in-a-web-application-via-flask-restf
word2vec cosine similarity greater than 1 arabic text,"<p>I have trained my <code>word2vec</code> model from <code>gensim</code> and I am getting the nearest neighbors for some words in the corpus. Here are the similarity scores:</p>
<pre><code> top neighbors for Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„:
Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„: 1.0000001192092896
Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù„: 0.9541053175926208
Ø§Ù„Ø§Ù‡ØªÙ„Ø§Ù„: 0.872565507888794
Ø§Ù„Ø§Ø­Ø«Ù„Ø§Ù„: 0.8386293649673462
Ø§Ù„Ø§ÙƒØªÙ„Ø§Ù„: 0.8209128379821777
</code></pre>
<p>It is odd to get a similarity greater than 1. I cannot apply any stemming to my text because the text includes many OCR spelling mistakes (I got the text from ORC-ed documents). How can I fix the issue ?</p>
<p><strong>Note</strong> I am using <code>model.similarity(t1, t2)</code></p>
<p><strong>This is how I trained my Word2Vec Model:</strong></p>
<pre><code>    documents = list()
    tokenize = lambda x: gensim.utils.simple_preprocess(x)
    t1 = time.time()
    docs = read_files(TEXT_DIRS, nb_docs=5000)
    t2 = time.time()
    print('Reading docs took: {:.3f} mins'.format((t2 - t1) / 60))
    print('Number of documents: %i' % len(docs))

    # Training the model
    model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE, min_count=5)
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
    model.save(os.path.join(MODEL_DIR, 'word2vec'))

    weights = model.wv.vectors
    index_words = model.wv.index2word

    vocab_size = weights.shape[0]
    embedding_dim = weights.shape[1]

    print('Shape of weights:', weights.shape)
    print('Vocabulary size: %i' % vocab_size)
    print('Embedding size: %i' % embedding_dim)

</code></pre>
<p>Below is the read_files function I defined:</p>
<pre><code>def read_files(text_directories, nb_docs):
    &quot;&quot;&quot;
    Read in text files
    &quot;&quot;&quot;
    documents = list()
    tokenize = lambda x: gensim.utils.simple_preprocess(x)
    print('started reading ...')
    for path in text_directories:
        count = 0
        # Read in all files in directory
        if os.path.isdir(path):
            all_files = os.listdir(path)
            for filename in all_files:
                if filename.endswith('.txt') and filename[0].isdigit():
                    count += 1
                    with open('%s/%s' % (path, filename), encoding='utf-8') as f:
                        doc = f.read()
                        doc = clean_text_arabic_style(doc)
                        doc = clean_doc(doc)
                        documents.append(tokenize(doc))
                        if count % 100 == 0:
                            print('processed {} files so far from {}'.format(count, path))
                if count &gt;= nb_docs and count &lt;= nb_docs + 200:
                    print('REACHED END')
                    break
        if count &gt;= nb_docs and count &lt;= nb_docs:
            print('REACHED END')
            break

    return documents

</code></pre>
<p>I tried <a href=""https://stackoverflow.com/questions/41387000/cosine-similarity-of-word2vec-more-than-1"">this</a> thread but it won't help me because I rather have <code>arabic</code> and <strong>misspelled</strong> text</p>
<p><strong>Update</strong>
I tried the following: (getting the similarity between the exact same word)</p>
<pre><code>print(model.similarity('Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„','Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„'))
</code></pre>
<p>and it gave me the following result:</p>
<pre><code>1.0000001
</code></pre>
","nlp, arabic, word2vec, similarity","<p>Definitionally, the cosine-similarity measure should max at 1.0.</p>
<p>But in practice, floating-point number representations in computers have tiny imprecisions in the deep-decimals. And, especially when a number of calculations happen in a row (as with the calculation of this cosine-distance), those will sometimes lead to slight deviations from what the expected maximum or exactly-right answer &quot;should&quot; be.</p>
<p>(Similarly: sometimes calculations that, mathematically, should result in the exact same answer no matter how they are reordered/regrouped deviate slightly when done in different orders.)</p>
<p>But, as these representational errors are typically &quot;very small&quot;, they're usually not of practical concern. (They are especially small in the range of numbers around -1.0 to 1.0, but can become quite large when dealing with giant numbers.)</p>
<p>In your original case, the deviation is just <code>0.000000119209289</code>. In the word-to-itself case, the deviation is just <code>0.0000001</code>. That is, about one-ten-millionth off. (Your other sub-<code>1.0</code> values have similar tiny deviations from perfect calculation, but they aren't noticeable.)</p>
<p>In most cases, you should just ignore it.</p>
<p>If you find it distracting to you or your users in numerical displays/logging, simply choosing to display all such values to a limited number of after-the-decimal-point digits â€“ say 4 or even 5 or 6 â€“ will hide those noisy digits. For example, using a Python 3 format-string:</p>
<pre class=""lang-py prettyprint-override""><code>sim = model.similarity('Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„','Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„')
print(f&quot;{sim:.6}&quot;)
</code></pre>
<p>(Libraries like <code>numpy</code> that work with large arrays of such floats can even set a global default for display precision â€“ see <a href=""https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html"" rel=""nofollow noreferrer""><code>numpy.set_print_options</code></a> â€“ though that shouldn't affect the raw Python floats you're examining.)</p>
<p>If for some reason you absolutely need the values to be capped at <code>1.0</code>, you could add extra code to do that. But, it's usually a better idea to choose your tests &amp; printouts to be robust to, &amp; oblivious with regard to, such tiny deviations from perfect math.</p>
",4,0,930,2020-12-15 18:21:16,https://stackoverflow.com/questions/65311534/word2vec-cosine-similarity-greater-than-1-arabic-text
"Gensim error with .most_similar(), jupyter kernel restarting","<p>I cannot get the .most_similar() function to work. I have tried both Gensim 3.8.3 version and now am on the beta version 4.0 . I am working right off of the Word2Vec Model tutorial on each documentation version.</p>
<p>The code giving me error and restarting my kernel:</p>
<pre><code>print(wv.most_similar(positive=['car', 'minivan'], topn=5))
</code></pre>
<p>The above code is verbatim in both 3.8.3 documentation and 4.0. Following tutorials verbatim.</p>
<p>As stated in other stack overflow answers I have tried model.wv.most_similar()</p>
<p>I don't think .most_similar() is depreciated.</p>
<p>Additionally the .doesnt_match() function is not working.</p>
<p>EDIT in regards to gojomo:</p>
<p>Right now I am on Genism 3.8.3. I am using the GloVe Model and Word2Vec models, actually just tried it and it worked with the GloVe model, maybe the Word2Vec model is having a memory problem like gojomo suggested my code below:</p>
<p>I am using linx laptop, I-7 core 1065 cpu, memory 7.4 GiB, 64 bit ubuntu</p>
<pre><code>%matplotlib inline

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

for i, word in enumerate(wv.vocab):
    if i == 10:
        break
    print(word)

pairs = [
    ('programming', 'linux'),   
    ('programming', 'bicycle'), 
    ('programming', 'apple'),  
    ('programming', 'cereal'),    
    ('programming', 'capitalism'),
    ('programming', 'computers'), 
    ('programming', 'python'),  
    ('programming', 'algebra'),  
    ('programming', 'logic'),    
    ('programming', 'math'),
]
for w1, w2 in pairs:
    print('%r\t%r\t%.2f' % (w1, w2, wv.similarity(w1, w2)))

print(wv.most_similar(positive=['math'], topn=5))
</code></pre>
","python-3.x, machine-learning, nlp, gensim, word2vec","<p>If the Jupyter kernel is dying without a clear error message, you are likely running out of memory.</p>
<p>There may be more information logged to the console where you started the Jupyter server. If you expand you question to include any info there, as well as details about the model you've loaded (size on disk) and system you're running on (especially, RAM available), it may be possible to make other suggestions.</p>
<p>Also:</p>
<p>Whereas <code>gensim-3.8.3</code> requires a big new increment of RAM when the first <code>.most_similar()</code> call is made, the <code>gensim-4.0.0beta</code> pre-release only needs a much-smaller increment at that time - so it is far more likely that if a model succeeds in loading, you should also be able to get <code>.most_similar()</code> results. So it would also be useful to know:</p>
<ul>
<li>How did you install the <code>gensim-4.0.0beta</code>, and did you confirm that's the version actually used by your notebook kernel's environment?</li>
<li>Are you certain that the prior steps (such as loading) have succeeded, and that it's only &amp;  exactly the <code>most_similar()</code> that's triggering the failure? (Is it in a separate cell, and before attempting the <code>most_similar()</code> can you query other aspects of the model, such as its length or whether it contains certain words, successfully?)</li>
</ul>
",1,0,503,2020-12-17 02:21:44,https://stackoverflow.com/questions/65333831/gensim-error-with-most-similar-jupyter-kernel-restarting
Is the Word2Vec Spark implementation distributed?,"<p>I'm relatively new to Spark and having some difficulty understanding Spark ML.</p>
<p>The problem I have is that I have 3TB of text, which I want to train a Word2Vec model on. The server I'm running on has around 1TB of ram and so I can't save the file temporarily.</p>
<p>The file is saved as a parquet that I import into Spark. The question I have is does the Spark ML library distribute the Word2Vec training? If so is there anything I need to worried about while processing such a large text file? If not, is there anyway to stream this data while training Word2Vec?</p>
","apache-spark, pyspark, nlp, word2vec, apache-spark-mllib","<p>From this <a href=""https://github.com/apache/spark/pull/1719"" rel=""nofollow noreferrer"">https://github.com/apache/spark/pull/1719</a>  already in 2014 you can glean that parallel processing is possible - per partition.</p>
<blockquote>
<p>Quote:</p>
<p>To make our implementation more scalable, we train each partition
separately and merge the model of each partition after each iteration.
To make the model more accurate, multiple iterations may be needed.</p>
</blockquote>
<p>But you have to have partitoned data.</p>
",1,0,466,2020-12-17 13:21:03,https://stackoverflow.com/questions/65341552/is-the-word2vec-spark-implementation-distributed
ValueError: invalid vector on line 440902 | while loading wiki.ar.vec using gensim.models.keyedvectors.word2vec() function,"<p>I'm trying to load wiki.ar.vec arabic word embedding file using word2vec function from gensim.</p>
<p>Below is the code use to load embedding file.</p>
<pre><code>import gensim.models.keyedvectors as word2vec 
print( &quot;Word Embedding is loading&quot;)
embedding = word2vec.KeyedVectors.load_word2vec_format('/home/user/Documents/wiki.ar.vec', binary=False)
print( &quot;Word Embedding is loaded&quot;)
</code></pre>
<p>Facing the Error describe in below screenshot:</p>
<p><a href=""https://i.sstatic.net/wZbVP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wZbVP.jpg"" alt=""enter image description here"" /></a></p>
<p>or any other way to load wiki.ar.vec embedding file?</p>
<p>Any suggestion and answers are highly appriciated.</p>
","python, arabic, gensim, word2vec, word-embedding","<p>This error indicates the file is not in the proper format, at that specified line/vector.</p>
<p>Where did the file come from? Are you sure it's a binary-format file of the right format?</p>
<p>Have you tried re-downloading the file to ensure it hasn't been corrupted or truncated?</p>
",0,0,277,2020-12-22 09:52:23,https://stackoverflow.com/questions/65406526/valueerror-invalid-vector-on-line-440902-while-loading-wiki-ar-vec-using-gens
How is the window size affect word2vec and how do we choose window size according to different tasks?,"<p>For example, if I choose two window size, 5 and 50, and train the word2vec model, will the 50 one takes more time to train? Will the embeddings of the 50 one concentrates more on semantics of the text and the 5 one concentrates more on single word?
BTW, above two questions are just my thinking/exmaples of what I am seeking. My real question is just the title &quot;How is the window size affect word2vec and how do we choose window size according to different tasks?&quot;</p>
","nlp, word2vec","<p>A larger <code>window</code> will take longer to train.</p>
<p>A larger window will have a stronger effect on runtime in 'skip-gram' mode, where a larger window means more individual center-word predictions &amp; error-backpropagations. It'll have a milder effect on runtime in 'CBOW' mode, where it just means more averaging of input-vectors and fan-out of the final effects for each prediction/backpropagation.</p>
<p>For how it affects the character of the resulting word-vectors, there's some discussion &amp; a related research paper in a prior answer: <a href=""https://stackoverflow.com/questions/22272370/word2vec-effect-of-window-size-used/30447723#30447723"">Word2Vec: Effect of window size used</a></p>
<p>Generally, you'd optimize the <code>window</code> value the same as any other tunable parameter, by devising some repeatable way to score the final word-vectors on your real task (or a close/correlated simulation), then trying a range of values to see which scores best on your evaluation.</p>
",1,0,4584,2020-12-23 09:57:35,https://stackoverflow.com/questions/65422312/how-is-the-window-size-affect-word2vec-and-how-do-we-choose-window-size-accordin
Number of dimensions for each word vector in a given model &#39;en&#39; is 0,"<p>I want to print the number of dimensions for each word vector from the spacy language model 'en'. I installed both the language model and spacy. I have following lines of code</p>
<pre><code>import spacy
nlp =spacy.load('en')
dim = nlp.vocab.vectors_length
print(dim)
</code></pre>
<p>It gives 0.</p>
<p>Can someone help me with this?</p>
","nlp, spacy, word2vec","<p>The default <code>en</code> model doesn't include word vectors. You should use the <code>en_core_web_md</code> or <code>en_core_web_lg</code> models instead, which do.</p>
",6,3,567,2020-12-24 07:22:17,https://stackoverflow.com/questions/65435209/number-of-dimensions-for-each-word-vector-in-a-given-model-en-is-0
Which trained embeddings vectors from Gensim (word2vec model) should be used for Tensorflow? Unnormalised or normalised ones?,"<p>I want to use the Gensim (word2vec model) trained vectors inside a neural network (Tensorflow). There are two kinds of weights I can use for this purpose. The first group is <code>model.syn0</code> and the second group is <code>model.vectors_norm</code> (after calling <code>model.init_sims(replace=True)</code>). The second one is the group of vectors we use for calculating similarity. Which one has the correct order (match with <code>model.wv.index2word</code> and <code>model.wv.vocab[X].index</code>) and weights for the embedding layer of a neural network?</p>
","tensorflow, keras, gensim, word2vec, word-embedding","<p>If you are using Google's<code>GoogleNews-vectors</code> as pretrained model you can use <code>model.syn0</code>. If you are using Facebook's <code>fastText</code> word embeddings you can directly load the binary file.<br />
Below are the example to load both the instances.</p>
<p><strong>Load GoogleNews pretrained embedding:</strong></p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=500000) # To load the model first time.
model.wv.save_word2vec_format(model_path) #You can save the loaded model to binary file to load the model faster
model = gensim.models.KeyedVectors.load(model_path,mmap='r')
model.syn0norm = model.syn0
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p><strong>Load fastText pretrained embeddings:</strong></p>
<pre><code>import gensim
from gensim.models import FastText
model = FastText.load_fasttext_format('cc.en.300') # to load the model for first time.
model.save(&quot;fasttext_en_bin&quot;) # Save the model to binary file to load faster.
model = gensim.models.KeyedVectors.load(&quot;fasttext_en_bin&quot;,mmap=&quot;r&quot;)
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p>General example:</p>
<pre><code>if word in index2word:
   feature_vec = model[word]
</code></pre>
",2,1,966,2020-12-29 16:55:46,https://stackoverflow.com/questions/65495775/which-trained-embeddings-vectors-from-gensim-word2vec-model-should-be-used-for
"Tensorflow embeddings InvalidArgumentError: indices[18,16] = 11905 is not in [0, 11905) [[node sequential_1/embedding_1/embedding_lookup","<p>I am using TF 2.2.0 and trying to create a Word2Vec CNN text classification model. But however I tried there has been always an issue with the model or embedding layers. I could not found clear solutions in the internet so decided to ask it.</p>
<pre><code>import multiprocessing
modelW2V = gensim.models.Word2Vec(filtered_stopwords_list, size= 100, min_count = 5, window = 5, sg=0, iter = 10, workers= multiprocessing.cpu_count() - 1)
model_save_location = &quot;3000tweets_notbinary&quot;
modelW2V.wv.save_word2vec_format(model_save_location)

word2vec = {}
with open('3000tweets_notbinary', encoding='UTF-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:], dtype='float32')
        word2vec[word] = vec

num_words = len(list(tokenizer.word_index))

embedding_matrix = np.random.uniform(-1, 1, (num_words, 100))
for word, i in tokenizer.word_index.items():
    if i &lt; num_words:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
        else:
          embedding_matrix[i] = np.zeros((100,))
</code></pre>
<p>I have created my word2vec weights by the code above and then converted it to embedding_matrix as I followed on many tutorials. But since there are a lot of words seen by word2vec but not available in embeddings, if there is no embedding I assign 0 vector. And then fed data and this embedding to tf sequential model.</p>
<pre><code>seq_leng = max_tokens
vocab_size = num_words
embedding_dim = 100
filter_sizes = [3, 4, 5]
num_filters = 512
drop = 0.5
epochs = 5
batch_size = 32

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim= vocab_size,
                                                              output_dim= embedding_dim,
                                                              weights = [embedding_matrix],
                                                              input_length= max_tokens,
                                                              trainable= False),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(2),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(),
                                    tf.keras.layers.Dropout(drop),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(32, activation= &quot;relu&quot;, kernel_regularizer= tf.keras.regularizers.l2(1e-4)),
                                    tf.keras.layers.Dense(3, activation= &quot;softmax&quot;)
])

model.compile(loss= &quot;categorical_crossentropy&quot;, optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001, epsilon= 1e-06),
              metrics= [&quot;accuracy&quot;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

model.summary()

history = model.fit(x_train_pad, y_train2, batch_size= 60, epochs= epochs, shuffle= True, verbose= 1)
</code></pre>
<p>But when I run this code, tensorflow gives me the following error in any random time of the training process. But I could not find any solution to it. I have tried adding + 1 to vocab_size but when I do that I get size mismatch error which does not let me even compile my model. Can anyone please help me?</p>
<pre><code>InvalidArgumentError:  indices[18,16] = 11905 is not in [0, 11905)
     [[node sequential_1/embedding_1/embedding_lookup (defined at &lt;ipython-input-26-ef1b16cf85bf&gt;:1) ]] [Op:__inference_train_function_1533]

Errors may have originated from an input operation.
Input Source operations connected to node sequential_1/embedding_1/embedding_lookup:
 sequential_1/embedding_1/embedding_lookup/991 (defined at /usr/lib/python3.6/contextlib.py:81)

Function call stack:
train_function
</code></pre>
","tensorflow, nlp, word2vec, embedding, word-embedding","<p>I solved this solution. I was adding a new dimension to vocab_size by doing it vocab_size + 1 as suggested by others. However, since sizes of layer dimensions and embedding matrix don't match I got this issue in my hands. I added a zero vector at the end of my embedding matrix which solved the issue.</p>
",2,1,7359,2020-12-30 23:43:28,https://stackoverflow.com/questions/65514944/tensorflow-embeddings-invalidargumenterror-indices18-16-11905-is-not-in-0
Difference between &quot;alpha&quot; and &quot;start_alpha&quot;,"<p>I want to train a Word2Vec model using &quot;gensim&quot;. I want to determine the initial rating rate. However, it is written that both &quot;alpha&quot; and &quot;start_alpha&quot; parameters can be used to do so. What is the difference between them? Are they the same?</p>
","python, gensim, word2vec","<p>The parameter <code>alpha</code> is used in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the constructor</a>:</p>
<blockquote>
<p>classgensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None,
vector_size=100, alpha=0.025, window=5, min_count=5,
max_vocab_size=None, sample=0.001, seed=1, workers=3,
min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75,
cbow_mean=1, hashfxn=, epochs=5, null_word=0,
trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,
callbacks=(), comment=None, max_final_vocab=None)</p>
</blockquote>
<p>whereas the parameter <code>start_alpha</code> is used in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.train"" rel=""nofollow noreferrer"">train</a> method:</p>
<blockquote>
<p>train(corpus_iterable=None, corpus_file=None, total_examples=None,
total_words=None, epochs=None, start_alpha=None, end_alpha=None,
word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False,
callbacks=(), **kwargs)</p>
</blockquote>
<p>As for which to use:</p>
<blockquote>
<p><code>start_alpha</code> (float, optional) â€“ Initial learning rate. If supplied,
replaces the starting <code>alpha</code> from the constructor, for this one call
to<code>train()</code>. Use only if making multiple calls to train(), when you
want to manage the alpha learning-rate yourself (not recommended).</p>
</blockquote>
",0,0,172,2021-01-03 12:10:01,https://stackoverflow.com/questions/65549685/difference-between-alpha-and-start-alpha
Embedding multiword ngram phrases with PathLineSentences in gensim word2vec,"<p>I have around 82 gzipped files (around 180MB each and 14GB total) where each file contains new line separated sentences. I am thinking of using <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.PathLineSentences"" rel=""nofollow noreferrer"">PathLineSentences</a> from gensim Word2Vec to train word2vec model on the vocabularies. In that way <a href=""https://stackoverflow.com/questions/58925659/how-to-incrementally-train-a-word2vec-model-with-new-vocabularies"">I do not have to worry about taking all the sentences</a> list into the RAM.</p>
<p>Now I also wanted to get the embedding to include multiword phrases. But from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#embeddings-with-multiword-ngrams"" rel=""nofollow noreferrer"">documentation</a>, it seems that I need to have an already trained phrase detector an all the sentences I have e.g.</p>
<pre><code>from gensim.models import Phrases
# Train a bigram detector.
bigram_transformer = Phrases(all_sentences)
# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.
model = Word2Vec(bigram_transformer[all_sentences], min_count=1)
</code></pre>
<p>Now, I have two questions:</p>
<ol>
<li>Is there any way I can do the Phrase Detection while running the Word2Vec on top of each of the individual files in a streaming manner?</li>
<li>If not, is there any way I can do the initial phrase detection in the similar fashion of PathLineSentences, as in doing the phrase detection in a streaming manner?</li>
</ol>
","python, gensim, word2vec","<p>The Gensim <code>Phrases</code> class will accept data in the exact same form as <code>Word2Vec</code>: an iterable of all the tokenized texts.</p>
<p>You can provide that both as the initial training corpus, then as the corpus to be transformed into paired bigrams.</p>
<p>However, I would highly suggest that you <em>not</em> try to do the phrase-combinations in a simultaneous stream as feeding to <code>Word2Vec</code>, for both clarity and efficiency reasons.</p>
<p>Instead, do the transformation once, writing the results to a new, single corpus file. Then:</p>
<ul>
<li>you can easily review the results of the bigram-combinations</li>
<li>the pair-by-pair calculations that decide which words will be combined will be done only once, creating a simple corpus of space-delimited tokens. (Otherwise, each of the <code>epochs + 1</code> passes done by `Word2Vec will need to repeat the same calculations.)</li>
</ul>
<p>Roughly that'd look like:</p>
<pre class=""lang-py prettyprint-override""><code>with open('corpus.txt', 'w') as of:
    for phrased_sentence in bigram_transformer[all_sentences]:
        of.write(' '.join(phrased_sentence)
        of.write('\n')
</code></pre>
<p>(You could instead write to a gzipped file like <code>corpus.txt.gz</code> instead, using <code>GzipFile</code> or <code>smart_open</code>'s gzip functionality, if you'd like.)</p>
<p>Then the new file shows you exact data <code>Word2Vec</code> is operating on, and can be fed as a simple corpus - wrapped as an iterable with <code>LineSentence</code> or even passed using the <code>corpus_file</code> option that can better use more <code>workers</code> threads.</p>
",1,0,755,2021-01-05 04:14:57,https://stackoverflow.com/questions/65573173/embedding-multiword-ngram-phrases-with-pathlinesentences-in-gensim-word2vec
How do I subtract and add vectors with gensim KeyedVectors?,"<p>I need to <strong>add and subtract word vectors</strong>, for a project in which I use <strong><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim.models.KeyedVectors</a></strong> (from the <code>word2vec-google-news-300</code> model)</p>
<p>Unfortunately, I've tried but can't manage to do it correctly.</p>
<p>Let's look at the poular example <em>queen ~= king - man + woman</em>.<br />
When I want to subtract <em>man</em> from <em>king</em> and add <em>woman</em>,<br />
I can do this with gensim by</p>
<pre class=""lang-py prettyprint-override""><code># model is loaded using gensim.models.KeyedVectors.load()
model.wv.most_similar(positive=[&quot;king&quot;, &quot;woman&quot;], negative=[&quot;man&quot;])[0]
</code></pre>
<p>which, as expected, returns <code>('queen', 0.7118192911148071)</code> for the model I use.</p>
<p>Now, to achieve the same with adding and subtracting vectors (all of them are unit-normed), I've tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code> vec_king, vec_man, vec_woman = model.wv[&quot;king&quot;], model.wv[&quot;man&quot;], model.wv[&quot;woman&quot;]
 result = model.similar_by_vector(vec_king - vec_man + vec_woman)[0]
</code></pre>
<p><code>result</code> in the code above is <code>('king', 0.7992597222328186)</code> which is not what I'd expect.</p>
<p><em>What is my mistake?</em></p>
","python, nlp, gensim, word2vec, vector-space","<p>You're generally doing the right thing, but note:</p>
<ul>
<li><p>the <code>most_similar()</code> method also disqualifies from its results any of the named words provided - so even if <code>'king'</code> is (still) the closest word to the result, it will be ignored. Your formulation might very well have <code>'queen'</code> as the next-closest word, after ignoring the input words - which is all that the 'analogy' tests need.</p>
</li>
<li><p>the <code>most_similar()</code> method also does its vector-arithmetic on versions of the vectors that are <em>normalized to unit length</em>, which can result in slightly different answers. If you change your uses of <code>model.wv['king']</code> to <code>model.get_vector('king', norm=True)</code>, you'll get the unit-normed vectors instead.</p>
</li>
</ul>
<p>See also similar earlier answer: <a href=""https://stackoverflow.com/a/65065084/130288"">https://stackoverflow.com/a/65065084/130288</a></p>
",0,2,1496,2021-01-07 12:03:11,https://stackoverflow.com/questions/65612062/how-do-i-subtract-and-add-vectors-with-gensim-keyedvectors
Gensim word2vec training doesn&#39;t callback on batch end,"<p>I am interested in placing a callback on the Gensim word2vec model to trigger some function after each batch. Per <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">documentation</a>, it is possible to place a callback on batch end or epoch end. However, as shown in the MVE below, only the epoch callback actually triggers.</p>
<p>To run the sample, let <code>corpus_filepath</code> direct to a line separated file of unpunctuated sentences (words in a sentence on given a line should be space separated). You may also need to change <code>workers</code> in the <code>Word2Vec</code> instantiation.</p>
<pre><code>from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

corpus_filepath = 'train.txt'
out_filepath = 'out.txt'

class MyCallback(CallbackAny2Vec):
    def __init__(self):
        pass

    def on_batch_end(self, model):
        print('batch end')

    def on_epoch_end(self, model):
        print('epoch end')


callback = MyCallback()
model = Word2Vec(size=300, window=5, min_count=0, workers=64)
print('Making vocabulary...')
model.build_vocab(corpus_file=corpus_filepath)
print('Beginning training...')
model.train(corpus_file=corpus_filepath, epochs=5, total_words=model.corpus_total_words, callbacks=[callback])
</code></pre>
<p>Incorrect output (missing batch printouts):</p>
<pre><code>Making vocabulary...
Beginning training...
epoch end
epoch end
epoch end
epoch end
epoch end
</code></pre>
<p>What am I doing wrong?</p>
","python, machine-learning, gensim, word2vec","<p>Looking at the code, it appears the <code>on_batch_begin</code> and <code>on_batch_end</code> callbacks have not been implemented by Gensim in the <code>corpus_file</code> mode you're using.</p>
<p>Thus, you could try changing to the traditional corpus-iterable mode to see the callbacks fire. (Overall training throughput in that mode tends to max out with around 8-12 workers, no matter how many CPU cores are available.)</p>
<p>However, note also that even there, the per-batch callbacks are run at arbitrary times in multiple threads - so many things are unwise/unsafe to attempt in those callbacks. Attempted saves of the model, for example, could result in errors or other file corruption, and even purely informational output might be mixed from multiple threads or reflect inconsistent changing state. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2182"" rel=""nofollow noreferrer"">Gensim's open bug report #2181 for more details</a>. It's possible the <code>on_batch</code> callbacks are removed entirely due to this risk in an upcoming release.</p>
<p>So I'd recommend adapting your code to use some other approach â€“ perhaps the <code>on_epoch</code> callbacks? â€“ instead. What operation did you want to do in such frequent/simultaneous worker-thread callbacks?</p>
",1,0,751,2021-01-18 00:24:18,https://stackoverflow.com/questions/65767390/gensim-word2vec-training-doesnt-callback-on-batch-end
ValueError: need at least one array to concatenate in Top2Vec Error,"<p>docs = ['Consumer discretionary, healthcare and technology are preferred China equity  sectors.',
'Consumer discretionary remains attractive, supported by Chinaâ€™s policy to revitalize domestic consumption. Prospects of further monetary and fiscal stimulus  should reinforce the Chinese consumption theme.',
'The healthcare sector should be a key beneficiary of the coronavirus outbreak,  on the back of increased demand for healthcare services and drugs.',
'The technology sector should benefit from increased demand for cloud services  and hardware demand as China continues to recover from the coronavirus  outbreak.',
'China consumer discretionary sector is preferred. In our assessment, the sector  is likely to outperform the MSCI China Index in the coming 6-12 months.']</p>
<p>model = Top2Vec(docs, embedding_model = 'universal-sentence-encoder')</p>
<p>while running the above command, I'm getting an error that is not clearly visible for debugging what could be the root cause for the error?</p>
<p>Error:</p>
<h2>2021-01-19 05:17:08,541 - top2vec - INFO - Pre-processing documents for training
INFO:top2vec:Pre-processing documents for training
2021-01-19 05:17:08,562 - top2vec - INFO - Downloading universal-sentence-encoder model
INFO:top2vec:Downloading universal-sentence-encoder model
2021-01-19 05:17:13,250 - top2vec - INFO - Creating joint document/word embedding
INFO:top2vec:Creating joint document/word embedding
WARNING:tensorflow:5 out of the last 6 calls to &lt;function recreate_function..restored_function_body at 0x7f8c4ce57d90&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to <a href=""https://www.tensorflow.org/guide/function#controlling_retracing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#controlling_retracing</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/function</a> for  more details.
WARNING:tensorflow:5 out of the last 6 calls to &lt;function recreate_function..restored_function_body at 0x7f8c4ce57d90&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to <a href=""https://www.tensorflow.org/guide/function#controlling_retracing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#controlling_retracing</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/function</a> for  more details.
2021-01-19 05:17:13,548 - top2vec - INFO - Creating lower dimension embedding of documents
INFO:top2vec:Creating lower dimension embedding of documents
2021-01-19 05:17:15,809 - top2vec - INFO - Finding dense areas of documents
INFO:top2vec:Finding dense areas of documents
2021-01-19 05:17:15,823 - top2vec - INFO - Finding topics
INFO:top2vec:Finding topics</h2>
<p>ValueError                                Traceback (most recent call last)
 in ()
----&gt; 1 model = Top2Vec(docs, embedding_model = 'universal-sentence-encoder')</p>
<p>2 frames
&lt;<strong>array_function</strong> internals&gt; in vstack(*args, **kwargs)</p>
<p>/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py in vstack(tup)
281     if not isinstance(arrs, list):
282         arrs = [arrs]
--&gt; 283     return _nx.concatenate(arrs, 0)
284
285</p>
<p>&lt;<strong>array_function</strong> internals&gt; in concatenate(*args, **kwargs)</p>
<p>ValueError: need at least one array to concatenate</p>
","python, arrays, concatenation, word2vec, word-embedding","<p>You need to use more docs and unique words for it to find at least 2 topics. As an example, I just multiply your list by 10 and it works:</p>
<pre><code>from top2vec import Top2Vec

docs = ['Consumer discretionary, healthcare and technology are preferred China equity  sectors.',
'Consumer discretionary remains attractive, supported by Chinaâ€™s policy to revitalize domestic consumption. Prospects of further monetary and fiscal stimulus  should reinforce the Chinese consumption theme.',
'The healthcare sector should be a key beneficiary of the coronavirus outbreak,  on the back of increased demand for healthcare services and drugs.',
'The technology sector should benefit from increased demand for cloud services  and hardware demand as China continues to recover from the coronavirus  outbreak.',
'China consumer discretionary sector is preferred. In our assessment, the sector  is likely to outperform the MSCI China Index in the coming 6-12 months.']

docs = docs*10 
model = Top2Vec(docs, embedding_model='universal-sentence-encoder')
print(model)
</code></pre>
<blockquote>
<p>&lt;top2vec.Top2Vec.Top2Vec object at 0x13eef6210&gt;</p>
</blockquote>
<p>I had few (30) long docs of up to 130 000 characters, so I just split them into smaller docs every 5000 characters:</p>
<pre><code>
docs_split = []
for doc in docs:
    skip_n = 5000
    for i in range(0,130000,skip_n):
        docs_split.append(doc[i:i+skip_n])
</code></pre>
",3,0,3157,2021-01-19 05:30:23,https://stackoverflow.com/questions/65785949/valueerror-need-at-least-one-array-to-concatenate-in-top2vec-error
Did we update an existing gemsim model with our own data correctly?,"<p>Purpose: We are exploring the use of word2vec models in clustering our data. We are looking for the ideal model to fit our needs and have been playing with using (1) existing models offered via Spacy and Gensim (trained on internet data only), (2) creating our own custom models with Gensim (trained on our technical data only) and (3) now looking into creating hybrid models that add our technical data to existing models (trained on internet + our data).</p>
<p>Here is how we created our hybrid model of adding our data to an existing Gensim model:</p>
<pre><code>model = api.load(&quot;word2vec-google-news-300&quot;)
model = Word2Vec(size=300, min_count =1)
model.build_vocab(our_data)
model.train(our_data, total_examples=2, epochs =1)
model.wv.vocab
</code></pre>
<p>Question: Did we do this correctly in terms of our intentions of having a model that is trained on the internet and layered with our data?</p>
<p>Concerns: We are wondering if our data was really added to the model. When using the most similar function, we see really high correlations with more general words with this model. Our custom model has much lower correlations with more technical words. See output below.</p>
<pre><code>Most Similar results for 'Python'

This model (internet + our data):
'technicians' = .99
'system'      = .99
'working'     = .99

Custom model (just our data):
'scripting'   = .65
'perl'        = .63
'julia'       = .58
</code></pre>
","python, nlp, spacy, gensim, word2vec","<p>No: your code won't work for your intents.</p>
<p>When you execute the line...</p>
<pre><code>model = Word2Vec(size=300, min_count=1)
</code></pre>
<p>...you've created an all-new, empty <code>Word2Vec</code> object, assigning it into the <code>model</code> variable, which discards anything that's already there. So the prior-loaded data will have no effect. You're just training a new model on your (tiny) data.</p>
<p>Further, the object you had loaded isn't a full <code>Word2Vec</code> model. The 'GoogleNews' vectors that Google released back in 2013 are <em>only the vectors</em>, not a full model. There's no straightforward &amp; reliable way to keep training that object, as it is missing lots of information a real full model would have (including word-frequencies and the model's internal weights).</p>
<p>There are some advanced ways you could try to seed your own model with those values - but they involve lots of murky tradeoffs &amp; poorly-documented steps, in order for the end-results to have any value, compared to just training your own model on your own sufficient data. There's no officially-documented/supported way to do it in Gensim.</p>
",0,1,59,2021-01-19 17:55:53,https://stackoverflow.com/questions/65796905/did-we-update-an-existing-gemsim-model-with-our-own-data-correctly
Shared memory among processes for pre-trained word2vec model?,"<p>I have a look-up object, specifically a pre-trained word2vec model from <code>gensim.models.keyedvectors.Word2VecKeyedVectors</code>. I need to do some data pre-processing and I am using multi-processing for the same. Is there a way in which all of my processes can use the object from the same memory location instead of each process loading the object into its own memory?</p>
","python, multiprocessing, word2vec","<p>Yes, if:</p>
<ul>
<li>the files were saved using Gensim's internal <code>.save()</code> method, and the relevant large-arrays of vectors are clearly separate <code>.npy</code> files</li>
<li>the files are loaded using Gensim's internal <code>.load()</code> method, with the <code>mmap</code> option</li>
<li>you avoid doing any operations which inadvertently cause each process's object to reallocate the backing array completely (breaking the mmap-sharing).</li>
</ul>
<p>See <a href=""https://stackoverflow.com/a/43067907/130288"">this prior answer</a> for an overview of the steps/concerns of a similar need.</p>
<p>(The concern &amp; extra steps listed there to avoid breaking the mmap-sharing â€“ by performing manual patch-ups of the <code>norm</code> properties â€“ should no longer be necessary in Gensim 4.0.0, currently available only as a prerelease version.)</p>
",1,0,247,2021-01-26 11:30:54,https://stackoverflow.com/questions/65900526/shared-memory-among-processes-for-pre-trained-word2vec-model
Sort dictionary python by value (word2vec),"<p>I want to sort my <code>dict</code> by value, but if I apply this code it doesn't work (it print only my <code>key-value</code> pairs without any kind of sorting). If I change <code>key=lambda x: x[1] to x[0]</code> it correctly sort by <code>key</code>, so I don't understand what I'm doing wrong.</p>
<p>My code:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
from scipy.spatial.distance import cosine

e_science = Word2Vec.load(&quot;clean_corpus_science.model&quot;)
e_pokemon = Word2Vec.load(&quot;clean_corpus_pokemon.model&quot;)

science_vocab = list(e_science.wv.vocab)
pokemon_vocab = list(e_pokemon.wv.vocab)

vocab_intersection = list(set(science_vocab).intersection(set(pokemon_vocab)))

similarity = []
for i in range(0, len(vocab_intersection)):
  similarity.append(1-cosine(e_science[vocab_intersection[i]], e_pokemon[vocab_intersection[i]]))

hashmap = {}
for i in range(0, len(similarity)):
  hashmap[vocab_intersection[i]] = {similarity[i]} 

dict(sorted(hashmap.items(), key=lambda x: x[1]))
</code></pre>
","python, sorting, word2vec, cosine-similarity","<p>You're trying to sort sets, and Python isn't sure how to order them. Take your scores out of the sets, and then you can sort as expected.</p>
<pre><code>dict(sorted(hashmap.items(), key=lambda x: tuple(x[1])[0]))
</code></pre>
<p>That's pretty ugly though, you may want to do the cleanup in a separate step.</p>
",1,1,144,2021-01-27 19:14:22,https://stackoverflow.com/questions/65925841/sort-dictionary-python-by-value-word2vec
Does Gensim handling pad index and UNK index in W2V models?,"<p>I'm using Gensim for building W2V models and, I didn't find a way for adding a vector for Unkown words or padding parts in Gensim and, I have to do it manually.
I also check the index of 0 in the created embedding and, it is also used for a specific word. This matter could cause a problem for padding words because they have the same index.</p>
<p>Am I missing something in here? Is Gensim handle this problem?</p>
<p>P.S: For handling this issue, I always append two vectors in the model weights after I train the model.</p>
","python, gensim, word2vec","<p>A Gensim <code>Word2Vec</code> model only learns, and reports, vectors for words that it learned during training.</p>
<p>If you want it to learn some vector for any synthetic 'unknown' or 'padding' symbols, you need to include them in the training data. (They may not be very interesting/useful vector-values, though, and having such synthetic token vectors may not outperform simply ignoring unknown-tokens or avoiding artificial padding entirely.)</p>
",2,1,1264,2021-01-31 10:49:35,https://stackoverflow.com/questions/65978214/does-gensim-handling-pad-index-and-unk-index-in-w2v-models
How to load pre-trained glove model with gensim load_word2vec_format?,"<p>I am trying to load a pre-trained glove as a word2vec model in gensim. I have downloaded the glove file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a>. I am using the following script:</p>
<pre><code>from gensim import models
model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)
</code></pre>
<p>but get the following error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-e0b48b51f433&gt; in &lt;module&gt;()
      1 from gensim import models
----&gt; 2 model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>
<p>What is the underlying problem? Does gensim need a specific format to be able to load it?</p>
","stanford-nlp, gensim, word2vec, word-embedding","<p>The GLoVe format is slightly different â€“ missing a 1st-line declaration of vector-count &amp; dimensions â€“ than the format that <code>load_word2vec_format()</code> supports.</p>
<p>There's a <code>glove2word2vec</code> utility script included you can run once to convert the file:</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>Also, starting in Gensim 4.0.0 (currentlyu in prerelease testing), the <code>load_word2vec_format()</code> method gets a new optional <code>no_header</code> parameter:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</a></p>
<p>If set as <code>no_header=True</code>, the method will deduce the count/dimensions from a preliminary scan of the file - so it can read a GLoVe file with that option â€“ but at the cost of two full-file reads instead of one. (So, you may still want to re-save the object with <code>.save_word2vec_format()</code>, or use the <code>glove2word2vec</code> script, to make future loads faster.)</p>
",7,4,8931,2021-02-03 04:08:58,https://stackoverflow.com/questions/66021131/how-to-load-pre-trained-glove-model-with-gensim-load-word2vec-format
How to find similar sentence from a corpus on word2vec?,"<p>I have implemented word2vec on my corpus using the TensorFlow tutorial: <a href=""https://www.tensorflow.org/tutorials/text/word2vec#next_steps"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec#next_steps</a>
Now I'm want to give a sentence as input and want to find a similar sentence in the corpus.</p>
<p>Any leads on how I can perform this?</p>
","nlp, word2vec, sentence-similarity","<p>A simple word2vec model is not capable of such task, as it only relates word semantics to each other, not the semantics of whole sentences. Inherently, such a model has no generative function, it only serves as a look-up table.</p>
<p>Word2vec models map word strings to vectors in the embedding space. To find similar words for a given sample word, one can simply go through all vectors in the vocabulary and find the ones that are closest (in terms of the 2-norm) from the sample word vector. For further information you could go <a href=""https://wiki.pathmind.com/word2vec"" rel=""nofollow noreferrer"">here</a> or <a href=""https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, this does not work for sentences as it would require a whole vocabulary of sentences of which to pick similar ones - which is not feasible.</p>
<p>Edit: This seems to be a duplicate of <a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt?rq=1"">this</a> question.</p>
",2,1,716,2021-02-03 10:59:13,https://stackoverflow.com/questions/66026102/how-to-find-similar-sentence-from-a-corpus-on-word2vec
"ValueError: Dimensions must be equal, but are 2 and 1 in time2vec example","<p>I have 2 inputs and 4 outputs. I want to use the time2vec to predict the outputs. I have used the code in  <a href=""https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e</a>, it works for one input and one output. But when I want to use for (2 inputs and four outputs) it gives me the following error:</p>
<pre><code>import numpy as np
import tensorflow as tf
from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Embedding, Input, concatenate, 
Lambda
from sklearn.preprocessing import MinMaxScaler
from keras.callbacks import EarlyStopping
import keras
import random
import os
from sklearn.metrics import mean_absolute_error
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.keras.callbacks import * 
from tensorflow.keras.optimizers import *
from tensorflow.keras import backend as K
from kerashypetune import KerasGridSearch
import matplotlib.pyplot as plt
w               = 5
ts              = 10              
nt              = 10           
ntest           = nt + int(percent*nt) 
X_train = np.random.rand(90,5,2)
X_test = np.random.rand(5,5,2)
y_train = np.random.rand(90,4)
y_test = np.random.rand(5,4)
</code></pre>
<p>&quot;&quot;&quot; ### DEFINE T2V LAYER ###</p>
<pre><code>class T2V(Layer):
    def __init__(self, output_dim=None, **kwargs):
        self.output_dim = output_dim
        super(T2V, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.W = self.add_weight(name='W', shape=(1, self.output_dim), initializer='uniform', 
        trainable=True)
        self.P = self.add_weight(name='P',shape=(1, 
        self.output_dim),initializer='uniform',trainable=True)
        self.w = self.add_weight(name='w',shape=(1, 1),initializer='uniform', trainable=True)
        self.p = self.add_weight(name='p',shape=(1, 1),initializer='uniform',trainable=True)
        super(T2V, self).build(input_shape)
    
    def call(self, x):
        original = self.w * x + self.p
        sin_trans = K.sin(K.dot(x, self.W) + self.P)
        return K.concatenate([sin_trans, original], -1)
</code></pre>
<h3>CREATE GENERATOR FOR LSTM AND T2V</h3>
<pre><code>sequence_length = w
def gen_sequence(id_df, seq_length, seq_cols):
    data_matrix  = id_df[seq_cols].values
    num_elements = data_matrix.shape[0]
for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):
    yield data_matrix[start:stop, :]

def gen_labels(id_df, seq_length, label):
    data_matrix  = id_df[label].values
    num_elements = data_matrix.shape[0]
    return data_matrix[seq_length:num_elements, :]
</code></pre>
<h3>DEFINE MODEL STRUCTURES</h3>
<pre><code>def set_seed_TF2(seed):
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)

def T2V_NN(param, dim):
    inp = Input(shape=(dim,2))
    x = T2V(param['t2v_dim'])(inp)
    x = LSTM(param['unit'], activation=param['act'])(x)
    x = Dense(2)(x)
    m = Model(inp, x)
    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))
    return m

def NN(param, dim):
    inp = Input(shape=(dim,2))
    x = LSTM(param['unit'], activation=param['act'])(inp)
    x = Dense(2)(x)
    m = Model(inp, x)
    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))
    return m
</code></pre>
<h2>Param grid</h2>
<pre><code>param_grid = {'unit': [64,32],'t2v_dim': [128,64],'lr': [1e-2,1e-3], 'act': ['elu','relu'], 'epochs': 1,'batch_size': [512,1024]}
</code></pre>
<h3>FIT T2V + LSTM</h3>
<pre><code>es = EarlyStopping(patience=5, verbose=0, min_delta=0.001, monitor='val_loss', mode='auto', 
restore_best_weights=True)

hypermodel = lambda x: T2V_NN(param=x, dim=sequence_length)

kgs_t2v = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, 
tuner_verbose=1)
kgs_t2v.set_seed(set_seed_TF2, seed=33)
kgs_t2v.search(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)
</code></pre>
<p>But when I run the model, I've got this error :</p>
<pre><code>ValueError: Dimensions must be equal, but are 2 and 1 for '{{node t2v_2/MatMul}} = MatMul[T=DT_FLOAT, 
transpose_a=false, transpose_b=false](t2v_2/Reshape, t2v_2/Reshape_1)' with input shapes: [?,2], [1,128].
</code></pre>
<p>Could you help me to solve this?</p>
","tensorflow, keras, deep-learning, lstm, word2vec","<p>You have to change the parameters inside the T2V layer and inside your network in order to correctly match the shapes</p>
<pre><code>class T2V(Layer):
    
    def __init__(self, output_dim=None, **kwargs):
        self.output_dim = output_dim
        super(T2V, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.output_dim), 
                                 initializer='uniform', trainable=True)
        self.P = self.add_weight(name='P', shape=(input_shape[1], self.output_dim), 
                                 initializer='uniform', trainable=True)
        self.w = self.add_weight(name='w', shape=(input_shape[1], 1), 
                                 initializer='uniform', trainable=True)
        self.p = self.add_weight(name='p', shape=(input_shape[1], 1), 
                                 initializer='uniform', trainable=True)
        super(T2V, self).build(input_shape)
    
    def call(self, x):
        original = self.w * x + self.p
        sin_trans = K.sin(K.dot(x, self.W) + self.P)
        return K.concatenate([sin_trans, original], -1)
</code></pre>
<p>create a dummy example</p>
<pre><code>n_sample = 90
timesteps = 5
feat_inp = 2
feat_out = 4

X = np.random.uniform(0,1, (n_sample, timesteps, feat_inp))
y = np.random.uniform(0,1, (n_sample, feat_out))

def T2V_NN():
    inp = Input(shape=(timesteps,feat_inp))
    x = T2V(32)(inp)
    x = LSTM(8)(x)
    x = Dense(feat_out)(x)
    m = Model(inp, x)
    m.compile(loss='mse', optimizer='adam')
    return m

model = T2V_NN()
model.fit(X,y, epochs=3)
</code></pre>
",1,1,435,2021-02-09 02:14:31,https://stackoverflow.com/questions/66112042/valueerror-dimensions-must-be-equal-but-are-2-and-1-in-time2vec-example
gensim - word2vec: AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;most_common&#39;,"<pre><code>PAD = 0
UNK = 1
START = 2
END = 3
def make_vocab(wc, vocab_size):
    word2id, id2word = {}, {}
    word2id['&lt;pad&gt;'] = PAD
    word2id['&lt;unk&gt;'] = UNK
    word2id['&lt;start&gt;'] = START
    word2id['&lt;end&gt;'] = END
    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):
        word2id[w] = i
    return word2id
</code></pre>
<p>I got this error &quot;AttributeError: 'Word2Vec' object has no attribute 'most_common'&quot; when calling this function. I tried with different version of gensim. Could you give me some hints to  solve this.</p>
","python, nlp, gensim, word2vec","<p>Gensim's Word2Vec doesn't contain a <code>most_common</code> method.</p>
<p>If, for whatever reason you must extract <code>word,frequency</code> pairs from your model you can use</p>
<pre><code>[(word, wc.w2v.vocab[word]) for word in wc.wv.vocab]
</code></pre>
<p>and sort the resulting list. This is a decidedly strange use case, however.</p>
",1,0,1626,2021-02-14 16:56:32,https://stackoverflow.com/questions/66197779/gensim-word2vec-attributeerror-word2vec-object-has-no-attribute-most-comm
Minimum number of words in the vocabulary for Word2Vec models?,"<p>I have a corpus of short text(~5000 sentences) which forms a vocabulary of ~2000 words. I used Gensim to build a Word2Vec model, but the output from most_similar doesn't look reasonable. Is this because I don't have enough words in the vocabulary? If so, is there any rule of thumbs for the vocabulary size?</p>
","gensim, word2vec","<p>Generally word2vec needs a lot of data, with many varied examples of each word, for good word-vectors. You can sometimes squeeze some usefulness out of smaller datasets with:</p>
<ul>
<li>smaller vector-dimensionality; and/or</li>
<li>more training epochs</li>
</ul>
<p>(While I've not formally tested this, my hunch/rule-of-thumb is vector-dimensionality should be no more than the square-root of the count of unique words. So with only 2000 unique words, even a dimensionality of 50 is pushing it.)</p>
<p>You may be tempted to use a lower-than-default <code>min_count</code> so those words that only appear one or two times train vectors. But such vectors without varied usage examples will themselves be poor - dominated by those one or two not-broadly-representative contexts. Also, in aggregate all such &quot;noise&quot; words, interspersed with the words that <strong>do</strong> have enough examples, tend to make those other word-vectors worse. (Discarding words with too few examples usually <em>improves</em> the surviving words' vectors.)</p>
<p>If at all possible, get more training data, from a similar usage domain, to mix in with your data of primary interest.</p>
",1,0,1690,2021-02-18 20:39:04,https://stackoverflow.com/questions/66267818/minimum-number-of-words-in-the-vocabulary-for-word2vec-models
What should be used between Doc2Vec and Word2Vec when analyzing product reviews?,"<p>I collected some product reviews of a website from different users, and I'm trying to find similarities between products through the use of the embeddings of the words used by the users.
I grouped each review per product, such that I can have different reviews succeeding one after the other in my dataframe (i.e: different authors for one product). Furthermore, I also already tokenized the reviews (and all other pre-processing methods). Below is a mock-up dataframe of what I'm having (the list of tokens per product is actually very high, as well as the number of products):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Product</th>
<th>reviews_tokenized</th>
</tr>
</thead>
<tbody>
<tr>
<td>XGame3000</td>
<td>absolutely amazing simulator feel inaccessible ...</td>
</tr>
<tr>
<td>Poliamo</td>
<td>production value effect tend cover rather  ...</td>
</tr>
<tr>
<td>Artemis</td>
<td>absolutely fantastic possibly good oil ...</td>
</tr>
<tr>
<td>Ratoiin</td>
<td>ability simulate emergency operator town ...</td>
</tr>
</tbody>
</table>
</div>
<p>However, I'm not sure of what would be the most efficient between doc2Vec and Word2Vec. I would initially go for Doc2Vec, since it has the ability to find similarities by taking into account the paragraph/sentence, and find the topic of it (which I'd like to have, since I'm trying to cluster products by topics), but I'm a bit worry about the fact that the reviews are from different authors, and thus might bias the embeddings? Note that I'm quite new to NLP and embeddings, so some notions may escape me. Below is my code for Doc2Vec, which giving me a quite good silhouette score (~0.7).</p>
<pre><code>product_doc = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(df.tokens)]
model3 = Doc2Vec(min_count=1, seed = SEED, ns_exponent = 0.5)
model3.build_vocab(product_doc)
model3.train(product_doc, total_examples=model3.corpus_count, epochs=model3.epochs)
product2vec = [model3.infer_vector((df['tokens'][i].split(' '))) for i in range(0,len(df['tokens']))]
dtv = np.array(product2vec)
</code></pre>
<p>What do you think would be the most efficient method to tackle this? If something is not clear enough, or else, please tell me.</p>
<p>Thank you for your help.</p>
<p>EDIT: Below is the clusters I'm obtaining:
<a href=""https://i.sstatic.net/STJq5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/STJq5.png"" alt=""Clusters obtained"" /></a></p>
","python, nlp, word2vec, doc2vec","<p>There's no way to tell which particular mix of methods will work best for a specific dataset and particular end-goal: you really have to try them against each other, in your own reusable pipeline for scoring them against your desired results.</p>
<p>It looks like you've already stripped the documents down to keywords rather than original natural text, which could hurt with these algorithms - you may want to try it both ways.</p>
<p>Depending on the size &amp; format of your texts, you may also want to look at doing &quot;Word Mover's Distance&quot; (WMD) comparisons between sentences (or other small logical chunks of your data). Some work has demo'd interesting results in finding &quot;similar concerns&quot; (even with different wording) in the review domain, eg: <a href=""https://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/"" rel=""nofollow noreferrer"">https://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/</a></p>
<p>Note, though, WMD gets quite costly to calculate in bulk with larger texts.</p>
",1,1,506,2021-03-01 13:10:43,https://stackoverflow.com/questions/66422766/what-should-be-used-between-doc2vec-and-word2vec-when-analyzing-product-reviews
Memory Error in Python using gensim.utils.simple_preprocess,"<p>I am a beginner with gensim word2vec, and I am encountering a memory error when preparing text for training the model. I am using Python 3.8.8. I have about 900,000 text files in 12 different folders. I was thinking I should send all text documents through gensim.utils.simple_preprocess, and then I'd have a list of lists for the model. After going through about 150,000 documents, I received a memory error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;word2vec_part1.py&quot;, line 58, in &lt;module&gt;
    documents = list(read_input(paths))
  File &quot;word2vec_part1.py&quot;, line 39, in read_input
    myfile = infile.read()
MemoryError
</code></pre>
<p>Is there a way to fix this memory issue? I included the code I am using below. I am new to Python, word2vec, and stackoverflow, so I apologize if my question is poorly worded or if this is a dumb question! Thank you for your time!</p>
<pre><code># imports and logging

import gensim 
import logging
import os
import os.path
import glob


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# define function

def read_input(inputs):

    # logging info
    logging.info(&quot;reading files&quot;)

    # set working directories and load files into a list
    for path in inputs:
        os.chdir(path)
        read_files=glob.glob(&quot;*.txt&quot;) 
        # preprocess and counting
        for i, file in enumerate(read_files):
            if(i%10000==0):
                logging.info(&quot;read {0} reviews&quot;.format(i))
            # preprocessing and return a list of words
            with open(file, &quot;rb&quot;) as infile:
                myfile = infile.read()
                yield gensim.utils.simple_preprocess(myfile)


# create a list of all file paths
paths = [#here is a list of file paths]

# call function
documents = list(read_input(paths))
logging.info(&quot;done reading files!!&quot;)
print(len(documents))
print(documents[1])

# training word2vec model
model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2)
model.train(documents,total_examples=len(documents),epochs=10)
model.save(&quot;word2vec.model&quot;)

# look up top 6 words similar to 'law'
w1 = [&quot;law&quot;]
model.wv.most_similar (positive=w1,topn=6)

logging.info(&quot;done!!!&quot;)
</code></pre>
","python, memory, gensim, word2vec","<p>It appears that attempting to hold all the documents in a <code>list</code> in memory requires more RAM than your system has. (Perhaps also: one of the files is gigantic. What's the largest single file?)</p>
<p>It's not necessary to hold all docs in memory. Gensim's <code>Word2Vec</code> (&amp; other algorithms) can almost always accept any Python <em>iterable</em> object - one that can iterate over its contents one-by-one, repeatedly, even if they're coming from some other back-end. This typically uses far less RAM.</p>
<p>The leader of the Gensim project has a useful post about iterables that could help you adapt your <code>read_input()</code> function into a wrapper class that can re-iterate over the files repeatedly:</p>
<p><a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/</a></p>
<p>Two other notes:</p>
<p>(1) <code>simple_preprocess()</code> isn't especially sophisticated, and you may want to do your own tokenization instead; but:</p>
<p>(2) If you re-tokenize on every iteration, especially if your tokenization does anything sophisticated or uses regular-expressions, you're doing a lot of redundant re-tokenizing of the same texts, which is likely to be a bottleneck in your training. So in fact you might want to just use your <code>read_input()</code> not to stuff all rokenized docs into a list, but write them to a new file, post-tokenization, one-document to a line and all tokens separated by single spaces. Then, a utility class like Gensim's <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer""><code>LineSentence</code></a> can provide the iterable-wrapper for feeding that almost-fully-ready file to <code>Word2Vec</code>.</p>
",0,0,375,2021-03-02 23:21:20,https://stackoverflow.com/questions/66448514/memory-error-in-python-using-gensim-utils-simple-preprocess
Pre-trained embedding layer: tf.constant with unsupported shape,"<p>I am going to use pre-trained word embeddings in Keras model. my matrix weights are stored in ;matrix.w2v.wv.vectors.npy; and it has shape (150854, 100).</p>
<p>Now when I add the embedding layer in the Keras model with different parameters as follows:</p>
<pre class=""lang-py prettyprint-override""><code>model.add(Embedding(5000, 100,
    embeddings_initializer=keras.initializers.Constant(emb_matrix),
    input_length=875, trainable=False))
</code></pre>
<p>I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                         Traceback (most recent call last)
&lt;ipython-input-61-8731e904e60a&gt; in &lt;module&gt;()
  1 model = Sequential()
  2 
----&gt; 3 model.add(Embedding(5000,100,
   embeddings_initializer=keras.initializers.Constant(emb_matrix),
   input_length=875,trainable=False))
  4 model.add(Conv1D(128, 10, padding='same', activation='relu'))
  5 model.add(MaxPooling1D(10))

  22 frames
 
 /usr/local/lib/python3.7/dist- 
 packages/tensorflow/python/framework/constant_op.py in 
_constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  323   raise TypeError(&quot;Eager execution of tf.constant with unsupported shape 
             &quot;
  324                   &quot;(value has %d elements, shape is %s with %d 
                        elements).&quot; %
--&gt; 325                   (num_t, shape, shape.num_elements()))
  326 
  327 

  TypeError: Eager execution of tf.constant with unsupported shape (value has 
  15085400 elements, shape is (5000, 100) with 500000 elements).
</code></pre>
<p>Kindly tell me where I am doing a mistake.</p>
","keras, word2vec, keras-layer, embedding","<p>Your embeddings layer expects a vocabulary of 5,000 words and initializes an embeddings matrix of the shape 5000Ã—100. However. the word2vec model that you are trying to load has a vocabulary of 150,854 words.</p>
<p>Your either need to increase the capacity of the embedding layer or truncate the embedding matrix to allow the most frequent words only.</p>
",1,0,621,2021-03-03 16:55:00,https://stackoverflow.com/questions/66461525/pre-trained-embedding-layer-tf-constant-with-unsupported-shape
what does &#39;corpus_count&#39; in gensim word2vec?,"<p>I want to train my word Embedding from scratch and I use gensim.models.word2vec as my model.
My corpus is so large that I can not read it at once , so I divide my corpus file into <strong>many parts</strong> and train my model iterativelyã€‚I find this is helpful:</p>
<pre><code>train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)
</code></pre>
<h2>I confused about the parameter &quot;total_words&quot; .
Is it means total words of all my corpus or the part corpus trained now?</h2>
<p>UPDATE:</p>
<p>my code is like this:</p>
<pre><code>model =  gensim.models.word2vec.Word2Vec.load(init_model)  
for i in range(parts):
    model.build_vocab(corpus_file=this_part_file_name, update=True)
    model.train(corpus_file = this_part_file_name, 
                   total_words=word_count(this_part_file_name) )

</code></pre>
<p>Should the parameter total_words be <code>word_count(this_part_file_name)</code> or <code>word_count(ALL_my_corpus_file)</code> ?</p>
","nlp, gensim, word2vec","<p><code>total_words</code> is the count of all raw words in the sentences in the corpus. You only have to provide one of the two: <code>total_examples</code> or <code>total_words</code>. If you ran <code>build_vocab()</code>, you may get the value for total words from <code>model.corpus_total_words</code>.
There is another count - <code>word_count</code> that refers to the count of words that are already trained. You can set this to 0 if you want to train on all the words, but this is optional.</p>
<p>More info: <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>
",1,0,1464,2021-03-07 15:09:42,https://stackoverflow.com/questions/66517974/what-does-corpus-count-in-gensim-word2vec
Should I split sentences in a document for Doc2Vec?,"<p>I am building a Doc2Vec model with 1000 documents using Gensim.
Each document has consisted of several sentences which include multiple words.</p>
<p>Example)</p>
<p>Doc1: [[word1, word2, word3], [word4, word5, word6, word7],[word8, word9, word10]]</p>
<p>Doc2: [[word7, word3, word1, word2], [word1, word5, word6, word10]]</p>
<p>Initially, to train the Doc2Vec, I first split sentences and tag each sentence with the same document tag using  &quot;TaggedDocument&quot;. As a result, I got the final training input for Doc2Vec as follows:</p>
<p>TaggedDocument(words=[word1, word2, word3], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word7, word3, word1, word2], tags=['Doc2'])</p>
<p>TaggedDocument(words=[word1, word5, word6, word10], tags=['Doc2'])</p>
<p>However, would it be okay to train the model with the document as a whole without splitting sentences?</p>
<p>TaggedDocument(words=[word1, word2, word3,word4, word5, word6, word7,word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7,word1, word5, word6, word10], tags=['Doc2'])</p>
<p>Thank you in advance :)</p>
","gensim, word2vec, doc2vec","<p>Both approaches are going to be very similar in their effect.</p>
<p>The slight difference is that in PV-DM modes (<code>dm=1</code>), or PV-DBOW with added skip-gram training (<code>dm=0, dbow_words=1</code>), if you split by sentence, words in different sentences will never be within the same context-window.</p>
<p>For example, your <code>'Doc1'</code> words <code>'word3'</code> and <code>'word4'</code> would never be averaged-together in the same PV-DM context-window-average, nor be used to PV-DBOW skip-gram predict-each-other, if you split by sentences. If you just run the whole doc's words together into a single <code>TaggedDocument</code> example, they would interact more, via appearing in shared context-windows.</p>
<p>Whether one or the other is better for your purposes is something you'd have to evaluate in your own analysis - it could depend a lot on the nature of the data &amp; desired similarity results.</p>
<p>But, I can say that your <em>second</em> option, all the words in one <code>TaggedDocument</code>, is the more common/traditional approach.</p>
<p>(That is, as long as the document is still no more than 10,000 tokens long. If longer, splitting the doc's words into multiple <code>TaggedDocument</code> instances, each with the same <code>tags</code>, is a common workaround for an internal 10,000-token implementation limit.)</p>
",2,2,384,2021-03-17 02:04:41,https://stackoverflow.com/questions/66665981/should-i-split-sentences-in-a-document-for-doc2vec
How to get back hyperparameters from a trained world2vec model gensim?,"<p>I have a trained word2vec model which I need to train further with more data. I want to use the same hyperparameters that is used while training the model for the new model as well. But I don't want to hardcode it. Is there a method which I can use to get the hyperparameters used while training the existing model.
I am using Gensim word2vec.</p>
","python, gensim, word2vec, hyperparameters","<p>Any full <code>Word2Vec</code> model has every metaparameter that was supplied at its initial creation somewhere in its object properties.</p>
<p>It's almost always on the model itself, using the exact same name as was used for the constructor parameters. So, <code>model.window</code> will return the <code>window</code>, etc - and thus you can just create a new model pulling each value from the old model.</p>
<p>Note that continuing training on an already-trained model involves a lot of thorny tradeoffs.</p>
<p>For example, the <code>.build_vocab(..., update=True)</code> on an existing model won't be applying <code>min_count</code> consistently against all word totals from all prior calls, but only those in the latest 'batch'.</p>
<p>The proper learning-rate (<code>alpha</code> to <code>min_alpha</code> values) for incremental updates isn't well-defined by theory or rules-of-thumb. And if the vocabulary &amp; word-balance in the new texts mainly train some words, not all, those recently-updated words can be pulled arbitrarily out of strong-comparability-alignment with earlier words that didn't get more training. (The underlying method of mdoel optimization, stochastic gradient descent, is best-grounded when all training texts are given equal training attention, without any subset begin intensely trained later.)</p>
",1,0,586,2021-03-26 09:05:13,https://stackoverflow.com/questions/66813857/how-to-get-back-hyperparameters-from-a-trained-world2vec-model-gensim
Using word2vec to substitute less frequent words in data frame R,"<p>I have a data frame <code>data1</code> with cleaned strings of text  matched to their ids</p>
<pre><code>    # A tibble: 2,000 x 2
      id text                                                                                                                            
   &lt;int&gt; &lt;chr&gt;                                                                                                                           

     1 decent scene guys visit spanish lady hilarious flamenco music background reâ€¦
     3 movie beautiful plot depth kolossal scenes battles moral rationale br br conclusion wondâ€¦
     4 fan scream killing astonishment story summarized don time move ii won regret plot ironical              
     5 mistake film guess minutes clunker fought hard stay seat lose hours life feeling br hisâ€¦
     6 phoned awful bed dog ranstuck br br positive grooming eldest daughter beeeatch br ousâ€¦
    
    # â€¦ with 1,990 more rows
</code></pre>
<p>And have created a new variable <code>freq</code> that for every word gives the tf, pdf and itidf. In order, the columns of <code>freq</code> indicate <code>id</code>, <code>word</code>, <code>n</code>, <code>tf</code>, <code>idf</code>, <code>tf_idf</code></p>
<pre><code># A tibble: 112,709 x 6
      id word           n    tf   idf tf_idf
   &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1   335 starcrash      1 0.5    7.60   3.80
 2  2974 carly          1 0.5    6.50   3.25
 3  1796 phillips       1 0.5    5.81   2.90
 4  1796 eric           1 0.5    5.40   2.70
 5  1398 wilson         1 0.5    5.20   2.60
 6   684 apolitical     1 0.333  7.60   2.53
 7  1485 saimin         1 0.333  7.60   2.53
 8  1398 charlie        1 0.5    4.77   2.38
 9  2733 shouldn        1 0.5    4.71   2.36
10  2974 jones          1 0.5    4.47   2.23
# â€¦ with 112,699 more rows
</code></pre>
<p>I am trying to create a loop that goes through this second variable and uses word2vec to substitute in <code>data1</code> any word of tf lower than the mean of all others, with the closest match.
I have tried the function</p>
<pre><code> replace_word &lt;- function(x) {
   x&lt;-hunspell_suggest(x)
   x&lt;-mutate(x)
   p&lt;-system.file(package = &quot;word2vec&quot;, &quot;models&quot;, &quot;example.bin&quot;)
   m&lt;-read.word2vec(p)
   s&lt;-predict(m, x, type='nearest', top_n=1)
   paste0(s)
  }
  
</code></pre>
<p>But when I run it it goes into an infinite loop. I originally wanted to check whether the spelling of the word was correct first, but because there are words not in the dictionary I kept on getting errors.
Because I have never done something like this before, I really don't know how to make it work. Could someone please help?</p>
<p>Thank you</p>
","r, dataframe, for-loop, word2vec","<p>Maybe this code is what you are looking for. You can also use a pretrained word2vec model, in the below example the word2vec model is trained upon your data (more info at <a href=""https://www.bnosac.be/index.php/blog/100-word2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/100-word2vec-in-r</a>)</p>
<pre><code>library(word2vec)
library(udpipe)
data(brussels_reviews, package = &quot;udpipe&quot;)
x &lt;- subset(brussels_reviews, language == &quot;nl&quot;)

data1 &lt;- data.frame(id = x$id, text = tolower(x$feedback), stringsAsFactors = FALSE) 
str(data1)
#&gt; 'data.frame':    500 obs. of  2 variables:
#&gt;  $ id  : int  19991431 21054450 22581571 23542577 40676307 46755068 23831365 23016812 46958471 28687866 ...
#&gt;  $ text: chr  &quot;zeer leuke plek om te vertoeven , rustig en toch erg centraal gelegen in het centrum van brussel , leuk adres o&quot;| __truncated__ &quot;het appartement ligt op een goede locatie: op loopafstand van de europese wijk en vlakbij verschilende metrosta&quot;| __truncated__ &quot;bedankt bettina en collin. ik ben heel blij dat ik bij jullie heb verbleven, in zo'n prachtige stille omgeving &quot;| __truncated__ &quot;ondanks dat het, zoals verhuurder joffrey zei, geen last minute maar een last seconde boeking was, is alles per&quot;| __truncated__ ...
freq &lt;- strsplit.data.frame(data1, term = &quot;text&quot;, group = &quot;id&quot;, split = &quot;[[:space:][:punct:][:digit:]]+&quot;)
freq &lt;- document_term_frequencies(freq)
freq &lt;- document_term_frequencies_statistics(freq)
freq &lt;- freq[, c(&quot;doc_id&quot;, &quot;term&quot;, &quot;freq&quot;, &quot;tf&quot;, &quot;idf&quot;, &quot;tf_idf&quot;)]
head(freq)
#&gt;      doc_id      term freq      tf       idf     tf_idf
#&gt; 1: 19991431      zeer    1 0.03125 1.5702172 0.04906929
#&gt; 2: 19991431     leuke    1 0.03125 1.9519282 0.06099776
#&gt; 3: 19991431      plek    1 0.03125 2.5770219 0.08053194
#&gt; 4: 19991431        om    2 0.06250 1.4105871 0.08816169
#&gt; 5: 19991431        te    2 0.06250 0.9728611 0.06080382
#&gt; 6: 19991431 vertoeven    1 0.03125 4.6051702 0.14391157

## Build word2vec model
set.seed(123456789)
w2v &lt;- word2vec(x = data1$text, dim = 15, iter = 20, min_count = 0, lr = 0.05, type = &quot;cbow&quot;)
vocabulary &lt;- summary(w2v, type = &quot;vocabulary&quot;)
## For each word, find the most similar one if it is part of the word2vec vocabulary
freq$similar_word &lt;- ifelse(freq$term %in% vocabulary, freq$term, NA)
freq$similar_word &lt;- lapply(freq$similar_word, FUN = function(x){
    if(!is.na(x)){
        x &lt;- predict(w2v, x, type = 'nearest', top_n = 1)
        x &lt;- x[[1]]$term2
    }
    x
})
head(freq)
#&gt;      doc_id      term freq      tf       idf     tf_idf  similar_word
#&gt; 1: 19991431      zeer    1 0.03125 1.5702172 0.04906929     plezierig
#&gt; 2: 19991431     leuke    1 0.03125 1.9519282 0.06099776         cafes
#&gt; 3: 19991431      plek    1 0.03125 2.5770219 0.08053194 opportuniteit
#&gt; 4: 19991431        om    2 0.06250 1.4105871 0.08816169    verblijven
#&gt; 5: 19991431        te    2 0.06250 0.9728611 0.06080382   overnachten
#&gt; 6: 19991431 vertoeven    1 0.03125 4.6051702 0.14391157  comfortabele
</code></pre>
<p>Now your threshold of 0.5. That's up to you to define.</p>
",0,0,165,2021-03-29 15:59:00,https://stackoverflow.com/questions/66857496/using-word2vec-to-substitute-less-frequent-words-in-data-frame-r
How to get three dimensional vector embedding for a list of words,"<p>I have been asked to create three dimensional vector embeddings for a series of words. Although I understand what an embedding is and that <code>word2vec</code> will be able to create the vector embeddings, I cannot find a resource that shows me how to create a <em>three</em> dimensional vector (all the resources show many more dimensions than this).</p>
<p>The format I have to create the file in is:</p>
<pre><code>house    34444     0.3232 0.123213 1.231231
dog    14444    0.76762 0.76767 1.45454
</code></pre>
<p>which is in the format <code>&lt;token&gt;\t&lt;word_count&gt;\t&lt;vector_embedding_separated_by_spaces&gt;</code></p>
<p>Can anyone point me towards a resource that will show me how to create the desired file format given some training text?</p>
","nlp, word2vec, word-embedding","<p>Once you've decided on a programming language, and word2vec library, its documentation will likely highlight a configurable parameter that lets you specify the dimensionality of the vectors it trains. So, you just need to change that parameter from its typical values , like <code>100</code> or <code>300</code>, to <code>3</code>.</p>
<p>(Note, though, that 3-dimensional word-vectors are unlikely to show the interesting &amp; useful property of higher-dimensional vectors.)</p>
<p>Once you've used such a library to create the vectors-in-memory, writing them out in your specified format becomes just a file-IO problem, unrelated to word2vec itself. In typical languages, you'd open a new file for writing, loop over your data printing each line properly, then close the file.</p>
<p>(To get a more detailed answer from StackOverflow, you'd want to pick a specific language/library, show what you've already tried with actual code, and show how the results/errors achieved fall short of your goal.)</p>
",1,0,454,2021-04-05 09:35:50,https://stackoverflow.com/questions/66950909/how-to-get-three-dimensional-vector-embedding-for-a-list-of-words
Is there a way to iterate through the vectors of Gensim&#39;s Word2Vec?,"<p>I'm trying to perform a simple task which requires iterations and interactions with specific vectors after loading it into gensim's Word2Vec.</p>
<p>Basically, given a txt file of the form:</p>
<pre><code>t1 -0.11307 -0.63909 -0.35103 -0.17906 -0.12349
t2 0.54553 0.18002 -0.21666 -0.090257 -0.13754
t3 0.22159 -0.13781 -0.37934 0.39926 -0.25967 
</code></pre>
<p>where t1 is the name of the vector and what follows are the vectors themselves. I load it in using the function <code>vecs = KeyedVectors.load_word2vec_format(datapath(f), binary=False)</code>.</p>
<p>Now, I want to iterate through the vectors I have and make a calculation, take summing up all of the vectors as an example. If this was read in using <code>with open(f)</code>, I know I can just use <code>.split(' ')</code> on it, but since this is now a KeyedVector object, I'm not sure what to do.</p>
<p>I've looked through the word2vec documentation, as well as used <code>dir(KeyedVectors)</code> but I'm still not sure if there is an attribute like <code>KeyedVectors.vectors</code> or something that allows me to perform this task.</p>
<p>Any tips/help/advice would be much appreciated!</p>
","python, gensim, word2vec","<p>There's a list of all words in the <code>KeyedVectors</code> object in its <code>.index_to_key</code> property. So one way to sum all the vectors would be to retrieve each by name in a list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>np.sum([vecs[key] for key in vecs.index_to_key], axis=0)
</code></pre>
<p>But, if all you really wanted to do is sum the vectors â€“ and the keys (word tokens) aren't an important part of your calculation, the set of all the raw word-vectors is available in the <code>.vectors</code> property, as a numpy array with one vector per row. So you could also do:</p>
<pre class=""lang-py prettyprint-override""><code>np.sum(vecs.vectors, axis=0)
</code></pre>
",-1,1,1926,2021-04-05 20:45:10,https://stackoverflow.com/questions/66959571/is-there-a-way-to-iterate-through-the-vectors-of-gensims-word2vec
words not available in corpus for Word2Vec training,"<p>I am totally new to Word2Vec. I want to find cosine similarity between word pairs in my data. My codes are as follows:</p>
<pre><code>import pandas as pd
from gensim.models import Word2Vec
model = Word2Vec(corpus_file=&quot;corpus.txt&quot;, sg=0, window =7, size=100, min_count=10, iter=4)
vocabulary = list(model.wv.vocab)
data=pd.read_csv(&quot;experiment.csv&quot;)
cos_similarity = model.wv.similarity(data['word 1'], data['word 2'])
</code></pre>
<p>The problem is some words in the data columns of my &quot;experiment.csv&quot; file: &quot;word 1&quot; and &quot;word 2&quot; are not present in the corpus file (&quot;corpus.txt&quot;). So this error is returned:</p>
<pre><code>&quot;word 'setosa' not in vocabulary&quot;
</code></pre>
<p>What should I do to handle words that are not present in my input corpus? I want to assign words in my experiment that are not present in the input corpus the vector zero, but I am stuck how to do it.</p>
<p>Any ideas for my problems?</p>
","python, nlp, word2vec","<p>It's really easy to give unknown words the origin (all 'zero') vector:</p>
<pre class=""lang-py prettyprint-override""><code>word = data['word 1']
if word in model.wv:
    vec = model[word]
else: 
    vec = np.zeros(100)
</code></pre>
<p>But, this is unlikely what you want. The zero vector can't be cosine-similarity compared to other vectors.</p>
<p>It's often better to simply ignore unknown words. If they were so rare that your training data didn't haven enough of them to create a vector, they can't contribute much to other analyses.</p>
<p>If they're still important, the best approach is to get more data, with realistic usage contexts, so they get meaningful vectors.</p>
<p>Another alternative is to use an algorithm, such as the word2vec variant <code>FastText</code>, which can always synthesize a guess-vector for any words that were out-of-vocabulary (OOV) based on the training data. It does this by learning word-vectors for word-fragments (charactewr n-grams), then assembling a vector for a new unknown word from those fragments. It's often better than random, because unknown words are often typos or variants of known words with which they share a lot of segments. But it's still not great, and for really odd strings, essentially returns a random vector.</p>
<p>Another tactic I've seen used, but wouldn't personally recommend, is to replace a lot of the words that would otherwise be ignored â€“ such as those with fewer than <code>min_count</code> occurrences â€“ with some single plug token, like say <code>'&lt;OOV&gt;'</code>. Then that synthetic token becomes a quite-common word, but gets an almost entirely meaningless: a random low-magnitude vector. (The prevalence of this fake word &amp; noise-vector in training will tend to make other surrounding words' vectors worse or slower-to-train, compared to simply eliding the low-frequency words.) But then, when dealing with later unknown words, you can use this same <code>'&lt;OOV&gt;'</code> pseudoword's vector as a not-too-harmful stand-in.</p>
<p>But again: it's almost always better to do some combination of â€“ (a) more data; (b) ignoring rare words; (c) using a algorithm like FastText which can synthesize better-than-nothing vectors â€“ than collapse <em>all</em> unknown words to a single nonsense vector.</p>
",2,0,1066,2021-04-13 20:13:27,https://stackoverflow.com/questions/67081781/words-not-available-in-corpus-for-word2vec-training
How to explain gensim word2vec output?,"<p>I run the following code and just wonder why the top 3 most similar words for &quot;exposure&quot; don't include &quot;charge&quot; and &quot;lend&quot;?</p>
<pre><code>from gensim.models import Word2Vec
corpus = [['total', 'exposure', 'charge', 'lend'],
          ['customer', 'paydown', 'rate', 'months', 'month']]
gens_mod = Word2Vec(corpus, min_count=1, vector_size=300, window=2, sg=1, workers=1, seed=1)
keyword=&quot;exposure&quot;
gens_mod.wv.most_similar(keyword)

Output:
[('customer', 0.12233059108257294),
 ('month', 0.008674687705934048),
 ('total', -0.011738087050616741),
 ('rate', -0.03600010275840759),
 ('months', -0.04291829466819763),
 ('paydown', -0.044823747128248215),
 ('lend', -0.05356598272919655),
 ('charge', -0.07367636263370514)]
</code></pre>
","python, nlp, gensim, word2vec, word-embedding","<p>The word2vec algorithm is only useful &amp; valuable with large amounts of training data, where every word of interest has a variety of realistic, subtly-contrasting usage examples.</p>
<p>A toy-sized dataset won't show its value. It's always a bad idea to set <code>min_count=1</code>. And, it's nonsensical to try to train 300-dimensional word-vectors from a corpus of only 9 words, 9 unique words, and most of the words having the exact same neighbors.</p>
<p>Try it on a more realistic dataset - tens-of-thousands of unique words, all with multiple usage examples â€“ and you'll see more intuitively-correct similarity results.</p>
",2,0,455,2021-04-14 17:37:56,https://stackoverflow.com/questions/67096547/how-to-explain-gensim-word2vec-output
When should I consider to use pretrain-model word2vec model weights?,"<p>Suppose my corpus is reasonably large - having tens-of-thousands of unique words. I can either use it to build a word2vec model directly(Approach #1 in the code below) or initialize a new word2vec model with pre-trained model weights and fine tune it with my own corpus(Approach #2). Is the approach #2 worth consideration? If so, is there a rule of thumb on when I should consider a pre-trained model?</p>
<pre><code># Approach #1
from gensim.models import Word2Vec
model = Word2Vec(my_corpus, vector_size=300, min_count=1)

# Approach #2
model = Word2Vec(vector_size=300, min_count=1)
model.build_vocab(my_corpus)
model.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
model.train(my_corpus, total_examples=len(my_corpus))
</code></pre>
","python, gensim, word2vec, word-embedding, pre-trained-model","<p>The general answer to this type of question is: you should try them both, and see which works better for your purposes.</p>
<p>No one without your exact data &amp; project goals can be sure which will work better in your situation, and you'll need to exact same kind of ability-to-evaluate alterante choices to do all sorts of very basic, necessary tuning of your work.</p>
<p>Separately:</p>
<ul>
<li>&quot;fine-tuning&quot; word2vec-vectors can mean many things, and can introduce a number of expert-leve thorny tradeoff-decisions - the sorts of tradeoffs that can only be navigated if you've got a robust way to test different choices against each other.</li>
<li>The specific simple tuning approach your code shows - which relies on an experimental method (<code>intersect_word2vec_format()</code>) that might not work in the latest Gensim â€“ is pretty limited, and since it discards all the words in the outside vectors that aren't already in your own corpus, also discards one of the major reasons people often want to mix older vectors in - to cover more words not in their training data. (I doubt that approach will be useful in many cases, but as per above, to be sure you'd want to try it with respect to your data/goals.</li>
<li>It's almost always a bad idea to use <code>min_count=1</code> with word2vec &amp; similar algorithms. If such rare words are truly important, find more training examples so good vectors can be trained for them. But without enough training examples, they're usually better to ignore - keeping them even makes the vectors for surrounding words worse.</li>
</ul>
",1,0,1219,2021-04-14 22:06:59,https://stackoverflow.com/questions/67099706/when-should-i-consider-to-use-pretrain-model-word2vec-model-weights
Calculate Cosine Similarity for a word2vec model in R,"<p>IÂ´m working with the package &quot;word2vec&quot; model in R and got a huge problem. I wanna figure out which words are the closest synonyms to &quot;uncertainty&quot; and &quot;economy&quot; like the paper of Azqueta-Gavaldon (2020): &quot;Economic policy uncertainty in the euro area: An unsupervised machine learning approach&quot;.So I did the word2vec function of the word2vec package to create my own word2vec model. With the function predict (object, ...) I can create a table which shows me the words which are closest to my considered words.The problem is that the similarity of this function is defined as the (sqrt(sum(x . y) / ncol(x))) which is not the cosine similarity.
I know that I can use the function cosine(x,y). This function but just works to calculate the cosine similarity between two vectors and canÂ´t do the output like the predict function which I described above.</p>
<p>Does anyone know how to determine the cosine similarity for each word in my Word2Vec model to the other and give me an output of the most similar words to a given word based on these values?</p>
<p>This would really help me a lot and I am already grateful for your answers.</p>
<p>Kind regards,
Tom</p>
","r, word2vec, word-embedding, cosine-similarity","<p>following github-code explains how you can use the cosine similarity in Word2Vec Models in R:
<a href=""https://gist.github.com/adamlauretig/d15381b562881563e97e1e922ee37920"" rel=""nofollow noreferrer"">https://gist.github.com/adamlauretig/d15381b562881563e97e1e922ee37920</a></p>
<p>You can use this function at every matrix in R and therefore for every Word2Vec Model built in R.</p>
<p>Kind Regards,
Tom</p>
",0,0,893,2021-04-15 08:34:57,https://stackoverflow.com/questions/67104985/calculate-cosine-similarity-for-a-word2vec-model-in-r
Why is my Doc2Vec model in gensim not reproducible?,"<p>I have noticed that my gensim Doc2Vec (DBOW) model is sensitive to document tags. My understanding was that these tags are cosmetic and so they should not influence the learned embeddings. Am I misunderstanding something? Here is a minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import numpy as np
import os
    
os.environ['PYTHONHASHSEED'] = '0'
    
reps = []
for a in [0,500]:
    documents = [TaggedDocument(doc, [i + a]) 
                 for i, doc in enumerate(common_texts)]
    model = Doc2Vec(documents, vector_size=100, window=2, min_count=0,
                    workers=1, epochs=10, dm=0, seed=0)
    reps.append(np.array([model.docvecs[k] for k in range(len(common_texts))])
    
reps[0].sum() == reps[1].sum()
</code></pre>
<p>This last line returns <code>False</code>. I am working with gensim 3.8.3 and Python 3.5.2. More generally, is there <em>any</em> role that the values of the tags play (assuming they are unique)? I ask because I have found that using different tags for documents in a classification task leads to widely varying performance.</p>
<p>Thanks in advance.</p>
","gensim, word2vec, random-seed, doc2vec","<p>Have you checked the magnitude of the differences?</p>
<p>Just running:</p>
<pre><code>delta = reps[0].sum() - reps[1].sum()
</code></pre>
<p>for the aggregate differences results with <code>-1.2598932e-05</code> when I run it.</p>
<p>Comparison dimension-wise:</p>
<pre><code> eps = 10**-4
 over = (np.abs(diff) &lt;= eps).all()
</code></pre>
<p>Returns <code>True</code> on a vast majority of the runs which means that you are getting quite reproducible results given the complexity of the calculations.</p>
<p>I would blame <a href=""https://nhigham.com/2020/08/04/what-is-numerical-stability/#:%7E:text=Numerical%20stability%20concerns%20how%20errors,can%20be%20from%20any%20source."" rel=""nofollow noreferrer"">numerical stability</a> of the calculations or uncontrolled randomness. Even though you do try to control the random seed, there is a different random seed in NumPy and different in <code>random</code> standard library so you are not controlling for all of the sources of randomness. This can also have an influence on the results but I did not check the actual implementation in <code>gensim</code> and it's dependencies.</p>
",1,0,614,2021-04-20 13:00:41,https://stackoverflow.com/questions/67179473/why-is-my-doc2vec-model-in-gensim-not-reproducible
Word2vec on documents each one containing one sentence,"<p>I have some unsupervised data (100.000 files) and each file has a paragraph containing one sentence. The preprocessing went wrong and deleted all stop points (.).
I used word2vec on a small sample (2000 files) and it treated each document as one sentence.
Should I continue the process on all remaining files? Or this would result to a bad model ?</p>
<p>Thank you</p>
","python, nlp, gensim, word2vec","<p>Did you try it, and get bad results?</p>
<p>I'm not sure what you mean by &quot;deleted all stop points&quot;. But, Gensim's <code>Word2Vec</code> is oblivious to what your tokens are, and doesn't really have any idea of 'sentences'.</p>
<p>All that matters is the lists-of-tokens you provide. (Sometimes people include puntuation like <code>'.'</code> as tokens, and sometimes it's stripped - and it doesn't make a very big different either way, and to the extent it does, whether it's good or bad may depend on your data &amp; goals.)</p>
<p>Any lists-of-tokens that include neighboring related tokens, for the sort of context-window training that's central to the word2vec algorithm, should work well.</p>
<p>For example, it can't learn anything from one-word texts, where there are no neighboring words. But running togther sentences, paragraphs, and even full documents into long texts works fine.</p>
<p>Even concatenating wholly-unrelated texts doesn't hurt much: the bit of random noise from unrelated words now in-each-others' windows is outweighed, with enough training, by the meaningful relationships in the much-longer runs of truly-related text.</p>
<p>The main limit to consider is that each training text (list of tokens) shouldn't be more than 10,000 tokens long, as internal implementation limits up through Gensim 4.0 mean tokens past the 10,000th position will be ignored. (This limit might eventually be fixed - but until then, just splitting overlong texts into 10,000-token chunks is a fine workaround with negligible effects via the lost contexts at the break points.)</p>
",1,0,238,2021-04-20 20:32:21,https://stackoverflow.com/questions/67185941/word2vec-on-documents-each-one-containing-one-sentence
Statistical reasoning: how and why does tf.keras.preprocessing.sequence skipgrams use sampling_table this way?,"<p>The <code>sampling_table parameter</code> is only used in the <code>tf.keras.preprocessing.sequence.skipgrams</code> method once to  test if the probability of the target word in the <code>sampling_table</code> is smaller than some random number drawn from 0 to 1 (<code>random.random()</code>).</p>
<p>If you have a large vocabulary and a sentence that uses a lot of infrequent words, doesn't this cause the method to skip a lot of the infrequent words in creating skipgrams? Given the values of a sampling_table that is log-linear like a zipf distribution, doesn't this mean you can end up with no skip grams at all?</p>
<p>Very confused by this. I am trying to replicate the <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">Word2Vec tutorial</a> hand don't understand or how the <code>sampling_table</code> is being used.</p>
<p>In the <a href=""https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py#L152-L240"" rel=""nofollow noreferrer"">source code</a>, this is the lines in question:</p>
<pre><code>            if sampling_table[wi] &lt; random.random():
                continue
</code></pre>
","python, keras, statistics, word2vec, subsampling","<p>This looks like the frequent-word-downsampling feature common in word2vec implementations. (In the original Google <code>word2vec.c</code> code release, and the Python Gensim library, it's adjusted by the <code>sample</code> parameter.)</p>
<p>In practice, it's likely <code>sampling_table</code> has been precalculated so that the rarest words are always used, common words skipped a little, and the very-most-common words skipped a lot.</p>
<p>That seems to be the intent reflected by the <a href=""https://github.com/keras-team/keras-preprocessing/blob/6701f27afa62712b34a17d4b0ff879156b0c7937/keras_preprocessing/sequence.py#L108"" rel=""nofollow noreferrer"">comment for <code>make_sample_table()</code></a>.</p>
<p>You could go ahead and call that with a probe value, like say 1000 for a 100-word vocabulary, and see what <code>sampleing_table</code> it gives back. I suspect it'll be numbers close to <code>1.0</code> early (drop lots of common words), and close to <code>0.0</code> late (keep most/all rare words).</p>
<p>This tends to improve word-vector quality, by reserving more relative attention for medium- and low-frequency words, and not exessively overtraining/overweighting plentiful words.</p>
",1,0,447,2021-04-23 22:04:54,https://stackoverflow.com/questions/67237327/statistical-reasoning-how-and-why-does-tf-keras-preprocessing-sequence-skipgram
"I had a problem using word2vec. Maybe it&#39;s a version problem, but I don&#39;t know how to solve it ï¼Ÿ","<p>This is my code</p>
<pre><code>w2v = Word2Vec(vector_size=150,min_count = 10)
w2v.build_vocab(x_train)
w2v.train(x_train)

def average_vec(text):
    vec = np.zeros(300).reshape((1,300))
    for word in text:
        try:
            vec += w2v[word].reshape((1,300))
        except KeyError:
            continue
        return vec
</code></pre>
<p><em><strong>And this throws the following error:</strong></em></p>
<pre><code>Traceback (most recent call last):   File
&quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in &lt;module&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in
&lt;listcomp&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 21, in
average_vec
    vec += w2v[word] TypeError: 'Word2Vec' object is not subscriptable

Process finished with exit code 1
</code></pre>
","python, gensim, word2vec","<p>The <code>Word2Vec</code> model object itself â€“ <code>w2v</code> in your code â€“ no longer supports direct access to individual vectors by lookup word key, in Gensim 4.0 and above.</p>
<p>Instead, you should use the subsidiary object in its <code>.wv</code> property - an object of type <code>KeyedVectors</code> which can be used to work with the set of word-vectors separately. (Separating functionality like this helps in cases where you only want the word-vectors, or only have the word-vectors from someone else, but not the full model's overhead.)</p>
<p>So, everywhere you might use <code>w2v[word]</code>, try <code>w2v.wv[word]</code> instead.</p>
<p>Or perhaps, name things more like the following, and hold a different variable reference to the word-vectors:</p>
<pre class=""lang-py prettyprint-override""><code>w2v_model = Word2Vec(...)
word_vectors = w2v_model.wv
print(word_vectors[word])
</code></pre>
<p>For other tips in adapting your own older code, or examples online, to Gensim 4.0, the following project wiki page may be helpful:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">Migrating from Gensim 3.x to 4</a></p>
",0,0,570,2021-04-26 06:47:10,https://stackoverflow.com/questions/67261993/i-had-a-problem-using-word2vec-maybe-its-a-version-problem-but-i-dont-know-h
Difference between VectorSize in word2Vec and numFeatures in TF-IDF,"<p>What is the difference between <strong>vectorSize</strong> in <strong>Word2Vec</strong> and <strong>numFeatures</strong> in <strong>HashingTF</strong>? I refer to class <em>Word2Vec</em> and <em>HashingTF</em> in pyspark:</p>
<p><strong>WORD2VEC</strong>: class pyspark.ml.feature.Word2Vec(*, <strong>vectorSize=100</strong>, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)</p>
<p><strong>HashingTF</strong>:  class pyspark.ml.feature.HashingTF(*, <strong>numFeatures=262144</strong>, binary=False, inputCol=None, outputCol=None)</p>
","python, gensim, word2vec, tf-idf","<p>They're both the dimensionality of the representation, but the values will be in different ranges and useful in different ways.</p>
<p>In <code>Word2Vec</code>, each word gets a vector of <code>vectorSize</code> dimensions - where each dimension is a floating-point number (rather than a whole number). The values will be both positive and negative, and essentially never zero. Thus all words have coordinates in a fuzzy 'cloud' of space around the origin point.</p>
<p>Thus a word2vec vector is considered a 'dense embedding' of the word: it represents the word into a smaller vector space ('embeds' it) in a way where every dimension varies and holds some of the info ('dense'). As a result, all (100 in your example) dimensions will be used to represent any one item (word).</p>
<p>In <code>HashingTF</code> (which probably stands for 'hashing term frequency' or 'hashing trick frequency'), a text document of many words gets a vector of <code>numFeatures</code> dimensions - where each dimension is a non-negative integer count of how many times certain words appear in the document.</p>
<p>By using a technique called the 'hashing trick', it ensures any word, whether seen before or not, is assigned (by a hash value) to one of a fixed-set of counting buckets. The value of each dimension in the vector is the count of the words assigned to one bucket. In typical cases, many if not nearly-all of the buckets will be empty â€“ and thus have zero values in the corresponding dimensions.</p>
<p>Thus a <code>HashingTF</code> vector is considered a 'sparse embedding' of a document: it represents the document into a smaller vector sapce ('embeds' it) in a way where most dimensions often stay zero, but a small relevant subset of dimensions become nonzero ('sparse'). As a result, the (262,144 in your example) dimensions might only be represented by a short list of which dimensions are non-zero and their value.</p>
",1,0,179,2021-04-28 09:09:09,https://stackoverflow.com/questions/67297183/difference-between-vectorsize-in-word2vec-and-numfeatures-in-tf-idf
invalid literal for int() with base 10: &#39;&lt;!DOCTYPE,"<p>I'm trying to use pre-trained word2vec in Google Colab. Previously I downloaded the model onto my C:/, and then uploaded it to my Google Drive. However, I get this error I can't seem to find anywhere.</p>
<p>My code is:</p>
<pre><code>from gensim.models import word2vec
import urllib.request

urllib.request.urlretrieve(&quot;https://drive.google.com/file/d/1lgCddPxJC__QA-qGtYTdNNoHRiYWyOpQ/view?usp=sharing/GoogleNews-vectors-negative300.bin&quot;, &quot;GoogleNews-vectors-negative300.bin&quot;)

word2vec_path = 'GoogleNews-vectors-negative300.bin'
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
</code></pre>
<p>Error Message:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-354-492ef9dcbbcc&gt; in &lt;module&gt;()
      1 word2vec_path = 'GoogleNews-vectors-negative300.bin'
----&gt; 2 word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)

2 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;!DOCTYPE'
</code></pre>
","python, gensim, word2vec","<p>As use ~deceze notes, that error hints that the file has some typical HTML boilerplate (<code>&lt;~DOCTYPE</code>) where the code is expecting 2 <code>int</code>s declaring the forthcoming count-of-vectors (<code>vocab_size</code>) &amp; their dimensionality (<code>vector_size</code>).</p>
<p>It's likely your <code>urlrequest()</code> action didn't receive the file you expected, and perhaps got a 'file not found' or other error instead. So:</p>
<ul>
<li>Check its size &amp; contents to see if it's what you expect.</li>
<li>Check your request code, to ensure it can even get what you need from a random cloud notebook. (Maybe the Google Drive URL requires a logged-in user, and your Colab notebook isn't able to make web requests as a logged-in version of you?)</li>
<li>If you have the valid file elsewhere, see if you can send that valid copy directly to the scratch storage space of the notebook.</li>
</ul>
",0,-1,765,2021-05-06 14:13:42,https://stackoverflow.com/questions/67419932/invalid-literal-for-int-with-base-10-doctype
Minimum Number of Words for Each Sentence for Training Gensim Word2vec Model,"<p>Suppose I have a corpus of short sentences of which the number of words ranges from 1 to around 500 and the average number of words is around 9. If I train a Gensim Word2vec model using window=5(which is the default), should I use all of the sentences? or I should remove sentences with low word count? If so, is there a rule of thumb for the minimum number of words?</p>
","nlp, gensim, word2vec, hyperparameters","<p>Texts with only 1 word are essentially 'empty' to the word2vec algorithm: there are no neighboring words, which are necessary for all training modes. You could drop them, but there's little harm in leaving them in, either. They're essentially just no-ops.</p>
<p>Any text with 2 or more words can contribute to the training.</p>
",1,0,271,2021-05-13 17:56:41,https://stackoverflow.com/questions/67523963/minimum-number-of-words-for-each-sentence-for-training-gensim-word2vec-model
IOPub data rate exceeded. in Google Colab,"<p>I'm trying to print words from a DATASET with length 8483448 bytes on google colab but i'm geting this error :</p>
<pre><code>words =list(model.wv.vocab)
print('this vocabulary for corpus')
print(words)
</code></pre>
<p>ERROR:</p>
<pre><code>IOPub data rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_data_rate_limit`.

Current values:
NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
NotebookApp.rate_limit_window=3.0 (secs)
</code></pre>
<p>thanks for giving me some help to fix this error.</p>
","python, google-colaboratory, word2vec","<p>Given the error, you seem to be hitting a Google Colab-specific limit on the output size.</p>
<p>Try printing <code>len(model.wv.vocab)</code> first to get a sense of how large of an output you're trying to display. It may not be practical to show in a notebook cell!</p>
<p>If you just need a peek at some of the large vocabulary, print a small subet, for example <code>print(words[0:10])</code>.</p>
<p>Note also: in the latest Gensim versions (&gt;=4.0.0), the <code>.vocab</code> dictionary goes away. But, a list of all known tokens (words), usually in descending-frequency order, is available in list <code>model.wv.index_to_key</code>. (So, in <code>gensim-4.0.0</code> &amp; up, you could look at the 100 most-frequent tokens with <code>print(model.wv.index_to_key[0:100])</code>.)</p>
",2,0,4004,2021-05-16 14:08:48,https://stackoverflow.com/questions/67557606/iopub-data-rate-exceeded-in-google-colab
How to interpret doc2vec classifier in terms of words?,"<p>I have trained a doc2vec (PV-DM) model in <code>gensim</code> on documents which fall into a few classes. I am working in a non-linguistic setting where both the number of documents and the number of unique words are small (~100 documents, ~100 words) for practical reasons. Each document has perhaps 10k tokens. My goal is to show that the doc2vec embeddings are more predictive of document class than simpler statistics and to explain which <em>words</em> (or perhaps word sequences, etc.) in each document are indicative of class.</p>
<p>I have good performance of a (cross-validated) classifier trained on the embeddings compared to one compared on the other statistic, but I am still unsure of how to connect the results of the classifier to any features of a given document. Is there a standard way to do this? My first inclination was to simply pass the co-learned word embeddings through the document classifier in order to see which words inhabited which classifier-partitioned regions of the embedding space. The document classes output on word embeddings are very consistent across cross validation splits, which is encouraging, although I don't know how to turn these effective labels into a statement to the effect of &quot;Document X got label Y because of such and such properties of words A, B and C in the document&quot;.</p>
<p>Another idea is to look at similarities between word vectors and document vectors. The ordering of similar word vectors is pretty stable across random seeds and hyperparameters, but the output of this sort of labeling does not correspond at all to the output from the previous method.</p>
<p>Thanks for help in advance.</p>
<p><strong>Edit</strong>: Here are some clarifying points. The tokens in the &quot;documents&quot; are ordered, and they are measured from a discrete-valued process whose states, I suspect, get their &quot;meaning&quot; from context in the sequence, much like words. There are only a handful of classes, usually between 3 and 5. The documents are given unique tags and the classes are not used for learning the embedding. The embeddings have rather dimension, always &lt; 100, which are learned over many epochs, since I am only worried about overfitting when the classifier is learned, not the embeddings. For now, I'm using a multinomial logistic regressor for classification, but I'm not married to it. On that note, I've also tried using the normalized regressor coefficients as vector in the embedding space to which I can compare words, documents, etc.</p>
","gensim, word2vec, word-embedding, doc2vec","<p>That's a very small dataset (100 docs) and vocabulary (100 words) compared to much published work of <code>Doc2Vec</code>, which has usually used tens-of-thousands or millions of distinct documents.</p>
<p>That each doc is thousands of words and you're using PV-DM mode that mixes both doc-to-word and word-to-word contexts for training helps a bit. I'd still expect you might need to use a smaller-than-defualt dimensionaity (vector_size&lt;&lt;100), &amp; more training epochs - but if it does seem to be working for you, great.</p>
<p>You don't mention how many classes you have, nor what classifier algorithm you're using, nor whether known classes are being mixed into the (often unsupervised) <code>Doc2Vec</code> training mode.</p>
<p>If you're only using known classes as the doc-tags, and your &quot;a few&quot; classes is, say, only 3, then to some extent you only have 3 unique &quot;documents&quot;, which you're training on in fragments. Using only &quot;a few&quot; unique doctags might be prematurely hiding variety on the data that could be useful to a downstream classifier.</p>
<p>On the other hand, if you're giving each doc a unique ID - the original 'Paragraph Vectors' paper approach, and then you're feeding those to a downstream classifier, that can be OK alone, but may also benefit from adding the known-classes as extra tags, in addition to the per-doc IDs. (And perhaps if you have many classes, those may be OK as the only doc-tags. It can be worth comparing each approach.)</p>
<p>I haven't seen specific work on making <code>Doc2Vec</code> models explainable, other than the observation that when you are using a mode which co-trains both doc- and word- vectors, the doc-vectors &amp; word-vectors have the same sort of useful similarities/neighborhoods/orientations as word-vectors alone tend to have.</p>
<p>You could simply try creating synthetic documents, or tampering with real documents' words via targeted removal/addition of candidate words, or blended mixes of documents with strong/correct classifier predictions, to see how much that changes either (a) their doc-vector, &amp; the nearest other doc-vectors or class-vectors; or (b) the predictions/relative-confidences of any downstream classifier.</p>
<p>(A wishlist feature for <code>Doc2Vec</code> for a while has been to synthesize a pseudo-document from a doc-vector. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2459"" rel=""nofollow noreferrer"">this issue</a> for details, including a link to one partial implementation. While the mere ranked list of such words would be nonsense in natural language, it might give doc-vectors a certain &quot;vividness&quot;.)</p>
<p>Whn you're not using real natural language, some useful things to keep in mind:</p>
<ul>
<li>if your 'texts' are really unordered bags-of-tokens, then <code>window</code> may not really be an interesting parameter. Setting it to a very-large number can make sense (to essentially put all words in each others' windows), but may not be practical/appropriate given your large docs. Or, trying PV-DBOW instead - potentially even mixing known-classes &amp; word-tokens in either <code>tags</code> or <code>words</code>.</li>
<li>the default <code>ns_exponent=0.75</code> is inherited from word2vec &amp; natural-language corpora, &amp; at least one research paper (linked from the class documentation) suggests that for other applications, especially recommender systems, very different values may help.</li>
</ul>
",2,0,652,2021-05-18 05:24:35,https://stackoverflow.com/questions/67580388/how-to-interpret-doc2vec-classifier-in-terms-of-words
Inner workings of Gensim Word2Vec,"<p>I have a couple of issues regarding Gensim in its Word2Vec model.</p>
<p>The first is what is happening if I set it to train for 0 epochs? Does it just create the random vectors and calls it done. So they have to be random every time, correct?</p>
<p>The second is concerning the WV object in the doc page says:</p>
<pre><code>This object essentially contains the mapping between words and embeddings.
After training, it can be used directly to query those embeddings in various ways.  
See the module level docstring for examples.
</code></pre>
<p>But that is not clear to me, allow me to explain I have my own created word vectors which I have substitute in the</p>
<pre><code>   word2vecObject.wv['word'] = my_own
</code></pre>
<p>Then call the train method with those replacement word vectors. But I would like to know which part am I replacing, is it the input to hidden weight layer or the hidden to input? This is to check if it can be called pre-training or not. Any help? Thank you.</p>
","python, gensim, word2vec","<p>I've not tried the nonsense parameter <code>epochs=0</code>, but it might behave as you expect. (Have you tried it and seen otherwise?)</p>
<p>However, if your real goal is to be able to tamper with the model after initialization, but before training, the usual way to do that is to not supply any corpus when constructing the model instance, and instead manually do the two followup steps, <code>.build_vocab()</code> &amp; <code>.train()</code>, in your own code - inserting extra steps between the two. (For even finer-grained control, you can examine the source of <code>.build_vocab()</code> &amp; its helper methods, and simply ensure you do all those necessary things, with your own extra steps interleaved.)</p>
<p>The &quot;word vectors&quot; in the <code>.wv</code> property of type <code>KeyedVectors</code> are essentially the &quot;input projection layer&quot; of the model: the data which converts a single word into a <code>vector_size</code>-dimensional dense embedding. (You can think of the keys â€“ word token strings â€“ as being somewhat like a one-hot word-encoding.)</p>
<p>So, assigning into that structure only changes that &quot;input projection vector&quot;, which is the &quot;word vector&quot; usually collected from the model. If you need to tamper with the hidden-to-output weights, you need to look at the model's <code>.syn1neg</code> (or <code>.syn1</code> for HS mode) property.</p>
",2,0,215,2021-05-19 19:21:58,https://stackoverflow.com/questions/67609635/inner-workings-of-gensim-word2vec
TypeError: &#39;Word2Vec&#39; object is not subscriptable,"<p>I am trying to build a Word2vec model but when I try to reshape the vector for tokens, I am getting this error. Any idea ?</p>
<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 100)) 
for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
wordvec_df = pd.DataFrame(wordvec_arrays) 
wordvec_df.shape

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-101-71156bf1c4a3&gt; in &lt;module&gt;
      1 wordvec_arrays = np.zeros((len(tokenized_tweet), 100))
      2 for i in range(len(tokenized_tweet)):
----&gt; 3     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
      4 wordvec_df = pd.DataFrame(wordvec_arrays)
      5 wordvec_df.shape

&lt;ipython-input-100-e3a82e60af93&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v[word].reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
","python-3.x, jupyter-notebook, gensim, word2vec","<p>As of Gensim 4.0 &amp; higher, the <code>Word2Vec</code> model doesn't support subscripted-indexed access (the <code>['...']') to individual words. (Previous versions would display a deprecation warning, </code>Method will be removed in 4.0.0, use self.wv.<strong>getitem</strong>() instead`, for such uses.)</p>
<p>So, when you want to access a specific word, do it via the <code>Word2Vec</code> model's <code>.wv</code> property, which holds just the word-vectors, instead. So, your (unshown) <code>word_vector()</code> function should have its line highlighted in the error stack changed to:</p>
<pre><code>            vec += model_w2v.wv[word].reshape((1, size))
</code></pre>
",13,8,21257,2021-05-25 12:30:41,https://stackoverflow.com/questions/67687962/typeerror-word2vec-object-is-not-subscriptable
How did online training work in the Word2vec model using Genism,"<p>Using the Genism library, we can load the model and update the vocabulary when the new sentence will be added. Thatâ€™s means If you save the model you can continue training it later. I checked with sample data, letâ€™s say I have a word in my vocabulary that was previously trained (i.e. â€œwomenâ€). And after that letâ€™s say I have new sentences and using model.build_vocab(new_sentence, update=True) and model.train(new_sentence), the model is updated. Now, in my new_sentence I have some word that already exists(â€œwomenâ€) in the previous vocabulary list and have some new word(â€œgirlâ€) that not exists in the previous vocabulary list. After updating the vocabulary, I have both old and new words in the corpus. And I checked using model.wv[â€˜womenâ€™], the vector is updated after update and training new sentence. Also, get the word embedding vector for a new word i.e. model.wv[â€˜girlâ€™]. All other words that were previously trained and not in the new_sentence, those word vectors not changed.</p>
<pre><code>model = Word2Vec(old_sentences, vector_size=100,window=5, min_count=1) 
model.save(&quot;word2vec.model&quot;)
model = Word2Vec.load(&quot;word2vec.model&quot;) //load previously save model 
model.build_vocab(new_sentences,update=True,total_examples=model.corpus_count, epochs=model.epochs)   
model.train(new_sentences)
</code></pre>
<p>However, just donâ€™t understand the inside depth explanation of how the online training is working. Please let me know if anybody knows in detail. I get the code but want to understand how the online training working in theoretically. Is it re-train the model on the old and new training data from scratch?</p>
<p>Here is the link that I followed: <a href=""https://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow noreferrer"">Online training</a></p>
","python-3.x, nlp, word2vec, word-embedding","<p>When you perform a new call to <code>.train()</code>, it only trains on the new data. So only words in the new data can possibly be updated.</p>
<p>And to the extent that the new data may be smaller, and more idiosyncratic in its word usages, any words in the new data will be trained to only be consistent with other words being trained in the new data. (Depending on the size of the new data, and the training parameters chosen like <code>alpha</code> &amp; <code>epochs</code>, they might be pulled via the new examples arbitrarily far from their old locations - and thus start to lose comparability to words that were trained earlier.)</p>
<p>(Note also that when providing an different corpus that the original, you shouldn't use a parameter like <code>total_examples=model.corpus_count</code>, reusing <code>model.corpus_count</code>, a value cahced in the model from the earlier data. Rather, parameters should describe the current batch of data.)</p>
<p>Frankly, I'm not a fan of this feature. It's possible it could be useful to advanced users. But most people drawn to it are likely misuing it, expecting any number of tiny incremental updates to constantly expand &amp; improve the model - when there's no good support for the idea that will reliably happen with naive use.</p>
<p>In fact, there's reasons to doubt such updates are generally a good idea. There's even an established term for the risk that incremental updates to a neural-network wreck its prior performance: <a href=""https://en.wikipedia.org/wiki/Catastrophic_interference"" rel=""nofollow noreferrer"">catastrophic forgetting</a>.</p>
<p>The straightforward &amp; best-grounded approach to updating word-vectors for new expanded data is to re-train from scratch, so all words are on equal footing, and go through the same interleaved training, on the same unified optimization (SGD) schedule. (The new new vectors at the end of such a process will not be in a compatible coordinate space, but should be equivalently useful, or better if the data is now bigger and better.)</p>
",2,0,649,2021-05-26 02:27:18,https://stackoverflow.com/questions/67697776/how-did-online-training-work-in-the-word2vec-model-using-genism
Training Word2Vec Model from sourced data - Issue Tokenizing data,"<p>I have recently sourced and curated a lot of reddit data from Google Bigquery.</p>
<p>The dataset looks like this:</p>
<p><a href=""https://i.sstatic.net/q1C1G.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q1C1G.png"" alt=""Data Preview"" /></a></p>
<p>Before passing this data to word2vec to create a vocabulary and be trained, it is required that I properly tokenize the 'body_cleaned' column.</p>
<p>I have attempted the tokenization with both manually created functions and NLTK's word_tokenize, but for now I'll keep it focused on using word_tokenize.</p>
<p>Because my dataset is rather large, close to 12 million rows, it is impossible for me to open and perform functions on the dataset in one go. Pandas tries to load everything to RAM and as you can understand it crashes, even on a system with 24GB of ram.</p>
<p>I am facing the following issue:</p>
<ul>
<li>When I tokenize the dataset (using NTLK word_tokenize), if I perform the function on the dataset as a whole, it correctly tokenizes and word2vec accepts that input and learns/outputs words correctly in its vocabulary.</li>
<li>When I tokenize the dataset by first batching the dataframe and iterating through it, the resulting token column is not what word2vec prefers; although word2vec trains its model on the data gathered for over 4 hours, the resulting vocabulary it has learnt consists of single characters in several encodings, as well as emojis - not words.</li>
</ul>
<p>To troubleshoot this, I created a tiny subset of my data and tried to perform the tokenization on that data in two different ways:</p>
<ul>
<li>Knowing that my computer can handle performing the action on the dataset, I simply did:</li>
</ul>
<pre><code>reddit_subset = reddit_data[:50]

reddit_subset['tokens'] = reddit_subset['body_cleaned'].apply(lambda x: word_tokenize(x))
</code></pre>
<p>This produces the following result:
<a href=""https://i.sstatic.net/FDrB8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FDrB8.png"" alt=""Tokenized Data Preview"" /></a></p>
<p>This in fact works with word2vec and produces model one can work with. Great so far.</p>
<p>Because of my inability to operate on such a large dataset in one go, I had to get creative with how I handle this dataset. My solution was to batch the dataset and work on it in small iterations using Panda's own batchsize argument.</p>
<p>I wrote the following function to achieve that:</p>
<pre><code>def reddit_data_cleaning(filepath, batchsize=20000):
    if batchsize:
        df = pd.read_csv(filepath, encoding='utf-8', error_bad_lines=False, chunksize=batchsize, iterator=True, lineterminator='\n')
    print(&quot;Beginning the data cleaning process!&quot;)
    start_time = time.time()
    flag = 1
    chunk_num = 1
    for chunk in df:
        chunk[u'tokens'] = chunk[u'body_cleaned'].apply(lambda x: word_tokenize(x))
        chunk_num += 1
    if flag == 1:
            chunk.dropna(how='any')
            chunk = chunk[chunk['body_cleaned'] != 'deleted']
            chunk = chunk[chunk['body_cleaned'] != 'removed']
            print(&quot;Beginning writing a new file&quot;)
            chunk.to_csv(str(filepath[:-4] + '_tokenized.csv'), mode='w+', index=None, header=True)
            flag = 0
        else:
            chunk.dropna(how='any')
            chunk = chunk[chunk['body_cleaned'] != 'deleted']
            chunk = chunk[chunk['body_cleaned'] != 'removed']
            print(&quot;Adding a chunk into an already existing file&quot;)
            chunk.to_csv(str(filepath[:-4] + '_tokenized.csv'), mode='a', index=None, header=None)
    end_time = time.time()
    print(&quot;Processing has been completed in: &quot;, (end_time - start_time), &quot; seconds.&quot;)
</code></pre>
<p>Although this piece of code allows me to actually work through this huge dataset in chunks and produces results where otherwise I'd crash from memory failures, I get a result which doesn't fit my word2vec requirements, and leaves me quite baffled at the reason for it.</p>
<p>I used the above function to perform the same operation on the Data subset to compare how the result differs between the two functions, and got the following:</p>
<p><a href=""https://i.sstatic.net/ZkGFD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZkGFD.png"" alt=""Comparison of Methods"" /></a></p>
<p>The desired result is on the new_tokens column, and the function that chunks the dataframe produces the &quot;tokens&quot; column result.</p>
<p>Is anyone any wiser to help me understand why the same function to tokenize produces a wholly different result depending on how I iterate over the dataframe?</p>
<p>I appreciate you if you read through the whole issue and stuck through!</p>
","python, pandas, tokenize, word2vec","<p>After taking gojomo's advice I simplified my approach at reading the csv file and writing to a text file.</p>
<p>My initial approach using pandas had yielded some pretty bad processing times for a file with around 12 million rows, and memory issues due to how pandas reads data all into memory before writing it out to a file.</p>
<p>What I also realized was that I had a major flaw in my previous code.
I was printing some output (as a sanity check), and because I printed output too often, I overflowed Jupyter and crashed the notebook, not allowing the underlying and most important task to complete.</p>
<p>I got rid of that, simplified reading with the csv module and writing into a txt file, and I processed the reddit database of ~12 million rows in less than 10 seconds.</p>
<p>Maybe not the finest piece of code, but I was scrambling to solve an issue that stood as a roadblock for me for a couple of days (and not realizing that part of my problem was my sanity checks crashing Jupyter was an even bigger frustration).</p>
<pre><code>def generate_corpus_txt(csv_filepath, output_filepath):
    import csv
    import time
    start_time = time.time()
    with open(csv_filepath, encoding = 'utf-8') as csvfile:
        datareader = csv.reader(csvfile)
        count = 0
        header = next(csvfile)
        print(time.asctime(time.localtime()), &quot; ---- Beginning Processing&quot;)
        with open(output_filepath, 'w+') as output:
            # Check file as empty
            if header != None:
                for row in datareader:
                        # Iterate over each row after the header in the csv
                        # row variable is a list that represents a row in csv
                    processed_row = str(' '.join(row)) + '\n'
                    output.write(processed_row)
                    count += 1
                    if count == 1000000:
                        print(time.asctime(time.localtime()), &quot; ---- Processed 1,000,000 Rows of data.&quot;)
                        count = 0
    print('Processing took:', int((time.time()-start_time)/60), ' minutes')
    output.close()
    csvfile.close()
</code></pre>
",1,0,907,2021-05-27 08:50:35,https://stackoverflow.com/questions/67718791/training-word2vec-model-from-sourced-data-issue-tokenizing-data
Does adding a list of Word2Vec embeddings give a meaningful represenation?,"<p>I'm using a pre-trained word2vec model (word2vec-google-news-300) to get the embeddings for a given list of words. Please note that this is NOT a list of words that we get after tokenizing a  sentence, it is just a list of words that describe a given image.</p>
<p>Now I'd like to get a single vector representation for the entire list. Does adding all the individual word embeddings make sense? Or should I consider averaging?
Also, I would like the vector to be of a constant size so concatenating the embeddings is not an option.</p>
<p>It would be really helpful if someone can explain the intuition behind considering either one of the above approaches.</p>
","nlp, word2vec, embedding, language-model","<p>Averaging is most typical, when someone is looking for a super-simple way to turn a bag-of-words into a single fixed-length vector.</p>
<p>You could try a simple sum, as well.</p>
<p>But note that the key difference between the sum and average is that the average divides by the number of input vectors. Thus they both result in a vector that's pointing in the exact same 'direction', just of different magnitude. And, the most-often-used way of comparing such vectors, cosine-similarity, is oblivious to magnitudes. So for a lot of cosine-similarity-based ways of later comparing the vectors, sum-vs-average will give identical results.</p>
<p>On the other hand, if you're comparing the vectors in other ways, like via euclidean-distances, or feeding them into other classifiers, sum-vs-average could make a difference.</p>
<p>Similarly, some might try unit-length-normalizing all vectors before use in any comparisons. After such a pre-use normalization, then:</p>
<ul>
<li>euclidean-distance (smallest to largest) &amp; cosine-similarity (largest-to-smallest) will generate identical lists of nearest-neighbors</li>
<li>average-vs-sum will result in different ending directions - as the unit-normalization will have upped some vectors' magnitudes, and lowered others, changing their relative contributions to the average.</li>
</ul>
<p>What <em>should</em> you do? There's no universally right answer - depending on your dataset &amp; goals, &amp; the ways your downstream steps use the vectors, different choices might offer slight advantages in whatever final quality/desirability evaluation you perform. So it's common to try a few different permutations, along with varying other parameters.</p>
<p>Separately:</p>
<ul>
<li>The <code>GoogleNews</code> vectors were trained on news articles back around 2013; their word senses thus may not be optimal for an image-labeling task. If you have enough of your own data, or can collect it, training your own word-vectors might result in better results. (Both the use of domain-specific data, &amp; the ability to tune training parameters based on your own evaluations, could offer benefits - especially when your domain is unique, or the tokens aren't typical natural-language sentences.)</li>
<li>There are other ways to create a single summary vector for a run-of-tokens, not just arithmatical-combo-of-word-vectors. One that's a small variation on the word2vec algorithm often goes by the name <code>Doc2Vec</code> (or 'Paragraph Vector') - it may also be worth exploring.</li>
<li>There are also ways to compare bags-of-tokens, leveraging word-vectors, that <em>don't</em> collapse the bag-of-tokens to a single fixed-length vector 1st - and while they're more expensive to calculate, sometimes offer better pairwise similarity/distance results than simple cosine-similarity. One such alternate comparison is called &quot;Word Mover's Distance&quot; - at some point,, you may want to try that as well.</li>
</ul>
",1,0,421,2021-06-01 11:38:23,https://stackoverflow.com/questions/67788151/does-adding-a-list-of-word2vec-embeddings-give-a-meaningful-represenation
I applied W2V on ML Algorithms. It gives error of negative value for NB and gives 0.48 accuracy for for all the other algorithms. How come?,"<pre><code>from gensim.models import Word2Vec
import time
# Skip-gram model (sg = 1)
size = 1000
window = 3
min_count = 1
workers = 3
sg = 1

word2vec_model_file = 'word2vec_' + str(size) + '.model'
start_time = time.time()
stemmed_tokens = pd.Series(df['STEMMED_TOKENS']).values
# Train the Word2Vec Model
w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)
print(&quot;Time taken to train word2vec model: &quot; + str(time.time() - start_time))
w2v_model.save(word2vec_model_file)

</code></pre>
<p>This is the code I have written. I applied this file on all ML algorithms for binary classification but all algorithms gives same result 0.48. How does it possible ? ANd also this result is very poor compare to BERT and TFIDF scores.</p>
","python, logistic-regression, random-forest, word2vec, naivebayes","<p>A vector <code>size</code> of 1000 dimensions is very uncommon, and would require massive amounts of data to train. For example, the famous <code>GoogleNews</code> vectors were for 3 million words, trained on something like 100 billion corpus words - and still only 300 dimensions. Your <code>STEMMED_TOKENS</code> may not be enough data to justify 100-dimensional vectors, much less 300 or 1000.</p>
<p>A choice of <code>min_count=1</code> is a bad idea. This algorithm can't learn anything valuable from words that only appear a few times. Typically people get better results by discarding rare words entirely, as the default <code>min_count=5</code> will do. (If you have a lot of data, you're likely to <em>increase</em> this value to discard even more words.)</p>
<p>Are you examining the model's size or word-to-word results at all to ensure it's doing what you expect? Despite your colum being named <code>STEMMED_TOKENS</code>, I don't see any actual splitting-into-tokens, and the <code>Word2Vec</code> class expects each text to be a <em>list-of-strings</em>, <strong>not</strong> a string.</p>
<p>Finally, without seeing all your other choices for feeding word-vector-enriched data to your other classification steps, it is possible (likely even) that there are other errors there.</p>
<p>Given that a binary-classification model can always get at least 50% accuracy by simply classifying every example with whichever class is more common, any accuracy result less than 50% should immediately cause suspicions of major problems in your process like:</p>
<ul>
<li>misalignment of examples &amp; labels</li>
<li>insufficient/unrepresentative training data</li>
<li>some steps not running at all due to data-prep or invocation errors</li>
</ul>
",0,0,195,2021-06-02 08:46:32,https://stackoverflow.com/questions/67801844/i-applied-w2v-on-ml-algorithms-it-gives-error-of-negative-value-for-nb-and-give
How to access to FastText classifier pipeline?,"<p>As we know <code>Facebook</code>'s <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">FastText</a> is a great open-source, free, lightweight library which can be used for text classification. But here a problem is the pipeline seem to be end-to end black-box. Yes, we can change the hyper-parameters from these <a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">options</a> for setting training configuration. But I couldn't manage to find a way to access to the vector embedding it generates internally.</p>
<p>Actually I want to do some manipulation on the vector embedding - like introducing <code>tf-idf</code> weighting apart from these <code>word2vec</code> representations and another thing I want to to is oversampling using <code>SMOTE</code> which requires numerical representation. For these reasons I need to introduce my custom code in between the overall pipeline which seems to be inaccessible for me. How introduce custom steps in this pipeline?</p>
","machine-learning, nlp, pipeline, word2vec, fasttext","<p>The full source code is available:</p>
<p><a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText</a></p>
<p>So, you can make any changes or extensions you can imagine - if you're comfortable reading &amp; modifying its C++ source code. Nothing is hidden or inaccessible.</p>
<p>Note that both FastText, and its <code>supervised</code> classification mode, are chiefly conventions for training a shallow neural-network. It may not be helpful to think of it as a &quot;pipeline&quot; like in the architecture of other classifier libraries - as none of the internal interfaces use that sort of language or modular layout.</p>
<p>Specifically, if you get the gist of word2vec training, FastText classifier mode really just replaces attempted-predictions of neighboring (in-context-window) vocabulary words, with attempted-predictions of known labels instead.</p>
<p>For the sake of understanding FastText's relationship to other techniques, and potential aspects for further extension, I think it's useful to also review:</p>
<ul>
<li>this skeptical blog post comparing FastText to the much-earlier 'vowpal wabbit' tool: <a href=""https://nlpers.blogspot.com/2016/08/fast-easy-baseline-text-categorization.html"" rel=""nofollow noreferrer"">&quot;Fast &amp; easy baseline text categorization with vw&quot;</a></li>
<li>Facebook's far-less discussed extension of such vector-training for more generic categorical or numerical tasks, <a href=""https://github.com/facebookresearch/StarSpace"" rel=""nofollow noreferrer"">&quot;StarSpace&quot;</a></li>
</ul>
",2,0,610,2021-06-06 09:55:59,https://stackoverflow.com/questions/67857840/how-to-access-to-fasttext-classifier-pipeline
AttributeError: Can&#39;t get attribute on &lt;module &#39;gensim.models.word2vec&#39;,"<p>I use python 3.9.1 on macOS Big Sur with an M1 chip.
And, gensim is 4.0.1</p>
<p>I tried to use the pre-trained Word2Vec model and I ran the code below:</p>
<pre><code>from gensim.models.word2vec import Word2Vec

model_path = '/path/to/word2vec.gensim.model'

model = Word2Vec.load(model_path)
</code></pre>
<p>However, I got an error below:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-4c1c2f93fadb&gt; in &lt;module&gt;
      1 from gensim.models.word2vec import Word2Vec
      2 
----&gt; 3 model = Word2Vec.load(model_path)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1932                 &quot;Try loading older model using gensim-3.8.3, then re-saving, to restore &quot;
   1933                 &quot;compatibility with current code.&quot;)
-&gt; 1934             raise ae
   1935 
   1936     def _load_specials(self, *args, **kwargs):

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1920         &quot;&quot;&quot;
   1921         try:
-&gt; 1922             model = super(Word2Vec, cls).load(*args, **kwargs)
   1923             if not isinstance(model, Word2Vec):
   1924                 rethrow = True

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in load(cls, fname, mmap)
    484         compress, subname = SaveLoad._adapt_by_suffix(fname)
    485 
--&gt; 486         obj = unpickle(fname)
    487         obj._load_specials(fname, mmap, compress, subname)
    488         obj.add_lifecycle_event(&quot;loaded&quot;, fname=fname)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in unpickle(fname)
   1456     &quot;&quot;&quot;
   1457     with open(fname, 'rb') as f:
-&gt; 1458         return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()
   1459 
   1460 

AttributeError: Can't get attribute 'Vocab' on &lt;module 'gensim.models.word2vec' from '/Users//opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py'&gt;

</code></pre>
<p>here is the link where I got the this model
<a href=""https://github.com/shiroyagicorp/japanese-word2vec-model-builder"" rel=""nofollow noreferrer"">https://github.com/shiroyagicorp/japanese-word2vec-model-builder</a></p>
<p>Thank you in advance.</p>
","python, nlp, gensim, word2vec","<p>The problem is that the referenced repository trained a model on an <a href=""https://github.com/shiroyagicorp/japanese-word2vec-model-builder/blob/master/requirements.txt"" rel=""nofollow noreferrer"">incredibly old version of GenSim</a>, which makes it incompatible with current versions.</p>
<p>You can potentially check whether the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2VecTrainables.add_lifecycle_event"" rel=""nofollow noreferrer"">lifecycle meta data</a> gives you any indication on the actual version, and then try to update your model from there.
The documentation also gives some tips for <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility#upgrading-trained-models"" rel=""nofollow noreferrer"">upgrading your older trained models</a>, but even those are relatively weak and point mostly to re-training. Similarly, even <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">migrating from GenSim 3.X to 4.X</a> is not referencing direct upgrade methods, but could give you ideas on what parameters to look out for specifically.</p>
<p>My suggestion would be to try loading it with any of the previous 3.X versions, and see if you have more success loading it there.</p>
",2,0,1425,2021-06-07 03:56:42,https://stackoverflow.com/questions/67865773/attributeerror-cant-get-attribute-on-module-gensim-models-word2vec
What is the network structure inside a Tensorflow Embedding Layer?,"<p>Tensoflow Embedding Layer (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a>) is easy to use,
and there are massive articles talking about
&quot;how to use&quot; Embedding (<a href=""https://machinelearningmastery.com/what-are-word-embeddings/"" rel=""noreferrer"">https://machinelearningmastery.com/what-are-word-embeddings/</a>, <a href=""https://www.sciencedirect.com/topics/computer-science/embedding-method"" rel=""noreferrer"">https://www.sciencedirect.com/topics/computer-science/embedding-method</a>)
.
However, I want to know the Implemention of the very &quot;Embedding Layer&quot; in Tensorflow or Pytorch.
Is it a word2vec?
Is it a Cbow?
Is it a special Dense Layer?</p>
","tensorflow, word2vec, embedding","<p>Structure wise, both <code>Dense</code> layer and <code>Embedding</code> layer are hidden layers with neurons in it. The difference is in the way they operate on the given inputs and weight matrix.</p>
<p>A <code>Dense</code> layer performs operations on the weight matrix given to it by multiplying inputs to it ,adding biases to it and applying activation function to it. Whereas <code>Embedding</code> layer uses the weight matrix as a look-up dictionary.</p>
<p>The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. Itâ€™s effectively a dictionary lookup.</p>
<pre><code>from keras.layers import Embedding

embedding_layer = Embedding(1000, 64)
</code></pre>
<p>Here 1000 means the number of words in the dictionary and 64 means the dimensions of those words. Intuitively, embedding layer just like any other layer will try to find vector (real numbers) of 64 dimensions <code>[ n1, n2, ..., n64]</code> for any word. This vector will represent the semantic meaning of that particular word. It will learn this vector while training using backpropagation just like any other layer.</p>
<blockquote>
<p>When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structureâ€”a kind of structure specialized for the specific problem for which youâ€™re training your model.</p>
</blockquote>
<p>-- <em>Deep Learning with Python by F. Chollet</em></p>
<hr />
<p>Edit - <strong>How &quot;Backpropagation&quot; is used to train the look-up matrix of the <code>Embedding Layer</code> ?</strong></p>
<p><code>Embedding</code> layer is similar to the linear layer without any activation function. Theoretically, <code>Embedding</code> layer also performs matrix multiplication but doesn't add any non-linearity to it by using any kind of activation function. So backpropagation in the <code>Embedding</code> layer is similar to as of any linear layer. But practically, we don't do any matrix multiplication in the embedding layer because the inputs are generally one hot encoded and the matrix multiplication of weights by a one-hot encoded vector is as easy as a look-up.</p>
",11,9,3055,2021-06-09 03:03:49,https://stackoverflow.com/questions/67896966/what-is-the-network-structure-inside-a-tensorflow-embedding-layer
Ploting function word2vec Error &#39;Word2Vec&#39; object is not subscriptable,"<p>I have my word2vec model.
I can use it in order to see the most similars words.
Now i create a function in order to plot the word as vector.
Here my function :</p>
<pre><code>def tsne_plot(model):

    labels = []
    tokens = []

    for word in model.wv.key_to_index:
        tokens.append(model[word])
        labels.append(word)
    
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
for value in new_values:
    x.append(value[0])
    y.append(value[1])
    
plt.figure(figsize=(16, 16)) 
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
plt.show()
</code></pre>
<p>When i call the function, I have the following error :</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-47-d0f4ea6902bf&gt; in &lt;module&gt;
----&gt; 1 tsne_plot(model)

&lt;ipython-input-46-b4714ffe935b&gt; in tsne_plot(model)
      5 
      6     for word in model.wv.key_to_index:
----&gt; 7         tokens.append(model[word])
      8         labels.append(word)
      9 

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
<p>I really don't how to remove this error. I think it's maybe because the newest version of Gensim do not use array [...].
Thanks for advance !</p>
","python, gensim, word2vec","<p>In Gensim 4.0, the <code>Word2Vec</code> object itself is no longer directly-subscriptable to access each word. Instead, you should access words via its subsidiary <code>.wv</code> attribute, which holds an object of type <code>KeyedVectors</code>.</p>
<p>So, replace <code>model[word]</code> with <code>model.wv[word]</code>, and you should be good to go.</p>
",9,1,3451,2021-06-10 14:04:46,https://stackoverflow.com/questions/67922777/ploting-function-word2vec-error-word2vec-object-is-not-subscriptable
How to use Word2Vec CBOW in statistical algorithm?,"<p>I have seen few examples of using CBOW in Neural Networks models (although I did not understand it)</p>
<p>I know that Word2Vec is not similar to BOW or TFIDF, as there is no single value for CBOW</p>
<p>and all examples I saw were using Neural Network.</p>
<p>I have 2 questions</p>
<p>1- Can we convert the vector to a single value and put it in a dataframe so we can use it in Logistic Regression Model?</p>
<p>2- Is there any simple code for CBOW usage with logistic Regression?</p>
<p>More Explanation.</p>
<p>In my case, I have a corpus that I want to make a comparison between top features in BOW and CBOW</p>
<p>after converting to BOW</p>
<p>I get this dataset</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       5     3     8       2       0 
2       0       1     0     0       6       9
3       1       4     1     5       1       7 
</code></pre>
<p>after converting to TFIDF</p>
<p>I get this dataset</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       0.38     0.42    0.02    0.22   0.00   0.19
2       0       0.75     0.20    0.08    0.12   0.37   0.21
3       1       0.17     0.84    0.88    0.11   0.07   0.44
</code></pre>
<p>I am observing the results of top 3 features in each model</p>
<p>so my dataset become like this</p>
<p>BOW (I put null here for the values that will be omitted)</p>
<pre><code>RepID   Label    Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1        5      null    8    null   null   7
2       0        null   null    null    6   9   2
3       1        4      null    5    null   7   null
</code></pre>
<p>TFIDF (I put null here for the values that will be omitted)</p>
<pre><code>RepID   Label   Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1       0.38     0.42    null    0.22   null   null
2       0       0.75     null    null    null   0.37   0.21
3       1       null     0.84    0.88    null   null   0.44
</code></pre>
<p>I want now to do the same with Word2Ven CBOW</p>
<p>I want to take the highest values in the CBOW model</p>
<pre><code>RepID   Label  Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1      v11     v12    v13    v14   v15   v16
2       0      v21     v22    v23    v24   v25   v26
3       1      v31     v32    v33    v34   v35   v36
</code></pre>
<p>to be like this</p>
<pre><code>RepID   Label    Cat   Dog   Snake   Rabbit  Apple Orange  ...
1       1        v11     null    v13    null   v15   null
2       0        null     null    v23    null   v25   v26
3       1        v31     null    v33    v34   null   null
</code></pre>
","nlp, logistic-regression, word2vec","<p>No matter the internal training method, CBOW or skip-gram, a word-vector is always a multidimensional vector: it contains many floating-point numbers.</p>
<p>So at one level, that is one &quot;value&quot; - where the &quot;value&quot; is a vector. But it's never a single number.</p>
<p>Word-vectors, even with all their dimensions, can absolutely serve as inputs for a downstream logistic regression task. But the exact particulars depend on exactly what data you're operating on, and what you intend to achieve - so you may want to expand your question, or ask a more specific followup, with more info about the specific data/task you're considering.</p>
<p>Note also: this is done more often with the pipeline of a library like <code>scikit-learn</code>. Putting dense high-dimensional word-vectors themselves (or other features derived from word-vectors) directly into &quot;dataframes&quot; is often a mistake, adding overhead &amp; indirection compared to working with such large feature-vectors in their more compact/raw format of (say) <code>numpy</code> arrays.</p>
",0,0,111,2021-06-12 18:32:31,https://stackoverflow.com/questions/67951891/how-to-use-word2vec-cbow-in-statistical-algorithm
What Metrics Are Used in the Output of Gensim&#39;s evaluate_word_pairs?,"<p>Gensim offers evaluate_word_pairs function for <a href=""https://radimrehurek.com/gensim_4.0.0/auto_examples/tutorials/run_word2vec.html#evaluating"" rel=""nofollow noreferrer"">evaluating semantic similarity</a>.</p>
<p>Here is an example from its page:</p>
<pre><code>model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

Out:
((0.1014236962315867, 0.44065378924434523), SpearmanrResult(correlation=0.07441989763914543, pvalue=0.5719973648460552), 83.0028328611898)
</code></pre>
<p>I would like to know what metrics are used to generate each value(0.1014236962315867, 0.44065378924434523,...) in the output?</p>
","gensim, word2vec, similarity","<p>Per the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">documentation for <code>evaluate_word_pairs()</code></a>:</p>
<blockquote>
<p><strong>Returns</strong></p>
<ul>
<li><strong>pearson</strong> <em>(tuple of (float, float))</em> â€“ Pearson correlation coefficient with 2-tailed p-value.</li>
<li><strong>spearman</strong> <em>(tuple of (float, float))</em> â€“ Spearman rank-order correlation coefficient between the similarities from the dataset and the similarities produced by the model itself, with 2-tailed p-value.</li>
<li><strong>oov_ratio</strong> <em>(float)</em> â€“ The ratio of pairs with unknown words.</li>
</ul>
</blockquote>
<p>Per your output, it looks like the Pearson result is still just a plain tuple, while the Spearman result has been reported as a named tuple. But in each case, it appears the correlation-coefficient is 1st, then the p-value.</p>
<p>Note that the <code>oov_ratio</code> is reported as the <em>percentage</em> of test words that weren't known to the model.</p>
<p>Consult other references for definitions/explanations of the <a href=""https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"" rel=""nofollow noreferrer"">Pearson</a> and <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman</a> coefficients/p-values.</p>
",1,0,391,2021-06-17 17:16:19,https://stackoverflow.com/questions/68023855/what-metrics-are-used-in-the-output-of-gensims-evaluate-word-pairs
Gensim: Is doc2vec a model or operation? Differences from R implementation,"<p>I have been tasked with putting a document vector model into production.
I am an R user, and so my original model is in R. One of the avenues we have is to recreate the code and the models in Python.</p>
<p><strong>I am confused by the Gensim implementation of Doc2vec</strong>.</p>
<p>The process that works in R goes like this:</p>
<p><strong>Offline</strong></p>
<hr />
<ul>
<li><p>Word vectors are trained using the functions in the <code>text2vec</code> package, namely GloVe or GlobalVectors, on a large corpus This gives me a large Word Vector text file.</p>
</li>
<li><p>Before the ML step takes place, the <code>Doc2Vec</code> function from the <code>TextTinyR</code> library is used to turn each piece of text from a smaller, more specific training corpus into a vector. <em>This is not a machine learning step. No model is trained</em>. The Doc2Vec function effectively aggregates the word vectors in the sentence, in the same sense that finding the sum or mean of vectors does, but in a way that preserves information about word order.</p>
</li>
<li><p>Various models are then trained on these smaller text corpuses.</p>
</li>
</ul>
<hr />
<p><strong>Online</strong></p>
<hr />
<ul>
<li>The new text is converted to Document Vectors using the pretrained word vectors.</li>
<li>The Document Vectors are fed into the pretrained model to obtain the output classification.</li>
</ul>
<hr />
<p><strong>The example code I have found for Gensim appears to be a radical departure from this.</strong></p>
<p>It appears in <code>gensim</code> that Doc vectors are a separate class of model from word vectors that you can train. It seems in some cases, the word vectors and doc vectors are all trained at once. Here are some examples from tutorials and stackoverflow answers:</p>
<p><a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5</a></p>
<p><a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors"">How to use Gensim doc2vec with pre-trained word vectors?</a></p>
<p><a href=""https://stackoverflow.com/questions/36815038/how-to-load-pre-trained-model-with-in-gensim-and-train-doc2vec-with-it?rq=1"">How to load pre-trained model with in gensim and train doc2vec with it?</a></p>
<p><a href=""https://stackoverflow.com/questions/45037860/gensim1-0-1-doc2vec-with-google-pretrained-vectors?noredirect=1&amp;lq=1"">gensim(1.0.1) Doc2Vec with google pretrained vectors</a></p>
<p>So my questions are these:</p>
<p><strong>Is the gensim implementation of Doc2Vec fundamentally different from the TextTinyR implementation?</strong></p>
<p><strong>Or is the gensim doc2vec model basically just encapsulating the word2vec model and the doc2vec process into a single object?</strong></p>
<p><strong>Is there anything else I'm missing about the process?</strong></p>
","python, r, gensim, word2vec, doc2vec","<p>In R, you can use text2vec (<a href=""https://cran.r-project.org/package=text2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=text2vec</a>) to train Glove embeddings, word2vec (<a href=""https://cran.r-project.org/package=word2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=word2vec</a>) to train word2vec embeddings or train fasttext embeddings (<a href=""https://cran.r-project.org/package=fastText"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=fastText</a> / <a href=""https://cran.r-project.org/package=fastTextR"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=fastTextR</a>). You can aggregate these embeddings to the document level by just taking e.g. the average of the words or relevant nouns/adjectives (if you tag the text using udpipe (<a href=""https://cran.r-project.org/package=udpipe"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=udpipe</a>) or use the approach from R package TextTinyR  (<a href=""https://cran.r-project.org/package=TextTinyR"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=TextTinyR</a>) which provides 3 other agrregation options: sum_sqrt / min_max_norm / idf</p>
<p>R package doc2vec (<a href=""https://cran.r-project.org/package=doc2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=doc2vec</a>) allows one to train paragraph vector embeddings (PV-DBOW / PV-DM in Gensim terminology) which is not just averaging of word vectors but trains a specific model (e.g. see <a href=""https://www.bnosac.be/index.php/blog/103-doc2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/103-doc2vec-in-r</a>).
ruimtehol (<a href=""https://cran.r-project.org/package=ruimtehol"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=ruimtehol</a>) allows to train Starspace embeddings which has the option of training sentence embeddings as well</p>
",2,1,697,2021-06-17 20:09:41,https://stackoverflow.com/questions/68025964/gensim-is-doc2vec-a-model-or-operation-differences-from-r-implementation
Why similar words will be close to each other under a word2vec model?,"<p>One of the tasks can be done with a word2vec model is to find most similar words for a give word using cosine similarity. What is the intuitive explanation for why similar words under a good word2vec model will be close to each other in the space?</p>
","gensim, word2vec, similarity, cosine-similarity","<p>The model is essentially performing compression.</p>
<p>It has to force a large number of distinct words (such as 100,000 or millions) into vectors of a much smaller number of dimensions (such as 100 or 300).</p>
<p>Necessarily, then, a simple &quot;one-hot&quot; encoding, where every word lights up one dimension entirely (<code>1.0</code>), and others not at all (<code>0.0</code>) is not possible. The words <em>must</em> use mixes of values across the small number of dimensions.</p>
<p>In order for such a model to become better on its training task â€“ predicting nearby words â€“ words that often have similar neighbors will grow to have similar coordinates. That allows a 'neighborhood' of the coordinate space to generate similar predictions.</p>
<p>But also, the remaining ways in which those words should predict <em>different</em> neighbors will also tend to be reflected in gradual coordinate changes (directions or neighborhoods).</p>
<p>For example, consider the case of polysemy. The words 'vault' &amp; 'bank' will both appear around 'money' (&amp; related concepts) - but 'bank' in another sense will also appear around 'river' &amp; 'water' (&amp; related concepts). So training in many contexts nudges 'bank' &amp; 'vault' to be more alike (to generate shared 'money'-related predictions). But then other interleaved example usage contexts then nudge 'bank' towards 'river'/'water'.</p>
<p>The net effect of these various tug-of-war backpropagation updates, by the end of sufficient &amp; well-parameterized training, tends to arrange the words in interesting ways - where similar words are nearer, and even certain directions/neighborhoods are vaguely associated with human-describable concepts. (For example, &quot;that direction&quot; might vaguely shift concepts towards money, &quot;this other direction&quot; in terms of gendered concepts, &quot;this 3rd direction&quot; in terms of colors, etc. - though such directions are necessarily fuzzy, imbued by patterns in the training data, can vary from run-to-run, and are not neatly aligned with each dimensions' perpendicula axes.)</p>
<p>Most generally: words that must, for model-optimization purposes, predict the same neighbor words tend to acqire similar coordinates. Their remaining differences are correlated with which neighboring words they need to predict diferently.</p>
",2,1,647,2021-06-22 19:21:57,https://stackoverflow.com/questions/68089626/why-similar-words-will-be-close-to-each-other-under-a-word2vec-model
Ask About The &quot;default&quot; Size of Vocabulary in Word2Vec in Deeplearning4j Library,"<p>I am currently learning about this library: Word2Vec from Deeplearning4j (<a href=""https://deeplearning4j.konduit.ai/language-processing/word2vec"" rel=""nofollow noreferrer"">Homepage</a>, <a href=""https://github.com/breandan/deep-learning-samples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java"" rel=""nofollow noreferrer"">Github</a>)</p>
<p>Following is the example usage of the method:</p>
<pre><code>//build Word2Vec model
Word2Vec vec = new Word2Vec.Builder()
                .layerSize(100)
                .windowSize(5)
                .stopWords(stopList)
                .tokenizerFactory(t)
                .learningRate(0.025)
                .build();
</code></pre>
<p>I know that I can limit the vocabulary size with this method:</p>
<pre><code>vec.limitVocabularySize(100) //limit the vocab size as 100
</code></pre>
<p>Above example is the command if I want to limit the vocab size into 100</p>
<p><strong>My question:</strong> <br/>
Could anyone inform me what is the <em>default</em> size of the vocab (i.e., if I do not set the limit)?</p>
<p>Best,</p>
","java, word2vec, deeplearning4j","<p>By default there is no limit. That means it will add all words it finds to the vocabulary.</p>
<p>Also note, the examples you linked to are over 4 years old. I suggest you use the official examples: <a href=""https://github.com/eclipse/deeplearning4j-examples"" rel=""nofollow noreferrer"">https://github.com/eclipse/deeplearning4j-examples</a></p>
",0,1,102,2021-06-29 14:57:49,https://stackoverflow.com/questions/68181203/ask-about-the-default-size-of-vocabulary-in-word2vec-in-deeplearning4j-library
Graph-based representation for land borders,"<p>I'm trying to get the 2D vectors from a set of countries. I've built my graph by the following process (see the picture):</p>
<ul>
<li>each node represents a country</li>
<li>each edge represents the land border between 2 countries (or nodes)</li>
</ul>
<p><a href=""https://i.sstatic.net/Efwpim.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Efwpim.png"" alt=""Graph representation of land borders"" /></a></p>
<p>I'm using Node2vec library to manage it but results are not relevant.</p>
<pre><code>countries = [
    &quot;France&quot;, &quot;Andorra&quot;, 
    &quot;Spain&quot;, &quot;Italy&quot;, &quot;Switzerland&quot;, 
    &quot;Germany&quot;, &quot;Portugal&quot;
]

crossing_borders = [
    (&quot;France&quot;, &quot;Andorra&quot;),
    (&quot;France&quot;, &quot;Spain&quot;),
    (&quot;Andorra&quot;, &quot;Spain&quot;),
    (&quot;France&quot;, &quot;Italy&quot;),
    (&quot;France&quot;, &quot;Switzerland&quot;),
    (&quot;Italy&quot;, &quot;Switzerland&quot;),
    (&quot;Switzerland&quot;, &quot;Italy&quot;),
    (&quot;Switzerland&quot;, &quot;Germany&quot;),
    (&quot;France&quot;, &quot;Germany&quot;),
    (&quot;Spain&quot;, &quot;Portugal&quot;)
]

graph.add_nodes_from(countries)
graph.add_edges_from(crossing_borders)

# Generate walks
node2vec = Node2Vec(graph, dimensions=2, walk_length=2, num_walks=50)

# Learn embeddings 
model = node2vec.fit(window=1)
</code></pre>
<p><a href=""https://i.sstatic.net/Tcqze.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tcqze.png"" alt=""enter image description here"" /></a></p>
<p>I would like to get countries which are sharing the land border closer each other. As below, Spain is too far from France. I only considered direct border that's why <code>walk-length = 2</code>.</p>
<p>Do you have any idea that would fit my problem ?</p>
","machine-learning, graph, deep-learning, word2vec","<p>If I understand correctly, <code>Node2Vec</code> is basd on word2Vec, and thus like word2vec, requires a large amount of varied training data, and shows useful results when learning dense high-dimensional vectors per entity.</p>
<p>A mere 7 'words' (country-nodes) with a mere 10 'sentences' of 2 words each (edge-pairs) thus isn't expecially likely to do anything useful. (It wouldn't in word2vec.)</p>
<p>These countries literally are regions on a sphere. A sphere's surface can be mapped to a 2-D plane - hence, 'maps'. If you just want a 2-D vector for each country, which reflects their relative border/distance relationships, why not just lay your 2-D coordinates over an actual map large enough to show all the countries, and treat each country as its 'geographical center' point?</p>
<p>Or more formally: translate the x-longitude/y-latitude of each country's geographical center into whatever origin-point/scale you need.</p>
<p>If this simple, physically-grounded approach is inadequate, then being explicit about why it's inadequate might suggest next steps. Something that's an incremental transformation of those starting points to meet whatever <em>extra</em> constraints you want may be the best solution.</p>
<p>For example, if your not-yet-stated formal goal is that &quot;every country-pair with an actual border should be closer than any country-pair without a border&quot;, then you could write code to check that, list any deviations, and try to 'nudge' the deviations to be more compliant with that constraint. (It might not be satisfiable; I'm not sure. And if you added other constraints, like &quot;any country pair with just 1 country between them should be closer than any country pair with 2 countries between them&quot;, satisfying them all at once could become harder.)</p>
<p>Ultimately, next steps may depend on exactly <em>why</em> you want these per-country vectors.</p>
<p>Another thing worth checking out might be the algorithms behind 'force-directed graphs'. There, after specifying a graph's desired edges/edge-lengths, and some other parameters, a physics-inspired simulation will arrive at some 2-d layout that tries to satisfy the inputs. See for example from the JS world:</p>
<p><a href=""https://github.com/d3/d3-force"" rel=""nofollow noreferrer"">https://github.com/d3/d3-force</a></p>
",1,0,89,2021-06-30 12:33:37,https://stackoverflow.com/questions/68194848/graph-based-representation-for-land-borders
Gensim word2vec and large amount of texts,"<p>I need to put the texts contained in a column of a MySQL database (about 3 million rows) into a list of lists of tokens. These texts (which are tweets, therefore they are generally short) must be preprocessed before being included in the list (stop words, hashtags, tags etc. must be removed). This list should be passed later as a <code>Word2Vec</code> parameter. This is the part of the code involved</p>
<pre><code>import mysql.connector
import re
from gensim.models import Word2Vec
import preprocessor as p
p.set_options(
    p.OPT.URL,
    p.OPT.MENTION,
    p.OPT.HASHTAG,
    p.OPT.NUMBER
)

conn = mysql.connector.connect(...)
cursor = conn.cursor()
query = &quot;SELECT text FROM tweet&quot;
cursor.execute(query)
table = cursor.fetchall()

stopwords = open('stopwords.txt', encoding='utf-8').read().split('\n')
sentences = []
for row in table:
    sentences = sentences + [[w for w in re.sub(r'[^\w\s-]', ' ', p.clean(row[0])).lower().split() if w not in stopwords and len(w) &gt; 2]]

cursor.close()
conn.close()

model = Word2Vec(sentences)
...
</code></pre>
<p>Obviously it takes a lot of time and I know that my method is probably inefficient. Can anyone recommend a better one? I know it is not a question directly related to <code>gensim</code> and <code>Word2Vec</code> but perhaps those who use them have already faced the problem of working with a large amount of texts.</p>
","python, nlp, gensim, word2vec","<p>You haven't mentioned how long your code takes to run, but some potential sources of slowdown in your current technique might include:</p>
<ul>
<li>the overhead of regex-based preprocessing, especially if a large number of independent regexes are each applied, separately, to the same texts</li>
<li>the inefficiency of expanding a Python list by appending one new item at a time - which as the list grows larger can sometimes be a factor</li>
<li>virtual-memory swapping, if the size of your data exceeds physical RAM</li>
</ul>
<p>You can check the swapping issue by monitoring memory use using a platform-specific tool (like <code>top</code> on Linux systems) to view memory usage during the operation. If that's a contributor, using a machine with more RAM, or making other code changes to reduce RAM usage (see below), will help.</p>
<p>Your full <code>prprocessing</code> code isn't shown, but a common  approach is a lot of independent steps, each of which involves one or more regular-expressions, but then returns a plain modified string (for future steps).</p>
<p>As appealingly simple &amp; pluggable as that is, it often becomes a source of avoidable slowness in preprocessing large amounts of text. For example, each regex/step itself might have to repeat detecting token-boundaries, or splitting then re-concatenating a string. Or, the regexes might use complex match patterns, or techniques (like backtracking) that can be expensive on worst-case inputs.</p>
<p>Often this sort of preprocessing can be greatly improved by one or more of:</p>
<ul>
<li>coalescing multiple regexes into a single step, so a string faces one front-to-back pass, rather than N</li>
<li>breaking into short tokens early, then leaving the text as a list-of-tokens for later steps - thus never redundantly splitting/joining, and letting later token-oriented steps to work on smaller strings and perhaps even simpler (non-regex) string-tests</li>
</ul>
<p>Also, even if the preprocessing is still a bit time-consuming, a big process improvement is usually to be sure to <em>only repeat it when the data changes</em>. That is, if you're going to try a bunch of different downstream steps, like different <code>Word2Vec</code> parameters, make sure you're not doing the expensive preprocessing every time. Do it once, write the results aside to a file, then reuse the results file until it needs to be regenerated (because the data or preprocessing rules have changed).</p>
<p>Finally, if the append-one-more pattern is contributing to your slowness, you could pre-allocate your <code>sentences</code> (<code>sentences = [Null,] * desired_length</code>), then replace each row in your loop rather than append (<code>sentences[row_num] = preprocessed_text</code>). But that might not be a major factor, and in fact the suggestion above, about &quot;reuse the results file&quot;, is a better way to minimize list-ops/RAM-usage, as well as enable reuse across alternate runs.</p>
<p>That is, open a new working file before your loop. Append each preprocessed text â€“Â with spaces between the tokens, and a newline at the end â€“ as one new line to this file. Then, have your <code>Word2Vec</code> step work directly from that file. (In Gensim, you can do this by wrapping the file with a <code>LineSentence</code> utility object, which reads a file of that format back as a re-iterable sequence, with each item being a list-of-tokens, <em>or</em> by using the <code>corpus_file</code> parameter to feed the filename directly to <code>Word2Vec</code>.)</p>
<p>From that list of possible tactics, I'd try:</p>
<ul>
<li>First, time your existing code for preprocessing (creating your <code>sentences</code></li>
<li>Then, <em>eliminate all fancy preprocessing</em>, doing nothing more complicated than <code>.split()</code>, and re-time. If there's a big change, then yes, the preprocessing is the major slowdown, and concentrate on improving that.</li>
<li>If even that minimal preprocessing still seems slower-than-desired, then maybe the RAM/concatenation issues are a concern, and try writing to an interim file.</li>
</ul>
<p>Separately: it's not strictly necessary to worry about removing stop-words in word2vec training - much published work doesn't bother with that step, and the algorithm already includes a <code>sample</code> parameter which causes it to skip a lot of the very-overrepresented words during training as less-interesting. Similarly, 2- and even 1- character tokens may still be interesting, especially in the domain of tweets, so you might not want to always discard them. (For example, lone emoji can be significant 'words'.)</p>
",2,0,434,2021-07-01 13:02:47,https://stackoverflow.com/questions/68210732/gensim-word2vec-and-large-amount-of-texts
Gensim Doc2Vec model returns different cosine similarity depending on the dataset,"<p>I trained two versions of doc2vec models with two datasets.</p>
<p>The first dataset was made with 2400 documents and the second one was made with 3000 documents including the documents which were used in the first dataset.</p>
<p>For an example,</p>
<p>dataset 1 = doc1, doc2, ... doc2400</p>
<p>dataset 2 = doc1, doc2, ... doc2400, doc2401, ... doc3000</p>
<p>I thought that both doc2vec models should return the same similarity score between doc1 and doc2, however, they returned different scores.</p>
<p>Does doc2vec model's result change upon the datasets even they include the same documents?</p>
","gensim, word2vec, doc2vec","<p>Yes, any addition to the training set will change the relative results.</p>
<p>Further, as explained in the Gensim FAQ, even re-training with the exact same data will typically result in different end coordinates for each training doc, though each run should be about equivalently useful:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism</a></p>
<p>What should remain roughly the same between runs is the <em>neighborhoods</em> around each document. That is, adding some extra training docs shouldn't change the general result that some candidate doc is &quot;very close&quot; or &quot;closer than other docs&quot; to some target doc - except to the extent that (1) the new docs might include some even-closer docs; and (2) a small amount of 'jitter' between runs, per the FAQ answer above.</p>
<p>If in fact you see <em>lots</em> of change in the relative neighborhoods and top-N neighbors of a document, either in repeated runs or runs with small increments of extra data, there's possibly something else wrong in the training.</p>
<p>In particular, 2400 docs is a pretty small dataset for <code>Doc2Vec</code> - smaller datasets might need smaller <code>vector_size</code> and/or more <code>epochs</code> and/or other tweaks to get more reliable results, and even then, might not show off the strengths of this algorithm on larger (tens-of-thousands to millions of docs) datasets.</p>
",0,0,258,2021-07-02 01:17:29,https://stackoverflow.com/questions/68218550/gensim-doc2vec-model-returns-different-cosine-similarity-depending-on-the-datase
Training SVM classifier (word embeddings vs. sentence embeddings),"<p>I want to experiment with different embeddings such Word2Vec, ELMo, and BERT but I'm a little confused about whether to use the word embeddings or sentence embeddings, and why. I'm using the embeddings as features input to SVM classifier.</p>
<p>Thank you.</p>
","svm, word2vec, bert-language-model, word-embedding, elmo","<p>Though both approaches can prove efficient for different datasets, as a rule of thumb I would advice you to use word embeddings when your input is of a few words, and sentence embeddings when your input in longer (e.g. large paragraphs).</p>
",2,4,754,2021-07-02 12:22:12,https://stackoverflow.com/questions/68225126/training-svm-classifier-word-embeddings-vs-sentence-embeddings
Set the parameters of Word2Vec for a practical example,"<p>I have a database containing about 2.8 million texts (more precisely tweets, so they are short texts). I put clean tweets (removing hashtags, tags, stop words...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each tweet).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>This was just an attempt, I would need some help to set the parameters (<code>size</code>, <code>window</code>, <code>min_count</code>, <code>workers</code>, <code>sg</code>) of <code>Word2Vec</code> in the most appropriate and consistent way.</p>
<p>Consider that my goal is to use</p>
<p><code>model.most_similar(terms)</code> (where <code>terms</code> is a list of words)</p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>The words in <code>terms</code> belong to the same topic and I would like to see if there are other words within the texts that could have to do with the topic.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Generally, the usual approach is:</p>
<ul>
<li>Start with the defaults, to get things initially working at a baseline level, perhaps only on a faster-to-work-with subset of the data.</li>
<li>Develop an objective way to determine whether one model is better than another, for your purposes. This might start as a bunch of ad hoc, manual comparisons of results for some representative probes - but <em>should</em> become a process that can automatically score each variant model, giving a higher score to the 'better' model according to some qualitative, repeatable process.</li>
<li>Either tinker with parameters one-by-one, or run a large search over many permutations, to find which model does best on your scoring.</li>
</ul>
<p>Separately: the quality of word2vec results is almost always improved by discarding the very rarest words, such as those appearing only once. (The default value of <code>min_count</code> is <code>5</code> for good reason.)</p>
<p>The algorithm can't make good word-vectors from words that only appear once, or a few times. It <em>needs</em> multiple, contrasting examples of its usage. But, given the typical Zipfian distribution of word usages in a corpus, there are a lot of such rare words. Discarding them speeds training, shrinks the model, &amp; eliminates what's essentially 'noise' from the training of other words - leaving those remaining word-vectors much better. (If you really need vectors for such words â€“ gather more data.)</p>
",2,1,1343,2021-07-02 12:56:58,https://stackoverflow.com/questions/68225624/set-the-parameters-of-word2vec-for-a-practical-example
Finding word similarities along the output or input vectors using gensim word2vec?,"<p>I know that you can use model.wv.most_similar(...) to get words by cosine similarity in gensim.</p>
<p>I also know gensim gives you input and output vectors in e.g. model.syn0 and model.syn1neg.</p>
<p>Is there a simple way to calculate cosine similarity and create a list of most similar using <strong>only</strong> the input or output vectors, one or the other? E.g. I want to try doing it using <strong>just</strong> output vectors.</p>
",word2vec,"<p>There's no built-in facility, but I believe you could achieve it by creating a separate <code>KeyedVectors</code> instance where you replace the usual ('input projection'/'syn0') vector-array with the same-sized output array (as exists in negative-sampling models).</p>
<p>Roughly the following may work:</p>
<pre class=""lang-py prettyprint-override""><code>full_w2v_model = ... # whatever training/loading is necessary
full_w2v_model.wv.save_word2vec_format(PATH)  # saves just the word-vectors
out_vecs = KeyedVectors.load_word2vec_format(PATH)  # reloads as separate object
out_vecs.vectors = full_w2v_model.syn1neg  # clobber raw vecs with model's output layer
</code></pre>
<p>(Let me know if this works as-is or neds some further touch-up to work!)</p>
",0,0,218,2021-07-04 02:10:43,https://stackoverflow.com/questions/68241170/finding-word-similarities-along-the-output-or-input-vectors-using-gensim-word2ve
Fine-tuning pre-trained Word2Vec model with Gensim 4.0,"<p>With Gensim &lt; 4.0, we can retrain a word2vec model using the following code:</p>
<pre><code>model = Word2Vec.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
model.train(my_corpus, total_examples=len(my_corpus), epochs=model.epochs)
</code></pre>
<p>However, what I understand is that Gensim 4.0 is no longer supporting <code>Word2Vec.load_word2vec_format</code>. Instead, I can only load the keyedVectors.</p>
<p>How to fine-tune a pre-trained word2vec model (such as the model trained on GoogleNews) with my domain-specific corpus using Gensim 4.0?</p>
","gensim, word2vec, transfer-learning, pre-trained-model","<p>I don't think that code would've ever have worked in Gensim versions before 4.0. A plain list-of-word-vectors, like <code>GoogleNews-vectors-negative300.bin</code>, does not (&amp; never has) had enough info to continue training.</p>
<p>It's missing the hidden-to-output layer weights &amp; word-frequency info essential for training.</p>
<p>Looking at past source code, as of release 1.0.0 (February 2017), that code wouldn've already given a <a href=""https://github.com/RaRe-Technologies/gensim/blob/1.0.0/gensim/models/word2vec.py#L1304"" rel=""nofollow noreferrer"">deprecation-error with a pointer to the method for loading a plain set-of-word-vectors</a> - to address people with the mistaken notion that could work â€“ and <a href=""https://github.com/RaRe-Technologies/gensim/blob/1.0.0/gensim/models/word2vec.py#L792"" rel=""nofollow noreferrer"">raised other errors on any attempts to <code>train()</code> such a model</a>. (Pre-1.0.0, docs also <a href=""https://github.com/RaRe-Technologies/gensim/blob/0.13.4.1/gensim/models/word2vec.py#L1137"" rel=""nofollow noreferrer"">warned that this would not work</a>, &amp; would have failed with a less-helpful error.)</p>
<p>As one of those errors mentioned, there has at times been experimental support for loading <em>some</em> of a prior set-of-word-vectors to clobber any words in an existing model's already-initialized vocabulary, via <code>.intersect_word2vec_format()</code>. But by default that both (1) locks the imported vectors against further change; (2) brings in no new words. That's unlike what people most often want from &quot;fine-tuning&quot;, so it's not a ready-made help for that goal.</p>
<p>I believe some people have cobbled together custom code to achieve various kinds of fine-tuning in their projects â€“ but I don't know of anyone who's published a reliable recipe or strong results. (And I suspect some of the people who <em>think</em> they're doing this well just haven't rigorously evaluated the steps they are taking.)</p>
<p>If you have any recipe you know worked pre-Gensim-4.0.0, it should be adaptable - 4.0 changes to the <code>Word2Vec</code>-related classes were mainly refactorings, optimizations, &amp; new options (with little-to-none removal of functionality). But a reliable description of what used-to-work, or which particular fine-tuning strategy is being pursued for what specific benefits, to make more specific recommendations.</p>
",-1,0,1811,2021-07-08 08:42:56,https://stackoverflow.com/questions/68298289/fine-tuning-pre-trained-word2vec-model-with-gensim-4-0
Gensim W2V - UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte,"<p>I trained and saved a word2Vec model 'myWord2Vec.model' to pass it to a logistic regression model for training, but the vector size is bigger than my training dataset so, I needed to reduce the vector size. I tried the code below:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('myWord2Vec.model', limit=2021)
</code></pre>
<p>It gave me this error:</p>
<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I have no clue how to fix it nor how to reduce the vector size.
I would appreciate the help!</p>
","python-3.x, utf-8, gensim, word2vec","<p>How did you save <code>myWord2Vec.model</code>?</p>
<p>If you saved it with <code>.save()</code>, you need to load it with <code>.load()</code>, of the same model class. (And note: <code>.load()</code> is all-or-nothing, without any <code>limit=</code> option to just load part of the the set-of-vectors.)</p>
<p>Only if you saved with <code>.save_word2vec_format()</code> would it then be appropriate to load with <code>.load_word2vec_format()</code>. (And, trying to load the wrong-format file with <code>.load_word2vec_format()</code> could generate the kind of error you're seeing.)</p>
<p>Separately: <code>limit=2021</code> is a very strange option. Do you really want just 2,021 vectors loaded? (Usually people want at least tens-of-thousands to load.)</p>
<p>Also, your observation &quot;the vector size is bigger than my training dataset&quot; doesn't really make sense. Vector sizes in word2vec are most often 100-400 dimensions, if you have enough training data to support that size. You would more likely reduce the vector-size if you had <em>less</em> data, which was insufficient to train a higher-dimensional model. Reducing vector size <em>does</em> save a bit of memory. But, the model size is more a function of the vocabulary size (number of unique words) than training dataset site. And, if you truly have too large of a vocabulary to fit in memory, usually discarding lower-frequency words, by choosing a higher <code>min_count</code> option, is better than shrinking the <code>vector_size</code>.</p>
",1,0,1153,2021-07-12 15:44:30,https://stackoverflow.com/questions/68350237/gensim-w2v-unicodedecodeerror-utf-8-codec-cant-decode-byte-0x80-in-positio
Inconsistencies between bigrams found by TfidfVectorizer and Word2Vec model,"<p>I am building a topic model from scratch, one step of which uses the TfidfVectorizer method to get unigrams and bigrams from my corpus of texts:</p>
<pre><code>    tfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.9, ngram_range = (1,2))
</code></pre>
<p>After topics are created, I use the similarity scores provided by gensim's Word2Vec to determine coherence of topics. I do this by training on the same corpus:</p>
<pre><code>    bigram_transformer = Phrases(corpus)
    model = Word2Vec(bigram_transformer[corpus], min_count=1)
</code></pre>
<p>For many of the bigrams in my topics however, I get a KeyError because that bigram was not picked up in the training of Word2Vec, despite them being trained on the same corpus. I think this is because Word2Vec decides on which bigrams to choose based on statistical analysis (<a href=""https://stackoverflow.com/questions/60108919/why-arent-all-bigrams-created-in-gensims-phrases-tool"">Why aren&#39;t all bigrams created in gensim&#39;s `Phrases` tool?</a>)</p>
<p>Is there a way to get the Word2Vec to include all those bigrams identified by TfidfVectorizer? I see trimming capabilities such as 'trim_rule' but not anything in the other direction.</p>
","python, nlp, gensim, word2vec, tfidfvectorizer","<p>The point of the <code>Phrases</code> model in Gensim is to pick <em>some</em> bigrams, which are calculated to be statistically-significant.</p>
<p>If you then apply that model's determinations as a preprocessing step on your corpus, certain pairs of unigrams will be outright replaced in your text with the combined bigram. (As such, it's possible some unigrams that were there originally will no longer appear even once.)</p>
<p>Thus the concepts of bigrams as used by Gensim's <code>Phrases</code> and the <code>TfidfVectorizer</code>'s <code>ngram_range</code> facility are different. <code>Phrases</code> is meant for destructive replacements where specific bigrams are inferred to be more interesting than the unigrams. <code>TfidfVectorizer</code> will add extra bigrams as additional dimensional features.</p>
<p>I suppose the right tuning of <code>Phrases</code> could cause it to consider every bigram as significant. Without checking, it looks like a super-tiny value, like <code>0.0000000001</code>, might have essentially that effect. (The <code>Phrases</code> class will reject a value of <code>0</code> as nonsensical given its usual use.)</p>
<p>But at that point, your later transformation (via <code>bigram_transformer[corpus]</code>) will combine every possible pair of words before <code>Word2Vec</code> training. For example, the sentence:</p>
<pre><code>['the', 'skittish', 'cat', 'jumped', 'over', 'the', 'gap',]
</code></pre>
<p>...would indiscriminately become...</p>
<pre><code>['the_skittish', 'cat_jumped', 'over_the', 'gap',]
</code></pre>
<p>It seems unlikely that you want that, for a number of reasons:</p>
<ul>
<li>There might then be no training texts with the <code>'cat'</code> unigram alone, leaving you with no word-vector for that word at all.</li>
<li>Bigrams that are rare or of little grammatical value (like  <code>'the_skittish'</code>) will receive trained word-vectors, &amp; take up space in the model.</li>
<li>The kinds of text corpus that are large enough for good <code>Word2Vec</code> results might have far more bigrams than are manageable. (A corpus small enought that you can afford to track every bigram may be on the thin side for good <code>Word2Vec</code> results.)</li>
</ul>
<p>Further, to perform that greedy-combination of <em>all</em> bigrams, the <code>Phrases</code> frequency-survey &amp; calculations aren't even necessary. (It can be done automatically with no preparation/analysis.)</p>
<p>So, you shouldn't expect every bigram of <code>TfidfVectorizer</code> to be get a word-vector, unless you take some extra steps, outside the normal behavior of <code>Phrases</code>, to ensure every such bigram was in the training texts.</p>
<p>To try to do so wouldn't necessarily need <code>Phrases</code> at all, and might be unmanageable, and involve other tradeoffs. (For example, I could imagine repeating the corpus many times, only combining a fraction of the bigrams each time â€“ so that each is sometimes surrounded by other unigrams, and sometimes by other bigrams â€“ to create a synthetic corpus with enough meaningful texts to create all your desired vectors. But the logic &amp; storage space for that model would be larger &amp; complicated, and without prominent precedent, so it'd be a novel experiment.)</p>
",2,1,206,2021-07-13 18:15:39,https://stackoverflow.com/questions/68367520/inconsistencies-between-bigrams-found-by-tfidfvectorizer-and-word2vec-model
Genesis most_similar find synonym only (not antonyms),"<p>Is there a way to let <code>model.wv.most_similar</code> in gensim return positive-meaning words only (i.e. that shows synonyms but not antonyms)?</p>
<p>For example, if I do:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], topn=2000)
</code></pre>
<p>Then the mode is also going to return words such as &quot;dishonest&quot;.</p>
<pre><code>('dishonest', 0.5542981028556824),
</code></pre>
<p>However, what if I want words with the positive-meaning only?</p>
<p>I have tried the following - subtracting &quot;not&quot; from &quot;honest&quot; in the vector space:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], negative=['not'], topn=2000)
</code></pre>
<p>But somehow it is still returning &quot;dishonest&quot; somehow.</p>
<pre><code>('dishonest', 0.23721608519554138)
('dishonesties', 0.16536088287830353)
</code></pre>
<p>Any idea how to do this in a better way?</p>
","python, nlp, gensim, word2vec, fasttext","<p>Unfortunately, the vector-space created by word2vec algorithm training <em>doesn't</em> neatly match our human, intuitive understandin of pure-synonymity.</p>
<p>Rather, word2vec's sense of 'similarity' is more general - and overall, antonyms tend to be quite similar to each other: they're used in similar contexts (the driving force of word2vec training), about the same topics.</p>
<p>And further even though many understandable contrasts do vaguely correlate with various directions, there is no universal &quot;opposite&quot; (or &quot;positive&quot;) direction. So composing <code>'not'</code> with a word <em>doesn't</em> neatly invert the dominant sense of a word, and <code>'honest' + 'not'</code> won't reliably help find the direction of <code>'dishonest'</code>.</p>
<p>Barring finding some extra technique for this task beyond basic word2vec (in other research literature or via your own experimentation), the best you may be able to do is using already-known unwanted answers to further refine the results. That is, something like the following <em>might</em> offer marginally-improved results:</p>
<pre><code>word_vecs.most_similar(positive=['honest'], negative=['dishonest'])
</code></pre>
<p>(Further expanding the examples with more related words, either of the kind you want or not, might also help.)</p>
<p>See also some of the comments &amp; links in a previous answer for more ideas: <a href=""https://stackoverflow.com/a/44491124/130288"">https://stackoverflow.com/a/44491124/130288</a></p>
",1,2,385,2021-07-19 03:32:57,https://stackoverflow.com/questions/68434829/genesis-most-similar-find-synonym-only-not-antonyms
Why is &#39;[UNK]&#39; word the first in word2vec vocabulary?,"<p>If the vocabulary is ordered from the more frequent word to the less frequent, placing '[UNK]' at the beginning means that it occurs most. But what if '[UNK]' isn't the most frequent word? Should I put it at another place in the vocabulary, according to its frequency?</p>
<p>I found such issue when doing this tutorial -&gt; <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec</a></p>
<p>When I'm doing negative sampling using the function tf.random.log_uniform_candidate_sampler, the negative samples with low token (s.g. 0,1,2 ...) will be sampled most. If '[UNK]' is the first (or second when using padding) in the vocabulary, which means that it has token 0 (or 1 when using padding), then the '[UNK]'  will be heavily sampled as negative sample. If '[UNK]' happens a lot, there is no problem, but what if it doesn't? Then it should receive a higher token, shouldn't?</p>
","word2vec, zipf","<p>The method which TextVectorization.get_vocabulary() calls will always put padding and the &quot;OOV&quot; characters as the first elements in the vector, which would imply that they're the most common as you've mentioned.</p>
<p>Not sure why it was written that way, as the OOV may not always be the most frequent as you've mentioned, but that's how it was implmented:</p>
<p>Source: <a href=""https://github.com/keras-team/keras/blob/v2.13.1/keras/layers/preprocessing/index_lookup.py#L370"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/v2.13.1/keras/layers/preprocessing/index_lookup.py#L370</a></p>
<p>However, in order to ensure that it (or any other stop-words) are not oversampled as you mentioned you were concerned about, the tutorial does show how to use the &quot;tf.keras.preprocessing.sequence.make_sampling_table&quot; function in order to downweight the probability that items earlier in the vocabulary will not be oversampled.</p>
<p>In order to simply not use the OOV character in the vocab you can always exclude it as well:</p>
<p>inverse_vocab = vectorize_layer.get_vocabulary(include_special_tokens=False)</p>
<p>Seems like you could manually shuffle the &quot;[UNK]&quot; value to its appropriate index too if you wanted it to be as accurate as possible as you suggested.</p>
",0,0,680,2021-07-19 12:26:58,https://stackoverflow.com/questions/68440502/why-is-unk-word-the-first-in-word2vec-vocabulary
Gensim sort_by_descending_frequency changes most_similar results,"<p>It seems that when retrieving the most similar word vectors, sorting by word frequency will change the results in <code>Gensim</code>.</p>
<p>Before sorting:</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences
print(len(common_texts))
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=1)  

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('interface', 0.7432922720909119),
 ('minors', 0.6719315052032471),
 ('time', 0.3513716757297516),
 ('computer', 0.05815044790506363),
 ('response', -0.11714297533035278),
 ('graph', -0.15643596649169922),
 ('eps', -0.2679084539413452),
 ('survey', -0.34035828709602356),
 ('trees', -0.63677978515625),
 ('user', -0.6500451564788818)]
</code></pre>
</blockquote>
<p>However, if I sort the vectors by descending frequency:</p>
<pre><code>model.wv.sort_by_descending_frequency()

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('minors', 0.9638221263885498),
 ('time', 0.6335864067077637),
 ('interface', 0.40014874935150146),
 ('computer', 0.03224882856011391),
 ('response', -0.14850640296936035),
 ('graph', -0.2249641716480255),
 ('survey', -0.26847705245018005),
 ('user', -0.45202943682670593),
 ('eps', -0.497650682926178),
 ('trees', -0.6367797255516052)]
</code></pre>
</blockquote>
<p>The most similar word ranking as well as the word similarities change. Any idea why?</p>
<p><strong>Update:</strong></p>
<p>Before calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'graph',
 'trees',
 'user',
 'minors',
 'eps',
 'time',
 'response',
 'survey',
 'computer',
 'interface',
 'human']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
<p>After calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'user',
 'trees',
 'graph',
 'human',
 'interface',
 'computer',
 'survey',
 'response',
 'time',
 'eps',
 'minors']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
","python, nlp, gensim, word2vec, fasttext","<p>That change-of-reported similarities definitely <strong>shouldn't</strong> happen, so something is surely going wrong here. (Maybe, cached-subword info isn't re-sorting.)</p>
<p>But also note:</p>
<ul>
<li>That method wasn't particularly meant for use <em>after</em> training - indeed, you should be seeing a warning message if using it that way.</li>
<li>Such a sort <em>should already happen by default</em> in all 2Vec algorithms at the end of vocab-discovery phase - it's the usual behavior, only rarely turned off. So requesting it again should at most be a no-op.</li>
</ul>
<p>To dig into what may be gowing wrong, can you edit your question to show the values of bothâ€¦</p>
<ul>
<li><code>model.wv.index_to_key</code></li>
<li><code>model.wv.expandos['count']</code></li>
</ul>
<p>â€¦before <em>and</em> after the <code>. sort_by_descending_frequency()</code> call?</p>
",1,1,382,2021-07-20 08:40:25,https://stackoverflow.com/questions/68451937/gensim-sort-by-descending-frequency-changes-most-similar-results
Structure of Gensim Word Embedding corpus,"<p>I want to train a word2vec model using Gensim. I preprocessed my corpus, which is made of hundreds of thousands of articles from a specific newspaper. I preprocessed them (lower casing, lemmatizing, removing stop words and punctuations, etc.) and then make a list of lists, in which each element is a list of words.</p>
<pre><code>corpus = [['first', 'sentence', 'second', 'dictum', 'third', 'saying', 'last', 'claim'],
          ['first', 'adage', 'second', 'sentence', 'third', 'judgment', 'last', 'pronouncement']]
</code></pre>
<p>I wanted to know if it is the right way, or it should be like the following:</p>
<pre><code>corpus = [['first', 'sentence'], ['second', 'dictum'], ['third', 'saying'], ['last', 'claim'], ['first', 'adage'], ['second', 'sentence'], ['third', 'judgment'], ['last', 'pronouncement']]
</code></pre>
","gensim, word2vec, word-embedding, corpus","<p>Both would minimally work.</p>
<p>But in the second, no matter how big your <code>window</code> parameter, the fact all texts are no more than 2 tokens long means words will only affect their immediate neighbors. That's probably not what you want.</p>
<p>There's no real harm in longer texts, except to note that:</p>
<ul>
<li>Tokens all in the same list will appear in each other's <code>window</code>-sized neighborhood - so don't run words together that shouldn't imply any realistic use alongside each other. (But, in large-enough corpuses, even the noise of some run-together unrelated texts won't make much difference, swamped by the real relationships in the bulk of the texts.)</li>
<li>Each text shouldn't be more than 10,000 tokens long, as an internal implementation limit will cause any tokens beyond that limit to be ignored.</li>
</ul>
",2,0,73,2021-07-27 21:45:48,https://stackoverflow.com/questions/68552107/structure-of-gensim-word-embedding-corpus
AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;most_similar&#39; (Word2Vec),"<p>I am using Word2Vec and using a wiki trained model that gives out the most similar words. I ran this before and it worked but now it gives me this error even after rerunning the whole program. I tried to take off  <code>return_path=True</code> but im still getting the same error</p>
<pre><code>print(api.load('glove-wiki-gigaword-50', return_path=True))
model.most_similar(&quot;glass&quot;)
</code></pre>
<p>#ERROR:</p>
<pre><code>/Users/me/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-153-3bf32168d154&gt; in &lt;module&gt;
      1 print(api.load('glove-wiki-gigaword-50', return_path=True))
----&gt; 2 model.most_similar(&quot;glass&quot;) 

AttributeError: 'Word2Vec' object has no attribute 'most_similar'
</code></pre>
<p>#MODEL
this is the model I used</p>
<pre><code>    print(
        '%s (%d records): %s' % (
            model_name,
            model_data.get('num_records', -1),
            model_data['description'][:40] + '...',
        )
    )
</code></pre>
<p>Edit: here is my gensim download &amp; output</p>
<pre><code>!python -m pip install -U gensim
</code></pre>
<p>OUTPUT:</p>
<p>Requirement already satisfied: gensim in ./opt/anaconda3/lib/python3.8/site-packages (4.0.1)</p>
<p>Requirement already satisfied: numpy&gt;=1.11.3 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)</p>
<p>Requirement already satisfied: smart-open&gt;=1.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.1.0)</p>
<p>Requirement already satisfied: scipy&gt;=0.18.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)</p>
","python, nlp, gensim, word2vec, doc2vec","<p>You are probably looking for <code>&lt;MODEL&gt;.wv.most_similar</code>, so please try:</p>
<pre><code>model.wv.most_similar(&quot;glass&quot;) 
</code></pre>
",18,5,17563,2021-08-06 05:41:06,https://stackoverflow.com/questions/68676637/attributeerror-word2vec-object-has-no-attribute-most-similar-word2vec
pyspark tokenizing the sentences and vectorizing them by using RegexTokenizer and Word2Vec,"<p>I have a spark DataFrame that I could tokenize the sentences in the &quot;body&quot; column. The DataFrame is shown below:</p>
<p><a href=""https://i.sstatic.net/jO5JC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jO5JC.jpg"" alt=""tokenized DataFrame"" /></a></p>
<p>I want to vectorize the created text_token column. I use the code below to do so</p>
<pre><code>word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'text_token', outputCol = 'result')
model = word2vec.fit(df_token)
result = word2Vec.transform(df_token)
</code></pre>
<p>But I get the error below:</p>
<pre><code>Py4JJavaError: An error occurred while calling o339.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 8) (3e87a4f2debc executor driver): org.apache.spark.SparkException: Failed to execute user defined function(RegexTokenizer$$Lambda$2656/0x0000000841079840: (string) =&gt; array&lt;string&gt;)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.NullPointerException
    at org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)
    ... 20 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
    at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
    at org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:191)
    at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:312)
    at org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:183)
    at org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:122)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function(RegexTokenizer$$Lambda$2656/0x0000000841079840: (string) =&gt; array&lt;string&gt;)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    ... 1 more
Caused by: java.lang.NullPointerException
    at org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)
    ... 20 more
</code></pre>
","python, pyspark, apache-spark-sql, tokenize, word2vec","<p>It seems like the column <code>body</code> had null values, so when you tried to tokenize using <code>RegexTokenizer</code> it gave you the <em>NullPointerException</em> above.</p>
<p>You should deal with those null values by either drop them like</p>
<pre><code>df2_token = regexTokenizer.transform(df2.dropna())
</code></pre>
<p>or fill them with a string of your choice using <code>.fillna</code>.</p>
",1,0,1257,2021-08-14 14:19:12,https://stackoverflow.com/questions/68784138/pyspark-tokenizing-the-sentences-and-vectorizing-them-by-using-regextokenizer-an
Modifying .trainables.syn1neg[i] with previously trained vectors in Gensim word2vec,"<p>My issue is the following.</p>
<p>In my code I'm modifying the .wv[word] before training but after .build_vocab(), which is fairly straight forward. Just instead of the vectors in there add mine for every word.</p>
<pre><code>for elem in setIntersection:
    if len(word_space[elem]) != 300:
        print('here', elem) #cast it to the fire
        sys.exit()
    w2vObjectRI.wv[elem] = np.asarray(word_space[elem], dtype=np.float32)
</code></pre>
<p>Where setIntersection is just a set of common words between gensim word2vec and RandomIndexing trained. Same size of 300 in both.</p>
<p>Now I want to also modify the hidden-to-output layer weights, which I was told that they are in .trainables.syn1neg[i], but here is my issue this matrix is not word addressable, is just a normal matrix with out names. How could I know which letter I will be modifying in this matrix? Also I see that they are initialised with 0s, I was just thinking if these weights are not reset before training? More clearly if I change those weights and then call train will it use the ones I provided? Thanks.</p>
<pre><code>for i in range(len(setIntersection)):
if len(word_space[setIntersection[i]]) != 300:
    print('here', setIntersection[i]) #cast it to the fire
    sys.exit()
w2vObjectRI.trainables.syn1neg[i] = np.asarray(word_space[setIntersection[i]], dtype=np.float32)
</code></pre>
<p>Cheers,</p>
<p>Pedro.</p>
","python, gensim, word2vec","<p>In Gensim 4.0+, that &quot;hidden to output layer&quot; is just in <code>w2v_model.syn1neg</code>, instead of a (now-removed) subcomponent <code>.trainables</code>.</p>
<p>Following the original <code>word2vec.c</code> on which Gensim's implementation is based, those weights begin training as uninitialized zeros.</p>
<p>As the output (predicted-word) nodes are exactly the same vocabulary as are considered in the input/projection layer, the correspondence of rows-to-words is exactly the same as in the input layer, aka the word-vectors being trained. (That was previously in an array called <code>.syn0</code>, more recently called just <code>.vectors</code>.)</p>
<p>So the word that's in slot 0 in <code>w2v_model.wv.vectors</code> is also the word represented by the output-node fed by <code>w2v_model.syn1neg[0]</code>.</p>
<p>In Gensim 4.0+, these word-to-slot values can be read from <code>w2v_model.wv.key_to_index[word]</code>. (Pre-4.0, I think it was <code>w2v_model.wv.vocab[word].index</code>.)</p>
",2,0,218,2021-08-18 13:49:23,https://stackoverflow.com/questions/68833707/modifying-trainables-syn1negi-with-previously-trained-vectors-in-gensim-word2
the repetitions in Gensim Word2Vec training corpus,"<p>I am using <code>Gensim</code> to train a <code>Word2Vec</code> embedding on different corpora, pertaining to different years, to compare the embedding vectors.
My question is: if I repeat the documents of a specific year twice and documents of another year just once, do the resulting embeddings give more weight to the repeated documents?
I have in my mind to make a corpus that gives more weight to recent documents and less weight to documents from far past.
I simply train the model on my <code>Line Sentence</code> corpus file.</p>
<pre><code>Word2Vec(corpus_file=corpus, vector_size=100, window=5, min_count=5, workers=4)
</code></pre>
","python, gensim, word2vec, word-embedding, corpus","<p>Sure, repeating some texts (even more than the re-iterations controlled by the <code>epochs</code> count) means they'll have more influence on the final model.</p>
<p>In general, repeating identical texts isn't as good as truly varied alternative examples of the same words. For example, if you only have one text using a certain word, repeating it 5 times might make the word survive the <code>min_count=5</code> cutoff, but the lack of many subtly-contrasting appearances means its final vector will only reflect that one peculiar, repeated use. The kind of good relative word-vector positions that people are usually seeking need a training tug-of-war between all the ways a word is used.</p>
<p>But, in this case, you should still have many examples, you're just overtraining on some of them.</p>
<p>Do note that it might be a <em>little</em> bit better to ensure the repeats are shuffled throughout the whole corpus, at least once before training begins, rather than clustered all together. (Repeating a text 10 times in a row will overtrain those words/contexts â€“ but not in as balanced of a way as if interleaved with all the other differently-weighted training.)</p>
<p>And, that you might not want to turn all the 1-occurrence words in a subset that you repeat 5 times to automatically survive the <code>min_count</code> cut - because they still just have that one true weak context example. So you might want to learn the vocabulary from a non-reweighted corpus, but then train on the new corpus with artificial repeats (being sure to provide your <code>.train()</code> call with the right new <code>total_examples</code> count for it to report progress &amp; adjust the learning-rate properly).</p>
",1,0,395,2021-08-21 11:03:35,https://stackoverflow.com/questions/68872427/the-repetitions-in-gensim-word2vec-training-corpus
Using weight from a Gensim Word2Vec model as a starting point of another model,"<p>I have two corpora that are from the same field, but with a temporal shift, say one decade. I want to train Word2vec models on them, and then investigate the different factors affecting the semantic shift.</p>
<p>I wonder how should I initialize the second model with the first model's embeddings to avoid as much as possible the effect of variance in co-occurrence estimates.</p>
","python, gensim, word2vec, word-embedding, fine-tuning","<p>At a naive &amp; easy level, you can just load one existing model, and <code>.train()</code> on new data. But note if doing that:</p>
<ul>
<li>Any words not already known by the model will be ignored, and the word-frequencies that feed algorithmic steps will only be from the initial survey</li>
<li>While all words in the current corpus will get as many training-updates as their appearances (&amp; your <code>epochs</code> setting) dictate, and thus be nudged arbitrarily-far from their original-model locations, other words from the seed model will stay exactly where they were. But, it's only the interleaved tug-of-war between words in the same training session that makes them usefully comparable. So doing this sequential training â€“ updating only some words in a new training session â€“ is likely to degrade the meaningfulness of word-to-word comparisons, in hard-to-measure ways.</li>
</ul>
<p>Another approach that might be woth trying could be to train single model over the combined corpus - but transform/repeat the era-specific texts/words in certain ways to be able to distinguish earlier-usages from later-usages. There are more details about this suggestion in the context of word-vectors varying over usage-eras in a couple previous answers:</p>
<p><a href=""https://stackoverflow.com/a/57400356/130288"">https://stackoverflow.com/a/57400356/130288</a></p>
<p><a href=""https://stackoverflow.com/a/59095246/130288"">https://stackoverflow.com/a/59095246/130288</a></p>
",1,0,403,2021-08-28 10:15:35,https://stackoverflow.com/questions/68963361/using-weight-from-a-gensim-word2vec-model-as-a-starting-point-of-another-model
Can a gensim word2vec model be trained in a federated way?,"<p>I am trying to find out how I could train a word2vec model in a federated way.</p>
<p>The data would be split into multiple parts, e.g. 4 &quot;institutions&quot;, and I would like to train the word2vec model on the data from each institution separately. They key restraint here is that the data from the institutions can not be moved to another location, so it can never be trained in a centralized way.</p>
<p>I know that it is possible to train the word2vec model iteratively, such that the data from the first institution is read and used to train &amp; update the word2vec model, but I wonder if its possible to do it simultaneously on all four institutions and then, for example, to merge all four word2vec models into one model.</p>
<p>Any ideas or suggestions are appreciated</p>
","python, gensim, word2vec, text-processing, federated","<p>There's no official support in Gensim, so any approach would involve a lot of custom research-like innovation.</p>
<p>Neural models like the word2vec algorithm (but not Gensim) have been trained in a very-distributed/parallel fashion â€“ see for example 'Hogwild' &amp; related followup work, for asynchronous SGD. Very roughly, many separate simultaneous processes train separately &amp; asynchronously, but keep updating each other intermittently, even without locking â€“ &amp; it works OK. (See more llinks in prior answer: <a href=""https://stackoverflow.com/a/66283392/130288."">https://stackoverflow.com/a/66283392/130288.</a>)</p>
<p>But:</p>
<ul>
<li>still this is usually done for performance, &amp; within a highly-connected datacenter â€“ <em>not</em> for the sake of keeping separate data sources private, between institutions that may be less connected/trusting, or where the shards of data might in fact be very different in vocabulary/word-senses</li>
<li>there's never been support in Gensim for this - though many years ago, in an older version of Gensim, someone whipped up a kinda sorta demo that purported to do such scatter/merge training via Spark â€“ see <a href=""https://github.com/dirkneumann/deepdist"" rel=""nofollow noreferrer"">https://github.com/dirkneumann/deepdist</a>.</li>
</ul>
<p>So: it's something a project could try to simulate, or test in practice, though the extra lags/etc of cross-&quot;institution&quot; updates might make it unpractical or ineffective. (And, they'd still have to initially consense on a shared vocabulary, which without due care would leak aspects of each's data.)</p>
<p>As you note, you could consider an approach where each trains one shared model in serial turns, which coudl very closely simulate a single training, albeit with the overhead of passing the interim model around, and no parallelism. Roughly:</p>
<ul>
<li>share word counts to reach a single consensus vocabulary</li>
<li>for each intended training epoch, each institution would train one pass on its whole dataset, then pass the model to the next institution</li>
<li>the calls to <code>.train()</code> would manually manage item counts &amp; <code>alpha</code>-related values to simulate one single SGD run</li>
</ul>
<p>Note that there'd still be some hints of each instititions relative co-occurrences of terms, which would leak some info about their private datasets â€“ perhaps most clearly on rare terms.</p>
<p>Still, if you weren't in a rush, that'd best simulate a single integrated model training.</p>
<p>I'd be tempted to try to fix the sharing concerns with some other trust-creating process or intermediary. (Is there an 3rd party that each could trust with their data, temporarily? Could a single shared training system be created which could <em>only</em> stream the individual datasets in for training, with no chance of saving/summarizing the full data? Might 4 cloud hosts, each under the separate institution's sole management but physically in a shared facility effect the above 'serial turns' approach with hardly any overhead?)</p>
<p>There's also the potential to map one model into another: taking a number of shared words as reference anchor points, learning a projection from one model to the other, which allows other non-reference-point words to be moved from one coordinate space to the other. This is has been mentioned as a tool for either extending a vocabulary with vectors from elsewhere (eg section 2.2 of the Kiros et al 'Skip-Thought Vectors' paper) or doing language translation (Mikolov et al 'Exploiting Similarities among Languages for Machine Translation' paper).</p>
<p>Gensim includes a <code>TranslationMatrix</code> class for learning such projections. Conceivably the institutions could pick one common dataset, or one institution with the largest dataset, as the creator of some canonical starting model. Then each institution creates their own models based on private data. Then, based on some set of 'anchor words' (that are assumed to have <em>stable</em> meaning across all models, perhaps because they are very common) each of these followup models are projected into the canonical space - allowing words that are either unique to each model to be moved into the shared model, or words that vary a lot across models to be projected to contrasting points in the same space (that it might then make sense to average together).</p>
",1,1,315,2021-09-06 11:05:05,https://stackoverflow.com/questions/69073499/can-a-gensim-word2vec-model-be-trained-in-a-federated-way
Gensim fasttext cannot get latest training loss,"
<h4>Problem description</h4>
<p>It seems that the <code>get_latest_training_loss</code> function in <code>fasttext</code> returns only 0. Both gensim <strong>4.1.0</strong> and <strong>4.0.0</strong> do not work.</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec
from pprint import pprint as print
from gensim.models.fasttext import FastText
from gensim.test.utils import datapath

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

# Set file names for train and test data
corpus_file = datapath('lee_background.cor')

model = FastText(vector_size=100, callbacks=[callback()])

# build the vocabulary
model.build_vocab(corpus_file=corpus_file)

# train the model
model.train(
    corpus_file=corpus_file, epochs=model.epochs,
    total_examples=model.corpus_count, total_words=model.corpus_total_words,
    callbacks=model.callbacks, compute_loss=True,
)

print(model)
</code></pre>
<pre><code>'Loss after epoch 0: 0.0'
'Loss after epoch 1: 0.0'
'Loss after epoch 2: 0.0'
'Loss after epoch 3: 0.0'
'Loss after epoch 4: 0.0'
</code></pre>
<p><strong>If currently FastText does not support <code>get_latest_training_loss</code>, the documentation here needs to be removed:</strong></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss</a></p>
<h4>Versions</h4>
<p>I have tried this in three different environments and neither of them works.</p>
<p><strong>First environment:</strong></p>
<pre><code>[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48)
[GCC 9.3.0]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.21.2
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Second environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Third environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
/Users/jinhuawang/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
gensim 4.0.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
","python, nlp, gensim, word2vec, fasttext","<p>Indeed, loss-tracking hasn't ever been implemented in Gensim's <code>FastText</code> model, at least through release 4.1.0 (August 2021).</p>
<p>The docs for that method appear in error, due to the inherited method from the <code>Word2Vec</code> superclass not being overriden to prevent the default assumption that superclass methods work.</p>
<p>There is a <a href=""https://github.com/RaRe-Technologies/gensim/issues/2617"" rel=""noreferrer"">long-open issue</a> to fill the gaps &amp; fix the problems in Gensim's loss-tracking (which is also somewhat buggy &amp; incomplete for <code>Word2Vec</code>). But, at the moment I don't think any contributor is working on it, &amp; it hasn't been prioritized for any upcoming release. It may require someone to volunteer to step forward &amp; fix things.</p>
",5,5,681,2021-09-10 04:02:38,https://stackoverflow.com/questions/69127120/gensim-fasttext-cannot-get-latest-training-loss
How to visualize Gensim Word2vec Embeddings in Tensorboard Projector,"<p>Following <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim word2vec embedding tutorial</a>, I have trained a simple word2vec model:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.save(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>I would like to visualize it <a href=""https://projector.tensorflow.org/"" rel=""noreferrer"">using the Embedding Projector in TensorBoard</a>. <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">There is another straightforward tutorial in gensim documentation</a>. I did the following in Colab:</p>
<pre><code>!python3 -m gensim.scripts.word2vec2tensor -i /content/word2vec.model -o /content/my_model

Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 94, in &lt;module&gt;
    word2vec2tensor(args.input, args.output, args.binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 68, in word2vec2tensor
    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py&quot;, line 1438, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py&quot;, line 172, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/utils.py&quot;, line 355, in any2unicode
    return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>Please note that I did check first this <a href=""https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector"">exact same question from 2018</a> - but the accepted answer no longer works as both in gensim and tensorflow have been updated so I considered it was worth asking again in Q4 2021.</p>
","python, tensorflow, gensim, word2vec, tensorboard","<p>Saving the model in the original C word2vec implementation format resolves the issue:
<code>model.wv.save_word2vec_format(&quot;/content/word2vec.model&quot;)</code>:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.wv.save_word2vec_format(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>There are two formats of storing word2vec models in <code>gensim</code>: keyed vector format from the original word2vec implementation and format that additionally stores hidden weights, vocabulary frequencies, and more. Examples and details can be found in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#usage-examples"" rel=""nofollow noreferrer"">documentation</a>. The script <code>word2vec2tensor.py</code> uses the original format and loads the model with <code>load_word2vec_format</code>: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/scripts/word2vec2tensor.py#L68"" rel=""nofollow noreferrer"">code</a>.</p>
",1,5,1041,2021-09-18 13:11:18,https://stackoverflow.com/questions/69234978/how-to-visualize-gensim-word2vec-embeddings-in-tensorboard-projector
gensim word2vec vocabulary size fluctuates up &amp; down as corpus grows despite `max_vocab_size` setting,"<p>I am training word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim Word2Vec</a> model with a multi-million sentence corpus that is made of 3 million unique tokens with <code>max_vocab_size = 32_000</code>.</p>
<p>Even though I set <code>min_count = 1</code>, model creates a vocabulary of far less than 32_000. When I use a subset of the corpus, vocabulary size increases!</p>
<p>In order to troubleshoot, I set up an experiment where I control the size of vocabulary with different sized subcorpus. The size of the vocabulary flactuates!</p>
<p>You can re-produce with the code below:</p>
<pre><code>import string
import numpy as np
from gensim.models import Word2Vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = &quot;&quot;
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# Sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('Number of tokens:', f'{len(flattened):,}')
print('Number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = Word2Vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('Vocab size of', duo[1], 'for', duo[0], 'number of sentences!')
</code></pre>
<p>Output:</p>
<pre><code>['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

Number of tokens: 748,383
Number of unique tokens: 171,485

Vocab size of 16929 for 5000 number of sentences!
Vocab size of 30314 for 10000 number of sentences!
Vocab size of 19017 for 15000 number of sentences!
Vocab size of 31394 for 20000 number of sentences!
Vocab size of 19564 for 25000 number of sentences!
Vocab size of 31831 for 30000 number of sentences!
Vocab size of 19543 for 35000 number of sentences!
Vocab size of 31744 for 40000 number of sentences!
Vocab size of 19536 for 45000 number of sentences!
Vocab size of 31642 for 50000 number of sentences!
Vocab size of 18806 for 55000 number of sentences!
Vocab size of 31255 for 60000 number of sentences!
Vocab size of 18497 for 65000 number of sentences!
Vocab size of 31166 for 70000 number of sentences!
Vocab size of 18142 for 75000 number of sentences!
Vocab size of 30886 for 80000 number of sentences!
Vocab size of 17693 for 85000 number of sentences!
Vocab size of 30390 for 90000 number of sentences!
Vocab size of 17007 for 95000 number of sentences!
Vocab size of 30196 for 100000 number of sentences!
</code></pre>
<p>I tried increasing <code>min_count</code> but it did not help this flactuation of vocabulary size. What am I missing?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>In Gensim, the <code>max_vocab_size</code> parameter is a <em>very</em> crude mechanism to limit RAM usage during the initial scan of the training corpus to discover the vocabulary. You should only use this parameter if it's the only way to work around RAM problems.</p>
<p>Essentially: try without using <code>max_vocab_size</code>. If you want control over which words are retained, use alternate parameters like <code>min_count</code> (to discard words less-frequent than a certain threshold) or <code>max_final_vocab</code> (to take no more than a set number of the most-frequent words).</p>
<p><em>If and only if</em> you hit out-of-memory errors (or massive virtual-memory swapping), then consider using <code>max_vocab_size</code>.</p>
<p>But even then, because of the way it works, you still wouldn't want to set <code>max_vocab_size</code> to the actual final size you want. Instead, you should set it to some value much, much larger - but just small enough to not exhaust your RAM.</p>
<p>This allows the most accurate possible word-counts <em>before</em> other parameters (like <code>min_count</code> &amp; <code>max_final_vocab</code>) are applied.</p>
<p>If you instead use a low <code>max_vocab_size</code>, the running survey will prematurely trim the counts any time the number of known words reaches that value. That is, as soon as the interim count reaches that many entries, say <code>max_vocab_size=32000</code>, many of the least-frequent counts are <em>forgotten</em> to cap memory usage (and more each time the threshold is reached).</p>
<p>That makes all final counts approximate (based on how often a term missed the cutoff), and means the final number of unique tokens in the full survey will be some value even less than <code>max_vocab_size</code>, somewhat arbitrarily based on how recently a forgetting-trim was triggered. (Hence, the somewhat random, but always lower than <code>max_vocab_size</code>, counts seen in your experiment output.)</p>
<p>So: <code>max_vocab_size</code> is unlikely to do what most people want, or in a predictable way. Still, it can help a fuzzy survey complete for extreme corpora where unique terms would otherwise overflow RAM.</p>
<p>Separately: <code>min_count=1</code> is usually a bad idea in word2vec, as words that lack sufficient varied usage examples won't themselves get good word-vectors, but leaving all such poorly-represented words in the training data tends to serve as noise that dilutes (&amp; delays) what can be learned about adequately-frequent words.</p>
",2,0,682,2021-09-19 21:01:40,https://stackoverflow.com/questions/69247049/gensim-word2vec-vocabulary-size-fluctuates-up-down-as-corpus-grows-despite-ma
Find most similar sentence in a large dataset of sentences,"<p>I currently have a text file with around a million sentences, each on a new line.
I am trying to build a solution where I can take a new sentence outside of this text file and have the program return the most similar sentence present in the file.</p>
<p>I have found some solutions which return the pair of sentences with the highest similarity INSIDE the existing dataset.For example <a href=""https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python"">this</a> one. But that is not what I am going for. I want to be able to compare a new sentence with all of those in the text file.</p>
<p>Also, I am not sure if I should be focusing on semantic similarity or cosine similarity.</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>I advise you to read about <a href=""https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"" rel=""nofollow noreferrer"">Damerauâ€“Levenshtein distance</a>.
I was also looking for a similar solution and settled on this algorithm.</p>
<p>There are implementations for Python:</p>
<ul>
<li><a href=""https://pypi.org/project/fastDamerauLevenshtein/"" rel=""nofollow noreferrer"">fastDamerauLevenshtein</a></li>
<li><a href=""https://pypi.org/project/pyxDamerauLevenshtein/"" rel=""nofollow noreferrer"">pyxDamerauLevenshtein</a></li>
</ul>
",1,0,1471,2021-09-21 18:53:09,https://stackoverflow.com/questions/69274178/find-most-similar-sentence-in-a-large-dataset-of-sentences
How do I feed an array of Tokenized Sentences to Word2vec to get embeddings?,"<p>Hi all: I cannot figure out the code required to get embeddings from a word2vec model.</p>
<p>Here is how my df is structured (it is some android based log):</p>
<p>logDateTime | lineNum | processID | threadID | priority | app | message | eventTemplate | eventID
ts            int       int         int        str        str   str       str             str</p>
<p>Essentially, I created a unique subset of events out of log messages and assigned a template with an associated id:</p>
<pre><code>def eventCreation(df):
    df['eventTemplate'] = df['message'].str.replace('\d+', '*')
    df['eventTemplate'] = df['eventTemplate'].str.replace('true', '*')
    df['eventTemplate'] = df['eventTemplate'].str.replace('false', '*')
    df['eventID'] = df.groupby(df.eventTemplate.tolist(), sort=False).ngroup() + 1
    df['eventID'] = 'E'+df['eventID'].astype(str)

def seqGen(arr, k):
    for i in range(len(arr)-k+1):
        yield arr[i:i+k]

#define the variables here
cwd = os.getcwd()
#create a dataframe of the logs concatenated
df = pd.DataFrame.from_records(process_files(cwd,getFiles))
# call functions to establish df
cleanDf(df)
featureEng(df)
eventCreation(df)
df['eventToken'] = df.eventTemplate.apply(lambda x: word_tokenize(x))
seq = []
eventArray = df[[&quot;eventToken&quot;]].to_numpy()
for sequence in seqGen(eventArray, 9):
    seq.append(eventArray)
</code></pre>
<p>So, 'seq' ends up looking like this:</p>
<pre><code>[array([['[*,com.blah.blach.blahMainblach] '],
        ['[*,*,*,com.blah.blah/.permission.ui.blah,finish-imm] '],
        ['[*,*,*,*,startingNewTask] '],
        ...,
        ['mfc, isSoftKeyboardVisible in WMS : * '],
        ['mfc, isSoftKeyboardVisible in WMS : * '],
        ['Calling a method in the system process without a qualified user: android.app.ContextImpl.startService:* android.content.ContextWrapper.startService:* android.content.ContextWrapper.startService:* com.blahblah.usbmountreceiver.USBMountReceiver.onReceive:* android.app.ActivityThread.handleReceiver:* ']],
       dtype=object),
</code></pre>
<p>The sequences are arrays with lists of tokenized log messages. The plan was after training the model, I can get the embedding of a log event by multiplying the onehot vector and the weight matrix... there is more to do, but I am stuck at getting the embeddings.</p>
<p>I am a newbie trying to develop a solution for anomaly detection.</p>
","python-3.x, logging, word2vec, word-embedding, anomaly-detection","<p>If you're using the Gensim library in Python for its <code>Word2Vec</code> implementation, it wants its corpus as a <em>re-iterable sequence</em> where each item is itself a <em>list of string tokens</em>.</p>
<p>A list which itself has each item as a list-of-string-tokens would work.</p>
<p>Your <code>seq</code> is close, but:</p>
<ol>
<li>It doesn't need to be (&amp; thus probably shouldn't be) a <code>numpy</code> array of objects.</li>
<li>Each of your <code>object</code> items is a <code>list</code> (good) but each has only has a single untokenized string inside (bad). You need to break those strings into the individual 'words' that you want the model to learn.</li>
</ol>
",0,0,489,2021-09-26 15:29:58,https://stackoverflow.com/questions/69336366/how-do-i-feed-an-array-of-tokenized-sentences-to-word2vec-to-get-embeddings
How to preprocess a text to remove stopwords?,"<p>I would like to remove a list of stopwords, namely the ones in</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
print(STOPWORDS)
</code></pre>
<p>In gensim, this should be pretty straightforward with <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py"" rel=""nofollow noreferrer""><code>remove_stopwords </code>function</a>.</p>
<p>My code to read the text and remove the stopwords is the following:</p>
<pre><code>def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      text.append(simple_preprocess(remove_stopwords(line)))
  return text

text = read_text('/content/text.txt')
text =  [x for x in text if x]
text[:3]
</code></pre>
<p>This is the output I get that contains words such as &quot;we&quot; or &quot;however&quot; which should have been removed from the <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">original text</a> though for instance &quot;the&quot; has been correctly removed from the first setence. I am very confused... what am I missing here?</p>
<pre><code>[['clinical', 'guidelines', 'management', 'ibd'],
 ['polygenetic',
  'risk',
  'scores',
  'add',
  'predictive',
  'power',
  'clinical',
  'models',
  'response',
  'anti',
  'tnfÎ±',
  'therapy',
  'inflammatory',
  'bowel',
  'disease'],
 ['anti',
  'tumour',
  'necrosis',
  'factor',
  'alpha',
  'tnfÎ±',
  'therapy',
  'widely',
  'management',
  'crohn',
  'disease',
  'cd',
  'ulcerative',
  'colitis',
  'uc',
  'however',
  'patients',
  'respond',
  'induction',
  'therapy',
  'patients',
  'lose',
  'response',
  'time',
  'to',
  'aid',
  'patient',
  'stratification',
  'polygenetic',
  'risk',
  'scores',
  'identified',
  'predictors',
  'response',
  'anti',
  'tnfÎ±',
  'therapy',
  'we',
  'aimed',
  'replicate',
  'association',
  'polygenetic',
  'risk',
  'scores',
  'response',
  'anti',
  'tnfÎ±',
  'therapy',
  'independent',
  'cohort',
  'patients',
  'establish',
  'clinical',
  'validity']]
</code></pre>
<p><strong>Text</strong> (complete file available <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>)</p>
<p>Clinical Guidelines for the Management of IBD.</p>
<p>Polygenetic risk scores do not add predictive power to clinical models for response to anti-TNFÎ± therapy in inflammatory bowel disease.
Anti-tumour necrosis factor alpha (TNFÎ±) therapy is widely used in the management of Crohn's disease (CD) and ulcerative colitis (UC). However, up to a third of patients do not respond to induction therapy and another third of patients lose response over time. To aid patient stratification, polygenetic risk scores have been identified as predictors of response to anti-TNFÎ± therapy. We aimed to replicate the association between polygenetic risk scores and response to anti-TNFÎ± therapy in an independent cohort of patients, to establish its clinical validity.</p>
","python, nlp, gensim, word2vec, stop-words","<p>Your remove_stopwords() function is case-sensitive and it doesn't ignore punctuation. For example, 'However' is not in STOPWORDS, but 'however' is in. You should call the simple_preprocess() function first. This should work:</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
from gensim.parsing.preprocessing import remove_stopword_tokens

def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      tokens = simple_preprocess(line)
      text.append(remove_stopword_tokens(tokens,stopwords=STOPWORDS))
  return text
</code></pre>
",3,0,627,2021-09-28 11:22:10,https://stackoverflow.com/questions/69360816/how-to-preprocess-a-text-to-remove-stopwords
cosine similarity doc vectors and word vectors for topical prevalence using doc2vec,"<p>I have a corpus of 250k Dutch news articles 2010-2020 to which I've applied word2vec models to uncover relationships between sets of neutral words and dimensions (e.g. good-bad). Since my aim is also to analyze the prevalence of certain topics over time, I was thinking of using doc2vec instead so as to simultaneously learn word and document embeddings. The 'prevalence' of topics in a document could then be calculated as the cosine similarities between doc vectors and word embeddings (or combinations of word vectors). In this way, I can calculate the annual topical prevalence in the corpus and see whether there's any changes over time. An example of such an approach can be found <a href=""https://export.arxiv.org/pdf/1707.03490"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My issue is that the avg. yearly cosine similarities yield really strange results. As an example, the cosine similarities between document vectors and a mixture of keywords related to covid-19/coronavirus show a decrease in topical prevalence since 2016 (which obviously cannot be the case).</p>
<p>My question is whether the approach that I'm following is actually valid. Or that maybe there's something that I'm missing. A 250k documents and 100k + vocabulary should be sufficient enough?</p>
<p>Below is the code that I've written:</p>
<pre><code># Doc2Vec model 
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]
d2vmodel = Doc2Vec(docs, min_count = 5, vector_size = 200, window = 10, dm = 1)
docvecs = d2vmodel.docvecs
wordvecs = d2vmodel.wv
    
# normalize vector 
from numpy.linalg import norm
def nrm(x):
  return x/norm(x)

# topical prevalence per doc
def topicalprevalence(topic, docvecs, wordvecs):
  proj_lst = []
  for i in range(0, len(docvecs)):
    topic_lst = []
    for j in topic: 
      cossim =  nrm(docvecs[i]) @ nrm(wordvecs[j])
      topic_lst.append(cossim)
    topic_avg = sum(topic_lst) / len(topic_lst)
    proj_lst.append(topic_avg)
  topicsyrs = { 
      'topic': proj_lst,
      'year': df['datetime'].dt.year
  }
  return pd.DataFrame(topicsyrs)

# avg topic prevalence per year
def avgtopicyear(topic, docvecs, wordvecs):
  docs = topicalprevalence(topic, docvecs, wordvecs)
  return pd.DataFrame(docs.groupby(&quot;year&quot;)[&quot;topic&quot;].mean())

# run 
covid = ['corona', 'coronapandemie', 'coronacrisis', 'covid', 'pandemie']
covid_scores = topicalprevalence(covid, docvecs, wordvecs)
</code></pre>
","python, gensim, word2vec, doc2vec","<p>The word-vec-to-doc-vec relatioships in modes that train both are interesting, but a bit hard to characterize as to what they really mean. In a sense the CBOW-like mode of <code>dm=1</code> (PV-DM) mixes doc-vectors in as one equal word among the whole <code>window</code>, when training to predict the 'target' word. But in the skip-gram-mixed mode <code>dm=0, dbow_words=1</code>, there'll be <code>window</code> count context-word-vec-to-target-word pair cycles to every 1 doc-vec-to-target-word pair cycle, changing the relative weight.</p>
<p>So if you saw a big improvement in <code>dm=0, dbow_words=1</code>, it might also be because that made the model relatively more word-to-word trained. Varying <code>window</code> is another way to change that balance, or increase <code>epochs</code>, in plain <code>dm=1</code> mode â€“ which should also result in doc/word compatible training, though perhaps not at the same rate/balance.</p>
<p>Whether a single <code>topicalprevalence()</code> mean vector for a full year would actually be reflective of individual word occurrences for a major topic may or may not be a valid conjecture, depending on possible other changes in the training data. Something like a difference in the relative mix of other major categories in the corpus might swamp even a giant new news topic. (EG: what if in y2020 some new section or subsidiary with a different focus, like entertainment, launched? It <em>might</em> swamp the effects of other words, especially when compressing down to a <em>single</em> vector of some particular dimensionality.)</p>
<p>Someting like a clustering of the year's articles, and identification of the closest 1 or N clusters to the target-words, with their similarities, might be more reflective even if the population of articles in changing. Or, a plot of each year's full set of articles as a histogram-of-similarities to the target-words - which might show a 'lump' of individual articles (not losing their distinctiveness to a full-year average) developing, over time, closer to the new phenomenon.</p>
",1,0,199,2021-09-28 12:21:04,https://stackoverflow.com/questions/69361669/cosine-similarity-doc-vectors-and-word-vectors-for-topical-prevalence-using-doc2
Default estimation method of Gensim&#39;s Word2vec Skip-gram?,"<p>I am now trying to use word2vec by estimating skipgram embeddings via NCE (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did (<a href=""https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO"" rel=""nofollow noreferrer"">https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO</a>). The paper has a replication GitHub repository (<a href=""https://github.com/sandeepsoni/semantic-progressiveness"" rel=""nofollow noreferrer"">https://github.com/sandeepsoni/semantic-progressiveness</a>), and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so I have no clue about how the authors implemented NCE estimation via gensim's word2vec.</p>
<p>The authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under Skip-gram embeddings. NCE? According to your manual,  it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. But then what estimation method is used?
negative (int, optional) â€“ If &gt; 0, negative sampling will be used, the int for negative specifies how many â€œnoise wordsâ€ should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>Thanks you in advance, and look forward to hearing from you soon!</p>
","python, nlp, gensim, word2vec","<p>You can view the default parameters for the Gensim <code>Word2Vec</code> model, in an unmodified Gensim library, in the Gensim docs. Here's a link to the current version (4.1) docs for the <code>Word2Vec</code> constructor method, showing all default parameter values:</p>
<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p>
<blockquote>
<p><em>class</em> gensim.models.word2vec.Word2Vec(<em>sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True</em>)</p>
</blockquote>
<p>Two of those parameters â€“ <code>hs=0, negative=5</code> â€“ mean the default mode has hierarchical-softmax disabled, and negative-sampling enabled with 5 negative words. These have been the default of Gensim's <code>Word2Vec</code> for many versions, so even other code is using an older version, this is likely the mode used (unless parameters or modified/overriden code changed them).</p>
",2,0,387,2021-10-05 05:49:03,https://stackoverflow.com/questions/69445437/default-estimation-method-of-gensims-word2vec-skip-gram
Word2Vec for network embedding ignores words (nodes) in corpus (walks),"<p>I am trying to run word2vec (Skipgram) to a set of walks for training a network embedding model, in my graph I have 169343 nodes, i.e; word in the context of Word2vec, and for each node I run a random walk with length 80. Therefore, I have (169343,80) walks, i.e; sentences in Word2vec. after running SkipGram for 3 epochs I only get 28015 vectors instead of 169343. and here is the code for my Network Embedding.</p>
<pre class=""lang-py prettyprint-override""><code>def run_skipgram(walk_path):

    walks = np.load(walk_path).tolist()

    skipgram = Word2Vec(sentences=walks, vector_size=128, negative=5, window=8, sg=1, workers=6, epochs=3)
    
    keys = list(map(int, skipgram.wv.index_to_key))
    keys.sort()

    vectors = [skipgram.wv[key] for key in keys]

    return np.array(vectors)
</code></pre>
","python, machine-learning, graph, gensim, word2vec","<p>Are you sure your <code>walks</code> corpus is what you expect, and what Gensim <code>Word2Vec</code> expects?</p>
<p>For example, is <code>len(walks)</code> equal to 169343? Is <code>len(walks[0])</code> equal to 80? Is <code>walks[0]</code> a list of 80 string-tokens?</p>
<p>Note also: by default <code>Word2Vec</code> uses a <code>min_count=5</code> - so any token that appears fewer than 5 times is ignored during training. In most cases, this minimum â€“ or an even higher one! â€“ makes sense, because tokens with only 1, or a few, usage examples in usual natural-language training data <em>can't</em> get good word-vectors (but can, in aggregate, function as dilutive noise that worsens other vectors).</p>
<p>Depending on your graph, one walk from each node might not ensure that node appears at least 5 times in all the walks. So you could try <code>min_count=1</code>.</p>
<p>But it'd probably be better to do <em>5</em> walks from every starting point, or enough walks to ensure all nodes appear at least 5 times. <code>169,343 * 80 * 5</code> is still only 67,737,200 training words, with a manageable 169,343 count vocabulary. (If there's an issue expanding the whole training set as one list, you could make an iterable that generates the walks as needed, one by one, rather than all up-front.)</p>
<p>Alternatively, something like 5 walks per starting-node, but of only 20 steps each, would keep the corpus about the same size bu guarantee each node appears at least 5 times.</p>
<p>Or even: adaptively keep adding walks <em>until</em> you're sure every node is represented enough times. For example, pick a random node, do a walk, keep a running tally of each node's appearances so far â€“ &amp; keep growing the net total of every node's
You could also try an adaptive corpus that keeps adding walks <em>until</em> every node is represented a minimum number of times.</p>
<p>Conceivably, for some remote nodes, that might take quite long to happen upon them, so another refinement might be: do some initial walk or walks, then tally how many visits each node got, &amp; while the least-frequent node is below the target <code>min_count</code>, start another walk from it â€“ guaranteeing it at least one more visit.</p>
<p>This could help oversample less-connected regions, which might be good or bad. Notably, with natural language text, the <code>Word2Vec</code> <code>sample</code> parameter is quite helpful to <em>discard</em> certain overrepresented words, preventing them from monopolizing training time redundantly, ensuring less-frequent words also get good representations. (It's a parameter which can sometimes provide the double-whammy of less training time <em>and</em> better results!) Ensuring your walks spend more time in less-connected areas might provide a similar advantage, especially if your downstream use for the vectors is just as interested in the vectors for the less-visited regions.</p>
",0,0,210,2021-10-17 10:20:40,https://stackoverflow.com/questions/69603325/word2vec-for-network-embedding-ignores-words-nodes-in-corpus-walks
Improve speed of python algorithm,"<p>I have used Sentiment140 dataset for twitter for sentiment analysis</p>
<p>Code:</p>
<p>getting words from tweets:</p>
<pre><code>tweet_tokens = []
[tweet_tokens.append(dev.get_tweet_tokens(idx)) for idx, item in enumerate(dev)]
</code></pre>
<p>getting unknown words from tokens</p>
<pre><code>words_without_embs = []
[[words_without_embs.append(w) for w in tweet if w not in word2vec] for tweet in tweet_tokens]
len(words_without_embs)
</code></pre>
<p>last part of code, calculate vector as the mean of left and right words (context)</p>
<pre><code>vectors = {} # alg
for word in words_without_embs:
  mean_vectors = []
  for tweet in tweet_tokens:
    if word in tweet:
      idx = tweet.index(word)
      try:
        mean_vector = np.mean([word2vec.get_vector(tweet[idx-1]), word2vec.get_vector(tweet[idx+1])], axis=0)
        mean_vectors.append(mean_vector)
      except:
        pass

    if tweet == tweet_tokens[-1]: # last iteration
      mean_vector_all_tweets = np.mean(mean_vectors, axis=0)
      vectors[word] = mean_vector_all_tweets
</code></pre>
<p>There are 1058532 words and the last part of this code works very slow, about 250 words per minute.</p>
<p>How can you improve the speed of this algorithm?</p>
","python, algorithm, machine-learning, nlp, word2vec","<p>More-common (&amp; probably better) strategies for dealing with unknown words include:</p>
<ul>
<li>training/using a model, like FastText, that can offer guess-vectors for out-of-vocabulary (OOV) words</li>
<li>acquiring more training data, so vectors for more unknown words can be learned from real usages</li>
<li>ignoring unknown words entirely</li>
</ul>
<p>It seems you've decided to instead synthesize new vectors for OOV words, by averaging all of their immediate neighbors. I don't think this would work especially well. In many kinds of downstream uses of the word-vectors, it just tends to overweight the word's in-context neighbors â€“ which can also be very simply/cheaply achieved by just ignoring the unknown word entirely.</p>
<p>But given what you want to do, the best approach would be to collect the neighboring words during the same pass that identifies the <code>words_without_embs</code>.</p>
<p>For example, make <code>words_without_embs</code> a <code>dict</code> (or perhaps a <code>DefaultDict</code>), where each key is a word that will need a vector, and each value is a <code>list</code> of all the neighboring words you've found so far.</p>
<p>Then, one loop over the <code>tweet_tokens</code> would both fill the <code>words_without_embs</code> with <em>keys</em> that are the words needing vectors, while stuffing those <em>values</em> with all the neighboring-words seen so far.</p>
<p>Then, one last loop over the <code>words_without_embs</code> keys would simply grab the existing lists of neighbor-words for the averaging. (No more multiple passes over <code>tweet_tokens</code>.)</p>
<p>But again: all this work might not outperform the baseline practice of simply dropping unknown words.</p>
",1,2,175,2021-10-29 14:18:23,https://stackoverflow.com/questions/69770465/improve-speed-of-python-algorithm
training custom word2vec model,"<p>i have my own dataset in which i want to use gensim word2vec to train but i'm not sure how to do it.</p>
<pre><code>from google.colab import files
import io
uploaded = files.upload()
data_path = 'chatbot_dataset.txt'
with open(data_path, 'r') as f:
    lines = f.read().split('\n')

for line in lines:
    input_text = line.split('\t')[0]
    if len(input_text.split()) &gt; MAX_SENTENCE_LENGTH:
      break
    target_text = '&lt;START&gt; ' + line.split('\t')[1] + &quot; &lt;END&gt;&quot;
    input_texts.append(input_text)
    target_texts.append(target_text)

model = Word2Vec(lines, min_count=1,workers=3,size=100,window=3,sg=1)
model.wv.get_vector('hello')
</code></pre>
<p>but i got this error while doing it, even though the word 'hello' is already in my dataset:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-15-b41c8cb17d3b&gt; in &lt;module&gt;()
    140 model.wv.vector_size
    141 #check out how 'PEM' is represented in an array of 100 numbers
--&gt; 142 model.wv.get_vector('hello')
    143 #find words with similar meaning to 'PEN'
    144 model.wv.most_similar('to')

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(&quot;word '%s' not in vocabulary&quot; % word)
    453 
    454     def get_vector(self, word):

KeyError: &quot;word 'hello' not in vocabulary&quot;
</code></pre>
","python, machine-learning, nlp, word2vec","<p>You're feeding <code>lines</code>, which appears to be a list of plain strings, to <code>Word2Vec</code>.</p>
<p><code>Word2Vec</code> is instead expecting a re-iterable sequence of items, where each item is a pre-tokenized list-of-strings. By passing it a sequence of plain strings instead, when <code>Word2Vec</code> interprets one string as a list, it will see it as a list-of-single-characters â€“ so the entire set of 'words' it learns will just be single-characters. (There may have been a warning in your logs about that, or if you were running with at least INFO logging, progress-reporting that shows a suspiciously-tiny number of discovered unique words.)</p>
<p>You can look at what your model's volcabulary wound up being by examining <code>model.wv.index_to_key</code> - for example, peeking at the 10 most-common words found by <code>print(model.wv.index_to_key[:10]</code>. If that doesn't look right, make sure you're properly preprocessing/tokenizing the corpus you'll be handing to <code>Word2Vec</code>.</p>
<p>Separately: <code>min_count=1</code> is never a good idea with <code>Word2Vec</code>. Only words with multiple varied usage examples can achieve useful word-vectors, and usually discarding the rarest words, as with the default <code>min_count=5</code>, ensures the best-quality vectors for all surviving words. (If there are words with fewer than 5 usage examples for which you need vectors, the best approach is obtain more varied-usage training data.)</p>
",0,0,717,2021-10-31 13:22:22,https://stackoverflow.com/questions/69787306/training-custom-word2vec-model
Pythonic way to obtain a distance matrix from word vectors in gensim 4.0,"<p>I am currently using gensim version 4.0.1 to generate word vectors. My ultimate goal is to compute cosine distances between all pairwise combinations word vectors and to use the obtained distance matrix for clustering the word vectors. So far I have been been generating the distance matrix with the following code:</p>
<pre><code>    print('Setting up Word2Vec model')
    model = gensim.models.Word2Vec (genome_tokens, vector_size=100, window=args.window_size, min_count=args.min_cluster_size, workers=args.threads, sg=1)

    print('Training Word2Vec model')
    model.train(genome_tokens,total_examples=len(genome_tokens),epochs=10)

    words = sorted(model.wv.index_to_key)
    scaled_data = [model.wv[w] for w in words]
    print('Calculating distribution distance among clusters')
    cluster_distrib_distance = pairwise_distances(scaled_data, metric=args.metric)
</code></pre>
<p>I was wondering if there is a specific function to obtain the distance matrix directly from the model object, without having to create the words and scaled data object.</p>
<p>Going through the gensim documentation I have mostly found information regarding ways to calculate similarities, rather than distances and often between documents rather than individual words. There does seem to be some discussion on this topic on the <a href=""https://github.com/RaRe-Technologies/gensim/issues/140"" rel=""nofollow noreferrer"">github repository</a>, but the methods described there seem to be specific to the older versions as is the case for the solution presented <a href=""https://stackoverflow.com/questions/45280020/getting-distance-matrix-and-features-matrix-from-word2vec-model"">here</a></p>
","python, nlp, gensim, word2vec","<p>There's no built-in utility method for that.</p>
<p>But, you can get the raw backing array, with all the vectors in it, in the <code>model.wv.vectors</code> property. Each row is the word-vector for the corresponding word in the same position in <code>index_to_key</code>.</p>
<p>You can feed this into <code>sklearn.metrics.pairwise_distances</code> (or similar) directly, without the need for the separate (&amp; differently-sorted) <code>scaled_data</code> outside.</p>
<p>Note that if using something like Euclidean distance, you <em>might</em> want the word-vectors to be unit-length-normalized before calculating distances. Then all distances will be in the range <code>[0.0, 2.0]</code>, and ranked distances will be the exact reverse of ranked cosine-similarities.</p>
<p>In that case you'd again want to work from an external set of vectors â€“ either by using <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_vector"" rel=""nofollow noreferrer""><code>get_vector(key, norm=True)</code></a> to get them 1-by-1, or <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_normed_vectors"" rel=""nofollow noreferrer""><code>get_normed_vectors()</code></a> to get a fully unit-normed version of the <code>.vectors</code> array.</p>
",1,0,498,2021-11-02 16:02:51,https://stackoverflow.com/questions/69813465/pythonic-way-to-obtain-a-distance-matrix-from-word-vectors-in-gensim-4-0
Word2Vec error: TypeError: unhashable type: &#39;list&#39;,"<p>I'm experimenting with peptide sequences and NLP right now and am trying to embed the peptide sequences using word2vec.
The peptides come in a long string format (ex: 'KCNTATCATQRLANFLVRSSNNLGPVLPPTNVGSNTY'), so I've split the peptide sequences into trigrams. But as I'm trying to embed them, I keep getting this error: <code>TypeError: unhashable type:'list.'</code></p>
<p>Not sure how to fix this error as I don't quite understand why it's coming up. My code is linked <a href=""https://colab.research.google.com/drive/1HWny5SfklWnhUcfO-y-QdOgRe8tmJcaZ?usp=sharing"" rel=""nofollow noreferrer"">here</a>, and here is the full error output:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-17-966c68819734&gt; in &lt;module&gt;()
      6 
      7 # embeddings pos
----&gt; 8 w2vpos = Word2Vec(kmersdatapos, size=EMB_DIM,window=5,min_count=5,negative=15,iter=10,workers=multiprocessing.cpu_count())

4 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1553                 )
   1554             for word in sentence:
-&gt; 1555                 vocab[word] += 1
   1556             total_words += len(sentence)
   1557 

TypeError: unhashable type: 'list'
</code></pre>
<p>Any suggestions are appreciated!</p>
","python, nlp, bioinformatics, gensim, word2vec","<p>You need to pass a list of list of strings to gensim's Word2Vec. In your code you are passing kmersdatapos to Word2Vec, which is list of list of list of strings.
For example:</p>
<pre><code>corpus = [[&quot;lorem&quot;, &quot;ipsum&quot;], [&quot;dolor&quot;], [&quot;sit&quot;, &quot;amet&quot;]]
</code></pre>
<p>is a valid parameter for the Word2Vec function. Whereas,</p>
<pre><code>corpus = [[[&quot;lorem&quot;, &quot;ipsum&quot;], [&quot;dolor&quot;]], [[&quot;sit&quot;, &quot;amet&quot;]]] 
</code></pre>
<p>is invalid.</p>
",2,0,485,2021-11-03 08:38:54,https://stackoverflow.com/questions/69821842/word2vec-error-typeerror-unhashable-type-list
How to seek for bigram similarity in gensim word2vec model,"<p>Here I have a word2vec model, suppose I use the google-news-300 model</p>
<pre><code>import gensim.downloader as api
word2vec_model300 = api.load('word2vec-google-news-300')
</code></pre>
<p>I want to find the similar words for &quot;AI&quot; or &quot;artifical intelligence&quot;, so I want to write</p>
<pre><code>word2vec_model300.most_similar(&quot;artifical intelligence&quot;)
</code></pre>
<p>and I got errors</p>
<pre><code>KeyError: &quot;word 'artifical intelligence' not in vocabulary&quot;
</code></pre>
<p>So what is the right way to extract similar words for bigram words?</p>
<p>Thanks in advance!</p>
","machine-learning, nlp, gensim, word2vec","<p>At one level, when a word-token isn't in a fixed set of word-vectors, the creators of that set of word-vectors chose not to train/model that word. So, anything you do will only be a crude workaround for its absence.</p>
<p>Note, though, that when Google prepared those vectors â€“ based on a dataset of news articles from before 2012 â€“ they also ran some statistical multigram-combinations on it, creating multigrams with connecting <code>_</code> characters. So, first check if a vector for <code>'artificial_intelligence'</code> might be present.</p>
<p>If it isn't, you could try other rough workarounds like averaging together the vectors for <code>'artificial'</code> and <code>'intelligence'</code> â€“ though of course that won't really be what people mean by the distinct combination of those words, just meanings suggested by the independent words.</p>
<p>The Gensim <code>.most_similar()</code> method can take either a raw vectors you've created by operations such as averaging, or even a list of multiple words which it will average for you, as arguments via its explicit keyword <code>positive</code> parameter. For example:</p>
<pre class=""lang-py prettyprint-override""><code>word2vec_model300.most_similar(positive=[average_vector])
</code></pre>
<p>...or...</p>
<pre class=""lang-py prettyprint-override""><code>word2vec_model300.most_similar(positive=['artificial', 'intelligence'])
</code></pre>
<p>Finally, though Google's old vectors are handy, they're a bit old now, &amp; from a particular domain (popular news articles) where senses may not match tose used in other domains (or more recently). So you may want to seek alternate vectors, or train your own if you have sufficient data from your area of interest, to have apprpriate meanings â€“ including vectors for any particular multigrams you choose to tokenize in your data.</p>
",3,3,826,2021-11-10 08:22:12,https://stackoverflow.com/questions/69909863/how-to-seek-for-bigram-similarity-in-gensim-word2vec-model
word2vec: optimize for low-frequency words,"<p>Given the classical implementation of <code>word2vec</code> by â€ªTomas Mikolovâ€¬, what set of parameters (<code>window</code>, <code>sample</code>, <code>negative</code>, maybe <code>cbow</code>)</p>
<pre><code>./word2vec -train corpus.txt \
    -output vec.txt \
    -min-count 5 -size 150 \
    -window 5 -sample 1e-5 -negative 10
    -threads 16
</code></pre>
<p>optimize for computing better embeddings for low-frequency words (say with frequency 5 to 25)?</p>
",word2vec,"<p>The only <code>word2vec.c</code> command-line parameters which differentially affect words by their frequency are <code>min_count</code>, which discards words below a certain threshold, and <code>sample</code>, which randomly discards <em>some</em> occurrences of highly-frequent words.</p>
<p>You can do that discarding because using all occurrences of highly-frequent words is overkill: it barely improves their vectors over fewer training samples, it takes extra training time, &amp; it essentially dilutes the influence of rarer words on the model's internal shared weights â€“ while for many applications, rare words are as important (or mote!) than frequent words.</p>
<p>So one definite way to make training spend more time/effort, relatively, on lower-frequency words is to use a <em>more-aggressive</em> <code>sample</code> value, which means a <em>smaller</em> number, and more of the most-frequent-words being randomly skipped.</p>
<p>The default is <code>1e-04</code>; especially as your corpus grows, you could try a 10x smaller value like <code>1e-05</code>, a 100x smaller value like <code>1e-06</code>, or try even lower. As with other parameter tweaks, you should have some repeatable evaluation of the final vector quality, for your project purposes, that can be used to guide such adjustments.</p>
<p>A more aggressive <code>sample</code> can sometimes deliver a double-whammy of both faster training â€“ by dropping the redundant high-frequency words â€“ &amp; better final results â€“ by both giving more weight to rarer words, &amp; effectively *shrinkingâ€¢ the context-windows whereever frequent-words are dropped. (The words are elided before the context-windows considered â€“ so will move retained words that were just outside the window into it.)</p>
<p>I've seen a very-aggressive <code>window</code> value of <code>1e-06</code> or higher discard a majority of the pre-downsampling corpus, in typical natural language distributions. The saved training time might also then make it thinkable to consider otherwise impractically-larger values for other parameters which tend to increase training time (like <code>epochs</code>, <code>size</code>, <code>negative</code>, <code>window</code>).</p>
<p>There's another parameter, controlling the rates of negative-example sampling, that I believe is called <em>alpha</em> in the original word2vec paper, and frozen at <code>0.75</code> in the original Google <code>word2vec.c</code> tool. However, <a href=""https://arxiv.org/abs/1804.04212"" rel=""nofollow noreferrer"">some research</a> has suggested other values of this parameter may be useful in some applications â€“ perhaps especially recommendation systems, and systems where the word-tokens don't have usual natural-language Zipfian distributions.</p>
<p>So, you may also want to try tinkering with that parameter. (Other implementations of word2vec, like Python Gensim's version, <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">offer this as a parameter <code>ns_exponent</code></a>.)</p>
<p>(Tinkering with the other parameters might help in your project aims, vis-a-vis the quality of less-frequent words' vectors, but not in an obvious way - you'd have to find such interactions by experiments in your domain.)</p>
",2,0,461,2021-11-11 09:24:11,https://stackoverflow.com/questions/69925900/word2vec-optimize-for-low-frequency-words
How to plot tsne on word2vec (created from gensim) for the most_similar 20 cases?,"<p>I am using TSNE to plot a trained word2vec model (created from gensim):</p>
<pre><code>labels = []
tokens = []

for word in model.wv.vocab:
    tokens.append(model[word])
    labels.append(word)

tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
new_values = tsne_model.fit_transform(tokens)

x = []
y = []
for value in new_values:
    x.append(value[0])
    y.append(value[1])
    
plt.figure(figsize=(50, 50)) 
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
plt.show()
</code></pre>
<p>Like as the inbuilt gensim method 'most_similar', per ex.</p>
<pre><code>w2v_model.wv.most_similar(postive=['word'], topn=20)
</code></pre>
<p>will output 20 of the most similar words to 'word', I will like to plot only the most similar words (n=20) of a given word. Any advice on how to modify the plot to do that?</p>
","python, gensim, word2vec, tsne","<p>Using an example from the package:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

model = Word2Vec(sentences=common_texts, window=5, min_count=1)

labels = [i for i in model.wv.vocab.keys()]
tokens = model[labels]

tsne_model = TSNE(init='pca',learning_rate='auto')
new_values = tsne_model.fit_transform(tokens)
</code></pre>
<p>tsne will look something like this:</p>
<pre><code>plt.figure(figsize=(7, 5)) 
for i in range(new_values.shape[0]):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
</code></pre>
<p><a href=""https://i.sstatic.net/bSfPS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bSfPS.png"" alt=""enter image description here"" /></a></p>
<p>Extract most similar for 'trees' (5 in my case) :</p>
<pre><code>most_sim_words = [i[0] for i in model.wv.most_similar(positive='trees', topn=5)]
most_sim_words
['human', 'graph', 'time', 'interface', 'system']
</code></pre>
<p>You can use code you have, just iterating through the most common words, and using <code>index()</code> to get their index in <code>tokens</code> :</p>
<pre><code>for word in most_sim_words:
    i = labels.index(word)
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
</code></pre>
<p><a href=""https://i.sstatic.net/KycV9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KycV9.png"" alt=""enter image description here"" /></a></p>
",1,1,851,2021-12-07 23:41:52,https://stackoverflow.com/questions/70268270/how-to-plot-tsne-on-word2vec-created-from-gensim-for-the-most-similar-20-cases
Gensim doc2vec&#39;s d2v.wv.most_similar() gives not relevant words with high similarity scores,"<p>I've got a dataset of job listings with about 150 000 records. I extracted skills from descriptions using NER using a dictionary of 30 000 skills. Every skill is represented as an unique identificator.</p>
<p>My data example:</p>
<pre><code>          job_title    job_id                                         skills
1  business manager         4               12 13 873 4811 482 2384 48 293 48
2    java developer        55    48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist        21    383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>Then, I train a doc2vec model using these data where job titles (their ids to be precise) are used as tags and skills vectors as word vectors.</p>
<pre><code>def tagged_document(df):
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [str(row['job_id'])])
        
        
data_for_training = list(tagged_document(data[['job_id', 'skills']]))

model_d2v = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, vector_size=80, min_count=3, epochs=100, window=100000)

model_d2v.build_vocab(data_for_training)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)
</code></pre>
<p>It works mostly okay, but I have issues with some job titles. I tried to collect more data from them, but I still have an unpredictable behavior with them.</p>
<p>For example, I have a job title &quot;Director Of Commercial Operations&quot; which is represented as 41 data records having from 11 to 96 skills (mean 32). When I get most similar words for it (skills in my case) I get the following:</p>
<pre><code>docvec = model_d2v.docvecs[id_]
model_d2v.wv.most_similar(positive=[docvec], topn=5)
</code></pre>
<pre><code>capacity utilization 0.5729076266288757
process optimization 0.5405482649803162
goal setting 0.5288119316101074
aeration 0.5124399662017822
supplier relationship management 0.5117508172988892
</code></pre>
<p>These are top 5 skills and 3 of them look relevant. However the top one doesn't look too valid together with &quot;aeration&quot;. The problem is that none of the job title records have these skills at all. It seems like a noise in the output, but why it gets one of the highest similarity scores (although generally not high)?
Does it mean that the model can't outline very specific skills for this kind of job titles?
Can the number of &quot;noisy&quot; skills be reduced? Sometimes I see much more relevant skills with lower similarity score, but it's often lower than 0.5.</p>
<p>One more example of correct behavior with similar amount of data:
BI Analyst, 29 records, number of skills from 4 to 48 (mean 21). The top skills look alright.</p>
<pre><code>business intelligence 0.6986587047576904
business intelligence development 0.6861011981964111
power bi 0.6589289903640747
tableau 0.6500121355056763
qlikview (data analytics software) 0.6307920217514038
business intelligence tools 0.6143202781677246
dimensional modeling 0.6032138466835022
exploratory data analysis 0.6005223989486694
marketing analytics 0.5737696886062622
data mining 0.5734485387802124
data quality 0.5729933977127075
data visualization 0.5691111087799072
microstrategy 0.5566076636314392
business analytics 0.5535123348236084
etl 0.5516749620437622
data modeling 0.5512707233428955
data profiling 0.5495884418487549
</code></pre>
","nlp, gensim, word2vec, word-embedding, doc2vec","<p>If the your gold standard of what the model should report is skills that appeared in the training data, are you sure you don't want a simple count-based solution? For example, just provide a ranked list of the skills that appear most often in <code>Director Of Commercial Operations</code> listings?</p>
<p>On the other hand, the essence of compressing N job titles, and 30,000 skills, into a smaller (in this case <code>vector_size=80</code>) coordinate-space model is to force some non-intuitive (but perhaps real) relationships to be reflected in the model.</p>
<p>Might there be some real pattern in the model â€“ even if, perhaps, just some idiosyncracies in the appearance of less-common skills â€“ that makes <code>aeration</code> necessarily slot near those other skills? (Maybe it's a rare skill whose few contextual appearances co-occur with other skills very much near 'capacity utilization' -meaning with the tiny amount of data available, &amp; tiny amount of overall attention given to this skill, there's no better place for it.)</p>
<p>Taking note of whether your 'anomalies' are often in low-frequency skills, or lower-freqeuncy job-ids, might enable a closer look at the data causes, or some disclaimering/filtering of <code>most_similar()</code> results. (The <code>most_similar()</code> method can limit its returned rankings to the more frequent range of the known vocabulary, for cases when the long-tail or rare words are, in with their rougher vectors, intruding in higher-quality results from better-reqpresented words. See the <code>restrict_vocab</code> parameter.)</p>
<p>That said, tinkering with training parameters may result in rankings that better reflect your intent. A larger <code>min_count</code> might remove more tokens that, lacking sufficient varied examples, mostly just inject noise into the rest of training. A different <code>vector_size</code>, smaller or larger, might better capture the relationships you're looking for. A more-aggressive (smaller) <code>sample</code> could discard more high-frequency words that might be starving more-interesting less-frequent words of a chance to influence the model.</p>
<p>Note that with <code>dbow_words=1</code> &amp; a large window, and records with (perhaps?) dozens of skills each, the words are having a much-more <em>neighborly</em> effect on each other, in the model, than the <code>tag</code>&lt;-&gt;<code>word</code> correlations. That might be good or bad.</p>
",0,1,459,2021-12-14 14:53:20,https://stackoverflow.com/questions/70350954/gensim-doc2vecs-d2v-wv-most-similar-gives-not-relevant-words-with-high-simila
stuck in feeding my SVM after creating my node2vec model,"<p>I am trying to do node classification using Node2Vec and SVM on a graph obtained from protein-protein interaction to predict disease genes related to a specific disease. to be accurate. the point is that, I have created a Graph using <code>networkx</code> , my nodes have <code>labels</code>(names of protein) and attributes=0/1(if this protein is causing a disease or not). I applied <code>node2vec</code> on this graph and I have my model. (I don't care about values of p and q at this stage) but I don't know how to proceed and feed it to SVM or more importantly, how to reduce dimensions of my vectorized graph before feeding it to SVM. plus, I don't know if in these vectors, the attributes of my node are included or not. separately however, I have a dictionary called lbls to have my nodes and their value
here is small piece of code</p>
<pre><code>node2vec = Node2Vec(G, dimensions=512, workers=4,p=1,q=2)
model = node2vec.fit(window=10, min_count=1, batch_words=4)
</code></pre>
","numpy, networkx, svm, word2vec","<p><code>node2vec</code> is an unsupervised node embedding method, which is based on <code>Word2Vec</code>. Have a look at the <a href=""https://snap.stanford.edu/node2vec/"" rel=""nofollow noreferrer"">snap documentation</a> for a brief description of the model. It will not use any of your attributes/features to create the embedding. It uses a shallow encoding, which are directly learned using a random walk based objective function. For details look at the paper or check the <a href=""https://web.stanford.edu/class/cs224w/index.html"" rel=""nofollow noreferrer"">Stanford Lecture</a> about node embeddings, which covers in detail Node2Vec. Your embedding dimension is currently <code>512</code> (for the node embedding of node2vec), which you probably can reduce.</p>
",0,0,212,2021-12-15 11:26:49,https://stackoverflow.com/questions/70362866/stuck-in-feeding-my-svm-after-creating-my-node2vec-model
Is there a way to use only the word result of most_similar function of gensim?,"<p>I am trying to use the most similar function of gensim but the results came in as a list that is let's say a=(word, cosine similarity). However I can't retrieve the word by a[0].Is there a way to access the word itself? I need to use it as an input.</p>
","trigonometry, gensim, word2vec, similarity","<p>The ranked list of results returned by <code>most_similar()</code> is a <em>list</em> where each item is a tuple of a word and its similarity value.</p>
<p>After...</p>
<pre><code>sims = model.wv.most_similar(my_word)
top_word = sims[0][0]
</code></pre>
<p>...<code>top_word</code> would have the word most-similar to <code>my_word</code>.</p>
",0,0,305,2021-12-16 06:24:45,https://stackoverflow.com/questions/70374705/is-there-a-way-to-use-only-the-word-result-of-most-similar-function-of-gensim
Can&#39;t load the pre-trained word2vec of korean language,"<p>I would like to download and load the pre-trained word2vec for analyzing Korean text.</p>
<p>I download the pre-trained word2vec here: <a href=""https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg</a>
from the Github Pre-trained word vectors of 30+ languages: <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors</a></p>
<p>My gensim version is 4.1.0, thus I used:
<code>KeyedVectors.load_word2vec_format('./ko.bin', binary=False)</code> to load the model. But there was an error that :</p>
<blockquote>
<p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</p>
</blockquote>
<p>I already tried many options including in stackoverflow and Github, but it still not work well.
Would you mind letting me the suitable solution?</p>
<p>Thanks,</p>
","gensim, word2vec","<p>While the page at <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors</a> isn't clear about the formats this author has chosen, by looking at their source code at...</p>
<p><a href=""https://github.com/Kyubyong/wordvectors/blob/master/make_wordvectors.py#L61"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors/blob/master/make_wordvectors.py#L61</a></p>
<p>...shows it using the Gensim model <code>.save()</code> method.</p>
<p>Such saved models should be reloaded using the <code>.load()</code> class method of the same model class. For example, if a <code>Word2Vec</code> model was saved with...</p>
<pre><code>model.save('language.bin')
</code></pre>
<p>...then it could be reloaded with...</p>
<pre><code>loaded_model = Word2Vec.load('language.bin')
</code></pre>
<p>Note, through, that:</p>
<ul>
<li>Models saved this way are often split over multiple files that should be kept together (and all start with the same root name) - but I don't see those here.</li>
<li>This work appears to be ~5 years old, based on a pre-1.0 version of Gensim â€“ so there might be issues loading the models directly into the latest Gensim. If you do run into such issues, &amp; absolutely need to make these vectors work, you might need to temporarily use a prior version of Gensim to <code>.load()</code> the model. Then, you could save the plain vectors out with <code>.save_word2vec_format()</code> for later reloading across any version. (Or, using the latest interim version that can load the model, re-save the model as <code>.save()</code>, then repeat the process with the latest version that can read <em>that</em> model, until you reach the current Gensim.)</li>
</ul>
<p>But, you also might want to find a more recent &amp; better-documented set of pretrained word-vectors.</p>
<p>For example, Facebook makes FastText pretrained vectors available in both a 'text' format and a 'bin' format for many languages at <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a> (trained on Wikipedia only) or <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a> (trained on Wikipedia plus web crawl data).</p>
<p>The 'text' format should in fact be loadable with <code>KeyedVectors.load_word2vec_format(filename, binary=False)</code>, but will only include full-word vectors. (It will also be relatively easy to view as text, or write simply code to massage into other formats.)</p>
<p>The 'bin' format is Facebook's own native FastText model format, and should be loadable with either the <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer""><code>load_facebook_model()</code></a> or <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer""><code>load_facebook_vectors()</code></a> utility methods. Then, the loaded model (or vectors) will be able to create the FastText algorithm's substring-based guesstimate vectors even for many words that weren't in the model or training data.</p>
",1,1,494,2021-12-23 07:03:09,https://stackoverflow.com/questions/70458726/cant-load-the-pre-trained-word2vec-of-korean-language
Text classification using Word2Vec and Pos tag,"<p>I have a medical dataset like</p>
<p>Text: &quot;weakness, diarrhea, neck pain&quot; Target:&quot;X.1, Y.1&quot; which is coded diagnosis</p>
<p>Also I am using pre-trained Word2Vec and pos tagging.
For example the word weakness has Word vector like</p>
<p>[0.2 0.04 ........ 0.05] (300 dim)</p>
<p>And pos tagging is &quot;Symptom, Noun&quot;</p>
<p>My question is how to combine pos tagging and word embedding to train with keras ?</p>
","keras, word2vec, pos-tagger","<p>There are multiple ways to deal with that.</p>
<ol>
<li><p>You can build an ensemble model, i.e., you can train with pos tags and word2vec seperately using two different models. If you get the prediction value at the final layer (or some interpretation of probability in any model), you can take the average for your final prediction.</p>
</li>
<li><p>You can combine word2vec with pos tags to run a neural network.</p>
</li>
</ol>
<p>However, I strongly believe POS tags will not be a good idea in these cases. You can see, all these words may have similar pos tags (most are isolated words and nouns), and data will have much less entropy.</p>
",0,0,744,2021-12-26 20:50:28,https://stackoverflow.com/questions/70489768/text-classification-using-word2vec-and-pos-tag
Gensim train not updating weights,"<p>I have a domain specific corpus for which I am trying to train embeddings. Since I want to be comprehensive in vocabulary, I am adding word vectors from <code>glove.6B.50d.txt</code>. Post adding vectors from here, I am training the model using the corpus I have.</p>
<p>I am trying the solutions from <a href=""https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights"">here</a> but the word embeddings don't seem to update.</p>
<p>This is the solution I have so far.</p>
<pre><code>#read glove embeddings
glove_wv = KeyedVectors.load_word2vec_format(GLOVE_PATH, binary=False)

#initialize w2v model
model =  Word2Vec(vector_size=50, min_count=0, window=20, epochs=10, sg=1, workers=10, 
                      hs=1, ns_exponent=0.5, seed=42, sample=10**-2, shrink_windows=True)
model.build_vocab(sentences_tokenized)
training_examples_count = model.corpus_count

# add vocab from glove
model.build_vocab([list(glove_wv.key_to_index.keys())], update=True)
model.wv.vectors_lockf = np.zeros(len(model.wv)) # ALLOW UPDATE OF WEIGHTS FROM BACK PROP; 0 WILL SUPPRESS

# add glove embeddings
model.wv.intersect_word2vec_format(GLOVE_PATH,binary=False, lockf=1.0)
</code></pre>
<p>Below I am training the model and checking word embedding of a particular word explicitly present in training</p>
<pre><code># train model
model.train(sentences_tokenized,total_examples=training_examples_count, epochs=model.epochs)

#CHECK IF EMBEDDING CHANGES FOR 'oyo'
print(model.wv.get_vector('oyo'))
print(glove_wv.get_vector('oyo'))
</code></pre>
<p>The word embeddings of the word <code>oyo</code> comes out to be same before and after the training. Where am I going wrong?</p>
<p>The input corpus- <code>sentences_tokenized</code> contains few sentences that contains the word <code>oyo</code>. One of such sentences-</p>
<pre><code>'oyo global platform empowers entrepreneur small business hotel home providing full stack technology increase earnings eas operation bringing affordable trusted accommodation guest book instantly india largest budget hotel chain oyo room one preferred hotel booking destination vast majority student country hotel chain offer many benefit include early check in couple room id card flexibility oyo basically network budget hotel completely different famous hotel aggregator like goibibo yatra makemytrip partner zero two star hotel give makeover room bring customer hotel website mobile app'
</code></pre>
","python, stanford-nlp, gensim, word2vec, word-embedding","<p>You're improvising a lot here with a bunch of potential errors or suboptimalities. Note especially that:</p>
<ul>
<li>While (because it's Python) you can always mutate the models however you want for interesting effects, seeding a model with outside word-vectors then continuing training isn't formally- or well-supported by Gensim. As far as I can tell â€“ &amp; I wrote a bunch of this code! â€“ there aren't any good docs/examples of doing it well, or doing the necessary tuning/validation of results, or demonstrating a reliable advantage of this technique. Most examples online are of eager people plowing ahead unaware of the tradeoffs, seeing a trivial indicator of completion or a tiny bit of encouraging results, and then overconfidently showing their work as if this were a well-grounded technique or best-practice. It isn't. Without a deep understanding of the model, &amp; review of the source code, &amp; regular re-checking of your results for sanity/improvement, there will be hidden gotchas. It is especially the case that fresh training on just a subset of all words could pull those words out of compatible coordinate alignment with other words not receiving training.</li>
<li>The <code>intersect_word2vec_format()</code> feature, and especially the <code>lockf</code> function, are also experimental - one stab at maybe offering a way to mix in other word-vectors, but without any theoretical support. (I also believe <code>intersect_word2vec_format()</code> remains slightly broken in recent (circa 4.1.2) Gensim versions, though there may be a <a href=""https://github.com/RaRe-Technologies/gensim/issues/3094"" rel=""nofollow noreferrer"">simple workaround</a>.) Still, the <code>lockf</code> functionality may require tricky manual initialization &amp; adaptation to other non-standard steps. To use it, it'd be best to read &amp; understand the Gensim source code where related variables appear.</li>
</ul>
<p>So, if you really need a larger vocabulary than your initial domain-specific corpus, the safest approach is probably to extend your training corpus with more texts that feature the desired words, as used in similar language contexts. (For example, if you rdomain is scientific discourse, you'd want to extend your corpus with more similar scientific text to learn compatible words â€“ not, say, classic fiction.) Then all words go through the well-characterized simultaneous training process.</p>
<p>That said, if you really want to continue experimenting with this potentially complicated and error-prone improvised approach, your main problems might be:</p>
<ul>
<li>using strings as your sentences instead of lists-of-tokens (so the training 'words' wind up actually just being single-characters)</li>
<li>something related to the <code>intersect_word2vec_format</code> bug; check if <code>.vectors_lockf</code> is the right length, with <code>1.0</code> in the all the right slots for word-updates, before training</li>
</ul>
<p>Separately, other observations:</p>
<ul>
<li><code>min_count=0</code> is usually a bad idea: these models improve when you discard rare words entirely. (Though, when doing a <code>.build_vocab(â€¦, update=True)</code> vocab-expansion, a bunch of things with the usual neat handling of low-frequency words and frequency-sorted vocabularies become screwy.)</li>
<li><code>hs=1</code> should generally not be set without also disabling the usually-preferred default negative-sampling with <code>negative=0</code>. (Otherwise, you're creating a hybrid franken-model, using both modes on one side of the internal neural network, that share the same input word-vectors: a much slower approach not especially likely to be better than either alone.)</li>
<li><code>ns_exponent=0.5</code> is non-standard, and using non-standard values for the parameter is most-likely to offer benefit in peculiar situations (like training texts that aren't true natural language sentences), and should only be tweaked within a harness for comparing results with alternate values.</li>
<li><code>sample=10**-2</code> is also non-standard, and such a large value might be nearly the same as turning off <code>sample</code> (say with a <code>0</code> value) entirely. It's more common to want to make this parameter more-aggressive (smaller than the default), if you have plentiful training data.</li>
</ul>
<p>In general, while the defaults aren't sacred, you generally should avoid tinkering with them until you have both (a) a good idea of why your corpus/goals might benefit from a different value; &amp; (b) a system for verifying which alterations are helping or hurting, such as a grid-search over many parameter combinations that scores the fitness of resulting models on (some proxy for) your true end task.</p>
",1,1,392,2021-12-30 14:48:04,https://stackoverflow.com/questions/70533179/gensim-train-not-updating-weights
gensim w2k - additional file,"<p>I trained w2v on rather big (&gt; 200 million sentences) corpus, and got, in addition to file w2v_model.model, files: w2v_model.model.trainables.syn1neg.npy and w2v.model_model.wv.vectors.npy. Model file was successfully loaded and read all npy files without any exceptions. The obtained model performed OK.</p>
<p>Now I retrained the model on much bigger corpus (&gt; 1 billion sentences). The same 3 files were automatically saved, as expected.</p>
<p>When I try to load my new retrained model:</p>
<pre><code>w2v_model = Word2Vec.load(path_filename)
</code></pre>
<p>I get:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/Users/...../w2v_US.model.trainables.vectors_lockf.npy'
</code></pre>
<p>But no .npy file with such extension was saved by gensim at the end of the training
(I save all output files in the same library, as required).</p>
<p>What should I do to obtain such file as a part of output .npy files (may be some option in  gensim w2v when training)? May be there are other ways to overcome this issue?</p>
","gensim, word2vec","<p>If a <code>.save()</code> is creating any files with the word <code>trainables</code> in it, you're using a older version fo Gensim. Any new training should definitely prefer using a current version. As of now (January 2022), that's <code>gensim-4.1.2</code>, released 2021-09.</p>
<p>If an attempt at a <code>.load()</code> generated that particular error, then there should've been that file, alongside the others you mention, created when the <code>.save()</code> had been done. (In fact, the only way that the <em>main</em> file you named with <code>path_filename</code> should be able to know that <em>other</em> filename is if that other file was written successfully, allowing the main file to complete writing.)</p>
<p>Are you sure that file wasn't written, but then somehow left behind, perhaps getting deleted or not moving alongside the other few files to some new filesystem path?</p>
<p>In general, I would suggest:</p>
<ul>
<li>using latest Gensim for any new training</li>
<li>always enable Python logging at the INFO level, &amp; watch the logging/console output of training/saving processes closely to see confirmation of expected activity/steps</li>
<li>keep all files from a <code>.save()</code> that begin with the same main filename (in your examples above, <code>w2v_US.model</code>) together - &amp; keep in mind that for larger models it may be a larger roster of files than for a small test model</li>
</ul>
<p>You will probably have to re-train the model, but you <em>might</em> be able to re-generate a compatible <code>lockf</code> file via steps like the following:</p>
<ul>
<li>save aside all files of any potential use</li>
<li>from the exact same configuration as your original <code>.save()</code> â€“ including the same outdated Gensim version, exact same model parameters, &amp; exact same training corpus â€“ repeat all the model-building steps you did before up through the <code>.build_vocab()</code> step. (That is: no extra need to <code>.train()</code>.) This will create an <em>untrained</em> dummy model that should exactly match the vocabulary 'shape' of your broken model.</li>
<li>use <code>.save()</code> to save <em>that</em> dummy model again - watching the logs/output for errors. There should be, alongside the other files, a file with a name like <code>dummy.model.trainables.vectors_lockf.npy</code>. If so, you <em>might</em> be able to copy that away, rename it to tbe the file expected by the original model whose load failed, then leave it alongside that original model - and the <code>.load()</code> might then succeed, or fail in a different way.</li>
</ul>
<p>(If there were other problems/corruption at the time of the original model creation, this might not work. In particular, I wonder if when you talk about retraining the model, you didn't start with a fresh <code>Word2Vec</code> instance, but somehow expanded the older one, which might've added other problems/complications. In that case, a full retraining, ideally in the latest Gensim, would be necessary, and also a better basis for going forward.)</p>
",1,1,142,2022-01-13 08:20:41,https://stackoverflow.com/questions/70693372/gensim-w2k-additional-file
How to get vocabulary size of word2vec?,"<p>I have a pretrained word2vec model in pyspark and I would like to know how big is its vocabulary (and perhaps get a list of words in the vocabulary).
Is this possible? I would guess it has to be stored somewhere since it can predict for new data, but I couldn't find a clear answer in the <a href=""https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.Word2Vec.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>I tried <code>w2v_model.getVectors().count()</code> but the result (<em>970</em>) seem too small for my use case. In case it may be relevant, I'm using short-text data and my dataset has tens of millions of messages each having from 10 to 30/40 words. I am using <code>min_count=50</code>.</p>
","machine-learning, pyspark, nlp, word2vec, apache-spark-ml","<p>Not quite sure why you doubt the result of <code>.getVectors().count()</code>, which gives the desired result indeed, as shown in the <a href=""https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.Word2Vec.html"" rel=""nofollow noreferrer"">documentation link</a> you have provided yourself.</p>
<p>Here is the example posted there, with a vocabulary of just three (3) tokens - <code>a</code>, <code>b</code>, and <code>c</code>:</p>
<pre><code>from pyspark.ml.feature import Word2Vec

sent = (&quot;a b &quot; * 100 + &quot;a c &quot; * 10).split(&quot; &quot;) # 3-token vocabulary
doc = spark.createDataFrame([(sent,), (sent,)], [&quot;sentence&quot;])
word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=&quot;sentence&quot;, outputCol=&quot;model&quot;)
model = word2Vec.fit(doc)
</code></pre>
<p>So, unsurprisingly, it is</p>
<pre><code>model.getVectors().count()
# 3
</code></pre>
<p>and asking for the vectors themselves</p>
<pre><code>model.getVectors().show()
</code></pre>
<p>gives</p>
<pre><code>+----+--------------------+
|word|              vector|
+----+--------------------+
|   a|[0.09511678665876...|
|   b|[-1.2028766870498...|
|   c|[0.30153277516365...|
+----+--------------------+
</code></pre>
<p>In your case, with <code>min_count=50</code>, every word that appears less than 50 times in your corpus will not be represented; reducing this number will result in more vectors.</p>
",0,0,1228,2022-01-16 18:59:54,https://stackoverflow.com/questions/70733235/how-to-get-vocabulary-size-of-word2vec
Understanding results of word2vec gensim for finding substitutes,"<p>I have implemented the word2vec model on transaction data <a href=""https://docs.google.com/spreadsheets/d/1mcFnagqajPm9XqJhBjgXW2waGraJ4mVq/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">(link)</a> of a single category. <br>
My goal is to find substitutable items from the data.<br>
The model is giving results but I want to make sure that my model is giving results based on customers historical data (considering context) and not just based on content (semantic data). Idea is similar to the recommendation system. <br>
I have implemented this using the gensim library, where I passed the data (products) in form of a list of lists.</p>
<p>Eg.</p>
<pre><code>[['BLUE BELL ICE CREAM GOLD RIM', 'TILLAMK CHOC CHIP CK DOUGH  IC'],
 ['TALENTI SICILIAN PISTACHIO GEL', 'TALENTI BLK RASP CHOC CHIP GEL'],
 ['BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS COFF ICE CREAM']]
</code></pre>
<p>Here, each of the sub lists is the past one year purchase history of a single customer. <br></p>
<pre><code># train word2vec model
model = Word2Vec(window = 5, sg = 0,
                 alpha=0.03, min_alpha=0.0007,
                 seed = 14)

model.build_vocab(purchases_train, progress_per=200)

model.train(purchases_train, total_examples = model.corpus_count, 
            epochs=10, report_delay=1)

# extract all vectors
X = []
words = list(model.wv.index_to_key)
for word in words:
    x = model.wv.get_vector(word)
    X.append(x)
Y = np.array(X)
Y.shape

def similar_products(v, n = 3):
    
    # extract most similar products for the input vector
    ms = model.wv.similar_by_vector(v, topn= n+1)[1:]
    
    # extract name and similarity score of the similar products
    new_ms = []
    for j in ms:
        pair = (products_dict[j[0]][0], j[1]) 
        new_ms.append(pair)
        
    return new_ms 

similar_products(model.wv['BLUE BELL ICE CREAM GOLD RIM'])
</code></pre>
<p>Results:</p>
<pre><code> [('BLUE BELL ICE CREAM BROWN RIM', 0.7322707772254944),
     ('BLUE BELL ICE CREAM LIGHT', 0.4575043022632599),
     ('BLUE BELL ICE CREAM NSA', 0.3731085956096649)]
</code></pre>
<p>To get intuitive understanding of word2vec and its working on how results are obtained, I created a dummy <a href=""https://docs.google.com/spreadsheets/d/16n2CbXiiZSgFy3wAroC4uV4QUdiS3T8-/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">dataset</a> where I wanted to find subtitutes of <code>'FOODCLUB VAN IC PAIL'</code>. <br>
If two products are in the same basket multiple times then they are substitutes. <br>
Looking at the data first substitute should be <code>'FOODCLUB CHOC CHIP IC PAIL'</code> but the results I obtained are:</p>
<pre><code>[('FOODCLUB NEAPOLITAN IC PAIL', 0.042492810636758804),
 ('FOODCLUB COOKIES CREAM ICE CREAM', -0.04012278839945793),
 ('FOODCLUB NEW YORK VAN IC PAIL', -0.040678512305021286)]
</code></pre>
<ol>
<li>Can anyone help me understand the intuitive working of word2vec model in gensim? Will each product be treated as word and customer list as sentence?</li>
<li>Why are my results so absurd in dummy dataset? How can I improve?</li>
<li>What hyperparameters play a significant role w.r.t to this model? Is negative sampling required?</li>
</ol>
","python, gensim, word2vec, word-embedding, recommendation-engine","<p>You may not get a very good intuitive understanding of usual word2vec behavior using these sorts of product-baskets as training data. The algorithm was originally developed for natural-language texts, where texts are runs of tokens whose frequencies, &amp; co-occurrences, follow certain indicative patterns.</p>
<p>People certainly do use word2vec on runs-of-tokens that aren't natural language - like product baskets, or logs-of-actions, etc â€“ but to the extent such tokens have very-different patterns, it's possible extra preprocessing or tuning will be necessary, or useful results will be harder to get.</p>
<p>As just a few ways customer-purchases might be different from real language, depending on what your &quot;pseudo-texts&quot; actually represent:</p>
<ul>
<li>the ordering within a text might be an artifact of how you created the data-dump rather than anything meaningful</li>
<li>the nearest-neighbors to each token within the <code>window</code> may or may not be significant, compared to more distant tokens</li>
<li>customer ordering patterns might in general not be as reflective of shades-of-relationships as words-in-natural-language text</li>
</ul>
<p>So it's not automatic that word2vec will give interesting results here, for recommendatinos.</p>
<p>That's especially the case with small datasets, or tiny dummy datasets. Word2vec requires <em>lots</em> of varied data to pack elements into interesting relative positions in a high-dimensional space. Even small demos usually have a vocabulary (count of unique tokens) of tens-of-thousands, with training texts that provide varied usage examples of every token dozens of times.</p>
<p>Without that, the model never learns anything interesing/generalizable. That's especially the case if trying to create a many-dimensions model (say the default <code>vector_size=100</code>) with a tiny vocabulary (just dozens of unique tokens) with few usage examples per example. And it only gets worse if tokens appear fewer than the default <code>min_count=5</code> times â€“ when they're ignored entirely. So don't expect anything interesting to come from your dummy data, at all.</p>
<p>If you want to develop an intuition, I'd try some tutorials &amp; other goals with real natural language text 1st, with a variety of datasets &amp; parameters, to get a sense of what has what kind of effects on result usefulness â€“ &amp; only after that try to adapt word2vec to other data.</p>
<p>Negative-sampling is the default, &amp; works well with typical datasets, especially as they grow large (where negative-sampling suffes less of a performance hit than hierarchical-softmax with large vocabularies). But a toggle between those two modes is unlike to cause giant changes in quality unless there are other problems.</p>
<p>Sufficient data, of the right kind, is the key â€“ &amp; then tweaking parameters may nudge end-result usefulness in a better direction, or shift it to be better for certain purposes.</p>
<p>But more specific parameter tips are only possible with clearer goals, once some baseline is working.</p>
",2,0,449,2022-01-25 05:32:55,https://stackoverflow.com/questions/70843845/understanding-results-of-word2vec-gensim-for-finding-substitutes
Use word2vec to expand a glossary in order to classify texts,"<p>I have a database containing about 3 million texts (tweets). I put clean texts (removing stop words, tags...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each text).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>I have also a list of words (belonging to the same topic, in this case: economics) called <code>terms</code>. I found that 7% of the texts contain at least one of these words (so we can say that 7% of total tweets talk about economics).</p>
<p>My goal is to expand the list <code>terms</code> in order to retrieve more texts belonging to the economic topic.</p>
<p>Then I use</p>
<p><code>results = model.most_similar(terms, topn=5000)</code></p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>Finally if I create the data frame</p>
<p><code>df = pd.DataFrame(results, columns=['key', 'similarity'])</code></p>
<p>I get something like that:</p>
<pre><code>key       similarity
word1     0.795432
word2     0.787954
word3     0.778942
...       ...
</code></pre>
<p>Now I think I have two possibilities to define the expanded glossary:</p>
<ul>
<li>I take the first N words (what should be the value of N?);</li>
<li>I look at the suggested words one by one and decide which one to include in the expanded glossary based on my knowledge (does this word really belong to the economic glossary?)</li>
</ul>
<p>How should I proceed in a case like this?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>There's no general answer for what the cutoff should be, or how much you should use your own manual judgement versus cruder (but fast/automatic) processes. Those are inherently decisions which will be heavily influenced by your data, model quality, &amp; goals â€“ so you have to try different approaches &amp; see what works there.</p>
<p>If you had a goal for what percentage of the original corpus you want to take â€“ say, 14% instead of 7% â€“ you could go as deeply into the ranked candidate list of 'similar words' as necessary to hit that 14% target.</p>
<p>Note that when you retrieve <code>model.most_similar(terms)</code>, you are asking the model to 1st average all words in <code>terms</code> together, then return words close to that one average point. To the extent your seed set of terms is tightly around the idea of <code>economics</code>, that might find words close to that generic average idea â€“ but might not find other interesting words, such as close sysnonyms of your seed words that you just hadn't thought of. For that, you might want to get not 5000 neighbors for one generic average point, but (say) 3 neighbors for every individual term. To the extent the 'shape' of the topic isn't a perfect sphere around someplace in the word-vector-space, but rather some lumpy complex volume, that might better reflect your intent.</p>
<p>Instead of using your judgement of the candidate words standing alone to decide whether a word is <code>economics</code>-related, you could instead look at the texts that a word uniquely brings in. That is, for new word X, look at the N texts that contain that word. How many, when applying your full judgement to their full text, deserve to be in your 'economics' subset? Only if it's above some threshold T would you want to move X into your glossary.</p>
<p>But such an exercise may just highlight: using a simple glossary â€“ &quot;for any of these hand-picked N words, every text mentioning at least 1 word is in&quot; â€“ is a fairly crude way of assessing a text's topic. There are other ways to approach the goal of &quot;pick a relevant subset&quot; in an automated way.</p>
<p>For example, you could view your task as that of training a text binary classifier to classify texts as 'economics' or 'not-economics'.</p>
<p>In such a case, you'd start with some training data - a set of example documents that are already labeled 'economics' or 'not-economics', perhaps via individual manual review, or perhaps via some crude bootstrapping (like labeling all texts with some set of glossary words as 'economics', &amp; all others 'not-economics'). Then you'd draw from the full range of potential text-preprocessing, text-feature-extracton, &amp; classification options to train &amp; evaluate classifiers that make that judgement for you. Then you'd evaluate/tune those â€“ a process wich might also improve your training data, as you add new definitively 'economics' or 'not-economics' texts â€“ &amp; eventually settle on one that works well.</p>
<p>Alternatively, you could use some other richer topic-modeling methods (LDA, word2vec-derived <code>Doc2Vec</code>, deeper neural models etc) for modeling the whole dataset, then from some seed-set of definite-'economics' texts, expand outward from them â€“ finding nearest-examples to known-good documents, either auto-including them or hand-reviewing them.</p>
<p>Separately: <code>min_count=1</code> is almost always a mistake in word2vec &amp; related algorihtms, which do better if you <em>discard</em> words so rare they lack the variety of multiple usage examples the algorithm needs to generate good word-vectors.</p>
",1,0,290,2022-01-30 14:57:09,https://stackoverflow.com/questions/70915829/use-word2vec-to-expand-a-glossary-in-order-to-classify-texts
Plotly - Highlight data point and nearest three points on hover,"<p>I have made a scatter plot of the word2vec model using plotly. <br>
I want functionality of highlighting the specific data point on hover along with the top 3 nearest vectors to that.
It would be of great help if anyone can guide me with this or suggest any other option
<br><br>
<a href=""https://drive.google.com/file/d/1VPVC8vnLF1V58EnX9Svi51DVOFcF_4dy/view?usp=sharing"" rel=""nofollow noreferrer"">model</a> <br>
<a href=""https://drive.google.com/file/d/1MBaM7sS-lh2eQ-k1OgNfaggkJcstmJq8/view?usp=sharing"" rel=""nofollow noreferrer"">csv</a>
<br><br>
Code:</p>
<pre><code>import gensim
import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
import plotly.express as px

def get_2d_coordinates(model, words):
    arr = np.empty((0,100), dtype='f')
    labels = []
    for wrd_score in words:
        try:
            wrd_vector = model.wv.get_vector(wrd_score)
            arr = np.append(arr, np.array([wrd_vector]), axis=0)
            labels.append(wrd_score)
        except:
            pass
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)
    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    return x_coords, y_coords

ic_model = gensim.models.Word2Vec.load(&quot;w2v_IceCream.model&quot;)
ic = pd.read_csv('ic_prods.csv')

icx, icy = get_2d_coordinates(ic_model, ic['ITEM_DESC'])
ic_data = {'Category': ic['SUB_CATEGORY'],
            'Words':ic['ITEM_DESC'],
            'X':icx,
            'Y':icy}
ic_df = pd.DataFrame(ic_data)
ic_df.head()
ic_fig = px.scatter(ic_df, x=icx, y=icy, color=ic_df['Category'], hover_name=ic_df['Words'], title='IceCream Data')
ic_fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/8AK1h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8AK1h.png"" alt=""enter image description here"" /></a></p>
","python, plotly, data-visualization, word2vec, plotly-python","<p>In plotly-python, I don't think there's an easy way of retrieving the location of the cursor. You can attempt to use go.FigureWidget to highlight a trace as described in <a href=""https://stackoverflow.com/questions/53327572/how-do-i-highlight-an-entire-trace-upon-hover-in-plotly-for-python"">this answer</a>, but i think you're going to be limited with with plotly-python and i'm not sure if highlighting the closest n points will be possible.</p>
<p>However, I believe that you can accomplish what you want in <code>plotly-dash</code> since callbacks are supported - meaning you would be able to retrieve location of your cursor and then calculate the <code>n</code> closest data points to your cursor and highlight the data points as needed.</p>
<p>Below is an example of such a solution. If you haven't seen it before, it looks complicated, but what is happening is that I am taking the point where you clicked as an input. plotly is plotly.js under the hood so it comes us in the form of a dictionary (and not some kind of plotly-python object). Then I calculate the closest three data points to the clicked input point by comparing the coordinates of every other point in the dataframe, add the information from the three closest points as traces to the input with the color <code>teal</code> (or any color of your choosing), and send this modified input back as the output, and update the figure.</p>
<p>I am using click instead of hover because hover would cause the highlighted points to flicker too much as you drag your mouse through the points.</p>
<p>Also the dash app doesn't work perfectly as I believe there is some issue when you double click on points (you can see me click once in the gif below before getting it to start working), but this basic framework is hopefully close enough to what you want. Cheers!</p>
<pre><code>import gensim
import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
import plotly.express as px
import plotly.graph_objects as go

import json

import dash
from dash import dcc, html, Input, Output

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']
app = dash.Dash(__name__, external_stylesheets=external_stylesheets)


def get_2d_coordinates(model, words):
    arr = np.empty((0,100), dtype='f')
    labels = []
    for wrd_score in words:
        try:
            wrd_vector = model.wv.get_vector(wrd_score)
            arr = np.append(arr, np.array([wrd_vector]), axis=0)
            labels.append(wrd_score)
        except:
            pass
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)
    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    return x_coords, y_coords

ic_model = gensim.models.Word2Vec.load(&quot;w2v_IceCream.model&quot;)
ic = pd.read_csv('ic_prods.csv')

icx, icy = get_2d_coordinates(ic_model, ic['ITEM_DESC'])
ic_data = {'Category': ic['SUB_CATEGORY'],
            'Words':ic['ITEM_DESC'],
            'X':icx,
            'Y':icy}

ic_df = pd.DataFrame(ic_data)
ic_fig = px.scatter(ic_df, x=icx, y=icy, color=ic_df['Category'], hover_name=ic_df['Words'], title='IceCream Data')

NUMBER_OF_TRACES = len(ic_df['Category'].unique())
ic_fig.update_layout(clickmode='event+select')

app.layout = html.Div([
    dcc.Graph(
        id='ic_figure',
        figure=ic_fig)
    ])

## we take the 4 closest points because the 1st closest point will be the point itself
def get_n_closest_points(x0, y0, df=ic_df[['X','Y']].copy(), n=4):

    &quot;&quot;&quot;we can save some computation time by looking for the smallest distance^2 instead of distance&quot;&quot;&quot;
    &quot;&quot;&quot;distance = sqrt[(x1-x0)^2 + (y1-y0)^2]&quot;&quot;&quot;
    &quot;&quot;&quot;distance^2 = [(x1-x0)^2 + (y1-y0)^2]&quot;&quot;&quot;
    
    df[&quot;dist&quot;] = (df[&quot;X&quot;]-x0)**2 + (df[&quot;Y&quot;]-y0)**2

    ## we don't return the point itself which will always be closest to itself
    return df.sort_values(by=&quot;dist&quot;)[1:n][[&quot;X&quot;,&quot;Y&quot;]].values

@app.callback(
    Output('ic_figure', 'figure'),
    [Input('ic_figure', 'clickData'),
    Input('ic_figure', 'figure')]
    )
def display_hover_data(clickData, figure):
    print(clickData)
    if clickData is None:
        # print(&quot;nothing was clicked&quot;)
        return figure
    else:
        hover_x, hover_y = clickData['points'][0]['x'], clickData['points'][0]['y']
        closest_points = get_n_closest_points(hover_x, hover_y)

        ## this means that this function has ALREADY added another trace, so we reduce the number of traces down the original number
        if len(figure['data']) &gt; NUMBER_OF_TRACES:
            # print(f'reducing the number of traces to {NUMBER_OF_TRACES}')
            figure['data'] = figure['data'][:NUMBER_OF_TRACES]
            # print(figure['data'])
        
        new_traces = [{
            'marker': {'color': 'teal', 'symbol': 'circle'},
            'mode': 'markers',
            'orientation': 'v',
            'showlegend': False,
            'x': [x],
            'xaxis': 'x',
            'y': [y],
            'yaxis': 'y',
            'type': 'scatter',
            'selectedpoints': [0]
        } for x,y in closest_points]

        figure['data'].extend(new_traces)
        # print(&quot;after\n&quot;)
        # print(figure['data'])
        return figure

if __name__ == '__main__':
    app.run_server(debug=True)
</code></pre>
<p><a href=""https://i.sstatic.net/gAtz9.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gAtz9.gif"" alt=""enter image description here"" /></a></p>
",2,2,2721,2022-02-01 16:45:31,https://stackoverflow.com/questions/70944316/plotly-highlight-data-point-and-nearest-three-points-on-hover
Convert grouped data in dataframe to documents in preparation for word2vec,"<p>I'm trying to replicate what the author of <a href=""https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9319034"" rel=""nofollow noreferrer"">this paper</a> has achieved with the publicly available Medicare dataset.</p>
<p>In summary, the author groups medical provider claims by the providers ID, their taxonomy and the HCPCS (codes of the procedures they performed) by most frequent to least frequent, see below image:</p>
<p><a href=""https://i.sstatic.net/vqvUv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vqvUv.png"" alt=""enter image description here"" /></a></p>
<p>Using the code below I have been able to recreate the top left hand table and also the bottom left table (I don't think it's necessary) but I don't know how to group each providers HCPCS codes by highest frequency to lowest frequency in preparation for feeding it into word2vec to train an embedding model.</p>
<p>If I could get some help preparing the data ready for word2vec training I would be very grateful.</p>
<pre><code>library(httr)
library(jsonlite)
library(tidyverse)

# CONNECT TO CMS DATA
res &lt;- GET(&quot;https://data.cms.gov/data-api/v1/dataset/5fccd951-9538-48a7-9075-6f02b9867868/data?size=5000&quot;)

# CONVERT TO DATA FRAME
data = fromJSON(rawToChar(res$content))

# GROUPING AND COUNTING OCCURANCES OF HCPCS PER PROVIDER ID
providerHCPCS &lt;- data %&gt;% 
  group_by(Rndrng_NPI,Rndrng_Prvdr_Type,HCPCS_Cd) %&gt;% 
  count(HCPCS_Cd, name = &quot;Line_Srvc_Cnt&quot;) %&gt;% 
  group_by(Rndrng_NPI) %&gt;% 
  arrange(desc(Line_Srvc_Cnt), .by_group = TRUE)
</code></pre>
","r, tidyverse, word2vec, word-embedding","<p>Is this what you want as a result?</p>
<pre><code>table2 &lt;- providerHCPCS %&gt;% group_by(Rndrng_NPI, Rndrng_Prvdr_Type) %&gt;% summarise(HCPCS_sequence = (paste(HCPCS_Cd, collapse=&quot;, &quot;)))

</code></pre>
",1,0,45,2022-02-03 23:55:27,https://stackoverflow.com/questions/70979931/convert-grouped-data-in-dataframe-to-documents-in-preparation-for-word2vec
"When applying word2vec, should I standardize the cosine values within each year, when comparing them across years?","<p>I'm a researcher, and I'm trying to apply NPL to understand the temporal changes of the meaning of some words.
So far I have obtained the trained embeddings (word2vec, sgn) of several years with identical parameters in the training.
For example, if I want to test the change of cosine similarity of word A and word B over 5 years, should I just compute them and plot the cosine values?</p>
<p>The reason I'm asking this is that I found the overall cosine values (mean of all possible pairs within that year) differ across the 5 years. **For example, 1990:0.21, 1991:0.19, 1992:0.31, 1993:0.22, 1994:0.31. Does it mean in some years, all words are more similar to each other than other years??</p>
<p>Base on my limited understanding, I think the vectors are odds in logistic functions, so they shouldn't be significantly affected by the size of the corpus? Is it necessary for me to standardize the cosine values (of all pairs within each year) so I can compare the relative ranking change across years? Or just trust the raw cosine values and compare them across years?</p>
","word2vec, word-embedding, cosine-similarity","<p>In general you should not think of cosine-similarities as an <em>absolute</em> measure that'd be comparable between models. That is, you should not think of &quot;0.7&quot; cosine-similarity as anything like &quot;70%&quot; similar, and choose some arbitrary &quot;70%&quot; threshold to be used across models.</p>
<p>Instead, it's only a measure within a single model's induced space - with its effective 'scale' affected by all the parameters &amp; the training data.</p>
<p>One small exercise that may help illustrate this: with the exact same data, train a 100d model, then a 200d model. Then look at some word pairs, or words alongside their nearest-neighbors ranked by cosine-similarity.</p>
<p>With enough training/data, generally the same highly-related words will be nearest-neighbors of each other. But the effective ranges of cosine-similarity values will be very different. If you chose a specific threshold in one model as meaning, &quot;close enough to feed some other analysis&quot;, the same threshold would <em>not</em> be sufficient in the other. Every model is its own world, induced by the training data &amp; parameters, as well as some sources of explicit or implicit randomness during training. (Several parts of the word2vec algorithm use random sampling, but also any efficient multi-threaded training will encounter arbitray differences in training-order via host OS thread-scheduling vagaries.)</p>
<p>If your parameters are identical, &amp; the corpora very-alike in every measurable internal proportion, these effects might be minimized, but never eliminated.</p>
<p>For example, even if people's intended word meanings were perfectly identical, one year's training data might include more discussion of 'war' or 'politics' or some medical-topic, than another. In that case, the iterative, interleaved tug-of-war in training updates will mean words from that overrepresented domain have far more push-pull influence on the final model word positions â€“ essentially warping subregions of the final space for <em>finer</em> distinctions some places, and thus *coarser distinctions in the less-updated zones.</p>
<p>That is, you shouldn't expect any global-per-model scaling factor (as you've implied might apply) to correct for any model-to-model differences. The influences of different data &amp; training runs are far more subtle, and might affect different 'neighborhoods' of words differently.</p>
<p>Instead, when comparing different models, a more stable grounds for comparison is <em>relative rankings</em> or <em>relative-proportions</em> of words with respect to their closeness-to-others. Did words move into, or out of, each others' top-N neighbors? Did A move more closely to B than C did to D? etc.</p>
<p>Even there, you might want to be careful about differences in the full vocabulary: if A &amp; B were each others' closest neighbors year 1, but 5 other words squeeze between them in year 2, did any word's meaning <em>really</em> change? Or might it simply be because those other words weren't even suitably represented in year 1 to receive any position, or previously had somewhat 'noisier' positions nearby? (As words get rarer their positions from run to run will be more idiosyncratic, based on their few usage examples, and the influences of those other sources of run-to-run 'noise'.)</p>
<p>Limiting all such analyses to very-well-represented words will minimize misinterpreting noise-in-the-models as something meaningful. Re-running models more than once, either with same parameters or slightly-different ones, or slightly-different training data subsets, and seeing which comparisons hold up across such changes, may also help determine which observed changes are robust, versus methodological artifacts such as jitter from run-to-run, or other sampling effects.</p>
<p>A few previous answers on similar questions about comparing word-vectors across different source corpora may have other useful ideas or caveats for you:</p>
<p><a href=""https://stackoverflow.com/questions/59084092/how-calculate-distance-between-2-node2vec-model/59095246#59095246"">how calculate distance between 2 node2vec model</a></p>
<p><a href=""https://stackoverflow.com/questions/57392103/word-embeddings-for-the-same-word-from-two-different-texts/57400356#57400356"">Word embeddings for the same word from two different texts</a></p>
<p><a href=""https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models/61741677#61741677"">How to compare cosine similarities across three pretrained models?</a></p>
",1,1,487,2022-02-08 12:13:45,https://stackoverflow.com/questions/71033726/when-applying-word2vec-should-i-standardize-the-cosine-values-within-each-year
Word2Vec returning vectors for individual character and not words,"<p>For the following list:</p>
<pre><code>words= ['S.B.MILS','Siblings','DEVASTHALY','KOTRESHA','unimodal','7','regarding','random','59','intimating','COMPETITION','prospects','2K15','gather','Mega','SENSOR','NCTT','NETWORKING','orgainsed','acts']
</code></pre>
<p>I try to:</p>
<pre><code>from gensim.models import Word2Vec
vec_model= Word2Vec(words, min_count=1, size=30)
vec_model['gather']
</code></pre>
<p>Which returns:</p>
<blockquote>
<p>KeyError: &quot;word 'gather' not in vocabulary&quot;</p>
</blockquote>
<p>But</p>
<pre><code>vec_model['g']
</code></pre>
<p>Does return a vector, so believe i'm returning all vectors for characters found in the list instead of vectors for all words found in the list.</p>
","python, nlp, word2vec","<p>Word2Vec expects a list of lists as input, where the corpus (main list) is composed of individual documents. The individual documents are composed of individual words (tokens). Word2Vec iterates over all documents and all tokens. In your example you have passed a single list to Word2Vec, therefore Word2Vec interprets each word as an individual document and iterates over each word character which is interpreted as a token. Therefore you have built a vocabulary of characters not words. To build a vocabulary of words you can pass a nested list to Word2Vec as in the example below.</p>
<pre><code>from gensim.models import Word2Vec

words= [['S.B.MILS','Siblings','DEVASTHALY','KOTRESHA'],
['unimodal','7','regarding','random','59','intimating'],
['COMPETITION','prospects','2K15','gather','Mega'],
['SENSOR','NCTT','NETWORKING','orgainsed','acts']]

vec_model= Word2Vec(words, min_count=1, size=30)
vec_model['gather']
</code></pre>
<p>Output:</p>
<pre><code>array([ 0.01106581,  0.00968017, -0.00090574,  0.01115612, -0.00766465,
       -0.01648632, -0.01455364,  0.01107104,  0.00769841,  0.01037362,
        0.01551551, -0.01188449,  0.01262331,  0.01608987,  0.01484082,
        0.00528397,  0.01613582,  0.00437328,  0.00372362,  0.00480989,
       -0.00299072, -0.00261444,  0.00282137, -0.01168992, -0.01402746,
       -0.01165612,  0.00088562,  0.01581018, -0.00671618, -0.00698833],
      dtype=float32)
</code></pre>
",1,1,419,2022-02-12 10:54:15,https://stackoverflow.com/questions/71091209/word2vec-returning-vectors-for-individual-character-and-not-words
Using gensim most_similar function on a subset of total vocab,"<p>I am trying to use the gensim word2vec <code>most_similar</code> function in the following way:</p>
<pre><code>wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;])
</code></pre>
<p>So basically, I multiple query words and I want to return the most similar outputs, but from a finite set. i.e. if vocab is 2000 words, then I want to return the most similar from a set of say 100 words, and not all 2000.</p>
<p>e.g.</p>
<pre><code>Vocab:
word_a, word_b, word_c, word_d, word_e ... words_z

Finite set:
word_d, word_e, word_f

</code></pre>
<p><em><strong>most_similar on whole vocab</strong></em></p>
<pre><code>wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;])
output = ['word_d', 'word_f', 'word_g', 'word_x'...]
</code></pre>
<p><em><strong>desired output</strong></em></p>
<pre><code>finite_set = ['word_d', 'word_e', 'word_f']
wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;], finite_set) &lt;-- some way of passing the finite set

output = ['word_d', 'word_f']
</code></pre>
","python, gensim, word2vec","<p>Depending on your specific patterns of use, you have a few options.</p>
<p>If you want to confine your results to a <em>contiguous range</em> of words in the <code>KeyedVectors</code> instance, a few optional parameters can help.</p>
<p>Most often, people want to confine results to the <em>most frequent</em> words. Those are generally those with the best-trained word-vectors. (When you get deep into less-frequent words, the few training examples tend to make their vectors somewhat more idiosyncratic â€“ both from randomization that's part of the algorithm, and from any ways the limited number of examples don't reflect the word's &quot;true&quot; generalizable sense in the wider world.)</p>
<p>Using the optional parameter <code>restrict_vocab</code>, with an integer value N, will limit the results to just the first N words in the <code>KeyedVectors</code> (which by usual conventions are those that were most-frequent in the training data). So for example, adding <code>restrict_vocab=10000</code> to a call against a set-of-vectors with 50000 words will only retun the most-similar words from the 1st 10000 known words. Due to the effect mentioned above, these will often be the most reliable &amp; sensible results - while nearby words from the longer-tail of low-frequency words are more likely to seem a little out of place.</p>
<p>Similarly, instead of <code>restrict_vocab</code>, you can use the optional <code>clip_start</code> &amp; <code>clip_end</code> parameters to limit results to any other contiguous range. For example, adding <code>clip_start=100, clip_end=1000</code> to your <code>most_similar()</code> call will only return results from the 900 words in that range (leaving out the 100 most-common words in the usual case). I suppose that might be useful if you're finding the most-frequent words to be too generic â€“ though I haven't noticed that being a typical problem.</p>
<p>Based on the way the underlying bulk-vector libraries work, both of the above options efficiently calculate <em>only</em> the needed similarities before sorting out the top-N, using native routines that might achieve nice parallelism without any extra effort.</p>
<p>If your words are a discontiguous mix throughout the whole <code>KeyedVectors</code>, there's no built-in support for limiting the results.</p>
<p>Two options you could consider include:</p>
<ul>
<li><p>Especially if you repeatedly search against the exact same subset of words, you could try creating a new <code>KeyedVectors</code> object with just those words - then every <code>most_similar()</code> against that separate set is just what you need. See the constructor &amp; <code>add_vector()</code> or <code>add_vectors()</code> methods in the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVectors</code> docs</a> for how that could be done.</p>
</li>
<li><p>Requesting a larger set of results, then filtering your desired subset. For example, if you supply <code>topn=len(wv_from_bin)</code>, you'll get back <em>every</em> word, ranked. You could then filter those down to only your desired subset. This does extra work, but that might not be a concern depending on your model size &amp; required throughput. For example:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>finite_set = set(['word_d', 'word_e', 'word_f'])  # set for efficient 'in'
all_candidates = wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;],
                                          topn=len(vw_from_bin))
filtered_results = [word_sim for word_sim in all_candidates if word_sim[0] in finite_set]
</code></pre>
<ul>
<li>You could save a <em>little</em> of the cost of the above by getting <em>all</em> the similarities, unsorted, using the <code>topn=None</code> option - but then you'd still have to subset those down to your words-of-interest, then sort yourself. But you'd still be paying the cost of all the vector-similarity calculations for all words, which in typical large-vocabularies is more of the runtime than the sort.</li>
</ul>
<p>If you were tempted to iterate over your subset &amp; calculate the similarities 1-by-1, be aware that can't take advantage of the math library's bulk vector operations â€“ which use vector CPU operations on large ranges of the underlying data â€“ so will usually be a lot slower.</p>
<p>Finally, as an aside: if your vocabulary is truly only ~2000 words, youre far from the bulk of data/words for which word2vec (and dense embedding word-vectors in general) usually shine. You may be disappointed in results unless you get a lot more data. (And in the meantime, such small vocabs may have problems effectively training typical word2vec dimensionalities (<code>vector_size</code>) of 100, 300, or more. (Using smaller <code>vector_size</code>, when you have a smaller vocab &amp; less training data, can help a bit.)</p>
<p>On the other hand, if you're in some domain other than real-language texts with an inherently limited unique vocabulary â€“ like say category-tags or product-names or similar â€“ and you have the chance to train your own word-vectors, you may want to try a wider range of training parameters than the usual defaults. Some recommendation-type apps may benefit from values very different from the <code>ns_exponent</code> default, &amp; if the source data's token-order is arbitrary, rather than meaningful, using a giant <code>window</code> or setting <code>shrink_windows=False</code> will deemphasize immediate-neighbors.</p>
",0,0,1161,2022-02-16 00:04:05,https://stackoverflow.com/questions/71134911/using-gensim-most-similar-function-on-a-subset-of-total-vocab
How to properly recover a Word2Vec model created using a SKLearn wrapper?,"<p>I am trying to create and store a gensin Word2Vec model using the <em>fit</em> function, then turn it into a SKLearn pipeline, pickle it, to later use it with <em>transform</em> on new data.</p>
<p>I created the wrapper, but the <em>self.w2v</em> object seems not to have been fitted and does not recognize any word. It is as if <em>self.w2v</em> had never seen <em>any</em> word.</p>
<p>Any ideas about how to address this?</p>
<pre><code>from sklearn.base import TransformerMixin, BaseEstimator
from gensim.models import Word2Vec

class SentenceVectorizer(TransformerMixin, BaseEstimator):

    def __init__(self, vector_size=50):
        self.vector_size = vector_size
   
    def sent_vectorizer(self, sentence, vectorizer):
        '''
        Applies the fitted W2V model for each token of each sentence and returns their vector representation.
        '''

        sent_vec =[]
        numw = 0

        for word in sentence:
            try:
                if numw == 0:
                    sent_vec = vectorizer.wv[word]       
                else:
                    sent_vec = np.add(sent_vec, vectorizer.wv[word])
                numw += 1

            except: # if word not present
                if numw == 0:
                    sent_vec = np.zeros(self.vector_size)
                else:
                    sent_vec = np.add(sent_vec, np.zeros(self.vector_size))

        if numw &gt; 0:
            return np.asarray(sent_vec) / numw
        else:
            return np.zeros(self.vector_size)

    def fit(self, X):
        self.w2v = Word2Vec(X, vector_size=self.vector_size)
        return self

    def transform(self, X):
        X_vec=[]
        for sentence in X:
            X_vec.append(self.sent_vectorizer(sentence, self.w2v))
        return X_vec
</code></pre>
<p>This code currently does well in training but returns zeroed vectors on inference (because no word has been recognized).</p>
<p>Most likely problem: fit method is not properly storing self.w2v, although when transform is called it seems to exist.</p>
","python, scikit-learn, gensim, word2vec","<p>Turns out I had an outdated gensim version which required vectorizer[word] instead of vectorizer.wv[word]. I'll leave the question here as it might be usefull to someone.</p>
",1,0,272,2022-02-17 18:50:05,https://stackoverflow.com/questions/71163820/how-to-properly-recover-a-word2vec-model-created-using-a-sklearn-wrapper
Replace objetive function in Word2Vec Gensim,"<p>I'm doing my final degree project. I need to create an extended version of the word2vec algorithm, changing the default objective function of the original paper. This has already been done (check this <a href=""https://www.frontiersin.org/articles/10.3389/fict.2018.00014/full"" rel=""nofollow noreferrer"">paper</a>). In that paper, they only say the new objective function, but they do not say how they have run the model.</p>
<p>Now, I need to extend that model too, with another function, but I'm not sure if I have to implement word2vec myself with the new function, or there is a way to replace it in the Gensim word2vec implementation.</p>
<p>I have checked the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec Gensim documentation</a> but I have not seen any parameter to do this. Do you have any idea how to do it? It is even possible?</p>
<p>I was unsure if this StackExchange site was the correct one, maybe <a href=""https://ai.stackexchange.com/"">https://ai.stackexchange.com/</a> is more appropriate.</p>
","python, gensim, word2vec","<p>There's no official support in Gensim for simply dropping in your own objective function.</p>
<p>However, the full source code is available â€“ <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim</a> â€“ so by editing it, or using it as a model for your own implementation, you could theoretically do anything.</p>
<p>Beware, though:</p>
<ul>
<li>the code has gone through a lot of optimization &amp; customization for new options that may not be relevant to your needs, so may not be the most clean &amp; simple starting point</li>
<li>for performance, the core routines are written in Cython â€“ see the <code>.pyx</code> files â€“ which can be especially hard to debug, and rely on library bulk array functions that may obscure how to implement your alternate function instead</li>
</ul>
",1,0,77,2022-02-19 07:55:38,https://stackoverflow.com/questions/71183157/replace-objetive-function-in-word2vec-gensim
Domain-specific word similarity,"<p>Does anyone know how of an accurate tool or method that can be used to compute word embeddings or find similarity among domain-specific words? I'm working on an NLP project that involves computing cosine similarity between technical terms, such as &quot;address&quot; and &quot;socket&quot;, but pre-trained models like word2vec aren't giving useful embeddings or accurate cosine similarities because they aren't specific to technical terms. Since the more general-nontechnical meanings of &quot;address&quot; and &quot;socket&quot; aren't similar to one another, these pretrained models aren't giving them sufficiently high similarity scores for the purposes of my project. Would appreciate any advice people would be able to offer. Thank you!</p>
","nlp, word2vec, similarity, cosine-similarity","<p>With sufficient data from your specific domain, you can train your own word2vec model - whose resulting word-vectors, being only influenced by your domain data, will be far more reflective of the in-domain meanings.</p>
<p>Similarly, if you have a mixture of data where you have hints that <em>some</em> word uses are for different senses of a polysemous word, you could try preprocessing your text, using those hints, replacing the ambiguous tokens (like say <code>'address'</code>) with a larger number of distinct tokens (like <code>'address*networking'</code>, <code>'address*delivery'</code>, etc). Even with a lot of error in such a process, its results might be sufficient for a specific purpose.</p>
<p>For example, maybe you'd assume all docs of a certain type â€“ like articles from a particular publication â€“ always mean <code>'address*networking'</code> when they write <code>'address'</code>. That crude replacement, on just some subset of docs sufficient to collect enough varied examples of <code>'address*networking'</code> usage, might leave you with a good-enough word-vector for <code>'address*networking'</code>.</p>
<p>(More generally, deciding which word sense of multiple candidates is meant by a particular word is called &quot;word sense disambiguation&quot;, and it might be possible to use other preexisting code for performing that to help preprocess texts - replacing ambiguous tokens with more-speciific stand-ins â€“ before performing word2vec training.)</p>
<p>Even without such assistive pre-processing, there've been a number of research attempts to extend word2vec to better model words with multiple contrasting meanings. Googling for <code>[word2vec polysemy]</code> or <code>[polysemous embeddings]</code> should turn up a bunch of examples.</p>
<p>But I don't know any of those techniques that have become widely-used, or that are explicitly supported by major word2vec libraries, so I can't specifically recommend or show working code for any. I don't know a standard best-practice or off-the-shelf solution â€“ you'd have to treat adopting those ideas from research papers as an R&amp;D project, performing a lot of your own implementation/evaluation to see if any help with your goals.</p>
",0,0,535,2022-02-20 13:13:26,https://stackoverflow.com/questions/71194758/domain-specific-word-similarity
nodevectors not returning all nodes,"<p>I'm trying to use <code>nodevector</code>'s <code>Node2Vec</code> class to get an embedding for my graph. I can't show the entire code, but basically this is what I'm doing:</p>
<pre><code>import networkx as nx
import pandas as pd
import nodevectors

n2v = nodevectors.Node2Vec(n_components=128,
                           walklen=80,
                           epochs=3,
                           return_weight=1,
                           neighbor_weight=1,
                           threads=4)
G = nx.from_pandas_edgelist(df, 'customer', 'item', edge_attr='weight', create_using=nx.Graph)
n2v.fit(G)
model = n2v.model
shape = model.ww.vectors.shape
</code></pre>
<p>I know <code>G</code> has all the nodes from my scope. Then, I fit the model, but <code>model.ww.vectors</code> has a number of rows smaller than my number of nodes.</p>
<p>I'm not successfully finding why do the number of nodes represented in my embedding by <code>model.ww.vectors</code> is lower than my actual number of nodes in <code>G</code>.</p>
<p>Does anyone know why it happens?</p>
","python, graph, word2vec, embedding","<p>TL;DR: Your non-default <code>epochs=3</code> can result in some nodes appearing only 3 times â€“ but the inner <code>Word2Vec</code> model by default ignores tokens appearing fewer than 5 times. Upping to <code>epochs=5</code> may be a quick fix - but read on for the reasons &amp; tradeoffs with various defaults.</p>
<p>--</p>
<p>If you're using the <code>nodevectors</code> package described <a href=""https://github.com/VHRanger/nodevectors"" rel=""nofollow noreferrer"">here</a>, it seems to be built on Gensim's <code>Word2Vec</code> â€“ which uses a default <code>min_count=5</code>.</p>
<p>That means any tokens â€“ in this case, nodes â€“ which appear fewer than 5 times are ignored. Especially in the natural-language contexts where <code>Word2Vec</code> was pioneered, discarding such rare words entirely usually has multiple benefits:</p>
<ul>
<li>from only a few idiosyncratic examples, such rare words themselves get peculiar vectors less-likely to generalize to downstream uses (other texts)</li>
<li>compared to other frequent words, each gets very little training effort overall, &amp; thus provides only a little pushback on shared model weights (based on their peculiar examples) - so the vectors are weaker &amp; retain more arbitrary influence from random-initialization &amp; relative positioning in the corpus. (More-frequent words provide more varied, numerous examples to extract their unique meaning.)</li>
<li>because of the Zipfian distribution of word-frequencies in natural language, there are <em>a lot</em> of such low-frequency words â€“ often even typos â€“ and altogether they take up a lot of the model's memory &amp; training-time. But they <em>don't</em> individually get very good vectors, or have generalizable beneficial influences on the shared model. So they wind up serving a lot like noise that weakens other vectors for more-frequent words, as well.</li>
</ul>
<p>So typically in <code>Word2Vec</code>, discarding rare words only gives up low-value vectors while simultaneously speeding training, shrinking memory requirements, &amp; improving the quality of the remaining vectors: a big win.</p>
<p>Although the distribution of node-names in graph random-walks may be very different from natural-language word-frequencies, some of the same concerns still apply for nodes that appear rarely. On the other hand, if a node truly only appears at the end of a long chain of nodes, every walk to or from it will include the exact same neighbors - and maybe extra appearances in more walks would add no new variety-of-information (at least within the inner <code>Word2Vec</code> <code>window</code> of analysis).</p>
<p>You may be able to confirm if the default <code>min_count</code> is your issue by using the <code>Node2Vec</code> <code>keep_walks</code> parameter to store the generated walks, then checking: are exactly the nodes that are 'missing' appearing fewer than <code>min_count</code> times in the walks?</p>
<p>If so, a few options may be:</p>
<ul>
<li>override <code>min_count</code> using the <code>Node2Vec</code> <code>w2vparams</code> option to something like <code>min_count=1</code>. As noted above, this is always a bad idea in traditional natural-language <code>Word2Vec</code> - but maybe it's not so bad in a graph application, where for rare/outer-edge nodes one walk is enough, and then at least you have whatever strange/noisy vector results from that minimal training.</li>
<li>try to influence the walks to ensure all nodes appear enough times. I suppose some values of the <code>Node2Vec</code> <code>walklen</code>, <code>return_weight</code>, &amp; <code>neighbor_weight</code> could improve coverage - but I don't think they could guarantee all nodes appear in at least N (say, 5, to match the default <code>min_count</code>) different walks. But it looks like the <code>Node2Vec</code> <code>epochs</code> parameter controls how many time <em>every</em> node is used as a starting point â€“ so <code>epochs=5</code> would guarantee every node appears at least 5 times, as the start of 5 separate walks. (Notably: the <code>Node2Vec</code> default is <code>epochs=20</code> - which would never trigger a bad interaction with the internal <code>Word2Vec</code> <code>min_count=5</code>. But setting your non-default <code>epochs=3</code> risks leaving some nodes with only 3 appearances.)</li>
</ul>
",1,0,395,2022-02-22 19:21:47,https://stackoverflow.com/questions/71227156/nodevectors-not-returning-all-nodes
What is right way to sum up word2vec vectors generated by Gensim?,"<p>I got four 300-dimention word2vec vectors like:</p>
<pre><code>v1=model.wv.get_vector('A')
v2=model.wv.get_vector('B')
v3=model.wv.get_vector('C')
v4=model.wv.get_vector('D')
</code></pre>
<p>I want to compare cosine similarity of <code>v1+v2</code> and <code>v3+v4</code>.</p>
<p>Should I reduce them two 2-dimention vectors first or not?</p>
<p>What <code>numpy</code> function should I use?</p>
","python, gensim, word2vec","<p>You can add the vectors with simple Python math operators:</p>
<pre><code>va = v1 + v2
vb = v3 + v4
</code></pre>
<p><code>numpy</code> actually doesn't have a cosine-similarity (or cosine-distance) function, so you'd have to use the formula for calculating from the dot-product &amp; unit-norm (both of which <code>numpy</code> has:</p>
<pre><code>cossim = np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb))
</code></pre>
<p>Or, you could leverage the cosine-distance function in <code>scipy</code>, and convert it to cosine-similarity by subtracting it from 1:</p>
<pre><code>cosdist = scipy.spatial.distance.cosine(va, vb)
cossim = 1 - cosdist
</code></pre>
",0,0,384,2022-02-23 15:57:42,https://stackoverflow.com/questions/71240225/what-is-right-way-to-sum-up-word2vec-vectors-generated-by-gensim
Why Word2Vec function returns me a lot of 0.99 values,"<p>I'm trying to apply a word2vec model on a review dataset. First of all I apply the preprocessing to the dataset:</p>
<pre><code>df=df.text.apply(gensim.utils.simple_preprocess)
</code></pre>
<p>and this is the dataset that I get:</p>
<pre><code>0       [understand, location, low, score, look, mcdon...
3       [listen, it, morning, tired, maybe, hangry, ma...
6       [super, cool, bathroom, door, open, foot, nugg...
19      [cant, find, better, mcdonalds, know, getting,...
27      [night, went, mcdonalds, best, mcdonalds, expe...
                              ...
1677    [mcdonalds, app, order, arrived, line, drive, ...
1693    [correct, order, filled, promptly, expecting, ...
1694    [wow, fantastic, eatery, high, quality, ive, e...
1704    [let, tell, eat, lot, mcchickens, best, ive, m...
1716    [entertaining, staff, ive, come, mcdees, servi...
Name: text, Length: 283, dtype: object
</code></pre>
<p>Now I create the Word2Vec model and train it:</p>
<pre><code>model = gensim.models.Word2Vec(sentences=df, vector_size=200, window=10, min_count=1, workers=6)
model.train(df,total_examples=model.corpus_count,epochs=model.epochs)
print(model.wv.most_similar(&quot;service&quot;,topn=10))
</code></pre>
<p>What I dont understand is that the function most_similar() returns to me a lot of 0.99 of similarity.</p>
<pre><code>[('like', 0.9999310970306396), ('mcdonalds', 0.9999251961708069), ('food', 0.9999234080314636), ('order', 0.999918520450592), ('fries', 0.9999175667762756), ('got', 0.999911367893219), ('window', 0.9999082088470459), ('way', 0.9999075531959534), ('it', 0.9999069571495056), ('meal', 0.9999067783355713)]
</code></pre>
<p>What am I doing wrong?</p>
","python, gensim, word2vec","<p>You're right that's not normal.</p>
<p>It is unlikely that your <code>df</code> is the proper format <code>Word2Vec</code> expects. It needs a re-iterable Python sequence, where each item is a <em>list</em> of <em>string tokens</em>.</p>
<p>Try displaying <code>next(iter(df))</code>, to see the 1st item in <code>df</code>, if iterated over as <code>Word2Vec</code> does. Does it look like a good piece of training data?</p>
<p>Separately regarding your code:</p>
<ul>
<li><code>min_count=1</code> is always a bad idea with <code>Word2Vec</code> - rare words can't get good vectors but do, in aggregate, serve a lot like random noise making nearby words harder to train. Generally, the default <code>min_count=5</code> shouldn't be lowered unless you're sure that will help your results, because you can compare that value's effects versus lower values. And if it seems like too much of your vocabulary disappears because words don't appear even a measly 5 times, you likely have too little data for this data-hungry algorithm.</li>
<li>Only 283 texts are unlikely to be enough training data unless each text has tens of thousands of tokens. (And even if it were possible to squeeze some results from this far-smaller-than-ideal corpus, you might need to shrink the <code>vector_size</code> and/or increase the <code>epochs</code> to get the most out of minimal data.</li>
<li>If you supply a corpus to <code>sentences</code> in the <code>Word2Vec()</code> construction, you <em>don't</em> need to call <code>.train()</code>. It will have already automatically used that corpus fully as part of the constructor. (You only need to call the indepdendent, internal <code>.build_vocab()</code> &amp; <code>.train()</code> steps if you <em>didn't</em> supply a corpus at construction-time.)</li>
</ul>
<p>I highly recommend you enable logging to at least the <code>INFO</code> level for the relevant classes (either all Gensim or just <code>Word2Vec</code>). Then you'll see useful logging/progress info which, if you read over, will tend to reveal problems like the redundant second training here. (That redundant training <em>isn't</em> the cause of your main problem, though.)</p>
",1,0,536,2022-02-25 16:24:18,https://stackoverflow.com/questions/71268790/why-word2vec-function-returns-me-a-lot-of-0-99-values
How does word2vec learn word relations?,"<p>Which part of the algorithm specifically makes the embeddings to have the <code>king - boy + girl = queen</code> ability? Did they just did this by accident?</p>
<p>Edit :</p>
<p>Take the CBOW as an example. I know about they use embeddings instead of one-hot vectors to encode the words and made the embeddings trainable instead of how we do when using one hot vectors that the data itself is not trainable. Then the output is a one-hot vector for target word. They just average all the surrounding word embeddings at some point then put some lego layers afterwards. So at the end they find the mentioned property by surprise, or is there a training procedure or network structure that gave the embeddings that property?</p>
","word2vec, embedding, word-embedding","<p>The algorithm simply works to train (optimize) a shallow neural-network model that's good at predicting words, from other nearby words.</p>
<p>That's the only internal training goal â€“ subject to the neural network's constraints on how the words are represented (N floating-point dimensions), or combined with the model's internal weights to render an interpretable prediction (forward propagation rules).</p>
<p>There's no other 'coaching' about what words 'should' do in relation to each other. All words are still just opaque tokens to word2vec. It doesn't even consider their letters: the whole-token is just a lookup key for a whole-vector. (Though, the word2vec variant FastText varies that somewhat by also training vectors for subwords â€“ &amp; thus can vaguely simulate the same intuitions that people have for word-roots/suffixes/etc.)</p>
<p>The interesting 'neighborhoods' of nearby words, and relative orientations that align human-interpretable aspects to vague directions in the high-dimensional coordinate space, fall out of the prediction task. And those relative orientations are what gives rise to the surprising &quot;analogical arithmetic&quot; you're asking about.</p>
<p>Internally, there's a tiny internal training cycle applied over and over: &quot;nudge this word-vector to be slightly better at predicting these neighboring words&quot;. Then, repeat with another word, and other neighbors. And again &amp; again, millions of times, each time only looking at a tiny subset of the data.</p>
<p>But the updates that contradict each other cancel out, and those that represent reliable patterns in the source training texts reinforce each other.</p>
<p>From one perspective, it's essentially trying to &quot;compress&quot; some giant vocabulary â€“ tens of thousands, to millions, of unique words â€“ into a smaller N-dimensional representation - usually 100-400 dimensions when you have enough training data. The dimensional-values that become as-good-as-possible (but never necessary great) at predicting neighbors turn out to exhibit the other desirable positionings, too.</p>
",0,0,218,2022-03-02 09:46:30,https://stackoverflow.com/questions/71320529/how-does-word2vec-learn-word-relations
Retrieve n-grams with word2vec,"<p>I have a list of texts. I turn each text into a token list. For example if one of the texts is <code>'I am studying word2vec'</code> the respective token list will be (assuming I consider n-grams with n = 1, 2, 3) <code>['I', 'am', 'studying ', 'word2vec, 'I am', 'am studying', 'studying word2vec', 'I am studying', 'am studying word2vec']</code>.</p>
<ol>
<li>Is this the right way to transform any text in order to apply <code>most_similar()</code>?</li>
</ol>
<p>(I could also delete n-grams that contain at least one stopword, but that's not the point of my question.)</p>
<p>I call this list of lists of tokens <code>texts</code>. Now I build the model:</p>
<p><code>model = Word2Vec(texts)</code></p>
<p>then, if I use</p>
<p><code>words = model.most_similar('term', topn=5)</code></p>
<ol start=""2"">
<li>Is there a way to determine what kind of results i will get? For example, if <code>term</code> is a 1-gram then will I get a list of five 1-gram? If <code>term</code> is a 2-gram then will I get a list of five 2-gram?</li>
</ol>
","python, gensim, word2vec","<p>Generally, the very best way to determine &quot;what kinds of results&quot; you will get if you were to try certain things is to try those things, and observe the results you actually get.</p>
<p>In preparing text for word2vec training, it is not typical to convert an input text to the form you've shown, with a bunch of space-delimited word n-grams added. Rather, the string <code>'I am studying word2vec'</code> would typically just be preprocessed/tokenized to a list of (unigram) tokens like <code>['I', 'am', 'studying', 'word2vec']</code>.</p>
<p>The model will then learn one vector per single word â€“ with no vectors for multigrams. And since it only knows such 1-word vectors, all the results its reports from <code>.most_similar()</code> will also be single words.</p>
<p>You can preprocess your text to combine some words into multiword entities, based on some sort of statistical or semantic understanding of the text. Very often, this process converts the runs-of-related-words to underscore-connected single tokens. For example, <code>'I visited New York City'</code> might become <code>['I', 'visited', 'New_York_City']</code>.</p>
<p>But any such preprocessing decisions are separate from the word2vec algorithm itself, which just considers whatever 'words' you feed it as 1:1 keys for looking-up vectors-in-training. It only knows tokens, not n-grams.</p>
",1,0,1254,2022-03-07 17:02:33,https://stackoverflow.com/questions/71384680/retrieve-n-grams-with-word2vec
Word2Vec dimensions incorrect,"<p>Data being used is saved in csv file:</p>
<pre><code>Sentence #  Word    POS Tag
Sentence1   YASHAWANTHA NNP B-PER
Sentence1   K   NNP I-PER
Sentence1   S   NNP I-PER
Sentence1   Mobile  NNP O
Sentence1   :   :   O
Sentence1   -7353555773 JJ  O
</code></pre>
<p>I am trying to take the dataset with the following columns: Sentence #, Word, POS, Tag and converting all entries within the Word column to Word2Vec vectors.</p>
<p>Here i am reading in the dataset and splitting into sentences:</p>
<pre><code>from gensim.models import Word2Vec
import pandas as pd

data = pd.read_csv(path_to_csv)

class SentenceGetter(object):
    def __init__(self, data):
        self.n_sent = 1#
        self.data = data

        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[&quot;Word&quot;].values.tolist(),s[&quot;POS&quot;].values.tolist(), s[&quot;Tag&quot;].values.tolist())]
        self.grouped = self.data.groupby(&quot;Sentence #&quot;).apply(agg_func)
        self.sentences = [s for s in self.grouped]
    
    def get_next(self):
        try:
            s = self.grouped[&quot;Sentence: {}&quot;.format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(data)
sentences = getter.sentences 

</code></pre>
<p>Now i convert all words to their corresponding Word2Vec vectors, where word2idx is a dictionary with the key being the string and its corresponding Word2Vec vector as the value:</p>
<pre><code>vec_words= [[i] for i in words]
vec_model= Word2Vec(vec_words, min_count=1, size=30)
word2idx = dict({})
for idx, key in enumerate(vec_model.wv.vocab):
    word2idx[key] = vec_model.wv[key]
</code></pre>
<p>Then for the tags column i use simple enumeration:</p>
<pre><code>tag2idx = {t: i for i, t in enumerate(tags)}
</code></pre>
<p>I then pad the words and tags:</p>
<pre><code>from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

max_len = 60
X = [[word2idx[w[0]] for w in s] for s in sentences]
X = pad_sequences(maxlen=max_len, sequences=X, padding=&quot;post&quot;, value=num_words-1)
y = [[tag2idx[w[2]] for w in s] for s in sentences]
y = pad_sequences(maxlen=max_len, sequences=y, padding=&quot;post&quot;, value=tag2idx[&quot;O&quot;])
y= [to_categorical(i, num_classes = num_tags) for i in y]
</code></pre>
<p>Then define the model:</p>
<pre><code>from sklearn.model_selection import train_test_split
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense
from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)
input_word = Input(shape=(max_len,))
model = Embedding(input_dim=num_words, output_dim=max_len, input_length=max_len)(input_word)
model = SpatialDropout1D(0.1)(model)
model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)
out = TimeDistributed(Dense(num_tags, activation=&quot;softmax&quot;))(model)
model = Model(input_word, out)

model.compile(optimizer=&quot;rmsprop&quot;,
              loss=&quot;categorical_crossentropy&quot;,
              metrics=[&quot;accuracy&quot;])
</code></pre>
<p>Then fit the model:</p>
<pre><code>history = model.fit(
    x_train, np.array(y_train),
    validation_split=0.2,
    batch_size=32, 
    epochs=1,
    verbose=1,    
)
</code></pre>
<p>This fitting step leads to the following error and i am unsure how to fix it</p>
<blockquote>
<p>Input 0 of layer &quot;spatial_dropout1d_2&quot; is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 60, 30, 60)</p>
</blockquote>
","python, tensorflow, word2vec","<p>The shape before padding of</p>
<pre><code>X = [[word2idx[w[0]] for w in s] for s in sentences]
X = np.array(X)
print(X.shape)
</code></pre>
<p>is <code>(3, 6, 30)</code> for 3 sentences in the csv file, and <code>(3, 60, 30)</code> after padding, 30 being the word2wec size.
but the model expects an input of size (3, 60)</p>
<hr />
<p>Without changing the rest, you can modify the network :</p>
<pre><code>wrd2vec_size = 30
input_word = Input(shape=(max_len, wrd2vec_size))
x = SpatialDropout1D(0.1)(input_word)
x = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(x)
out = TimeDistributed(Dense(num_tags, activation=&quot;softmax&quot;))(x)

model = Model(input_word, out)
</code></pre>
",0,0,225,2022-03-20 12:20:15,https://stackoverflow.com/questions/71546678/word2vec-dimensions-incorrect
word not in vocabulary error in gensim model,"<pre><code>from bs4 import BeautifulSoup
import requests
cont = requests.get(&quot;https://ichi.pro/tr/veri-biliminde-uzaklik-olculeri-159983401462266&quot;).content
soup = BeautifulSoup(cont,&quot;html.parser&quot;)
metin = soup.text

import re
sonuÃ§1 = re.search(&quot;1. Ã–klid Mesafesi&quot;,metin)
sonuÃ§2 = re.search('OkuduÄŸunuz iÃ§in teÅŸekkÃ¼rler!',metin)
metin = metin[sonuÃ§1.start():sonuÃ§2.start()].split(&quot;\n&quot;)

from gensim.models import Word2Vec
model = Word2Vec(metin,size=200,window=15,min_count=5,sg=5)

model.wv[&quot;Jaccard mesafesi&quot;]
</code></pre>
<p>metin is:</p>
<pre><code>....'     Jaccard mesafesi',
 '    ',
 '',
 'DezavantajlarÄ±',
 'Jaccard endeksinin Ã¶nemli bir dezavantajÄ±, verilerin bÃ¼yÃ¼klÃ¼ÄŸÃ¼nden oldukÃ§a etkilenmesidir. BÃ¼yÃ¼k veri kÃ¼melerinin endeks Ã¼zerinde bÃ¼yÃ¼k bir etkisi olabilir, Ã§Ã¼nkÃ¼ kesiÅŸme noktasÄ±nÄ± benzer tutarken birleÅŸmeyi Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rabilir.',
 'KullanÄ±m DurumlarÄ±',
 'Jaccard indeksi, genellikle ikili veya ikili verilerin kullanÄ±ldÄ±ÄŸÄ± uygulamalarda kullanÄ±lÄ±r. Bir gÃ¶rÃ¼ntÃ¼nÃ¼n segmentlerini, Ã¶rneÄŸin bir arabayÄ± tahmin eden bir derin Ã¶ÄŸrenme modeliniz olduÄŸunda, Jaccard indeksi daha sonra, doÄŸru etiketler verilen tahmin edilen segmentin ne kadar doÄŸru olduÄŸunu hesaplamak iÃ§in kullanÄ±labilir.',
 'Benzer ÅŸekilde, belgeler arasÄ±nda ne kadar kelime seÃ§iminin Ã¶rtÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ Ã¶lÃ§mek iÃ§in metin benzerlik analizinde kullanÄ±labilir. BÃ¶ylece, desen setlerini karÅŸÄ±laÅŸtÄ±rmak iÃ§in kullanÄ±labilir.',
 '8. Haversine',
 '',
 '',
 '',
 '     Haversine mesafesi. Yazar tarafÄ±ndan gÃ¶rÃ¼ntÃ¼.',
 '    ',
....
</code></pre>
<p>note: Ä± am turkish so my content is turkish but this not important Ä± think if you are stranger this is not problem
second note:Ä± try another words but Ä± can not train the model?
what should Ä± do?</p>
","web-scraping, nlp, nltk, gensim, word2vec","<p>There are multiple problems:</p>
<ol>
<li><p>If you want a Turkish model, you can try to find a pretrained Word2Vec model for Turkish (e.g. check out <a href=""https://github.com/akoksal/Turkish-Word2Vec"" rel=""nofollow noreferrer"">this repository</a>) or train a model for Turkish yourself. The way you use it now you seem to train a model but only from a single website, which will barely do anything because the model needs a large amount of sentences to learn anything (like at least 10.000, better much more). Also you set <code>min_count=5</code> anyway, so any word appearing less than 5 times is ignored generally. Try something like training it on the Turkish Wikipedia, see the linked repository.</p>
</li>
<li><p>Word2Vec by default is a unigram model, so the input is a <em>single</em> word. If you hand it a bigram consisting of two words like <code>&quot;Jaccard mesafesi&quot;</code> it will not find anything. Also you should catch the case that the word is not in vocabulary, otherwise each unknown word will cause an error and your program to cancel. Search for the unigram representation of each token, then combine the two, e.g. by using the statistical mean of the vectors:</p>
<pre><code>import numpy

ngram = &quot;Jaccard mesafesi&quot;
split_ngram = ngram.split()
try:
    ngram_vector = []
    for w in split_ngram:
        ngram_vector.append(model.wv[w])
    ngram_vector = numpy.mean(unigram_vectors, axis=0)
except:
    ngram_vector = None
    print(f&quot;Word {word} is not in vocabulary&quot;)
</code></pre>
</li>
<li><p>The Word2Vec class for training takes as argument a list of <code>tokenized</code> sentences, so a list word lists. You handed it entire, untokenized sentences instead.</p>
</li>
</ol>
",1,0,157,2022-03-21 09:05:07,https://stackoverflow.com/questions/71555062/word-not-in-vocabulary-error-in-gensim-model
&#39;utf-8&#39; codec can&#39;t decode byte 0x93 in position 0: invalid start byte,"<p>I want to use Word2Vec, and i have download a Word2Vec's corpus in indonesian language, but when i call it, it was give me an error, this is what i try :</p>
<pre><code>Model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Feature Extraction Lexicon Based/Word2Vec/idwiki_word2vec_100_new_lower.model.wv.vectors.npy', binary=True,)
</code></pre>
<p>and it was give me an error, like this :</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-73-219e152ee7d9&gt; in &lt;module&gt;()
----&gt; 1 Model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Feature Extraction Lexicon Based/Word2Vec/idwiki_word2vec_100_new_lower.model.wv.vectors.npy', binary=True,)

2 frames
/usr/local/lib/python3.7/dist-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    353     if isinstance(text, unicode):
    354         return text
--&gt; 355     return unicode(text, encoding, errors=errors)
    356 
    357 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte
</code></pre>
","utf-8, gensim, word2vec","<p>A file named <code>idwiki_word2vec_100_new_lower.model.wv.vectors.npy</code> is unlikely to be in the format needed by <code>load_word2vec_format()</code>.</p>
<p>The <code>.npy</code> suggests it is a raw <code>numpy</code> array, which is not the format expected.</p>
<p>Also, the <code>.wv.vectors.</code> section suggests this could be part of a full, multi-file Gensim <code>.save()</code> of a complete <code>Word2Vec</code> model. That's more than just the vectors, &amp; requires all associated files to re-load.</p>
<p>You should double-check the source of the vectors and what their claims are about its format and the proper ways to load. (If you're still having problems &amp; need more guidance, you should specify more details about the origin of the file â€“ for example a link to the website where it was obtained â€“ to support other suggestions.)</p>
",0,0,2499,2022-03-23 01:50:19,https://stackoverflow.com/questions/71580934/utf-8-codec-cant-decode-byte-0x93-in-position-0-invalid-start-byte
When to use Word2vec and bag of words?,"<p>I'm still unsure about when to use word2vec and when to rely on the bag of words. For example, if I want to develop a text clustering model that takes text as an input and outputs a cluster for each input, should I care about the word representation and use word2vec or should I rely on the bag of words and treat the input text as a document?
Please share any more reading and understanding resources with me; I'm very interested in text preprocessing and clustering and want to learn everything I can about it.</p>
<p>Furthermore, if I want to use k-Means for the clustering, should I split the data or it's okay to just work with the whole data in one?</p>
","text, nlp, word2vec","<p>There's no hard rules. Generally, for any set of techniques you consider plausibly-appropriate, &amp; within your skills/budget, you try them all against your specific data &amp; task, and pick the ones that go better.</p>
<p>(You might develop some vague intuitions over time about situations where certain approaches are more likely to reflect the 'essential' parts of your task - but they can hadly be communicated in a StackOverflow answer over all possibilities.)</p>
<p>If you've tried specific things &amp; been surprised or disappointed by the result, that might create a more-answerable question, where you supply the specifics of your data/task, &amp; what you've tried, &amp; what your results are, and ask about specific unexpected behaviors, or specific aspects you'd want corrected/improved.</p>
",0,0,245,2022-03-30 06:11:35,https://stackoverflow.com/questions/71672310/when-to-use-word2vec-and-bag-of-words
How do I adapt code to make CNN model compatible with a higher dimension word embedding?,"<p>I have been following an online tutorial on 1D CNN for text classification. I have got the model to work with a  self trained word2vec embedding of 100 dimensions, but I want to see how the model would preform when given a higher dimensional word embedding.</p>
<p>I have tried  downloading a 300 dimension word2vec model and adding the .txt file  in the  CNN model and changing any dimensions from a 100 to 300. The model runs but produces bad results, the accuracy is 'nan' and the loss is 0.000 for all epochs.</p>
<p>What would i have to change for the model to work with the 300 dimension word2vec model?
Thanks
i have added the code below:</p>
<pre><code># load doc into memory
def load_doc(filename):
    # open the file as read only
    file = open(filename, 'r')
    # read all text
    text = file.read()
    # close the file
    file.close()
    return text
 
# turn a doc into clean tokens
def clean_doc(doc, vocab):
    # split into tokens by white space
    tokens = doc.split()
    # remove punctuation from each token
    table = str.maketrans('', '', punctuation)
    tokens = [w.translate(table) for w in tokens]
    # filter out tokens not in vocab
    tokens = [w for w in tokens if w in vocab]
    tokens = ' '.join(tokens)
    return tokens
 
# load all docs in a directory
def process_docs(directory, vocab, is_trian):
    documents = list()
    # walk through all files in the folder
    for filename in listdir(directory):
        # skip any reviews in the test set
        if is_trian and filename.startswith('cv9'):
            continue
        if not is_trian and not filename.startswith('cv9'):
            continue
        # create the full path of the file to open
        path = directory + '/' + filename
        # load the doc
        doc = load_doc(path)
        # clean doc
        tokens = clean_doc(doc, vocab)
        # add to list
        documents.append(tokens)
    return documents
 
# load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r')
    lines = file.readlines()[1:]
    file.close()
    # create a map of words to vectors
    embedding = dict()
    for line in lines:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = asarray(parts[1:], dtype='float32')
    return embedding
 
# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = zeros((vocab_size, 100))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = embedding.get(word)
    return weight_matrix
 
# load the vocabulary
vocab_filename = 'vocab.txt'
vocab = load_doc(vocab_filename)
vocab = vocab.split()
vocab = set(vocab)
 
# load all training reviews
positive_docs = process_docs('txt_sentoken/pos', vocab, True)
negative_docs = process_docs('txt_sentoken/neg', vocab, True)
train_docs = negative_docs + positive_docs
 
# create the tokenizer
tokenizer = Tokenizer()
# fit the tokenizer on the documents
tokenizer.fit_on_texts(train_docs)
 
# sequence encode
encoded_docs = tokenizer.texts_to_sequences(train_docs)
# pad sequences
max_length = max([len(s.split()) for s in train_docs])
Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
# define training labels
ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])
 
# load all test reviews
positive_docs = process_docs('txt_sentoken/pos', vocab, False)
negative_docs = process_docs('txt_sentoken/neg', vocab, False)
test_docs = negative_docs + positive_docs
# sequence encode
encoded_docs = tokenizer.texts_to_sequences(test_docs)
# pad sequences
Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
# define test labels
ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])
 
# define vocabulary size (largest integer value)
vocab_size = len(tokenizer.word_index) + 1
 
# load embedding from file
raw_embedding = load_embedding('embedding_word2vec.txt')
# get vectors in the right order
embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)
# create the embedding layer
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)
 
# define model
model = Sequential()
model.add(embedding_layer)
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
print(model.summary())
# compile network
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit network
model.fit(Xtrain, ytrain, epochs=10, verbose=2)
# evaluate
loss, acc = model.evaluate(Xtest, ytest, verbose=0)
print('Test Accuracy: %f' % (acc*100))
</code></pre>
","python, tensorflow, machine-learning, keras, word2vec","<p>If you are using <code>300</code>-dimensional vectors you need to change two things in your code.
This line:</p>
<pre><code>weight_matrix = zeros((vocab_size, 100))
</code></pre>
<p>To:</p>
<pre><code>weight_matrix = zeros((vocab_size, 300))
</code></pre>
<p>And this line:</p>
<pre><code>embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)
</code></pre>
<p>To</p>
<pre><code>embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)
</code></pre>
<p>That way, each integer representing a word will be mapped to its 300-dimensional word vector.</p>
<p><strong>Update 1</strong></p>
<p>If you download <code>w2v</code> from <code>gensim</code>:</p>
<pre><code>import gensim.downloader as api

wv = api.load('word2vec-google-news-300')
</code></pre>
<p>This seems to work:</p>
<pre><code>from string import punctuation
from os import listdir
from numpy import array
from numpy import asarray
from numpy import zeros
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D


# load doc into memory
def load_doc(filename):
    # open the file as read only
    file = open(filename, 'r')
    # read all text
    text = file.read()
    # close the file
    file.close()
    return text


# turn a doc into clean tokens
def clean_doc(doc, vocab):
    # split into tokens by white space
    tokens = doc.split()
    # remove punctuation from each token
    table = str.maketrans('', '', punctuation)
    tokens = [w.translate(table) for w in tokens]
    # filter out tokens not in vocab
    tokens = [w for w in tokens if w in vocab]
    tokens = ' '.join(tokens)
    return tokens


# load all docs in a directory
def process_docs(directory, vocab, is_trian):
    documents = list()
    # walk through all files in the folder
    for filename in listdir(directory):
        # skip any reviews in the test set
        if is_trian and filename.startswith('cv9'):
            continue
        if not is_trian and not filename.startswith('cv9'):
            continue
        # create the full path of the file to open
        path = directory + '/' + filename
        # load the doc
        doc = load_doc(path)
        # clean doc
        tokens = clean_doc(doc, vocab)
        # add to list
        documents.append(tokens)
    return documents


# load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename, 'r')
    lines = file.readlines()[1:]
    file.close()
    # create a map of words to vectors
    embedding = dict()
    for line in lines:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = asarray(parts[1:], dtype='float32')
    return embedding


# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = zeros((vocab_size, 300))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
      try:
          weight_matrix[i] = embedding[word]
      except KeyError:
          weight_matrix[i] = np.random.uniform(size=300)
    return weight_matrix

# load the vocabulary
vocab_filename = 'vocab.txt'
vocab = load_doc(vocab_filename)
vocab = vocab.split()
vocab = set(vocab)

# load all training reviews
positive_docs = process_docs('/content/txt_sentoken/pos', vocab, True)
negative_docs = process_docs('/content/txt_sentoken/neg', vocab, True)
train_docs = negative_docs + positive_docs

# create the tokenizer
tokenizer = Tokenizer()
# fit the tokenizer on the documents
tokenizer.fit_on_texts(train_docs)

# sequence encode
encoded_docs = tokenizer.texts_to_sequences(train_docs)
# pad sequences
max_length = max([len(s.split()) for s in train_docs])
Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
# define training labels
ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])

# load all test reviews
positive_docs = process_docs('/content/txt_sentoken/pos', vocab, False)
negative_docs = process_docs('/content/txt_sentoken/neg', vocab, False)
test_docs = negative_docs + positive_docs
# sequence encode
encoded_docs = tokenizer.texts_to_sequences(test_docs)
# pad sequences
Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
# define test labels
ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])

# define vocabulary size (largest integer value)
vocab_size = len(tokenizer.word_index) + 1

# load embedding from file
# get vectors in the right order
embedding_vectors = get_weight_matrix(wv, tokenizer.word_index)
# create the embedding layer
embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)

# define model
model = Sequential()
model.add(embedding_layer)
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
print(model.summary())
# compile network
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit network
model.fit(Xtrain, ytrain, epochs=10, verbose=2)
# evaluate
loss, acc = model.evaluate(Xtest, ytest, verbose=0)
print('Test Accuracy: %f' % (acc * 100))
</code></pre>
",1,1,193,2022-04-07 23:21:15,https://stackoverflow.com/questions/71789971/how-do-i-adapt-code-to-make-cnn-model-compatible-with-a-higher-dimension-word-em
How to get the dimensions of a word2vec vector?,"<p>I have run a word2vec model on my data <code>list_of_sentence</code>:</p>
<pre><code>from gensim.models import Word2Vec

w2v_model=Word2Vec(list_of_sentence,min_count=5, workers=4)

print(type(w2v_model))

&lt;class 'gensim.models.word2vec.Word2Vec'&gt;
</code></pre>
<p>I would like to know the dimensionality of <code>w2v_model</code> vectors. How can I check it?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>The vector dimensionality is included as an argument in <code>Word2Vec</code>:</p>
<ul>
<li>In gensim versions up to 3.8.3, the argument was called <code>size</code> (<a href=""https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">docs</a>)</li>
<li>In the latest gensim versions (4.0 onwards), the relevant argument is renamed to <code>vector_size</code> (<a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">docs</a>)</li>
</ul>
<p>In both cases, the argument has a default value of <strong>100</strong>; this means that, if you do not specify it explicitly (as you do here), the dimensionality will be 100.</p>
<p>Here is a reproducible example using gensim 3.6:</p>
<pre><code>import gensim
gensim.__version__
# 3.6.0

from gensim.test.utils import common_texts
from gensim.models import Word2Vec

model = Word2Vec(sentences=common_texts, window=5, min_count=1, workers=4) # do not specify size, leave the default 100

wv = model.wv['computer']  # get numpy vector of a word in the corpus
wv.shape # verify the dimension of a single vector is 100
# (100,)
</code></pre>
<p>If you want to change this dimensionality to, say, 256, you should call <code>Word2Vec</code> with the argument <code>size=256</code> (for gensim versions up to 3.8.3) or <code>vector_size=256</code> (for gensim versions 4.0 or later).</p>
",1,1,2010,2022-04-08 06:53:43,https://stackoverflow.com/questions/71792841/how-to-get-the-dimensions-of-a-word2vec-vector
Keras Semantic Similarity model from pre-trained embeddings,"<p>I want to implement a Keras model to predict the similarity between two sentences from words embeddings as follows (I included my full script at the end):</p>
<ol>
<li>Load words embeddings models, e.g., Word2Vec and fastText.</li>
<li>Generate samples (<code>X1</code> and <code>X2</code>) by computing the average word vectors for all words in a sentence. If two or more models are used, calculate the arithmetic mean of all embeddings (<a href=""https://arxiv.org/abs/1804.05262"" rel=""nofollow noreferrer""><em>Frustratingly Easy Meta-Embedding -- Computing Meta-Embeddings by Averaging Source Word Embeddings</em></a>).</li>
<li>Concatenate <code>X1</code> and <code>X2</code> into one array before feeding them to the network.</li>
<li>Compile (and evaluate) the Keras model.</li>
</ol>
<p>The entire script is as follows:</p>
<pre><code>import numpy as np
from gensim.models import Word2Vec
from keras.layers import Dense
from keras.models import Sequential
from sklearn.model_selection import train_test_split


def encoder_vector(v: str, model: Word2Vec) -&gt; np.array:
    wv_dim = model.vector_size
    if v in model.wv:
        return model.wv[v]
    else:
        return np.zeros(wv_dim)


def encoder_words_avg(words: list[str], model: Word2Vec) -&gt; np.array:
    dim = model.vector_size
    words = [word for word in words if word in model.wv]
    if len(words) &gt;= 1:
        return np.mean(model.wv[words], axis=0)
    else:
        return np.zeros(dim)


def load_samples(mappings, w2v_model, fast_model):
    dim = w2v_model.vector_size
    num = len(mappings)

    X1 = np.zeros((num, dim))
    X2 = np.zeros((num, dim))
    y = np.zeros((num, 1))

    for i in range(num):
        mapping = mappings[i].split(&quot;|&quot;)
        sentence_1, sentence_2 = mapping[1:]

        e = np.zeros((2, dim))

        # Compute meta-embedding by averaging all embeddings.
        e[0, :] = encoder_words_avg(words=sentence_1.split(), model=w2v_model)
        e[1, :] = encoder_words_avg(words=sentence_1.split(), model=fast_model)
        X1[i] = e.mean(axis=0)

        e[0, :] = encoder_words_avg(words=sentence_2.split(), model=w2v_model)
        e[1, :] = encoder_words_avg(words=sentence_2.split(), model=fast_model)
        X2[i] = e.mean(axis=0)

        y[i] = 0.0 if mapping[0].startswith(&quot;-&quot;) else 1.0

    return X1, X2, y


def baseline_model(X_train, X_test, y_train, y_test):
    model = Sequential()
    model.add(
        Dense(
            200,
            input_shape=(X_train.shape[1],),
            activation=&quot;relu&quot;,
            kernel_initializer=&quot;he_uniform&quot;,
        )
    )
    model.add(Dense(1, activation=&quot;sigmoid&quot;))
    model.compile(optimizer=&quot;sgd&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
    model.fit(X_train, y_train, batch_size=8, epochs=14)

    # Evaluate the trained model, using the train and test data
    _, train_acc = model.evaluate(X_train, y_train, verbose=0)
    _, test_acc = model.evaluate(X_test, y_test, verbose=0)

    print(&quot;Train: %.3f, Test: %.3f\n&quot; % (train_acc, test_acc))

    return model


def main():
    w2v_model = Word2Vec.load(&quot;&quot;)
    fast_model = Word2Vec.load(&quot;&quot;)

    mappings = [
        &quot;1|boiled chicken egg|hen egg whole boiled&quot;,
        &quot;2|tomato|tomato substance&quot;,
        &quot;3|sweet potatoes|potato chip&quot;,
        &quot;-1|watering plants|cornsalad plant&quot;,
        &quot;-2|butter|butane&quot;,
        &quot;-3|olive plant|black olives&quot;,
    ]

    X1, X2, y = load_samples(mappings, w2v_model=w2v_model, fast_model=fast_model)

    # Concatenate both arrays into one before feeding to the network.
    X = np.concatenate([X1, X2], axis=1)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    model = baseline_model(X_train, X_test, y_train, y_test)

    model.summary()
</code></pre>
<p>The above script seems to work, but the prediction result is very poor even when using only Word2Vec (which makes me think there could be an issue with the Keras model...). Any ideas on how to improve the outcome? Am I doing something wrong?</p>
<p>Thank you.</p>
","keras, neural-network, word2vec, similarity, word-embedding","<p>It's unclear what you're intending to predict.</p>
<p>Do you want your Keras NN to report the <em>same</em> value as the precise cosine-similarity calculation, between the two text summary vectors, would report? If so, why not just... do the calculation? It's not something I'd necessarily expect a neural-architecture to approxmate better.</p>
<p>Alternatively, if your tiny 6-pair dataset is the target:</p>
<ol>
<li><p>Your existing 'gold standard' answers don't seem obviously correct to me. Superficially, 'olive plant' &amp; 'black olives' seem nearly as 'similar' as 'tomato' &amp; 'tomato substance'. Similarly, 'watering plants' &amp; 'cornsalad plant' about-as-similar as 'sweet potatoes' &amp; 'potato chip'.</p>
</li>
<li><p>A mere 6 examples (maybe 5 after train/test split?) is both inadequate to usefully train a larger neural classifier, <em>and</em> to the extent the classifer might be easily trained (indeed 'overfit') to the 5 training examples, it won't necessarily have learned anything generalizable to the one hold-out example (which is using vectors quite far from the training texts). (With such a paucity of training data, &amp; testing using inputs that might be arbitrarily different than the training data, &quot;very poor&quot; performance would be expected. Neural nets require lots of varied training examples!)</p>
</li>
</ol>
<p>Finally, the strategy of creating combined-embeddings-by-averaging, as investigated by your linked paper, is another atypical practice that seems fishy to me. Even if it could offer some benefits, there's no reason to mix that atypical, somewhat non-intuitive extra practice into your experiment before even having things work with a more typical and simple baseline approach, for comparison, to be sure the extra 'meta'/averaging is worth the complication.</p>
<p>The paper itself doesn't really show any advantage over concatenation, which has a stronger theoretical basis (preserving each model's full independent spaces) than averaging, except by a tiny amount in 1-of-6 tests. Further, average of GLoVe &amp; CBOW performs <em>the same or worse</em> than GLoVe alone on 3 on their 6 evaluations â€“ and pretty minimally better on the 3 other evaluations. That implies to me the outperformance might be mainly random jitter introduced by the extra steps, and the averaging is â€“ at best â€“ a cheap option to consider as a tiny boost, not a generally-better approach.</p>
<p>The paper also leaves many natural related questions unaddressed:</p>
<ul>
<li>Is averaging better than, say, just picking a random half of each models' dimensions for concatenation? That'd be even cheaper!</li>
<li>Might some of the <em>slight</em> lift in <em>some</em> tasks be due not to the averaging, but the other transformations they've applied â€“ the l2-normalization applied to each source model, or across the whole of each dimension for the GLoVE model? (It's unclear if this model-postprocessing was only applied before dual-model averaging, or also to GLoVe in its solo evaluation.)</li>
</ul>
<p>There's other work suggesting post-training transformations of word-vector spaces may improve performance on downstream tasks â€“ see for example <a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">'All But The Top'</a> â€“ so which steps, exactly, get which advantages is important to distinguish.</p>
",1,1,458,2022-04-08 20:15:55,https://stackoverflow.com/questions/71802729/keras-semantic-similarity-model-from-pre-trained-embeddings
word2vec/gensim â€” RuntimeError: you must first build vocabulary before training the model,"<p>I am having trouble training my own <code>word2vec</code> model on the <code>.txt</code> files.</p>
<p>The code:</p>
<pre><code>import gensim
import json
import pandas as pd
import glob
import gensim.downloader as api
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors


# loading the .txt files

sentences = []
sentence = []
for doc in glob.glob('./data/*.txt'): 
     with(open(doc, 'r')) as f:
        for line in f:
            line = line.rstrip()
            if line == &quot;&quot;:
                if len(sentence) &gt; 0:
                    sentences.append(sentence)
                    sentence = []
            else:
                cols = line.split(&quot;\t&quot;)
                if len(cols) &gt; 4:
                    form = cols[1]
                    lemma = cols[2]
                    pos = cols[3]
                    if pos != &quot;PONCT&quot;:
                        sentence.append(form.lower())


# trying to train the model

from gensim.models import Word2Vec
model_hugo = Word2Vec(sentences, vector_size=200, window=5, epochs=10, sg=1, workers=4)
</code></pre>
<p>Message error:</p>
<pre><code>RuntimeError: you must first build vocabulary before training the model
</code></pre>
<p>How do I build the vocabulary?</p>
<p>The code works with the sample <code>.conll</code> files, but I want to train the model on my own data.</p>
","python, gensim, word2vec, word-embedding","<p>Thanks to the @gojomo's suggestion and to <a href=""https://stackoverflow.com/a/55091252/9737944"">this answer</a>, I resolved the empty <code>sentences</code> issue. I needed the following block of code:</p>
<pre><code># make an iterator that reads your file one line at a time instead of reading everything in memory at once
# reads all the sentences

class SentenceIterator: 
    def __init__(self, filepath): 
        self.filepath = filepath 

    def __iter__(self): 
        for line in open(self.filepath): 
            yield line.split() 

</code></pre>
<p>before training the model:</p>
<pre><code># training the model

sentences = SentenceIterator('/content/drive/MyDrive/rousseau/rousseau_corpus.txt') 
model = gensim.models.Word2Vec(sentences, min_count=2) # min_count is for pruning 
                                                       # the internal dictionary. 
                                                       # Words that appear only once 
                                                       # in the corpus are probably 
                                                       # uninteresting typos and garbage. 
                                                       # In addition, thereâ€™s not enough 
                                                       # data to make any meaningful 
                                                       # training on those words, so itâ€™s
                                                       # best to ignore them
</code></pre>
",1,0,1325,2022-04-13 21:11:20,https://stackoverflow.com/questions/71863897/word2vec-gensim-runtimeerror-you-must-first-build-vocabulary-before-training
How to see on what words the clusters were based on,"<p>I'm using this code to cluster my documents.
The graph output looks like this:
<a href=""https://i.sstatic.net/dqayh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dqayh.png"" alt=""enter image description here"" /></a>
I'm trying to accomplish a way of printing out the most common words on which the clusters were based on. Is it possible with gensim d2v?</p>
<pre><code>kmeans_model = KMeans(n_clusters=3, init='k-means++', max_iter=100) 
X = kmeans_model.fit(d2v_model.dv.vectors)
labels=kmeans_model.labels_.tolist()
l = kmeans_model.fit_predict(d2v_model.dv.vectors)
pca = PCA(n_components=2).fit(d2v_model.dv.vectors)
datapoint = pca.transform(d2v_model.dv.vectors)

data['cluster'] = kmeans_model.labels_
clusters = data.groupby('cluster')    


for i, cluster in enumerate(clusters.groups):
    with open('cluster'+str(cluster)+ '.csv', 'w', encoding=&quot;utf-8&quot;, newline='') as f:
        data = data.replace(r'\r+|\n+|\t+',' ', regex=True)
        data = clusters.get_group(cluster)[['NR_SOLICITACAO','DS_ANALISE','PRE_PROCESSED']] # get title and overview columns
        f.write(data.to_csv(index_label='id')) # set index to id

import matplotlib.pyplot as plt
label1 = [&quot;#0000FF&quot;, &quot;#006400&quot;, &quot;#FFFF00&quot;, &quot;#CD5C5C&quot;, &quot;#FF0000&quot;, &quot;#FF1493&quot;]
color = [label1[i] for i in labels]
plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)
centroids = kmeans_model.cluster_centers_
centroidpoint = pca.transform(centroids)
plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')
plt.show()
</code></pre>
","data-science, cluster-analysis, k-means, word2vec","<p>You seem to be using a Gensim <code>Doc2Vec</code> model (the 'Paragraph Vectors' algorithm), and your code shows the clusters being calculated from the <code>.dv</code> property. That's the full-document vectors, rather than individual word-vectors.</p>
<p>The clusters so derived won't have the same sort of directly reportable relationship to individual words that (for example) LDA topics might have. There, the LDA model itself can directly report the topics implied by a word, or the words most inicative of a topic.</p>
<p>But there are indirect ways you can probe for similar relationships from individual word-tokens to your groupings. Whether any gives acceptable results for your needs you'll have to try &amp; evaluate. Some possibilities:</p>
<ol>
<li><p><strong>Survey the sets-of-documents directly</strong>: You could simply tally up all the words in each of your clusters, individually - as if the entire cluster was one document. Then pick the words that are most-distinct, by some measure, in each cluster. This could be as simple as ordinally ranking all words by the number of times they appear in each cluster, &amp; reporting the words whose rankings for one cluster are the most positions 'higher' than the same word in the other(s). Or, you could use other measure of term-importance, like TF-IDF.</p>
</li>
<li><p><strong>Doc-vectors to word-vector correlations</strong>: If you've used a <code>Doc2Vec</code> mode that also trains word-vectors at the same time â€“ such as the PV-DM mode (<code>dm=1</code>), or using PV-DBOW (<code>dm=0</code>) while also adding the non-default <code>dbow_words=1</code> argument â€“ then the models' <code>d2v_model.wv</code> property will have valid word-vectors, in a compatible coordinate space.</p>
</li>
</ol>
<p>Thus you can look for words that are similar-to either the centroid points of your clusters, or similar-to a larger subset of each cluster (perhaps even every document), to get a sampling of words-that-may-be-descriptive.</p>
<p>You do this by performing a <code>.most_similar()</code> on the <code>.wv</code> set-of-word-vectors, using doc-vectors (whether for a full document, or centroids of a cluster) as the <code>positive</code> example(s). EG:</p>
<pre><code>words_near_one_doc = d2v_model.wv.most_similar(positive=[doc_vec1])
</code></pre>
<p>...or...</p>
<pre><code>words_near_centroid_of_3_docs = d2v_model.wv.most_similar(positive=[doc_vec1, doc_vec2, doc_vec3])
</code></pre>
<ol start=""3"">
<li><strong>Probe with synthetic documents, such as one-word documents, or real documents perturbed by word additions/removals</strong>: You could go through your vocabulary, and use the <code>Doc2Vec</code> model to infer new doc-vectors for synthetic documents of just a single word each. (Consider far more <code>epochs</code> since the document lacks the normal size which would prompt lots of training before settling on a vector, and keep in mind this unnatural process, without the normal variety of a real document, might generate weird results). See where those degenerate cases of documents land among your pre-existing clusters, and consider them as labels. Or similarly, take existing documents â€“ perhaps those especially near the boundaries between clusters â€“ and try adding/removing words from them, &amp; re-inferring new vectors. See which words most strongly 'move' a document towards or away-from your existing clusters, &amp; consider using those words as positive or negative labels.</li>
</ol>
",1,0,304,2022-04-19 12:42:11,https://stackoverflow.com/questions/71925163/how-to-see-on-what-words-the-clusters-were-based-on
Word2Vec + LSTM Good Training and Validation but Poor on Test,"<p>currently I'am training my Word2Vec + LSTM for Twitter sentiment analysis. I use the pre-trained GoogleNewsVectorNegative300 word embedding. The reason I used the pre-trained GoogleNewsVectorNegative300 because the performance much worse when I trained my own Word2Vec using own dataset. The problem is why my training process had validation acc and loss stuck at 0.88 and 0.34 respectively. Then, my confussion matrix also seems wrong. Here several processes that I have done before fitting the model</p>
<p>Text Pre processing:</p>
<ol>
<li>Lower casing</li>
<li>Remove hashtag, mentions, URLs, numbers, change words to numbers, non-ASCII characters, retweets &quot;RT&quot;</li>
<li>Expand contractions</li>
<li>Replace negations with antonyms</li>
<li>Remove puncutations</li>
<li>Remove stopwords</li>
<li>Lemmatization</li>
</ol>
<p>I split my dataset into 90:10 for train:test as follows:</p>
<pre><code>def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return X_train, X_test, y_train, y_test
</code></pre>
<p>The split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class</p>
<p>Then, I implemented the text augmentation that is EDA (Easy Data Augmentation) on all the training data as follows:</p>
<pre><code>class TextAugmentation:
    def __init__(self):
        self.augmenter = EDA()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = TextAugmentation()
</code></pre>
<p>The data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class</p>
<p>Then, I tokenized the sequence as follows:</p>
<pre><code># Tokenize the sequence
pfizer_tokenizer = Tokenizer(oov_token='OOV')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

X_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
X_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# Pad the sequence
X_pfizer_train_padded = pad_sequences(X_pfizer_train_tokenized, maxlen=100)
X_pfizer_test_padded = pad_sequences(X_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# Encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)
</code></pre>
<p>Resulting in 8869 unique words and 100 maximum sequence length</p>
<p>Finally, I fit the into my model using pre trained GoogleNewsVectorNegative300 word embedding but only use the weight and LSTM, and I split my training data again with 10% for validation as follows:</p>
<pre><code># Build single LSTM model
def build_lstm_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,), dtype='int32')
    
    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)
    
    # LSTM model layer
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=True)(embedding_layer)
    batch_normalization = BatchNormalization()(lstm_layer)
    
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=False)(batch_normalization)
    batch_normalization = BatchNormalization()(lstm_layer)

    # Dense model layer
    dense_layer = Dense(units=128, activation='relu')(batch_normalization)
    dropout_layer = Dropout(rate=0.5)(dense_layer)
    batch_normalization = BatchNormalization()(dropout_layer)
    
    output_layer = Dense(units=3, activation='softmax')(batch_normalization)

    lstm_model = Model(inputs=input_layer, outputs=output_layer)

    return lstm_model

# Building single LSTM model
sinovac_lstm_model = build_lstm_model(SINOVAC_EMBEDDING_MATRIX, SINOVAC_MAX_SEQUENCE)
sinovac_lstm_model.summary()
sinovac_lstm_model.compile(loss='categorical_crossentropy',
                               optimizer=Adam(learning_rate=0.001),
                               metrics=['accuracy'])
sinovac_lstm_history = sinovac_lstm_model.fit(x=X_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=64,
                                                  epochs=20,
                                                  validation_split=0.1,
                                                  verbose=1)
</code></pre>
<p><a href=""https://i.sstatic.net/csV70.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/csV70.png"" alt=""enter image description here"" /></a></p>
<p>The training result:
<a href=""https://i.sstatic.net/G0nto.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G0nto.png"" alt=""enter image description here"" /></a></p>
<p>The evaluation result:
<a href=""https://i.sstatic.net/Rh1qL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Rh1qL.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/Xw2yz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xw2yz.png"" alt=""enter image description here"" /></a></p>
<p>I really need some suggestions or insights to have a good accuracy on my test</p>
","python, keras, nlp, lstm, word2vec","<p>Without reviewing everything, a few high-order things that may be limiting your results:</p>
<ul>
<li><p>The <code>GoogleNews</code> vectors were trained on media-outlet news stories from 2012 and earlier. Tweets in 2020+ use a very different style of language. I wouldn't necessarily expect those pretrained vectors, from a different era &amp; domain-of-writing, to be very good at modeling the words you'll need. A well-trained word2vec model (using plenty of modern tweet data, with good preprocessing/tokenization &amp; parameterization choices) has a good chance of working better, so you may want to revisit that choice.</p>
</li>
<li><p>The <code>GoogleNews</code> training texts preprocessing, while as far as I can tell never fully-documented, did not appear to flatten all casing, nor remove stopwords, nor involve lemmatization. It didn't mutate obvious negations into antonyms, but it did perform a statistical combinations of some single-words into multigram tokens instead. So some of your steps are potentially causing your tokens to have less concordance with that set's vectors â€“ even throwing away info, like inflectional variations of words, that could be beneficially retained. Be sure every step you're taking is worth the trouble â€“ and note that a suffiicient modern word2vec moel, on Tweets, built using the same preprocessing for word2vec training then later steps, would match vocabularies perfectly.</p>
</li>
<li><p>Both the word2vec model, and any deeper neural network, often need lots of data to train well, and avoid overfitting. Even disregarding the 900 million parameters from <code>GoogleNews</code>, you're trying to train ~130k parameters â€“ at least 520KB of state â€“ from an initial set of merely 2060 tweet-sized texts (maybe 100KB of data). Models that learn generalizable things tend to be <em>compressions</em> of the data, in some sense, and a model that's much larger than the training data brings risk of severe overfitting. (Your mechanistic process for replacing words with synonyms may not be really giving the model any info that the word-vector similarity between synonyms didn't already provide.) So: consider shrinking your model, and getting much more training data - potentially even from other domains than your main classification interest, as long as the use-of-language is similar.</p>
</li>
</ul>
",1,0,815,2022-04-22 03:06:16,https://stackoverflow.com/questions/71963038/word2vec-lstm-good-training-and-validation-but-poor-on-test
Word2Vec + CNN Overfitting,"<p>Currently I'am training my Word2Vec + CNN for Twitter sentiment analysis about COVID-19 vaccine domain. I used the pre-trained GoogleNewsVectorNegative300 word embedding. The problem is why I heavily overfit on training proses. The reason I used the pre-trained GoogleNewsVectorNegative300 because the performance much worse when I trained my own Word2Vec using own dataset. Here several processes that I have done before fitting the model:</p>
<p>Text Pre processing:</p>
<ol>
<li>Lower casing</li>
<li>Remove hashtag, mentions, URLs, numbers, change words to numbers, non-ASCII characters, retweets &quot;RT&quot;</li>
<li>Expand contractions</li>
<li>Replace negations with antonyms</li>
<li>Remove puncutations</li>
<li>Remove stopwords</li>
<li>Lemmatization</li>
</ol>
<p>I split my dataset into 90:10 for train:test as follows:</p>
<pre><code>def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return X_train, X_test, y_train, y_test
</code></pre>
<p>The split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class
Training:
<a href=""https://i.sstatic.net/xeLgv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xeLgv.png"" alt=""enter image description here"" /></a>
Testing:
<a href=""https://i.sstatic.net/E0fpC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E0fpC.png"" alt=""enter image description here"" /></a></p>
<p>Then, I implemented the text augmentation that is EDA (Easy Data Augmentation) on all the training data as follows:</p>
<pre><code>class TextAugmentation:
    def __init__(self):
        self.augmenter = EDA()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = TextAugmentation()
</code></pre>
<p>The data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class</p>
<p>Then, I tokenized the sequence as follows:</p>
<pre><code># Tokenize the sequence
pfizer_tokenizer = Tokenizer(oov_token='OOV')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

X_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
X_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# Pad the sequence
X_pfizer_train_padded = pad_sequences(X_pfizer_train_tokenized, maxlen=100)
X_pfizer_test_padded = pad_sequences(X_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# Encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)
</code></pre>
<p>Resulting in 8869 unique words and 100 maximum sequence length</p>
<p>Finally, I fit the into my model using pre trained GoogleNewsVectorNegative300 word embedding and CNN, and I split my training data again with 10% for validation as follows:</p>
<pre><code># Build single CNN model
def build_cnn_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,))

    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)

    # CNN model layer
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(embedding_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)

    
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)

    
    cnn_layer = Conv1D(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = BatchNormalization()(cnn_layer)


    flatten = Flatten()(batch_norm_layer)
    
    # Dense model layer
    dense_layer = Dense(units=10, activation='relu')(flatten)
    batch_norm_layer = BatchNormalization()(dense_layer)
    output_layer = Dense(units=3, activation='softmax')(batch_norm_layer)
  
    cnn_model = Model(inputs=input_layer, outputs=output_layer)
  
    return cnn_model

    return lstm_model

sinovac_cnn_history = sinovac_cnn_model.fit(x=X_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=128,
                                                  epochs=100,
                                                  validation_split=0.1,
                                                  verbose=1)
</code></pre>
<p><a href=""https://i.sstatic.net/S2oef.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S2oef.png"" alt=""enter image description here"" /></a></p>
<p>The training result:
<a href=""https://i.sstatic.net/7soxo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7soxo.png"" alt=""enter image description here"" /></a></p>
<p>I really need some suggestions or insights because I have been doing this without any performance progress to my model</p>
","python, keras, nlp, conv-neural-network, word2vec","<p>That's quite a complex problem. It sure looks like overfitting as you said yourself. Meaning the model can't generalize well from your training set to new data.</p>
<p>Intuitively, I would suggest for you to cycle hyperparameters (epochs, batch size, learning rate, dropout layers), if you didn't already, to seek a better combination. Also, I would suggest to use <a href=""https://scikit-learn.org/stable/modules/cross_validation.html"" rel=""nofollow noreferrer"">cross-validation</a> to get a better idea of the performance of your classifier. This would also shuffle the training data and avoid that the model learns the data by heart.</p>
<p>Have you tried classifying the original data without the data augmentation? It's not a lot of data, but it could be enough to see if the performance on the test set is better than the final version, and thus see whether the data augmentation might be screwing something up in your data.</p>
<p>Did you try another embedding? I don't really think this is the problem, but in the search for the error I would probably switch it to see what happens.</p>
<p>Last but not least, do you know for a fact that this model structure can handle this task? Meaning did you find a working example somewhere? It sure sounds like it could do it, but there is the chance that the CNN model for example just doesn't generalize well over the embeddings. Have you considered using a model specified on text classification, like a Transformer or an LSTM?</p>
",2,0,373,2022-04-22 09:19:07,https://stackoverflow.com/questions/71966350/word2vec-cnn-overfitting
Word2vec raise KeyError(f&quot;Key &#39;{key}&#39; not present&quot;),"<p>Currently using gensim 4.0 library to write the code. However, I don't know why it keeps failing in finding a similar word. At first, when I set up min_count = 5, the error is, that it wants me to build a vocab first, but after I reduce it to min_count = 1, it says, key error not present...Full code with datasets over here: <a href=""https://github.com/JYjunyang/FYPDEMO"" rel=""nofollow noreferrer"">https://github.com/JYjunyang/FYPDEMO</a> Am I writing something wrong or missing some important steps? Everything works fine but just this word2vec implementation...Will appreciate for every guidance provided...
<strong>Take note: LemmaColumn is a dataframe after lemmatization</strong></p>
<pre><code>def FeaturesExtraction():
    word2vec = 
Word2Vec(sentences=LemmaColumn,vector_size=100,window=5,min_count=1,workers=8,sg=1)
    b1 = time.time()
    train_time = time.time() - b1
    print(word2vec.wv.most_similar('virus', topn=10))
</code></pre>
<p>And I not sure why, after training with 10k data, unique words in vocabulary only have 7:<br />
word #0/7 is t<br />
word #1/7 is l<br />
word #2/7 is x<br />
word #3/7 is e<br />
word #4/7 is _<br />
word #5/7 is u<br />
word #6/7 is f</p>
","python, machine-learning, data-science, word2vec","<p>Your <code>LemmaColumn</code> variable probably isn't in the format <code>Word2Vec</code> needs for the <code>sentences</code> argument. It needs a Python sequence: something than can be iterated over multiple times, like a list, or another re-iterable object. And in that sequence, every individual item must itself be a list-of-string-tokens (words).</p>
<p>Your tiny vocabulary is instead what I'd expect to see if instead:</p>
<pre><code>LemmaColumn = [ 
    ['f', 'u', 'l', 'l', '-', 't', 'e', 'x', 't'],
]
</code></pre>
<p>â€¦or evenâ€¦</p>
<pre><code>LemmaColumn = [ 
    ['full-text'],
]
</code></pre>
<p>â€¦because Python will happily treat a plain string (like <code>'full-text'</code>) as if it were a list filled with 1-character strings. Thus your entire training vocabular is only the characters of that single string â€“ likely a column-name, rather than the column-data you want to be using.</p>
<p>Double-check what's in <code>LemmaColumn</code>. Perform the necessary transformations on the column's data to make it the kind of sequence <code>Word2Vec</code> expects, &amp; confirm it looks sensible before trying <code>Word2Vec</code>.</p>
<p>Also: running with logging on to at least the <code>INFO</code> level will show a lot more of the model's progress, and as you learn to understand the reported steps/progress, things like weirdly-low counts of texts/words, or steps that'd take time if they were working on the right amount (lots) of data completing instantly, will be evident sooner.</p>
<p>Finally, note that <code>min_count=1</code> is essentially always a bad idea with an algorithm like word2vec. Good vectors only come from multiple varied examples of the same word's usage â€“ hence the default <code>min_count=5</code>. Keeping rare words not only tends to get poor vectors for those rare words, but the fact that natural-language text tends to have lots of such rare words means so much of the model's time &amp; space is devoted to the (nearly hopeless) task of improving those junk words' vectors that other nearby words' vectors suffer as well.</p>
",3,1,3898,2022-04-25 09:27:23,https://stackoverflow.com/questions/71997293/word2vec-raise-keyerrorfkey-key-not-present
Similarity between multiple vectors having same length,"<p><strong>Objective</strong> : Compute a similarity between two users on the basis of their skills</p>
<p><strong>Approach</strong> : Trained a <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">word2vec</a> model using <code>gensim</code> library on the set of skills obtained from Job Descriptions. Model seems to be working pretty fine when used <code>model.wv.most_similar</code>
e.g.
<a href=""https://i.sstatic.net/7O4uG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7O4uG.png"" alt=""most_similar skill example"" /></a></p>
<p><strong>Problem</strong> : Vocabulary of the skills on which model was trained doesn't match with the skills which I currently have so I went ahead and found a replacement of the current skills from the model's vocabulary by finding a similarity w.r.t spelling using <code>SequenceMatcher</code> from module <code>difflib</code>. e.g. &quot;PyTorch&quot; was there in my current skills but the model's vocabulary had &quot;torch&quot; present as a skill. So using <code>SequenceMatcher</code> I found that &quot;torch&quot; has the highest similarity from model's vocabulary so I replaced &quot;Pytorch&quot; with &quot;torch&quot; and computed the vector representation of the same by passing &quot;torch&quot; into the model, <code>model.wv[&quot;torch&quot;]</code>
and stored it in a dictionary so that I won't have to compute it again and again.</p>
<p>Function to compute the same :</p>
<pre class=""lang-py prettyprint-override""><code>def new_to_old_embedding(skill_embeddings, new_skill, model)
    &quot;&quot;&quot; Computing embeddings for new skills from app by mapping new skills with old skills from model's vocabulary
    
    Returns:
        dict: Embeddings of new skills after mapping with old skills
    &quot;&quot;&quot;
    if new_skill not in old_skills:
        thresh = 0.6
        replaced_skill = ''
        for old_skill in old_skills :
            spell_sim = SequenceMatcher(None, old_skill, new_skill).ratio()
            if spell_sim &gt; thresh :
                thresh = spell_sim
                replaced_skill = old_skill
        skill_embeddings[new_skill] = model.wv[replaced_skill]
    else :
        skill_embeddings[new_skill] = model.wv[new_skill]
    return skill_embeddings
</code></pre>
<p>Similarly for all of my current skills, I found a nearest skill w.r.t spelling and computed its vector representation and stored it in a python dictionary.</p>
<p>Now if user1 has skills = [&quot;OpenCV&quot;, &quot;Python&quot;] and user2 has skills = [&quot;Machine Learning&quot;, &quot;Deep Learning&quot;, &quot;Python&quot;] and I already have vector representations of each skill stored in a dictionary then how can I compute the similarity between these two sets of skills ?</p>
<p>OR</p>
<p>In other words, I have to find a similarity between two matrices of dimensions (m, L) and (n, L)
where,</p>
<ul>
<li>m is number of skills for user1</li>
<li>n is the number of skills for user2</li>
<li>L is the length of the vector representing skill which is fixed (300 in my case)</li>
</ul>
<p>I did found <a href=""https://math.stackexchange.com/questions/507742/distance-similarity-between-two-matrices"">this</a> question but since my problem is a NLP problem I was not sure whether or not this will work.</p>
","python, nlp, word2vec","<p>One option would be to average the multiple vectors together for each set-of-skills, then compute the cosine-similarity between those average vectors.</p>
<p>The next version of Gensim will have a utility method on <code>KeyedVectors</code> that will let you supply a list of keys (words), and return the average of all those vectors. Until that's released, you could use its source code as a model for your own calculations:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/97cef997032c3222645ebdc898c199a7b63e5395/gensim/models/keyedvectors.py#L462"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/97cef997032c3222645ebdc898c199a7b63e5395/gensim/models/keyedvectors.py#L462</a></p>
<p>Thee's also a utility method to calculate the cosine-similarity between one vector and a list of others, <code>KeyedVectors.cosine_similarities()</code>, that you could use on those averages:</p>
<p>docs: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.cosine_similarities"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.cosine_similarities</a></p>
<p>source: <a href=""https://github.com/RaRe-Technologies/gensim/blob/97cef997032c3222645ebdc898c199a7b63e5395/gensim/models/keyedvectors.py#L1147"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/97cef997032c3222645ebdc898c199a7b63e5395/gensim/models/keyedvectors.py#L1147</a></p>
<p>But, this way of comparing sets-of-vectors â€“ by their average â€“ while straightforward &amp; common, is only one of many possible ways.</p>
<p>Another option is something called &quot;Word Mover's Distance&quot; (WMD), which is  more expensive to calculate (especially on larger sets), because it actually uses a search for a minimal set of changes to 'shift' the different sets-of-meanings to match. But the resulting distances (smaller for more-similar sets) can sometmes better capture what's meaningful.</p>
<p>It's available as a method on <code>KeyedVectors</code> where you supply two lists of keys (word) that should be in the set-of-<code>KeyedVectors</code>, and it returns the calculated distance:</p>
<p>docs: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.wmdistance"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.wmdistance</a></p>
",1,0,1056,2022-04-30 07:20:03,https://stackoverflow.com/questions/72066483/similarity-between-multiple-vectors-having-same-length
Can we deduce the relationship b/w a dimension of a word vector with the linguistic characteristic it represents?,"<p>Let's imagine we generated a 200 dimension word vector using any pre-trained model of the word ('hello') as shown in the below image.</p>
<p><a href=""https://i.sstatic.net/5TfJp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5TfJp.png"" alt=""Word_Vector"" /></a></p>
<p>So, by any means can we tell which linguistic feature is represented by each d_i of this vector?</p>
<p>For example, d1 might be looking at whether the word is a noun; d2 might tell whether the word is a named entity or not and so on.</p>
","nlp, stanford-nlp, word2vec, word-embedding","<p>Because these word vectors are dense distributional representations, it is often difficult / impossible to interpret individual neurons, and such models often do not localize interpretable features to a single neuron (though this is an active area of research). For example, see <a href=""https://arxiv.org/abs/2010.02695"" rel=""nofollow noreferrer"">Analyzing Individual Neurons in Pre-trained Language Models
</a> for a discussion of this with respect to pre-trained language models).</p>
<p>A common method for studying how individual dimensions contribute to a particular phenomenon / task of interest is to train a linear model (i.e., logistic regression if the task is classification) to perform the task from fixed vectors, and then analyze the weights of the trained linear model.</p>
<p>For example, if you're interested in part of speech, you can train a linear model to map from the word vector to the POS [1]. Then, the weights of the linear model represent a linear combination of the dimensions that are predictive of the feature. For example, if the weight on the 5th neuron has large magnitude (very positive or very negative), you might expect that neuron to be somewhat correlated with the phenomenon of interest.</p>
<p>[1]: Note that defining a POS for a particular word is nontrivial, since the POS often depends on context. For example, &quot;play&quot; can be a noun (&quot;he saw a play&quot;) or a verb (&quot;I will play in the grass&quot;).</p>
",2,0,54,2022-05-03 05:25:50,https://stackoverflow.com/questions/72095099/can-we-deduce-the-relationship-b-w-a-dimension-of-a-word-vector-with-the-linguis
"Infer &quot;shapes&quot;, or infer analogous relations in Word2Vec","<p>Gensim Word2Vec offers a system for inferring analogous relationships, that is, with the &quot;same shape&quot; as those already found?</p>
<p>Es:
Starting from <strong>King, Queen</strong></p>
<p>I would like to get other couples with male / female gender.</p>
<p>In other word:
most_similar(positive=['king', X], negative=['queen']) -&gt; Y</p>
<p>I would like to find as many xy pairs.</p>
","gensim, word2vec","<p>There's no built-in facility resembling what I think you're asking.</p>
<p>But, you are of course free to cycle through any number of candidate words (as <code>X</code>, or the other arguments to <code>most_similar()</code>), to see what top-neighbors are reported (candidate <code>Y</code> values) - perhaps applying some threshold of similarity.</p>
<p>Note the famous <code>man:king :: woman: _?_</code> is usually presented to a word2vec model in Gensim as <code>most_similar(positive=['king', 'woman'], negative=['man'])</code>, which sort of achieves <code>king - man + woman = _?_</code>. I'm not sure your alternate formulation, effectively <code>king - queen + X = Y</code> has an analogical meaning, for arbitrary <code>X</code> or responses <code>Y</code>.</p>
<p>And, note that <code>most_similar()</code> suppresses the reporting of any candidate wards that are already arguments to <code>positive</code> or <code>negative</code>. Often, the results of the 'artihmetic' are still closer to the input words than anything else - but that won't be reported, showing next-best words instead.</p>
",1,0,17,2022-05-09 00:40:06,https://stackoverflow.com/questions/72165998/infer-shapes-or-infer-analogous-relations-in-word2vec
What is the data type of X in pca.fit_transform(X)?,"<p>I got a word2vec model <code>abuse_model</code> trained by Gensim. I want to apply PCA and make a plot on CERTAIN words that I only care about (vs. all words in the model). Therefore, I created a dict <code>d</code> whose keys are words that I care about and the values are vectors to the key.</p>
<pre><code>vocab = list(abuse_model.wv.key_to_index)
vocab = [v for v in vocab if v in positive_terms]
d = {}
for word in vocab:
    d[word] = abuse_model.wv[word]
</code></pre>
<p>No errors so far.</p>
<p>I encountered an error when passing the dict into <code>pca.fit_transform</code>. I'm new to it and am wondering if the data format that I passed in (list of tuples) is not correct. What data type that the argument has to be?</p>
<pre><code>from sklearn.decomposition import PCA

pca = PCA(n_components=2)
result = pca.fit_transform(list(d.items()))
</code></pre>
<p>Thanks in advance!</p>
","scikit-learn, pca, gensim, word2vec","<p>Per <code>scikit-learn</code> docs â€“ <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform</a> â€“ the argument to <code>.fit_transform()</code>, as is usual for <code>scikit-learn</code> models, is &quot;array-like of shape (n_samples, n_features)&quot;.</p>
<p>Here, that'd mean your samples/rows are words, and features/columns the word-vector dimensions. And, you'll want to remember <em>outside</em> of the <code>PCA</code> object which words correspond to which rows. (In Python 3.x, the fact your <code>d</code> <code>dict</code> will always iterate in the order of insertion should have you covered there.)</p>
<p>So, it may be enough to change your use of <code>.items()</code> to <code>.values()</code>, so that you wind up supplying <code>PCA</code> with your <code>list</code> (which is suitably array-like) of vectors.</p>
<p>A few other notes:</p>
<ul>
<li>the <code>.key_to_index</code> property is already a <code>list</code>, so you don't need to convert/copy it</li>
<li>if your <code>positive_terms</code> is a large <code>list</code>, changing it to a <code>set</code> could offer faster <code>in</code> membership-testing</li>
<li>rather than using a <code>d</code> <code>dict</code>, which involves a little more overhead (including when you then make a <code>list</code> of its values), if your sets-of-words and vectors are large, you might want to preallocate a <code>numpy</code> array of the right size and collect your vectors in it. For example:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>X = np.empty((len(vocab), abuse_model.wv.vector_size)
for i, word in enumerate(vocab):
    X[i] = abuse_model.wv[word]

#...
#...

result = pca.fit_transform(X)
</code></pre>
<ul>
<li>Even though your hunch is you only want the dimensionality-reduction on your subset of words, you may also want to try keeping all words, or some random subset of other words â€“ it <em>might</em> help retain some of the original structure that otherwise, your subsampling will have prematurely removed. (Unsure of this; just noting it could be a factor.) Even if you do the PCA on a larger set of words, you could still choose to only later plot/analyze your desired subset for clarity.</li>
</ul>
",0,0,611,2022-05-12 19:17:57,https://stackoverflow.com/questions/72221035/what-is-the-data-type-of-x-in-pca-fit-transformx
movie similarity using Word2Vec and deep Convolutional Autoencoders,"<p>i am new to python and i am trying to create a model that can measure how similar movies are based on the movies description,the steps i followed so far are:</p>
<p>1.turn each movie description into a vector of 100*(maximum number of words possible for a movie description) values using Word2Vec, this results in a 21300-values vector for each movie description.
2.create a deep convolutional autoencoder that tries to compress each vector(and hopefully extract meaning from it).</p>
<p>while the first step was successful and i am still struggling with the autoencoder, here is my code so far:</p>
<pre><code>encoder_input = keras.Input(shape=(21300,), name='sum')
encoded= tf.keras.layers.Reshape((150,142,1),input_shape=(21300,))(encoder_input)
x = tf.keras.layers.Conv2D(128, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;,input_shape=(1,128,150,142))(encoded)
x = tf.keras.layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv2D(64, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)#49*25*64
x = tf.keras.layers.Conv2D(32, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)#25*13*32
x = tf.keras.layers.Conv2D(16, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv2D(8, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)
x=tf.keras.layers.Flatten()(x)
encoder_output=keras.layers.Dense(units=90, activation='relu',name='encoder')(x)
x= tf.keras.layers.Reshape((10,9,1),input_shape=(28,))(encoder_output)

# Decoder

decoder_input=tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(decoder_input)
x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
decoder_output = keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same')(x)

autoencoder = keras.Model(encoder_input, decoder_output)
opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)

autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')

autoencoder.compile(opt, loss='mse')
print(&quot;STARTING FITTING&quot;)


history = autoencoder.fit(
movies_vector,
movies_vector,
epochs=25,

        )


print(&quot;ENCODER READY&quot;)
#USING THE MIDDLE LAYER 
encoder = keras.Model(inputs=autoencoder.input,
                    outputs=autoencoder.get_layer('encoder').output)
</code></pre>
<p>running this code gives me the following error:</p>
<pre><code>required broadcastable shapes [[node mean_squared_error/SquaredDifference (defined at tmp/ipykernel_52/3425712667.py:119) ]] [Op:__inference_train_function_1568]
</code></pre>
<p>i have two questions:</p>
<p>1.how can i fix this error?</p>
<p>2.how can i improve my autoencoder so that i can use the compressed vectors to test for movie similarity?</p>
","tensorflow, keras, deep-learning, word2vec, autoencoder","<ol>
<li><p>The output of your model is (batch_size, 260, 228, 1), while your targets appear to be (batch_size, 21300). You can solve that problem by either adding a <code>tf.keras.layers.Flatten()</code> layer to the end of your model, or by not flattening your input.</p>
</li>
<li><p>You probably should not be using 2D convolutions, as there is no spatial or temporal correlation between adjacent feature channels in most text embedding. You should be able to safely reshape to (150,142) rather than (150, 142, 1) and use 1D convolution, pooling, and upsampling layers.</p>
</li>
</ol>
",0,0,118,2022-05-14 19:03:03,https://stackoverflow.com/questions/72243158/movie-similarity-using-word2vec-and-deep-convolutional-autoencoders
Deep Convolutional Autoencoder for movie similarity,"<p>i am new to python and i have a dataset that contains movie descriptions and i am trying to create a model that can calculate movie similarity based on these descriptions.
so i started by turning each movie description into a Word2Vec vector where each word has a size 100,since the longest movie description in my dataset has 213 words, each movie description is turned into a vector of size 21300.
now my next step is to reduce the dimensionality of these vectors using a convolutional autoencoder.
it was recommended to me that i turn each 21300-sized vector into a 150 by 142 matrix so i did that, my goal is to compress these matrices from 150 by 142 to 5 by 5 matrix which i will then flatten and use to calculate cosine similarity between different compressed movie vectors.
now here is my faulty code so far:</p>
<pre><code>encoder_input = keras.Input(shape=(21300,), name='sum')
encoded= tf.keras.layers.Reshape((150,142),input_shape=(21300,))(encoder_input)
x = tf.keras.layers.Conv1D(32, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;,input_shape=(16,150,142))(encoded)
x = tf.keras.layers.MaxPooling1D(2, padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv1D(32, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling1D(2, padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv1D(16, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling1D(2, padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv1D(16, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling1D(2, padding=&quot;same&quot;)(x)
x = tf.keras.layers.Conv1D(8, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
x = tf.keras.layers.MaxPooling1D(2, padding=&quot;same&quot;)(x)
x=tf.keras.layers.Flatten()(x)
encoder_output=keras.layers.Dense(units=25, activation='relu',name='encoder')(x)
x= tf.keras.layers.Reshape((5,5),input_shape=(25,))(encoder_output)

# Decoder

decoder_input=tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(x)
x = tf.keras.layers.UpSampling1D(2)(decoder_input)
x = tf.keras.layers.Conv1D(16, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(16, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
#x=tf.keras.layers.Flatten()(x)
decoder_output = keras.layers.Conv1D(1, 3, activation='relu', padding='same')(x)




opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)

autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')

autoencoder.compile(opt, loss='mse')
autoencoder.summary()

history = autoencoder.fit(
movies_vector,
movies_vector,
epochs=25

        )
   

print(&quot;ENCODER READY&quot;)
#USING THE MIDDLE LAYER 
encoder = keras.Model(inputs=autoencoder.input,
   outputs=autoencoder.get_layer('encoder').output)
</code></pre>
<p>running this code produces the following error:</p>
<pre><code>ValueError: Dimensions must be equal, but are 100 and 21300 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](mean_squared_error/remove_squeezable_dimensions/Squeeze, IteratorGetNext:1)' with input shapes: [?,100], [?,21300].
</code></pre>
<p>how can i fix this autoencoder?</p>
","tensorflow, keras, deep-learning, word2vec, autoencoder","<p>I was able to reproduce the error with dummy data. Changing the decoder model  as follows will help.</p>
<pre><code>decoder_input=tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(x)
x = tf.keras.layers.UpSampling1D(2)(decoder_input)
x = tf.keras.layers.Conv1D(16, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(16, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)
x = tf.keras.layers.UpSampling1D(2)(x)
x=tf.keras.layers.Conv1D(213, 3, activation='relu', padding='same')(x)
decoder_output = tf.keras.layers.Flatten()(x)
</code></pre>
<p>Please find the gist <a href=""https://colab.sandbox.google.com/gist/synandi/21a36e06372ad0be516f73ee7fcef8f1/untitled91.ipynb"" rel=""nofollow noreferrer"">here</a>. Thank you.</p>
",0,0,94,2022-05-16 11:26:09,https://stackoverflow.com/questions/72258450/deep-convolutional-autoencoder-for-movie-similarity
How to create word embedding using Word2Vec on Python?,"<p>I have seen many tutorials online on how to use Word2Vec (gensim).</p>
<p>Most tutorials are showing on how to find the <code>.most_similar</code> word or similarity between two words.</p>
<p>But, how if I have text data <code>X</code> and I want to produce the word embedding vector <code>X_vector</code>?</p>
<p>So that, this <code>X_vector</code> can be used for classification algorithms?</p>
","python, gensim, word2vec, text-classification, word-embedding","<p>If <code>X</code> is a word (string token), you can look up its vector with <code>word_model[X]</code>.</p>
<p>If <code>X</code> is a text - say, a list-of-words â€“ well, a <code>Word2Vec</code> model only has vectors for words, not texts.</p>
<p>If you have some desired way to use a list-of-words plus per-word-vectors to create a text-vector, you should apply that yourself. There are many potential approaches, some simple, some complicated, but no one 'official' or 'best' way.</p>
<p>One easy popular baseline (a fair starting point especially on very small texts like titles) is to average together all the word vectors. That can be as simple as (assuming <code>numpy</code> is imported as <code>np</code>):</p>
<pre><code>np.mean([word_model[word] for word in word_list], axis=0)
</code></pre>
<p>But, recent versions of Gensim also have a convenience <code>.get_mean_vector()</code> method for averaging together sets of vectors (specified as their word-keys, or raw vectors), with some other options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
",2,0,1902,2022-05-23 09:42:48,https://stackoverflow.com/questions/72346412/how-to-create-word-embedding-using-word2vec-on-python
Compute sentence similarity between predicted sentence and a list of sentences(Using TDIDF),"<p>i am trying to find the a method that uses TDIDF to see how 'new' a predicted sentence is compared to the list it was generated from.</p>
<p>So for example:</p>
<p>New sent. = &quot;Hello world&quot;</p>
<p>Then i have a list of sentences and i want to find for example the top 5 sentence that are most comparable to the new sentence.</p>
<p>I know i need to vectorize the sentences, but how do i then get a score for each sentence in the list and return the top 5 most comparable.</p>
","python, pandas, word2vec","<p>One of the intro 'Core Concepts' sections of the documentation for Gensim (a popular Python library for modeling text) shows TFIDF-vectorization, then creating a helper index (which lets you check one vector against a bunch, listing the top results).</p>
<p>See: <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts</a></p>
",1,0,33,2022-05-23 15:40:59,https://stackoverflow.com/questions/72351292/compute-sentence-similarity-between-predicted-sentence-and-a-list-of-sentencesu
How to handle KeyError(f&quot;Key &#39;{key}&#39; not present&quot;) wor2vec with gensim,"<p>I have build a model with gensim library and am trying to get the vector of word that not present in the vocabulary but i have an error, and i want to handle this error with the best i way.
If i can get the vector of word not present in the model that well be perfect.</p>
<p><strong>The code</strong></p>
<pre><code>model = KeyedVectors.load('nice.model')
token_vector = model.wv['bla bla bla']
</code></pre>
<p><strong>Error</strong></p>
<pre><code>  File &quot;/home/ahmed/PycharmProjects/WebScarping/venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py&quot;, line 421, in get_index
    raise KeyError(f&quot;Key '{key}' not present&quot;)
KeyError: &quot;Key 'hmed' not present&quot;
</code></pre>
<p>please help me in resolving the error</p>
","python, nlp, gensim, word2vec, keyerror","<p>If the token is not present in the model, it can't give you a vector for it.</p>
<p>Your model doesn't have a vector for the (pseudo-)word <code>'bla bla bla'</code>, all it can do is report that.</p>
<p>You could avoid the exception by pre-checking whether the token is present, and only requesting it if present:</p>
<pre class=""lang-py prettyprint-override""><code>if token in model.wv:
    token_vector = model.wv[token]
else:
    # whatever your next-best step is when a vector not available
    ...
</code></pre>
<p>Or, you could catch the exception:</p>
<pre class=""lang-py prettyprint-override""><code>try:
    token_vector = model.wv[token]
except KeyError:
    # whatever your next-best step is when a vector not available
    ...
</code></pre>
<p>But there's no magic way to create a good vector for an unknown token. You'll have to ignore such words, or make-up some plug stand-in value, or figure some other project-appropriate workaround.</p>
<p>(If you have sufficient training data with varied examples of the token's real usage, you could train a model that includes the token. You could also consider finding or training a word2vec variant model like <code>FastText</code>, which can synthesize guess-vectors for unknown tokens based on which substrings they might share with words learned in training â€“ but such vectors may be quite poor in quality.)</p>
",2,2,7872,2022-06-02 17:42:32,https://stackoverflow.com/questions/72480289/how-to-handle-keyerrorfkey-key-not-present-wor2vec-with-gensim
Gensim- KeyError: &#39;word not in vocabulary&#39;,"<p>I am trying to achieve something similar in calculating product similarity used in this example. <a href=""https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/"" rel=""nofollow noreferrer"">how-to-build-recommendation-system-word2vec-python/</a></p>
<p>I have a dictionary where the key is the item_id and the value is the product associated with it. For eg: <code>dict_items([('100018', ['GRAVY MIX PEPPER']), ('100025', ['SNACK CHEEZIT WHOLEGRAIN']), ('100040', ['CAULIFLOWER CELLO 6 CT.']), ('100042', ['STRIP FRUIT FLY ELIMINATOR'])....)</code></p>
<p>The data structure is the same as in the example (as far as I know). However, I am getting <strong>KeyError: &quot;word '100018' not in vocabulary&quot;</strong> when calling the similarity function on the model using the key present in the dictionary.</p>
<pre><code># train word2vec model
model = Word2Vec(window = 10, sg = 1, hs = 0,
             negative = 10, # for negative sampling
             alpha=0.03, min_alpha=0.0007,
             seed = 14)
model.build_vocab(purchases_train, progress_per=200)
model.train(purchases_train, total_examples = model.corpus_count, 
        epochs=10, report_delay=1)

def similar_products(v, n = 6): #similarity function

# extract most similar products for the input vector
ms = model.similar_by_vector(v, topn= n+1)[1:]

# extract name and similarity score of the similar products
new_ms = []
for j in ms:
    pair = (products_dict[j[0]][0], j[1])
    new_ms.append(pair)
    
return new_ms    
</code></pre>
<p>I am calling the function using:</p>
<pre><code>similar_products(model['100018'])
</code></pre>
<p>Note: I was able to run the example code with the very similar data structure input which was also a dictionary. Can someone tell me what I am missing here?</p>
","python, gensim, word2vec","<p>If you get a <code>KeyError</code> telling you a word isn't in your model, then the word genuinely isn't in the model.</p>
<p>If you've trained the model yourself, and expected the word to be in the resulting model, but it isn't, something went wrong with training.</p>
<p>You should look at the corpus (<code>purchases_train</code> in your code) to make sure each item is of the form the model expects: a list of words. You should enable logging during training, and watch the output to confirm the expected amount of word-discovery and training is happening. You can also look at the exact list-of-words known-to-the-model (in <code>model.wv.key_to_index</code>) to make sure it has all the words you expect.</p>
<p>One common gotcha is that by default, for the best operation of the word2vec algorithm, the <code>Word2Vec</code> class uses a default <code>min_count=5</code>. (Word2vec only works well with multiple varied examples of a word's usage; a word appearing just once, or just a few times, usually won't get a good vector, and further, might make other surrounding word's vectors worse. So the usual best practice is to discard very-rare words.</p>
<p>Is the (pseudo-)word <code>'100018'</code> in your corpus less than 5 times? If so, the model will ignore it as a word too-rare to get a good vector, or have any positive influence on other word-vectors.</p>
<p>Separately, the site you're using example code from may not be a quality source of example code. It's changed a bunch of default values for no good reason - such as changing the <code>alpha</code> and <code>min_alpha</code> values to peculiar non-standard values, with no comment why. This is usually a signal that someone who doesn't know what they're doing is copying someone else who didn't know what they were doing's odd choices.</p>
",1,1,1931,2022-06-06 15:03:32,https://stackoverflow.com/questions/72519612/gensim-keyerror-word-not-in-vocabulary
How to convert small dataset into word embeddings instead of one-hot encoding?,"<p>I have a dataset of 33 words that are a mix of verbs and nouns, for eg. father, sing, etc. I have tried converting them to 1-hot encoding but for my use case, it has been suggested to look into word2vec embedding. I have looked in gensim and glove but struggling to make it work.</p>
<p><strong>How could I convert my data into an embedding?</strong> Such that two words that may be semantically closer may have a lesser distance between their respective vectors. How may this be achieved or any helpful material on the same?</p>
<p>Such as this<img src=""https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png"" alt=""embedding"" /></p>
","nlp, stanford-nlp, gensim, word2vec","<p>Since your dataset is quite small, and I'm assuming it doesn't contain any jargon, it's best to use a pre-trained model in order to save up on training time.</p>
<p>With gensim, it's as simple as:</p>
<pre><code>import gensim.downloader as api
wv = api.load('word2vec-google-news-300')
</code></pre>
<p>The 'word2vec-google-news-300' model has been pre-trained on a part of the Google News Dataset and generalizes well enough to most tasks. Following this, you can create word embeddings/vectors like so:</p>
<pre><code>vec = wv['father']
</code></pre>
<p>And, finally, for computing word similarity:</p>
<pre><code>similarity_score = wv.similarity('father', 'sing')
</code></pre>
<p>Lastly, one major limitation of Word2Vec is it's inability to deal with words that are OOV(out of vocabulary). For such cases, it's best to train a custom model for your corpus.</p>
",3,1,1014,2022-06-08 12:30:53,https://stackoverflow.com/questions/72545744/how-to-convert-small-dataset-into-word-embeddings-instead-of-one-hot-encoding
Is it possible to extract the matrices WI and WO from blazingtext word2vec?,"<p>Similar to the question linked below, I would like to access the input and output matricies WI and WO. However, I am using the Blazingtext implementation of Word2Vec.</p>
<p>After fitting the model.tar.gz atrifact contains: vectors.txt which corresponds to WI; and the binary model.bin for hosting</p>
<p>Does anyone know if it's posisble to acess the WO matrix if using blazingtext?</p>
<p><a href=""https://stackoverflow.com/questions/42554289/how-can-i-access-output-embeddingoutput-vector-in-gensim-word2vec"">How can I access output embedding(output vector) in gensim word2vec?</a></p>
<p>Thanks in advance</p>
",word2vec,"<p>From a quick glance through the BlazingText docs &amp; examples notebooks, I don't see Amazon choosing to expose any access to the 'output' vector weights, nor any examples which indirectly reveal where they might be.</p>
<p>They <em>might</em> be encoded somewhere in the <code>vectors.bin</code>, but there seem to be no docs of that format. If the source code for BlazingText (or even a few key parts were available, it might be straightforward to figure out if (&amp; where) those get stored... but it seems no source code is available.</p>
<p>So it may be the case that only Amazon engineers with proprietary info can answer that question.</p>
<p>Are you sure you need to use Amazon's BlazingText rather than some better-documented, source-available alternate implementation?</p>
<p>Its main benefit seems to be training speed, which may only be decisive for extra-large training sets, or situations where short-lag reindexing occurs regularly. (In cases where some large historic corpus is used for training once, then the vectors used for many downstream purposes, a one-time training session that runs &quot;over lunch&quot; or &quot;overnight&quot; is often fast enough.)</p>
",0,0,23,2022-06-21 09:29:44,https://stackoverflow.com/questions/72698389/is-it-possible-to-extract-the-matrices-wi-and-wo-from-blazingtext-word2vec
&#39;MeanEmbeddingVectorizer&#39; object has no attribute &#39;transform&#39;,"<p>Hello i'm working with text classification.
I've a dataset with 2 columns one made of text and the other one is the label.
Since i'm a beginner i'm following step by step a tutorial on W2vec trying to understand if it can work for my usecase but i keep getting this error.</p>
<p>This is my code</p>
<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
def fit(self, X, y):
        return self
def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

train_df['clean_text_tok']=[nltk.word_tokenize(i) for i in train_df['clean_text']]
model = Word2Vec(train_df['clean_text_tok'],min_count=1)
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))
modelw = MeanEmbeddingVectorizer(w2v)
# converting text to numerical data using Word2Vec
X_train_vectors_w2v = modelw.transform(X_train_tok)
X_val_vectors_w2v = modelw.transform(X_test_tok)
</code></pre>
<p>the error i'm getting is :</p>
<pre><code>Dimension:  100
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-127-289141692350&gt; in &lt;module&gt;
      4 modelw = MeanEmbeddingVectorizer(w2v)
      5 # converting text to numerical data using Word2Vec
----&gt; 6 X_train_vectors_w2v = modelw.transform(X_train_tok)
      7 X_val_vectors_w2v = modelw.transform(X_test_tok)

AttributeError: 'MeanEmbeddingVectorizer' object has no attribute 'transform'

</code></pre>
","python, nlp, word2vec","<p>If your <code>MeanEmbeddingVectorizer</code> is defined in your code exactly as its shows here, the failure-to-indent the <code>.fit()</code> and <code>.transform()</code> functions means they're <em>not</em> part of the class, as you likely intended.</p>
<p>Indenting those each an extra 4 spaces â€“ as was likely the intent of any source you copied this code from! â€“ will put them &quot;inside&quot; the <code>MeanEmbeddingVectorizer</code> class, as class methods. Then, objects of that class won't give the same &quot;no attribute&quot; error.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
    def fit(self, X, y):
        return self
    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])
</code></pre>
",0,0,1127,2022-06-21 12:00:15,https://stackoverflow.com/questions/72700395/meanembeddingvectorizer-object-has-no-attribute-transform
What is the ideal &quot;size&quot; of the vector for each word in Word2Vec?,"<p>I have a dataset of over 1 million rows. Each row has 40 token words. Based on these tokens, a classification is made with a neural network. The vocabulary is 20,000 unique words. It is a <strong>binary classification</strong> problem. I set the size (dimension) of the vectors in <code>gensim</code> <code>Word2Vec</code> as 150 and saved these vectors for each data point in a <code>json</code> file. The <code>json</code> file's size is really huge: 250 GB. I cannot load this file into memory in one scoop as my RAM is only 128 GB. I am trying to see if I can reduce the physical size of these vectors by reducing them to the right size. I went through some of the suggestions made in this website such as <a href=""https://stackoverflow.com/questions/46560861/relation-between-word2vec-vector-size-and-total-number-of-words-scanned"">Relation between Word2Vec vector size and total number of words scanned?</a>. But the vector size is mentioned to be 100-300 and also depends on the problem.</p>
<p>Here is what I am doing:</p>
<pre><code># for training the word2vec model
w2vmodel = gensim.models.Word2Vec(one_mil_tokens,vector_size=150, window=2, min_count=1, sg=0, seed=1)
w2vmodel.save(&quot;w2model.trained&quot;)
</code></pre>
<p>and</p>
<pre><code>model = gensim.models.Word2Vec.load(&quot;w2model.trained&quot;)
vec = []
finalvecs = []

#tokens is a list of over a 1 million rows
for token in tokens:
  for word in token:
    vec.append(model.wv[eachtoken].tolist())
  finalvecs.append(vec)
</code></pre>
<p>I am doing <code>json.dump()</code> for <code>finalvecs</code>.</p>
<ol>
<li>How can I determine the right size (dimension) of the vector for each token based on the given problem?</li>
<li>I use skip-gram model to train Word2Vec. Should I use CBOW to optimize the size?</li>
<li>Is <code>json</code> the fight format to store/retrieve these vectors or are there other efficient ways?</li>
</ol>
","python, python-3.x, machine-learning, nlp, word2vec","<p>Each dimension of a dense vector is typically a 32-bit float.</p>
<p>So, storing 20,000 token-vectors of 150 dimensions each will take at least <code>20000 vectors * 150 floats * 4 bytes/float</code> = 12MB for the raw vector weights, plus some overhead for remembering which token associates with which line.</p>
<p>Let's say you were somehow changing each of your rows into a single summary vector of the same size. (Perhaps, by averaging together each of the ~40 token vectors into a single vector â€“ a simple baseline approach, though there are many limitations of that approach, &amp; other techniques that might be used.) In that case, storing the 1 million vectors will necessarily take about <code>1000000 vectors * 150 floats * 4 bytes/float</code> = 600MB for the raw vector weights, plus some overhead to remember which row associates with which vector.</p>
<p>That neither of these is anywhere close to 250GB implies you're making some other choices expanding things significantly. JSON is a poor choice for compactly representing dense floating-point numerical data, but even that is unlikely to explain the full expansion.</p>
<p>Your description that you &quot;saved these vectors for each data point in a json file&quot; isn't really clear what vectors are being saved, or in what sort of JSON conventions.</p>
<p>Perhaps you're storing the 40 separate vectors for each row? That'd give a raw baseline weights-only size of <code>1000000 rows * 40 tokens/row * 150 floats * 4 bytes/float</code> = 24GB. It is plausible inefficient JSON is expanding the stored-size by something like 10x, so I guess you're doing something like this.</p>
<p>But in addition to the inefficiency of JSON, given that the 40 tokens (from a vocabulary of 20k) each given enough info to reconstitute any other per-row info that's solely a function of the tokens &amp; the 20k word-vectors, there's not really any reason to expand the representations this way.</p>
<p>For example: If the word <code>'apple'</code> is already in your dataset, and appears many thousands of times, there's no reason to re-write the 150 dimensions of <code>'apple'</code> many thousands of times. The word <code>'apple'</code> alone is enough to call-back those 150 dimensions, whenever you want them, from the much-smaller (12MB) set-of-20k token-vectors, that's easy to keep in RAM.</p>
<p>So mainly: ditch JSON, don't (unnecessarily) expand each row into <code>40 * 150</code> dimensions.</p>
<p>To your specific questions:</p>
<ol>
<li><p>The optimal size will vary based on lots of things, including your data &amp; other goals. The only way to rationally choose is to figure out some way to score the trained vectors on your true end goals: some repeatable way of comparing multiple alternate choices of <code>vector_size</code>. Then you run it every plausible way &amp; pick the best. (Barring that, you take a random stab based on some precedent work that seems roughly similar in data/goals, and hope that works OK until you have the chance to compare it against other choices.)</p>
</li>
<li><p>The choice of skip-gram or CBOW wont affect the size of the model at all. It might affect end result quality &amp; training times a bit, but the only way to choose is to try both &amp; see which works better for your goals &amp; constraints.</p>
</li>
<li><p>JSON is an awful choice for storing dense binary data. Representing numbers as just text involves expansion. The JSON formatting characters add extra overhead, repeatedly on every tow, that's redundant if every row is the exact same 'shape' of raw data. And, typical later vector operations in RAM usually work best on the same sort of maximally-compact raw in-memory representation that would also be best on disk. (In fat, the best on-disk representation will often be data that <em>exactly matches</em> the format in memory, so that data can be &quot;memory-mapped&quot; from disk to RAM in a quick direct operation that minimizes format-wrangling &amp; even defers accesses until reads needed.)</p>
</li>
</ol>
<p>Gensim will efficiently save its models via their build-in <code>.save()</code> method, into one (or more often several) related files on disk. If your Gensim <code>Word2Vec</code> model is in the variable <code>w2v_model</code>, you can just save the whole model with <code>w2v_model.save(YOUR_FILENAME)</code> â€“ &amp; later reload it with <code>Word2Vec.load(YOUR_FILENAME)</code>.</p>
<p>But if after training you only need the (20k) word-vectors, you can just save the <code>w2v_model.wv</code> property â€“ just the vectors: <code>w2v_model.wv.save(YOUR_FILENAME)</code>. Then you can reload them as an instance of <code>KeyedVectors</code>: <code>KeyedVectors.load(YOUR_FILENAME)</code>.</p>
<p>(Note in all cases: the save may be spread over multiple files, which if ever copied/moved elsewhere, should be kept together - even though you only ever specify the 'root' file of each set in save/load operations.)</p>
<p>How &amp; whether you'd want to store any vectorization of your 1 million rows would depend on other things not yet specified, including the character of the data, and the kinds of classification applied later. I doubt that you want to turn your rows into <code>40 * 150</code> 6000-dimensions â€“Â that'd be counter to some of the usual intended benefits of a <code>Word2Vec</code>-based analysis, where the word <code>apple</code> has much the same significance no matter where it appears in a textual list-of-words.</p>
<p>You'd have to say more about your data, &amp; classification goals, to get a better recommendation here.</p>
<p>If you haven't already done a 'bag-of-words' style representation of your rows (no word2vec), where every row is represented by a 20000-dimension one-hot sparse vector, and run a classifier on that, I'd recommend that first, as a baseline.</p>
",1,0,1899,2022-06-21 20:48:09,https://stackoverflow.com/questions/72706958/what-is-the-ideal-size-of-the-vector-for-each-word-in-word2vec
subsampling formula skipgram NLP,"<p>I'm studying how to implement a Skip-Gram model using Pytorch, I follow this <a href=""https://naturale0.github.io/2021/02/08/understanding-skip-gram#pytorch-implementation"" rel=""nofollow noreferrer"">tutorial</a>, in the subsampling part the author used this formula:</p>
<pre><code>import random
import math

def subsample_prob(word, t=1e-3):
    z = freq[word_to_ix[word]] / sum_freq
    return (math.sqrt(z/t) + 1) * t/z

words_subsample = [w for w in words if random.random() &lt; subsample_prob(w)]
</code></pre>
<p>where <code>z</code> variable is the proportion of counts of a certain word by the total of words in the corpus. my doubt is that depending on the proportion of words this formula gives a result greater than one, then the word is always added to the sub sample corpus, <strong>shouldn't it return a value between zero and one?</strong></p>
","nlp, word2vec, subsampling","<p>The frequent-word downsampling option ('subsampling') introduced in the original word2vec (as a <code>-sample</code> argument) indeed applies downsampling <em>only</em> to a small subset of the very-most-frequent words. (And, given the 'tall head'/Zipfian distributions of words in natural-language texts, that's plenty.)</p>
<p>Typical values leave most words <em>fully sampled</em>, as reflected in this formula by a sampling-probability greater-than <code>1.0</code>.</p>
<p>So: there's no error here. It's how the original word2vec implementation, and others, interpret the <code>sample</code> parameter. Most words are exempt from any thinning, but some of the most-common words are heavily dropped. (But, there's still plenty of their varied usage examples in the training set â€“ and indeed spending <em>fewer</em> training updates redundantly on those words lets <em>other</em> words get better vectors, facing less contention/dilution from overtraining of common words.)</p>
",0,0,298,2022-06-24 16:28:49,https://stackoverflow.com/questions/72746948/subsampling-formula-skipgram-nlp
Using a Word2Vec Model to Extract Data,"<p>I've used gensim Word2Vec to learn the embedding of monetary amounts and other numeric data in bank transaction memos. The goal is to use this to be able to extract these amounts and currencies from future input strings.</p>
<p><strong>Design</strong>
Our input strings are something like</p>
<pre><code>&quot;AMAZON.COM TXNw98e7r3347 USD 49.00 @ 1.283&quot;
</code></pre>
<p>During preprocessing, I tokenize and also replace all tokens that have the possibility of being a monetary amount (string consisting only of digits, commas, and &lt;= 1 decimal point/period) with a special VALUE_TOKEN. And I also manually replace exchange rates with RATE_TOKEN. The result would be</p>
<pre><code>[&quot;AMAZON&quot;, &quot;.COM&quot;, &quot;TXNw&quot;, &quot;98&quot;, &quot;e&quot;, &quot;7&quot;, &quot;r&quot;, &quot;3347&quot;, &quot;USD&quot;, &quot;VALUE_TOKEN&quot;, &quot;@&quot;, &quot;RATE_TOKEN&quot;]
</code></pre>
<p>With all my preprocessed lists of strings in list <code>data</code>, I generate model</p>
<pre><code>model = Word2Vec(data, window=3, min_count=3)
</code></pre>
<p>The embeddings of model that I'm most interested in are that of VALUE_TOKEN, RATE_TOKEN, as well as any currencies (USD, EUR, CAD, etc.). Now that I generated the model, I'm not sure what to do with it.</p>
<p><strong>Problem</strong>
Say I have a new string that the model has never seen before,</p>
<pre><code>new_string = &quot;EUR 299.99 RATE 1.3289 WITH FEE 5.00&quot;
</code></pre>
<p>I would like to use <code>model</code> to identify which tokens of <code>new_string</code> is most contextually similar to VALUE_TOKEN (which should return [&quot;299.99&quot;, &quot;5.00&quot;]), which is closest to RATE_TOKEN (&quot;1.3289&quot;). It should be able to classify these based on the learned embedding. I can preprocess <code>new_string</code> the way I do with the training data, but because I don't know the exchange rate before hand, all three tokens of [&quot;299.99&quot;, &quot;5.00&quot;, &quot;1.3289&quot;] will be tagged the same (either with VALUE_TOKEN or a new UNIDENTIFIED_TOKEN).</p>
<p>I've looked into methods like <code>most_similar</code> and <code>similarity</code> but don't think they work for tokens that are not necessarily in the vocabulary. What methods should I use to do this? Is this the right approach?</p>
","machine-learning, nlp, data-science, word2vec, word-embedding","<p>Word2vec's fuzzy, dense embedded token representations don't strike me as the right tool for what you're doing, though they might perhaps be an indirect contributor to a hybrid approach.</p>
<p>In particular:</p>
<ul>
<li>The word2vec algorithm originated from, &amp; has the most consistent public results, when applied to natural-language texts, with their particular patterns of relative token frequences, and varied co-occurrences. Certainly, many ahave applied it, with success, to other kinds of text/record data, but such uses may require a lot more preprocessing/parameter-tuning, and to the extent the underlying data has some fixed, highly-repetitive scheme, might be more amenable to other approaches.</li>
<li>If you replace all <em>known</em> values with <code>'VALUE_TOKEN'</code>, &amp; all known rates with <code>'RATE_TOKEN'</code>, then the model is only going to learn token-vectors for <code>'VALUE_TOKEN'</code> &amp; <code>'RATE_TOKEN'</code>. Such a model won't be able to supply any vector for non-replaced tokens it's never seen like <code>'$1.2345'</code> or <code>'299.99'</code>. Even collapsing all those to <code>'UNIDENTIFIED_TOKEN'</code> just limits the model to whatever it learned earlier was the vector for <code>'UNIDENTIFIED_TOKEN'</code> (if any, in the training data).</li>
<li>I've not noticed existing word2vec implementations offering an interface for inferring the word-vector for new unknown-vectors, from just one or several new examples of its appearance in-context. They <em>could</em>, in the same style of new-document-vector inference used by 'Paragraph Vectors'/<code>Doc2Vec</code>, but just don't.) The closest I've seen is <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word"" rel=""nofollow noreferrer"">Gensim's <code>predict_output_word()</code></a>, which does a CBOW-like forward-propagation on negative-sampling models, to every 'output node' (one per known word), to give a ranked list of the known-words most-likely to appear given some context words.</li>
</ul>
<p>That <code>predict_output_word()</code> <em>might</em>, if fed surrounding known-tokens, contribute to your needs by whether it says your <code>'VALUE_TOKEN'</code> or <code>'RATE_TOKEN'</code> is a more-likely model-prediction. You could adapt its code to <em>only</em> evaluate those two candidates, if you're always sure the right answer is one or the other, for a speed-up. A simple comparison of the average-of-context-word-vectors, and the candidate-answer vectors, <em>might</em> be as effective as the full forward-propagation.</p>
<p>Alternatively, you might want use the word2vec model solely as a source of features (via context-words) for some other classifier, which is trained to answer <code>VALUE</code> or <code>TOKEN</code>. This other classifier's input might include things like:</p>
<ul>
<li>some average of the vectors of all nearby tokens</li>
<li>the full vectors of closest neighbors</li>
<li>a one-hot encoding ('bag-of-words') of all nearby (or 'preceding') or 'following) known-tokens, assuming the vocabulary of non-numerical tokens is fairly short &amp; highly indicative</li>
<li>?</li>
</ul>
<p>If the data streams might include arbitrary new or corrupted tokens whose meaning might be inferrable from substrings, you could consider a <code>FastText</code> model as well.</p>
",1,0,263,2022-07-08 17:21:37,https://stackoverflow.com/questions/72914889/using-a-word2vec-model-to-extract-data
What is the meaning of size(embedding_model)?,"<p>I want to be sure I understand correctly:</p>
<p>Using the length of embedding model means number of different tokens it contains?</p>
<p>i.e:</p>
<pre><code>from gensim import downloader
embedding_model = downloader.load('glove-wiki-gigaword-50')
print(len(embedding_model))
</code></pre>
<p>output:</p>
<pre><code>400000 
</code></pre>
<p>means: <code>glove-wiki-gigaword-50</code> has 400000 different tokens (words) and each token (word) has the size of 50 bytes ?</p>
","gensim, word2vec, word-embedding","<p>Yes, <code>len(model)</code> in this case gives you the count of words inside it.</p>
<p><code>model.vector_size</code> will give you the number of dimensions (not bytes) per vector. (The actual size of the vector in bytes will be 4 times the count of dimensions, as each <code>float32</code>-sized value takes 4 bytes.)</p>
<p>I generally recommend against ever using the Gensim <code>api.downloader</code> functionality: if you instead find &amp; manually download from the original source of the files, you'll better understand their contents, formats, &amp; limitations â€“ and where the file has landed in your local filesystem. And, by then using a specific class/method to load the file, you'll better understand what kinds of classes/objects you're using, rather than whatever mystery-object <code>downloader.load()</code> might have given you.</p>
",1,0,68,2022-07-09 02:59:25,https://stackoverflow.com/questions/72918624/what-is-the-meaning-of-sizeembedding-model
Get most similar words for matrix of word vectors,"<p>So I computed a matrix of word vectors manually using keras which looks like this:</p>
<pre><code>&gt;&gt;&gt; word_embeddings

        0           1           2           3 
movie   0.007964    0.004251    -0.049078   0.032954    ...
film    -0.006703   0.045888    -0.020975   0.012483    ...
one     -0.011733   0.003348    -0.022017   -0.006476   ...
make    0.045888    -0.011219   0.037796    -0.041868   ...

1000 rows Ã— 25 columns
</code></pre>
<p>What I want now is get the <code>n</code> most similar words to a given input word, eg. <code>input='movie'</code> -&gt; <code>output=['film', 'cinema', ...]</code></p>
<p>I computed a matrix of euclidian distances, but how do I get the above result?</p>
<pre><code>&gt;&gt;&gt; from sklearn.metrics.pairwise import euclidean_distances
&gt;&gt;&gt; distance_matrix = euclidean_distances(word_embeddings)

array([[0.       , 2.4705646, 2.363872 , ..., 3.1345532, 2.9737253,
        2.791427 ],
       [2.4705646, 0.       , 2.3540049, ..., 3.6580865, 3.4589343,
        3.494087 ],
       [2.363872 , 2.3540049, 0.       , ..., 3.9583569, 3.692863 ,
        3.5237448],
       ...,
       [3.1345532, 3.6580865, 3.9583569, ..., 0.       , 4.0572405,
        4.0648513],
       [2.9737253, 3.4589343, 3.692863 , ..., 4.0572405, 0.       ,
        4.156624 ],
       [2.791427 , 3.494087 , 3.5237448, ..., 4.0648513, 4.156624 ,
        0.       ]], dtype=float32)

1000 rows Ã— 1000 columns
</code></pre>
","python, pandas, keras, word2vec, euclidean-distance","<p>try this:</p>
<pre><code>top_k_similar_indexes = np.argsort(distance_matrix, axis=1)[:, :k]
</code></pre>
<p>then you will have the indexes of the k top similar words for each row. If you want the indexes of the k top most different words it will be <code>np.argsort(distance_matrix, axis=1)[:, -k:]</code></p>
",2,2,596,2022-07-14 09:35:14,https://stackoverflow.com/questions/72978354/get-most-similar-words-for-matrix-of-word-vectors
Gensim word2vec model with unordered sentences,"<p>Hello I'm trying to tune word2vec for finding related categories on a large set of categories list.</p>
<p>My main problem compare to natural language is that my categories list are not ordered in a logic manner.</p>
<p>For example I have a lists of fruits:</p>
<pre><code>[banana, mango, apple...], 
[mango, lemon, pineapple...]
</code></pre>
<p>Let's assume mango usually comes in the same list as banana.
I want the model to detect this relationship such that when I call most_similar to mango I'll get banana first.</p>
<p>The problem is that the order of the fruit is meaningless. Mango and banana distance in the list can differate without any meaning.</p>
<p>I thought to set a very high window so &quot;everything is related to everything&quot; but I'm not sure it's the best approach.</p>
<p>I have a dataset of 12M sentences with 500K unique categories.</p>
<p>What is a good started for aloha rate, window and adjusting the model in general? Does word2vec even fits this?</p>
","gensim, word2vec","<p>Setting a very-large window â€“ far larger than any of your texts â€“ does essentially put all words into each others' context-windows. (If your texts are long, it will also significantly increase runtime, especially in skip-gram  mode.) You can also use the optional non-default setting in recent Gensim releases <code>shrink_windows=False</code> tofurther ensure that a step which normally might probabilistically reduce effective window-sizes is skipped.</p>
<p>Whether word2vec will work well for you is something best answered empirically, by trying it versus other approaches on your data and needs. You're not quite using typical texts, with normal natural-language word frequencies &amp; co-occurrences. But lots of people have found success with word2vec-like approaches on not-quite-real-language tokenized data.</p>
<p>While the default values of parameters are often reasonable starting points, with enough data and a typical task, you may have to vary them even more from the defaults, for optimal results, when using other kinds of data, or pursuing less typical goals.</p>
<p>If using the default negative-sampling on categorical data, you may especially want to look at the optional parameter <code>ns_exponent</code> â€“ frozen in early implementations at <code>ns_exponent=0.75</code>, but now adjustable in Gensim â€“ which one paper suggested could have far better values in models for recommendation-systems. (See the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">class docs for that parameter</a> for a link to that paper.)</p>
<p>But more generally: to find optimal values, you'll need some way to explore many options in an automated fashion. That means some robust repeatable way to score your end model on your real intended end-task (or a close proxy), so that you can run it many different ways then pick the one that scores best. And, parameters like <code>epochs</code>, <code>vector_size</code>, <code>window</code>, <code>negative</code>, and <code>min_count</code> are those most-often tuned.</p>
<p>Note though that if all you really want is to get the ranked list of things that most-often come in the same list as a target query â€“ get back <code>'banana'</code> for <code>'mango'</code>, if and only if <code>'banana'</code> co-appears most-often â€“ then you can use a much simpler approach than word2vec: just count, and retain, <em>all</em> the co-occurrences, or all the top-N co-occurrences for each unique key.</p>
<p>A fairly straightforward &amp; usual way to do this in Python would be to use a <code>dict</code> with one entry per unique key (category), and have the value stored at that key be an instance of the Python utility dictionary <code>Counter</code>. Iterate once over the whole dataset, tallying every co-occurence.</p>
<p>At the end, when you look up the <code>Counter</code> for <code>'mango'</code>, just list its contents in highest-to-lowest count order. You'll have a precise answer, from a single pass over the data â€“ rather than the 'dense' vectors that an algorithm like word2vec builds over many passes, probabilistically.</p>
<p>(If instead of using sorted <code>Counter</code>s per key, but sparse vectors, as wide as your count of unique keys, containing counts of each co-occurence in the respective slots-per-category, then you'll have a 'bag-of-words'-like large sparse vector per primary key. Those can also be pairwise compared by cosine-similarity, similar to how dense embeddings like smaller-dimensional word-vectors can be.)</p>
",1,0,140,2022-07-27 12:04:32,https://stackoverflow.com/questions/73137615/gensim-word2vec-model-with-unordered-sentences
Gensim 4.2.0 downloader function is missing,"<p>I'm using the Gensim package. However, when I want to load the word2vec model, the <code>gensim.downloader</code> function seems not to exist.</p>
<pre><code>w2v = gensim.downloader.load('word2vec-google-news-300')
</code></pre>
<p>Got error message:</p>
<pre><code>AttributeError: module 'gensim' has no attribute 'downloader'
</code></pre>
<p>I checked the directory of gensim using dir() method and here's what I got:</p>
<pre><code>['__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','__version__','_matutils','corpora','interfaces','logger','logging','matutils','models','parsing','similarities','topic_coherence','utils']
</code></pre>
<p>Seems like the downloader method is not in the directory. I wonder if there's another way to download a specific pretrained model with gensim library and also what's wrong with the gensim downloader.</p>
<p>My gensim version is 4.2.0.</p>
","python, nlp, gensim, word2vec","<p>If you're following some example code, you should copy its imports &amp; code exactly. I don't think you'll find any docs/examples suggesting to use the <code>gensim.downloader</code> module the way you've attempted.</p>
<p>More generally: I recommend against using <code>gensim.downloader</code>. It hides the actual sources, local paths, &amp; return types of the data it retrieves, and also runs new code, from the net, that's not part of the Gensim project source-control nor part of versioned Gensim releases. (It's a sketchy software-engineering practice.)</p>
<p>Instead, download the <code>GoogleNews</code> dataset directly from some host, saving the exact original file(s) to a specific place of your choosing. Examine the downloads to understand their filenames/formats (decompressing if necessary).</p>
<p>Then use other Gensim methods â€“ such as <code>KeyedVectors.load_word2vec_format()</code> â€“ to load from a specific known local file path, with a returned object of a specific documented type.</p>
<p>Your code (and your own understanding) will be more clear, robust, &amp; secure.</p>
",0,0,469,2022-08-01 04:19:36,https://stackoverflow.com/questions/73188799/gensim-4-2-0-downloader-function-is-missing
Using Word2Vec for word embedding of sentences,"<p>I am trying to create an emotion recognition model and for that I am using Word2Vec. I have a tokenized pandas data frame <code>x_train['Utterance']</code> and I have used</p>
<pre><code>model = gensim.models.Word2Vec(x_train['Utterance'], min_count = 1, vector_size = 100)
</code></pre>
<p>to create a vocabulary. Then, I created a dictionary embeddings_index that has as key the words and as value the vector embedding. I created a new column in my data frame where every word is replaced by the respective vector.</p>
<pre><code>x_train['vector'] = x_train['Utterance'].explode().map(embeddings_index).groupby(level=0).agg(list)
</code></pre>
<p>Finally, I used pad_sequences so that each instance of the data set is padded to the size of the instance with biggest length (because the data set initially was made of sentences of different sizes):</p>
<pre><code>x_train['vector'] = tf.keras.utils.pad_sequences(x_train.vector, maxlen = 30, dtype='float64', padding='post', truncating='post', value=0).tolist()
</code></pre>
<p>If <code>min_count = 1</code>, one of the parameters of Word2Vec, everything is alright and <code>x_train['vector']</code> is what I pretend, a column of the embeddings vectors of the tokenized sentences in <code>x_train['Utterance']</code>. However, when <code>min_count != 1</code>, the created vocabulary only has the words which appears more than the <code>min_count</code> value in <code>x_train['Utterance']</code>. Because of this, when creating <code>x_train['vector']</code> mapping the dictionary <code>embeddings_index</code>, the new column will contain lists like the following <code>[nan, [0.20900646, 0.76452744, 2.3117824], [0....</code>, where <code>nan</code> corresponds to words that are not in the dictionary. Because of this <code>nan</code>, when using the <code>tf.keras.utils.pad_sequences</code> I get the following error message: ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.</p>
<p>I would like to remove the <code>nan</code> from each list but I am not being able. Tried the <code>fillna('')</code> however it just removes the <code>nan</code> but keeps an empty index on the list. Any idea?</p>
","python, pandas, nlp, word2vec, word-embedding","<p>It seems the problem may that <code>x_train['Utterance']</code> includes a bunch of words that (after <code>min_count</code> trimming) aren't in the model. As a result you may be both miscalculating the true longest-text (because you're counting with unknown words), and get some nonsense values (where no word-vector was available for a low-frequency word)</p>
<p>The most simple fix would be to stop using the original <code>x_train['Utterance']</code> as your texts for steps that will be limited to a smaller vocabulary of only those words with word-vectors. Instead, pre-filter those text to eliminate words not present in the word-vector model. For example:</p>
<pre class=""lang-py prettyprint-override""><code>cleaned_texts = [[word for word in text if word in model.wv] 
                 for text in x_train['Utterance']]
</code></pre>
<p>Then, only use <code>cleaned_texts</code> for anything driving word-vector lookups, including your calculation of the longest text.</p>
<p>Other notes:</p>
<ul>
<li><p>you probably don't need to create your own <code>embeddings_index</code> dict-like object: the <code>Word2Vec</code> model already offers a dict-like interface, returning a word-vector per lookup key, via the instance of <code>KeyedVectors</code> in its <code>.wv</code> property.</p>
</li>
<li><p>if your other libraries or hardware considerations don't require <code>float64</code> values, you might just want to stick with <code>float32</code>-width values â€“ that's what the <code>Word2Vec</code> model will train into word-vectors, they take half as much memory, and results from these kinds of models are rarely improved, and sometimes slowed, by using higher-precisions.</p>
</li>
<li><p>you could also consider creating a <code>FastText</code> model instead of plain <code>Word2Vec</code> - such a model will always return a vector, even for unknown words, synthesized from word-fragment-vectors that it learns while training.</p>
</li>
</ul>
",1,0,1003,2022-08-06 18:23:47,https://stackoverflow.com/questions/73262309/using-word2vec-for-word-embedding-of-sentences
How to convert several-hot encoding to dence vector?,"<p>Now I am doing an NLP experiment. What I am thinking of is very similar to Word2Vec. I think my way must already exist. Maybe there is out-of-the-box code. But I don't know where to find.</p>
<p>Word2Vec's input word vector is one-hot. So the size of each word vector is equal to the size of the vocab.</p>
<p>But my input word vector is a catenation of several one-hot vectors. Maybe it can be called 'several-hot'. It's much shorter than one-hot but still sparse. I still want to dencify it using Word2Vec's scheme.</p>
<p>I have used Gensim's Word2Vec model. It seems to accept only tokens as input. That means it converts tokens to one-hot vectors internally right? I would like to know if there exist any Word2Vec code that accepts custom input vectors.</p>
","nlp, word2vec, one-hot-encoding","<p>In practice, <code>Word2Vec</code> models like in Gensim never truly intantiate a one-hot representation (sparse or not). Instead, they use the lookup key (word string) to pull up a dense vector, either in-training (where that vector is being adjusted) or post-training (when that vector is being returned for use elsewhere).</p>
<p>(Abstractly, that dense vector is still a neural networks' internal weights, from the virtual &quot;one-hot&quot; input-layer to the hidden layer of smaller dimensionality. But in implementations, it's a dictionary lookup of a word key to a row in a matrix, that row being the traditional &quot;word vector&quot;.)</p>
<p>If you have clusters of N words that you want to use an existing model, which only has one vector per word, you may just want to look-up all N words individually, and either add, or average, them together. That's effectively what the neural-network is doing during training, in certain modes (like CBOW), where N words are the input to predict one target 'center' word.</p>
<p>(If instead you are training your own word2vec model, and certain tri-grams are known to be relevant entities for which you want to <em>learn</em> new unique vectors, possibly unrelated to the unigram vectors for the same words, that would require some level of preprocessing of your training data to essentially promote those trigrams to be pseudowords, and let them go through the same iterative training process as true unigrams do in the usual case.)</p>
<p><strong>ADDITIONAL THOUGHTS AFTER COMMENT BELOW:</strong></p>
<p>I'm a bit unclear about what sort of text/goals might give rise to your specific needs, but vaguely, in addition to considering an average-of-multiple-words, you may also want to look into the <code>FastText</code> variant of word2vec.</p>
<p>FastText will learn (alongside full-word vectors) additional vectors for <em>substrings</em> of words seen in training. For languages where word-morphology (word roots) give good hints to meaning, or situations with typos &amp; other corruption in data, these subword vectors can later help synthesize better-than-nothing guess-vectors for new out-of-vocabulary (&quot;OOV&quot;) words that weren't seen during training.</p>
<p>It does this by combining other subword vectors learned from the training data. So, an OOV word (whether typo or truly not-in-training-data) that shares lots of substrings with seen words winds up getting a very-similar vector.</p>
<p>To the extent you might preprocess your original fragments to combine your original multigrams into single &quot;words&quot;, according to some best guesses, the way that FastText still learns fragment-vectors might ensure you're still learning something about the subsegments.</p>
<p>Also: the <code>Phrases</code> model in Gensim implements a statistical method for sometimes combining unigram tokens into pairs, based on the idea that certain pairs, if appearing together at a (configurable) statistically-notable rate, might be better modeled as a new combined bigram &quot;word&quot;.</p>
<p>The results aren't typically aesthetically-pleasing, nor do they match a human's sense of which word-groups are really logical-phrases, no matter how much the parameters are tuned. (Always, some unwanted pairs are combined, and wanted pairs are missed.)</p>
<p>But, such combinations, warts and all, sometimes help the resulting text representations on objective evaluations of downstream tasks like classification or info-retrieval. (And, applying <code>Phrases</code> repeatedly can create de facto trigrams, quadgrams, etc.)</p>
",1,1,504,2022-08-07 16:24:18,https://stackoverflow.com/questions/73269139/how-to-convert-several-hot-encoding-to-dence-vector
Build vocab in doc2vec,"<p>I have a list of abstracts and articles approx 500 in csv each paragraph contains approx 800 to 1000 words whenever I build vocab and print with words giving none and how I can improve results?</p>
<pre><code>    lst_doc = doc.translate(str.maketrans('', '', string.punctuation))

    target_data = word_tokenize(lst_doc)

    train_data = list(read_data())

    model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)

    train_vocab = model.build_vocab(train_data)

    print(train_vocab)

   {train = model.train(train_data, total_examples=model.corpus_count, 
   epochs=model.epochs) }
</code></pre>
<p>Output:
None</p>
","machine-learning, nlp, word2vec, doc2vec","<p>A call to <code>build_vocab()</code> only builds the vocabulary <em>inside</em> the model, for further usage. That function call doesn't return anything, so your <code>train_vocab</code> variable will be Python <code>None</code>.</p>
<p>So, the behavior you're seeing is as expected, and you should say more about what your ultimate aims are, and what you'd want to see as steps towards those aims, if you're stuck.</p>
<p>If you want to see reporting of the progress of your calls to <code>build_vocab()</code> or <code>train()</code>, you can set the logging level to <code>INFO</code>. This is always a usually a good idea working to learn a new library: even if initially the copious info shown is hard to understand, by reviewing it you'll start to see the various internal steps, and internal counts/timings/etc, that hint whehter things are doing well or poorly.</p>
<p>You can also examine the state of the <code>model</code> and its various internal properties after the code has run.</p>
<p>For example, the <code>model.wv</code> property contains, after <code>build_vocab()</code>, a Gensim <code>KeyedVectors</code> structure holding all the untrained ready-for-training vectors. You can ask for its length (<code>len(model.wv</code>) or examine the discovered active list of words (<code>model.wv.index_to_key</code>).</p>
<p>Other comments:</p>
<ul>
<li><p>It's not clear your 1st two lines â€“ assigning into <code>lst_doc</code> and <code>target_data</code> â€“ affect anything further, since it's unclear what <code>read_data()</code> might be doing to fill the <code>train_corpus</code>.</p>
</li>
<li><p>Often low <code>min_count</code> values <em>worsen</em> results, by including more words that have so few usage examples that they're little more than noise during training.</p>
</li>
<li><p>only 500 documents is rather small compared to most published work showing impressive results with this algorithm, which uses tens-of-thousands of documents (if not millions). So, keep in mind that results on such a small dataset may be unrepresentative of what's possible with a larger corpus - in terms of quality, optimal parameters, etc.</p>
</li>
</ul>
",0,0,449,2022-08-11 10:01:01,https://stackoverflow.com/questions/73318795/build-vocab-in-doc2vec
Spacy models with different word2vec embeddings give same results,"<p>I am trying to improve the performance of my spacy NER model by implementing my pretrained vectors. I have created my own vectors with word2vec using different texts and I have saved them in .txt files. However I get the exact same scores and this doesn't seem right.</p>
<p>Here are the steps I have been following for one file with custom pretrained embeddings:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_uncleaned_sents_model.txt ./uncl_txt --name JSTOR_unlceaned_sents_model

nlp = spacy.load(&quot;./uncl_txt&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./uncl_txt&quot;)

!python -m spacy train /content/uncl_txt/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./uncl_txt --paths.vectors ./uncl_txt

!python -m spacy evaluate /content/uncl_txt/model-best ./Spacy/eval.spacy --output ner_with_uncleaned_sents_vectors.jsonl
</code></pre>
<p>Here are the steps for the other embeddings file:</p>
<pre><code>!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_abs_model.txt ./abs --name JSTOR_abs_model

nlp = spacy.load(&quot;./abs&quot;)
nlp.add_pipe(&quot;ner&quot;)
nlp.to_disk(&quot;./abs&quot;)

!python -m spacy train /content/abs/config.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./abs/ --paths.vectors ./abs

!python -m spacy evaluate ./abs/model-best ./Spacy/eval.spacy --output ner_with_abs_vectors.jsonl
</code></pre>
<p>Am I doing something wrong? Should I add something in the config file?</p>
","python, spacy, word2vec, named-entity-recognition, word-embedding","<p>The model created using <code>nlp.add_pipe(&quot;ner&quot;)</code> does not have embeddings enabled by default.</p>
<p>The easiest way to create a config for <code>ner</code> with embeddings enabled is to use <code>spacy init config</code> with <code>-o accuracy</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy init config -p ner -o accuracy ner.cfg
</code></pre>
<p>And then train with:</p>
<pre class=""lang-bash prettyprint-override""><code>spacy train ner.cfg --paths.train train.spacy --paths.dev dev.spacy --paths.vectors ./vectors
</code></pre>
<p>(You can also enable it using custom config settings with <code>nlp.add_pipe(&quot;ner&quot;, config=...)</code>, but this requires digging into the details about the internal default model config, which might also change depending on the version of spacy, so <code>spacy init config</code> is easier to use.)</p>
",1,1,200,2022-09-01 11:20:26,https://stackoverflow.com/questions/73568510/spacy-models-with-different-word2vec-embeddings-give-same-results
How to extract matrix together with vocab from gensim word2vec model,"<p>I've trained a word2vec model like so</p>
<pre><code>from gensim.models import Word2Vec

# create model without initializing
model = Word2Vec(min_count=20,
                 window=5,
                 sample=6e-5, 
                 negative=20,
                 workers=cores-1,
                 vector_size=300)

# build vocabulary
w2v_model.build_vocab(sentences, progress_per=10000)

# train model
model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

</code></pre>
<p>I'd like to export the model as a dataframe, but not sure how to extract the matrix and vocab together correctly, with the right index positions.</p>
<p>Something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>label</th>
<th>V1</th>
<th>V2</th>
<th>V...</th>
</tr>
</thead>
<tbody>
<tr>
<td>government</td>
<td>0.560774564</td>
<td>-0.0464625023</td>
<td>...</td>
</tr>
<tr>
<td>state</td>
<td>0.0106112240</td>
<td>0.0464625023</td>
<td>...</td>
</tr>
<tr>
<td>....</td>
<td>...</td>
<td>..</td>
<td>.</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried this:</p>
<pre><code>tmp = pd.DataFrame(model.syn1neg)
tmp.insert(0, 'label', model.wv.index_to_key)
</code></pre>
<p>which does not square up when comparing</p>
<pre><code>&gt;&gt;&gt; model.wv.get_index('government')
10
&gt;&gt;&gt; tmp.loc[[0]]
0 government 0.329972  0.160003 -0.516633  ...  0.460873 -0.170273 -1.621128  1.255289
</code></pre>
","python, extract, gensim, word2vec","<p>For anyone else looking for a solution to this with gensim 4.x.x here's what I wound up doing:</p>
<pre><code>vocab, vectors = model.wv.key_to_index, model.wv.vectors

# get label and vector index.
label_index = np.array([(voc[0], voc[1]) for voc in vocab.items()])

# init dataframe using embedding vectors and set index as node name
tmp =  pd.DataFrame(vectors[label_index[:,1].astype(int)])
tmp.index = label_index[:, 0]
tmp.to_csv(&quot;matrix_with_labels.csv&quot;)

</code></pre>
<p>Not sure this is the best or proper way but it works.</p>
",1,0,809,2022-09-19 16:08:54,https://stackoverflow.com/questions/73776321/how-to-extract-matrix-together-with-vocab-from-gensim-word2vec-model
Gensim Word2Vec exhausting iterable,"<p>I'm getting the following prompt when calling model.train() from gensim word2vec</p>
<pre><code>INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s
</code></pre>
<p>The only solutions I found on my search for an answer point to the <em>itarable</em> vs <em>iterator</em> difference, and at this point, I tried everything I could to solve this on my own, currently, my code looks like this:</p>
<pre><code>class MyCorpus:
    def __init__(self, corpus):
        self.corpus = corpus.copy()

    def __iter__(self):
        for line in self.corpus:
            x = re.sub(&quot;(&lt;br ?/?&gt;)|([,.'])|([^ A-Za-z']+)&quot;, '', line.lower())
            yield utils.simple_preprocess(x)

sentences = MyCorpus(corpus)
w2v_model = Word2Vec(
    sentences = sentences,
    vector_size = w2v_size, 
    window = w2v_window, 
    min_count = w2v_min_freq, 
    workers = -1
    )

</code></pre>
<p>The <code>corpus</code> variable is a list containing sentences, and each sentence is a string.</p>
<p>I tried the numerous &quot;tests&quot; to see if my class is indeed iterable, like:</p>
<pre><code>    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
</code></pre>
<p>For instance, all of them suggest that my class is iterable, so at this point, I think the problem must be something else.</p>
","python, nlp, gensim, word2vec","<p><code>workers=-1</code> is not a supported value for Gensim's <code>Word2Vec</code> model; it essentially means you're using no threads.</p>
<p>Instead, you must specify the actual number of worker threads you'd like to use.</p>
<p>When using an iterable corpus, the optimal number of workers is usually some number up to your number of CPU cores, but not higher than 8-12 if you've got 16+ cores, because of some hard-to-remove inefficiencies in both the Python's Global Interpreter Lock (&quot;GIL&quot;) and the Gensim master-reader-thread approach.</p>
<p>Generally, also, you'll get better throughput if your iterable isn't doing anything expensive or repetitive in its preprocessing - like any regex-based tokenization, or a tokenization that's repeated on every epoch. So best to do such preprocessing <em>once</em>, writing the resulting  simple space-delimited tokens to a new file. Then, read that file with a very-simple, no-regex, space-splitting only tokenization.</p>
<p>(If performance becomes a major concern on a large dataset, you can also look into the alternate <code>corpus_file</code> method of specifying your corpus. It expects a single file, where each text is on its own line, and tokens are already just space-delimited. But it then lets every worker thread read its own range of the file, with far less GIL/reader-thread bottlenecking, so using <code>workers</code> equal to the CPU core count is then roughly optimal for throughput.)</p>
",2,2,131,2022-10-06 02:11:50,https://stackoverflow.com/questions/73968024/gensim-word2vec-exhausting-iterable
Extracting embeddings from a keras neural network&#39;s intermediate layer,"<p>I have this neural network based on <a href=""https://www.kaggle.com/code/haibaral/spanish-word2vec"" rel=""nofollow noreferrer"">this</a></p>
<pre><code>words_input = Input(shape=(500,),dtype='int32',name='words_input')
words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1], weights=[wordEmbeddings], trainable=False)(words_input)
conv_1 = Conv1D(filters=100, kernel_size=10, strides=2, activation='relu')(words)
avgpool_1 = AveragePooling1D(pool_size=10, strides=10)(conv_1)
b_lstm = Bidirectional(LSTM(200, activation='tanh', return_sequences=False))(avgpool_1)
dense_1 = Dense(128, activation='relu')(b_lstm)
dropout = Dropout(0.1)(dense_1)
dense_2 = Dense(5, activation='softmax')(dropout)


sgd = keras.optimizers.Adam(lr=0.0001)
model = Model(inputs=words_input, outputs=dense_2)
extractor = Model(inputs=model.inputs, outputs=model.get_layer(words).output)
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.summary()
</code></pre>
<p>I added the line</p>
<pre><code>extractor = Model(inputs=model.inputs, outputs=model.get_layer(words).output)
</code></pre>
<p>cause I want to extract the word2vec embeddings of the words from the inputs like they show <a href=""https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction"" rel=""nofollow noreferrer"">here</a></p>
<p>But I'm getting this error</p>
<pre><code>TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_6732/2108362002.py in &lt;module&gt;
     11 sgd = keras.optimizers.Adam(lr=0.0001)
     12 model = Model(inputs=words_input, outputs=dense_2)
---&gt; 13 extractor = Model(inputs=model.inputs, outputs=model.get_layer(words).output)
     14 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
     15 model.summary()

~/.local/lib/python3.8/site-packages/keras/engine/training.py in get_layer(self, name, index)
   3271         if name is not None:
   3272             for layer in self.layers:
-&gt; 3273                 if layer.name == name:
   3274                     return layer
   3275             raise ValueError(

~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~/.local/lib/python3.8/site-packages/keras/layers/core/tf_op_layer.py in handle(self, op, args, kwargs)
    117             for x in tf.nest.flatten([args, kwargs])
    118         ):
--&gt; 119             return TFOpLambda(op)(*args, **kwargs)
    120         else:
    121             return self.NOT_SUPPORTED

~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer &quot;tf.__operators__.eq&quot; (type TFOpLambda).

Expected float32 passed to parameter 'y' of op 'Equal', got 'words_input' of type 'str' instead. Error: Expected float32, but got words_input of type 'str'.

Call arguments received by layer &quot;tf.__operators__.eq&quot; (type TFOpLambda):
  â€¢ self=tf.Tensor(shape=(None, 500, 300), dtype=float32)
  â€¢ other='words_input'
</code></pre>
<p>Any idea what I am doing wrong? Why is it passing the name of the first layer &quot;words_input&quot; to the parameter y? Which is what I assume it is doing?</p>
","python-3.x, keras, neural-network, word2vec, feature-extraction","<p>You are not passing the correct name to the get_layer of the model, try this code</p>
<pre><code>tf.keras.backend.clear_session()
words_input = keras.Input(shape=(500,),dtype='int32',name='words_input')
words = keras.layers.Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1], weights=[wordEmbeddings], trainable=False, name='words')(words_input)
conv_1 = keras.layers.Conv1D(filters=100, kernel_size=10, strides=2, activation='relu')(words)
avgpool_1 = keras.layers.AveragePooling1D(pool_size=10, strides=10)(conv_1)
b_lstm = keras.layers.Bidirectional(keras.layers.LSTM(200, activation='tanh', return_sequences=False))(avgpool_1)
dense_1 = keras.layers.Dense(128, activation='relu')(b_lstm)
dropout = keras.layers.Dropout(0.1)(dense_1)
dense_2 = keras.layers.Dense(5, activation='softmax')(dropout)


sgd = keras.optimizers.Adam(learning_rate=0.0001)
model = keras.Model(inputs=words_input, outputs=dense_2)
extractor = keras.Model(inputs=model.inputs, outputs=model.get_layer('words').output)
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.summary()
</code></pre>
<pre><code>extractor.summary()

Output:
</code></pre>
<p><a href=""https://i.sstatic.net/UlLrY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UlLrY.png"" alt=""enter image description here"" /></a></p>
",1,0,464,2022-11-07 10:12:48,https://stackoverflow.com/questions/74344921/extracting-embeddings-from-a-keras-neural-networks-intermediate-layer
Problem with training Word2Vec after opening csv,"<p>I'm trying to train Word2Vec model. When I try to train the model directly from the Series I get everything is fine, but when I save the DataFrame to csv and then open it, I have a problem.</p>
<pre><code>data = pd.read_csv('test.txt', sep='\r\n', names=['input'], engine=&quot;python&quot;)
data = data.dropna().drop_duplicates()
data = data['input'].iloc[:1000].apply(lemmatize)
print(data)
</code></pre>
<p>So I get this Series:</p>
<pre><code>0                       [two, households, alike, dignity]
1                          [in, fair, verona, lay, scene]
2             [from, ancient, grudge, break, new, mutiny]
3       [where, civil, blood, makes, civil, hands, unc...
4                  [from, forth, fatal, loins, two, foes]
                              ...                        
1152    [and, therefore, thou, mayst, think, havior, l...
1153              [but, trust, gentleman, i, prove, true]
1154                              [than, coying, strange]
1155                       [i, strange, i, must, confess]
1156             [but, thou, overheard, st, ere, i, ware]
Name: input, Length: 911, dtype: object
</code></pre>
<p>When I try to train Word2Vec directly from this Series, everything is fine:</p>
<pre><code>w2v_model = Word2Vec(
    min_count=10,
    window=2,
    vector_size=300,
    negative=10,
    alpha=0.03,
    min_alpha=0.0007,
    sample=6e-5,
    sg=1)

w2v_model.build_vocab(data)
w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
print(w2v_model.wv.key_to_index)
</code></pre>
<pre><code>{'i': 0, 'thou': 1, 'and': 2, 'love': 3, 'romeo': 4, 'shall': 5, 'what': 6, 'fair': 7, 'thee': 8, 'but': 9, 'to': 10, 'thy': 11, 'tis': 12, 'the': 13, 'come': 14, 'o': 15, 'night': 16, 'a': 17, 'sampson': 18, 'that': 19, 'capulet': 20, 'montague': 21, 'servingman': 22, 'good': 23, 'gregory': 24, 'one': 25, 'lady': 26, 'my': 27, 'go': 28, 'he': 29, 'nurse': 30, 'she': 31, 'you': 32, 'let': 33, 'would': 34, 'well': 35, 'say': 36, 'sir': 37, 'this': 38, 'for': 39, 'enter': 40, 'old': 41, 'take': 42, 'ay': 43, 'hath': 44, 'benvolio': 45, 'of': 46, 'tell': 47, 'art': 48, 'mercutio': 49, 'know': 50, 'by': 51, 'light': 52, 'see': 53, 'eyes': 54, 'juliet': 55, 'er': 56, 'must': 57, 'word': 58, 'name': 59, 'man': 60, 'men': 61, 'wilt': 62, 'nay': 63, 'peace': 64, 'much': 65, 'it': 66, 'yet': 67, 'house': 68, 'upon': 69, 'which': 70, 'as': 71, 'doth': 72, 'think': 73, 'in': 74}
</code></pre>
<p>But when I try to save the DataFrame to csv and then open it and train Word2Vec, I get this:</p>
<pre><code>data = pd.read_csv('test.txt', sep='\r\n', names=['input'], engine=&quot;python&quot;)
data = data.dropna().drop_duplicates()
data = data['input'].iloc[:1000].apply(lemmatize)
data = data.dropna()
data.to_csv('test.csv', index=False)
</code></pre>
<pre><code>data = pd.read_csv('test.csv')['input']
w2v_model = Word2Vec(
    min_count=10,
    window=2,
    vector_size=300,
    negative=10,
    alpha=0.03,
    min_alpha=0.0007,
    sample=6e-5,
    sg=1)
w2v_model.build_vocab(data)
w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
print(w2v_model.wv.key_to_index)
</code></pre>
<pre><code>{&quot;'&quot;: 0, ',': 1, ' ': 2, 'e': 3, 'a': 4, 't': 5, 'o': 6, 's': 7, 'r': 8, 'i': 9, 'n': 10, 'h': 11, 'l': 12, ']': 13, '[': 14, 'd': 15, 'u': 16, 'm': 17, 'g': 18, 'c': 19, 'w': 20, 'y': 21, 'p': 22, 'f': 23, 'b': 24, 'v': 25, 'k': 26, 'j': 27, 'q': 28, 'x': 29, 'z': 30}
</code></pre>
<p>Series after opening:</p>
<pre><code>0              ['two', 'households', 'alike', 'dignity']
1               ['in', 'fair', 'verona', 'lay', 'scene']
2      ['from', 'ancient', 'grudge', 'break', 'new', ...
3      ['where', 'civil', 'blood', 'makes', 'civil', ...
4      ['from', 'forth', 'fatal', 'loins', 'two', 'fo...
                             ...                        
906    ['and', 'therefore', 'thou', 'mayst', 'think',...
907    ['but', 'trust', 'gentleman', 'i', 'prove', 't...
908                        ['than', 'coying', 'strange']
909             ['i', 'strange', 'i', 'must', 'confess']
910    ['but', 'thou', 'overheard', 'st', 'ere', 'i',...
Name: input, Length: 911, dtype: object
</code></pre>
<p>What could be the problem?</p>
","python, pandas, machine-learning, gensim, word2vec","<p>First, it'd help to name the variable holding data that's come from a different place different from the original data, for clarity of reference/comparison.</p>
<p>For example, instead of loading your saved data as...</p>
<pre><code>data = pd.read_csv('test.csv')['input']
</code></pre>
<p>...give it a distinctive name instead:</p>
<pre><code>data_from_csv = pd.read_csv('test.csv')['input']
</code></pre>
<p>Then, check whether your original <code>Series</code> <code>data</code> &amp; later <code>data_from_csv</code> actually look the same, as the exact same type of objects in each item of the iterable corpus, to <code>Word2Vec</code>.</p>
<p>For example, to look at the 1st item in each iterable object, look at:</p>
<pre><code>print(next(iter(data)))
</code></pre>
<p>â€¦and also for comparisonâ€¦</p>
<pre><code>print(next(iter(data_from_csv)))
</code></pre>
<p>If these aren't identical, then your second <code>data_from_csv</code> case <em>isn't</em> showing <code>Word2Vec</code> the same type of corpus. In particular, each individual text item in the <code>Word2Vec</code> training corpus should be a Python <em>list</em> of individual string tokens.</p>
<p>If you instead pass it only strings, it will instead see each individual item in that string (single characters) as if they were string tokens, resulting the problem you've described: all the model's known words are single-characters.</p>
<p>(Are you sure you didn't see a WARNING in your logs/console-output to that effect, on the 2nd run? The <code>.build_vocab()</code> step checks if the 1st item in the corpus is a plain-string, rather than the proper list, and prints a warning when it is.)</p>
<p>Make sure to do one or the other of:</p>
<p>(1) write the CSV as space-delimited texts â€“ not Python <code>list</code> literals â€“ then re-<code>.split()</code> into a list after CSV-reading the raw strings; or</p>
<p>(2) write the CSV as Python list literals â€“ eg with brackets and string-quoting â€“ but then also, after loading those literals as raw strings, interpret them as Python objects (as if using an <code>eval()</code>) to turn them back into lists-of-tokens</p>
<p>Then, the corpus you're feeding to <code>Word2Vec</code> will be of the right format to get the intended individual words. (Option (1) above is usually the preferred approach: space-delimited strings are a simpler/safer/faster format to later re-parse/split, with no risk that and <code>eval()</code> could potentially run unintended code.)</p>
<p>(As a totally separate issue: using odd non-default values like <code>alpha=0.03, min_alpha=0.0007</code> usually indicates you're following a <em>bad</em> tutorial, that's changed these for no good reason. Such a change is unlikely to either hurt or help your results much - it's just an odd &amp; unnecessary choice hinting at random guesswork &amp; unthinking copying rather than true understanding.)</p>
",1,0,53,2022-11-10 17:16:06,https://stackoverflow.com/questions/74392926/problem-with-training-word2vec-after-opening-csv
How can color names be more accurately recognised and extracted from strings?,"<p>It may be a naÃ¯ve approach that I use to recognise and extract colour names despite slight variations or misspellings in texts, which in a first throw also works better in English than in German, but the challenges seem to be approximately the same.</p>
<ul>
<li><p>Different spellings <code>grey/gray</code> or <code>weiÃŸ/weiss</code> where the similarity from a human perspective does not seem to be huge but from word2vec <code>grey</code> and <code>green</code> are more similar.</p>
</li>
<li><p>Colours not yet known or available in <code>color_list</code>, in the following case <code>brown</code> <em>May not best example, but perhaps it can be deduced from the context in the sentence. Just as you as a human being get an idea that it could be a color.</em></p>
</li>
</ul>
<p>Both cases could presumably be covered by an extension of the vocabulary with a lot of other color names. But, not knowing about such combinations in the first place seems difficult.</p>
<p>Does anyone see another adjusting screw or even a completely different procedure that could possibly achieve even better results?</p>
<pre><code>from collections import Counter
from math import sqrt
import pandas as pd

#list of known colors
colors = ['red','green','yellow','black','gray']

#dict or dataframe of sentences that contains color/s or not
df = pd.DataFrame({
    'id':[1,2,3,4],
    'text':['grey donkey with black mane',
    'brown dog with sharp teeth',
    'red cat with yellowish / seagreen glowing eyes',
    'proud rooster with red comb']
    }
)

#creating vector of the word
def word2vec(word):
    cw = Counter(word)
    sw = set(cw)
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

#check cosin distance between word and color
def cosdis(v1, v2):
    common = v1[1].intersection(v2[1])
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


df['color_matches'] = [[(w,round(cd, 2),c) for w in s.split() for c in colors if (cd:=cosdis(word2vec(c), word2vec(w))) &gt;= 0.85] for s in df.text]
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;"">id</th>
<th style=""text-align: left;"">text</th>
<th style=""text-align: left;"">color_matches</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">grey donkey with black mane</td>
<td style=""text-align: left;"">[('black', 1.0, 'black')]</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">brown dog with sharp teeth</td>
<td style=""text-align: left;"">[]</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">red cat with yellowish / seagreen glowing eyes</td>
<td style=""text-align: left;"">[('red', 1.0, 'red'), ('yellowish', 0.85, 'yellow'), ('seagreen', 0.91, 'green')]</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">proud rooster with red comb</td>
<td style=""text-align: left;"">[('red', 1.0, 'red')]</td>
</tr>
</tbody>
</table>
</div>","python, string, search, word2vec, cosine-similarity","<p>Your best strategy is to begin by having a list of colors ahead of time. A color includes adjectives which contain the word &quot;color&quot; in their definition. I said includes because this doesn't cover all cases: The type of edge case that can kill you would be something like</p>
<p>&quot;The yellowish jacket matched his yellow shoes&quot;.</p>
<p>This has the problem that &quot;yellowish&quot; in the oxford dictionary is defined as:</p>
<p><code>adjective: having a yellow tinge; slightly yellow.</code></p>
<p>Now you can do a little bit of recursion here:</p>
<p>First colors are adjectives which contain the word &quot;color&quot; in their definition</p>
<p>Second colors are adjectives which contain a <code>&lt;first color&gt;</code> in their definition</p>
<p>Third colors are adjectives which contain a <code>&lt;second color&gt;</code> in their definition.</p>
<p>etc...</p>
<p>Mining this from a dictionary data set can let you scoop up as many colors as possible. You might need to be a little careful here though and only select adjectives whose definitions include a phrase of the form <code>adverb color_of_lower_rank</code></p>
<p>Once you have a set of colors then compound colors example &quot;blue-green&quot; become tractable. There are also ill defined colors such as &quot;royal blue&quot;. Parsing these is more difficult because you need to know if the &quot;royal&quot; refers to the blue OR to the object ex:</p>
<p>&quot;The prince's royal blue cloak was beautiful&quot;</p>
<p>The royal property here has to do with the fact its a prince's cloak.</p>
<p>vs</p>
<p>&quot;The shirt was a beautiful royal blue&quot;.</p>
<p>Here you can just imagine a shirt that colored a beautiful shade of blue that you consider &quot;royal blue&quot;.</p>
<p>So in general parsing adverb-adjective phrases can get a bit complex.</p>
",1,2,344,2022-12-11 00:21:11,https://stackoverflow.com/questions/74757531/how-can-color-names-be-more-accurately-recognised-and-extracted-from-strings
Converting word2vec output into dataframe for sklearn,"<p>I am attempting to use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim's word2vec</a> to transform a column of a pandas dataframe into a vector that I can pass to a <a href=""https://scikit-learn.org/stable/supervised_learning.html"" rel=""nofollow noreferrer""><code>sklearn</code> classifier</a> to make a prediction.</p>
<p>I understand that I need to average the vectors for each row. I have tried <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">following this guide</a> but I am stuck, as I am getting models back but I don't think I can access the underlying embeddings to find the averages.</p>
<p>Please see <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">a minimal, reproducible example</a> below:</p>
<pre><code>import pandas as pd, numpy as np
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.feature_extraction.text import CountVectorizer

temp_df = pd.DataFrame.from_dict({'ID': [1,2,3,4,5], 'ContData': [np.random.randint(1, 10 + 1)]*5, 
                                'Text': ['Lorem ipsum dolor sit amet', 'consectetur adipiscing elit.', 'Sed elementum ultricies varius.',
                                         'Nunc vel risus sed ligula ultrices maximus id qui', 'Pellentesque pellentesque sodales purus,'],
                                'Class': [1,0,1,0,1]})
temp_df['text_lists'] = [x.split(' ') for x in temp_df['Text']]

w2v_model = Word2Vec(temp_df['text_lists'].values, min_count=1)

cv = CountVectorizer()
count_model = pd.DataFrame(data=cv.fit_transform(temp_df['Text']).todense(), columns=list(cv.get_feature_names_out()))
</code></pre>
<p>Using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer""><code>sklearn's CountVectorizer</code></a>, I am able to get a simple frequency representation that I can pass to a classifier. How can I get that same format using Word2vec?</p>
<p>This toy example produces:</p>
<pre><code>adipiscing  amet    consectetur dolor   elementum   elit    id  ipsum   ligula  lorem   ... purus   qui risus   sed sit sodales ultrices    ultricies   varius  vel
0   0   1   0   1   0   0   0   1   0   1   ... 0   0   0   0   1   0   0   0   0   0
1   1   0   1   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
2   0   0   0   0   1   0   0   0   0   0   ... 0   0   0   1   0   0   0   1   1   0
3   0   0   0   0   0   0   1   0   1   0   ... 0   1   1   1   0   0   1   0   0   1
4   0   0   0   0   0   0   0   0   0   0   ... 1   0   0   0   0   1   0   0   0   0
</code></pre>
<p>While this runs without error, I cannot access the embedding that I can pass with this current format. I would like to produce the same format, with the exception of instead of there being counts, its the <code>word2vec</code> value embeddings</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>While yo might not be able to help it if your original data comes from a Pandas <code>DataFrame</code>, neither Gensim nor Scikit-Learn work with <code>DataFrame</code>-style data natively. Rather, they tend to use raw <code>numpy</code> arrays, or base Python datastructures like <code>list</code>s or iterable sequences.</p>
<p>Trying to shoehorn interim raw vectors into the Pandas style of data structure tends to add code complication &amp; wasteful overhead.</p>
<p>That's especially true if the vectors are dense vectors, where essentially all of a smaller-number of dimensions are nonzero, as in word2vec-like algorthms. But that's also true if the vectors are the kinds of sparse vectors, with a giant number of dimensions, but most dimensions 0, that come from <code>CountVectorizer</code> and various &quot;bag-of-words&quot;-style text models.</p>
<p>So first, I'd recommend <em>against</em> putting the raw outputs of <code>Word2Vec</code> or <code>CountVectorizer</code>, which are usually interim representations on the way to completing some other task, into a <code>DataFrame</code>.</p>
<p>If you want to have the final assigned-labels in the <code>DataFrame</code>, for analysis or reporting in the Pandas style, only add those final outputs in the end. But to understand the interim vector representations, and then to pass them to things like Scikit-Learn classifiers in the formats those classes expect, keep those vectors (and inspect them yourself for clarity) in the their raw <code>numpy</code> vector formats.</p>
<p>In particular, after <code>Word2Vec</code> runs (with the parameters you've shown), there'll be a 100-dimensional vector <em>per word</em>. Not per multi-word text. And the 100-dimensions have no names other than their indexes 0 to 99.</p>
<p>And unlike the dimensions of the <code>CountVectorizer</code> representation, which are counts of individual words, each dimension of the &quot;dense embedding&quot; will be some floating-point decimal value that has no clear or specific interpretation alone: it's only directions/neighborhoods in the whole space, shearing across many dimensions, that vaguely correspond with useful or human-nameable concepts.</p>
<p>If you want to turn the per-word 100-dimensional vectors into vectors for a multi-word text, there are many potential ways to do so â€“ but one simple choice is to simply average together the N word-vectors into 1 summary vector. Gensim's class holding the word-vectors inside the <code>Word2Vec</code> model, <code>KeyedVectors</code>, has a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer""><code>.get_mean_vector()</code> method</a> that can help. For example:</p>
<pre class=""lang-py prettyprint-override""><code>texts_as_wordlists = [x.split(' ') for x in temp_df['Text']]
text_vectors = [w2v_model.wv.get_mean_vector(wordlist) for wordlist in texts_as_wordlists]
</code></pre>
<p>There are many other potential ways to use word-vectors to model a longer text. For example, you might reweight the words before averagine. But a simple average is a reasonable first baseline approach. (Other algorithms related to word2vec, like the 'Paragraph Vector' algorihtm implemented by the <code>Doc2Vec</code> class, can also create a vector for a multi-word text, and such a vector is <strong>not</strong> just the average of its word-vectors.)</p>
<p>Two other notes on using <code>Word2Vec</code>:</p>
<ul>
<li>word2vec vectors only get good when trained on lots of word-usage data. Toy-sized examples trained on only hundreds, or even tens-of-thousands, of words rarely show anything useful, or anything resembling the power of this algorithm on larger data set.</li>
<li><code>min_count=1</code> is essentially always a bad idea with this algorithm. Related to the point above, the algorithm needs multiple subtly-contrasting usage examples of any word to have any chance of placing it meaningfully in the shared-coordinate space. Words with just one, or even a few, usages tend to get awful vectors not generalizable to the word's real meaning as would be evident from a larger sample of its use. And, in natural-language corpora, such few-example words are very numerous - so they wind up taking a lot of the training time, and achieving their <em>bad</em> representations actually worsens the vectors for <em>surrounding</em> words, that could be better because there are enough training examples. So, the best practice with word2vec is usually to <em>ignore</em> the rarest words â€“ train as if they weren't even there. (The class's default is <code>min_count=5</code> for good reasons, and if that results in your model missing vectors for words you think you need, get more data showing uses of those words in real contexts, rather than lowering <code>min_count</code>.)</li>
</ul>
",2,1,1131,2022-12-12 05:48:08,https://stackoverflow.com/questions/74767053/converting-word2vec-output-into-dataframe-for-sklearn
Classic king - man + woman = queen example with pretrained word-embedding and word2vec package in R,"<p>I am really desperate, I just cannot reproduce the allegedly classic example of <code>king - man + woman = queen</code> with the <code>word2vec</code> package in R and any (!) pre-trained embedding model (as a <code>bin</code> file).</p>
<p>I would be very grateful if anybody could provide working code to reproduce this example... including a link to the necessary pre-trained model which is also downloadable (many are not!).</p>
<p>Thank you very much!</p>
","r, nlp, word2vec, word-embedding","<p>An overview of using word2vec with R is available at <a href=""https://www.bnosac.be/index.php/blog/100-word2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/100-word2vec-in-r</a> which even shows an example of king - man + woman = queen.</p>
<p>Just following the instructions there and downloading the first English 300-dim embedding word2vec model from <a href=""http://vectors.nlpl.eu/repository"" rel=""nofollow noreferrer"">http://vectors.nlpl.eu/repository</a> ran on the British National Corpus which I encountered, downloaded and unzipped the model.bin on my drive and next inspecting the terms in the model (words are there apparently appended with pos tags), getting the word vectors, displaying the vectors, getting the king - man + woman and finding the closest vector to that vector gives ... queen.</p>
<pre><code>&gt; library(word2vec)
&gt; model &lt;- read.word2vec(&quot;C:/Users/jwijf/OneDrive/Bureaublad/model.bin&quot;, normalize = TRUE)
&gt; head(summary(model, type = &quot;vocabulary&quot;), n = 10)
 [1] &quot;vintage-style_ADJ&quot; &quot;Sinopoli_PROPN&quot;    &quot;Yarrell_PROPN&quot;     &quot;en-1_NUM&quot;          &quot;74Â°â€“78Â°F_X&quot;       
 [6] &quot;bursa_NOUN&quot;        &quot;uni-male_ADJ&quot;      &quot;37541_NUM&quot;         &quot;Menuetto_PROPN&quot;    &quot;Saxena_PROPN&quot;     
&gt; wv &lt;- predict(model, newdata = c(&quot;king_NOUN&quot;, &quot;man_NOUN&quot;, &quot;woman_NOUN&quot;), type = &quot;embedding&quot;)
&gt; head(t(wv), n = 10)
       king_NOUN    man_NOUN  woman_NOUN
 [1,] -0.4536242 -0.47802860 -1.03320265
 [2,]  0.7096733  1.40374041 -0.91597748
 [3,]  1.1509652  2.35536361  1.57869458
 [4,] -0.2882653 -0.59587735 -0.59021348
 [5,] -0.2110678 -1.05059254 -0.64248675
 [6,]  0.1846713 -0.05871651 -1.01818573
 [7,]  0.5493720  0.13456300  0.38765019
 [8,] -0.9401053  0.56237948  0.02383301
 [9,]  0.1140556 -0.38569298 -0.43408644
[10,]  0.3657919  0.92853492 -2.56553030
&gt; wv &lt;- wv[&quot;king_NOUN&quot;, ] - wv[&quot;man_NOUN&quot;, ] + wv[&quot;woman_NOUN&quot;, ]
&gt; predict(model, newdata = wv, type = &quot;nearest&quot;, top_n = 4)
             term similarity rank
1       king_NOUN  0.9332663    1
2      queen_NOUN  0.7813236    2
3 coronation_NOUN  0.7663506    3
4   kingship_NOUN  0.7626975    4
</code></pre>
<p>Do you prefer to build your own model based on your own text or a more larger corpus e.g. the text8 file. Follow the instructions shown at <a href=""https://www.bnosac.be/index.php/blog/100-word2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/100-word2vec-in-r</a>.
Get a text file and use R package word2vec to build the model, wait untill the model finished training and next interact with it.</p>
<pre><code>download.file(&quot;http://mattmahoney.net/dc/text8.zip&quot;, &quot;text8.zip&quot;)
unzip(&quot;text8.zip&quot;, files = &quot;text8&quot;)

&gt; library(word2vec)
&gt; set.seed(123456789)
&gt; model &lt;- word2vec(x = &quot;text8&quot;, type = &quot;cbow&quot;, dim = 100, window = 10, lr = 0.05, iter = 5, hs = FALSE, threads = 2)
&gt; wv    &lt;- predict(model, newdata = c(&quot;king&quot;, &quot;man&quot;, &quot;woman&quot;), type = &quot;embedding&quot;)
&gt; wv    &lt;- wv[&quot;king&quot;, ] - wv[&quot;man&quot;, ] + wv[&quot;woman&quot;, ]
&gt; predict(model, newdata = wv, type = &quot;nearest&quot;, top_n = 4)
      term similarity rank
1     king  0.9743692    1
2    queen  0.8295941    2
</code></pre>
",3,4,1885,2022-12-12 10:15:29,https://stackoverflow.com/questions/74769552/classic-king-man-woman-queen-example-with-pretrained-word-embedding-and-wo
&#39;Word2Vec&#39; object has no attribute &#39;infer_vector&#39;,"<p>This is the version of gensim I am using:</p>
<pre><code>Name: gensim
Version: 4.3.0
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
Author: Radim Rehurek
Author-email: me@radimr
</code></pre>
<p>I want to convert sentences into vectors using <code>Word2Vec</code>. So is there any other method than <code>infer_vector</code> that converts a sentence into a vector. [Using <code>Word2Vec</code> is a compulsion]</p>
<p><strong>Current code:</strong></p>
<pre><code>In:clean_data[:3]
Out:[['good'],
 ['nice'],
 ['its',
  'ok',
  'but',
  'still',
  'not',
  'work',
  'some',
  'times',
  'please',
  'upgrade',
  'a',
  'valuable',
  'process']]
In:from gensim.models import Word2Vec

In:model= Word2Vec(clean_data, vector_size=100, min_count=2, sg=1)

In:model.train(clean_data,total_examples=model.corpus_count,epochs=model.epochs)

In:model.infer_vector(['its','ok','but','still','not','work','some','times','please','upgrade','a','valuable','process'])

</code></pre>
<p><strong>Error:</strong></p>
<pre><code>AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_11408/92733804.py in &lt;module&gt;
----&gt; 1 model.infer_vector(['its','ok','but','still','not','work','some','times','please','upgrade','a','valuable','process'])

AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>
","python, vectorization, word2vec, word-embedding","<p><code>.infer_vector()</code> is only available on the <code>Doc2Vec</code> model, Its underlying algorithm, &quot;Paragraph Vectors&quot;, describes a standard way to learn fixed-length vectors associated with multi-word texts. The <code>Doc2Vec</code> class follows that algorithm, first during bulk training, than as an option in the frozen trained model via the <code>.infer_vector()</code> method.</p>
<p><code>Word2Vec</code>, on the other hand, is a model only for learning vectors for individual words. As an algorithm, word2vec says nothing about what a vector for a multi-word text should be.</p>
<p>Many people choose to use the average of all a multi-word text's individual words as a simple vector for the text as a whole. It's quick &amp; easy to calculate, but fairly limited in its power. Still, for some applications, especially broad topical-classifications that don't rely on any sort of grammatical/ordering understanding, such text-vectors work OK â€“ especially as a starting baseline against which to compare additional techniques.</p>
<p>Gensim's <code>KeyedVectors</code> class, which is how the <code>Word2Vec</code> model stores its learned word-vectors inside its <code>.wv</code> property, has a utility method to help calculation the mean (aka average) of multiple word-vectors. Its documentation is here:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
<p>You could use it with a list-of-words like so:</p>
<pre><code>multiword_average_vector = model.wv.get_mean_vector([
        'its','ok','but','still','not','work','some',
        'times','please','upgrade','a','valuable','process'
    ])
</code></pre>
<p>Note that it will by default ignore any words not present in the model, but if you'd prefer it to raise an error, you can use the optional <code>ignore_missing=True</code> parameter.</p>
<p>Separately: note that tiny toy-sized uses of <code>Word2Vec</code> generally <em>won't</em> show any useful properties &amp; may mislead you about how the algorithm works on the larger datasets for which it is most valuable. You generally will want to train on corpuses of at least hundreds-of-thousands (if not millions) of words, to create vocabularies with at least tens-of-thousands of known words (each with many contrasting realistic usage examples in your training data), in order to see the real behavior/value of this algorithm.</p>
",0,0,562,2023-01-05 19:42:32,https://stackoverflow.com/questions/75023586/word2vec-object-has-no-attribute-infer-vector
Word2Vec / Doc2Vec training fails: Supplied example count (0) did not equal expected count,"<p>I am learning Word2Vec and was trying to replicate a Word2Vec model from my textbook. Unlike what the textbook shows, however, my model gives a warning saying that <code>supplied example count (0) did not equal expected count (2381)</code>. Apparently, my model was not trained at all. The corpus I fed to the model was apparently an re-usable iterator (it was a list) as it passed this test:</p>
<pre><code>&gt;&gt;&gt; print(sum(1 for _ in corpus))
&gt;&gt;&gt; print(sum(1 for _ in corpus))
&gt;&gt;&gt; print(sum(1 for _ in corpus))

2381
2381
2381
</code></pre>
<p>I tried with gensim 3.6 and gensim 4.3, and both versions gave me the same warning. Here is a code snippet I used with gensim 3.6:</p>
<pre><code>word2vec_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)
word2vec_model.build_vocab(corpus)
word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)
word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)
</code></pre>
<p>This is the warning message:</p>
<pre><code>WARNING:gensim.models.base_any2vec:EPOCH - 1 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 2 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 3 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 4 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 5 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 6 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 7 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 8 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 9 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 10 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 11 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 12 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 13 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 14 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 15 : supplied example count (0) did not equal expected count (2381)
(0, 0)
</code></pre>
<p>I tried to train a different model with Doc2Vec with different corpus that is in the form of TaggedDocument, it gave me the same warning message.</p>
","gensim, word2vec, doc2vec","<p>Gensim's <code>Word2Vec</code> &amp; <code>Doc2Vec</code> (&amp; related models) don't take a <code>workers=-1</code> value. You have to set a specific count of worker threads.</p>
<p>Setting <code>-1</code> means no threads, and then the no-training situation you've observed. (There might be some better messaging of what's gone wrong in the latest Gensim or with loggin to at least the INFO level.)</p>
<p>Generally the <code>worker</code> count should never be higher than the number of CPU cores â€“ but also, when training using a corpus iterable on a machine with more than 8 cores, optimal throughput is more likely to be reached in the 6-12 thread range than anything higher, because of some contention/bottlnecking in the single-reader-thread, fan-out-to-many-workers approach Gensim uses, and the Python &quot;GIL&quot;.</p>
<p>Unfortunately, the exact best throughput value will vary based on your other parameters, especially <code>window</code> and <code>vector_size</code> and <code>negative</code>, and can only be found via trial-and-error. I often start with 6 on an 8-core machine, and 12 on any machine with 16 or more cores. (Another key tip is to make sure your corpus iterable is doing as little as possible â€“ such as reading a pre-tokenized file from disk, rather than doing any other preprocessing every iteration, in the main thread.)</p>
<p>If you can get all your text from a pretokenized text file, you can also consider the <code>corpus_file</code> mode, which lets each worker read its own unique range of the file, and thus better achieves maximum throughput by setting workers to the number of cores.</p>
<p>Separate tips:</p>
<ul>
<li><p>A <code>min_count=2</code> value so low usually hurts word2vec results: rare words don't learn good representation for themselves from a small number of usage examples, but can in aggregate dilute/interfere-with other words. Discarding more rare words, as the size of the corpus allows, often improves all surviving words enough to improve overall downstream evaluations.</p>
</li>
<li><p><code>.intersect_word2vec_format()</code> is an advanced/experimental option with no sure best practices; try to understand what it does from the source code, and the weird ways it changes the usual SGD tradeoffs, before trying it â€“ and be sure to run extra checks that it's doing what you want over more typical approaches.</p>
</li>
</ul>
",1,0,480,2023-01-12 02:38:23,https://stackoverflow.com/questions/75091018/word2vec-doc2vec-training-fails-supplied-example-count-0-did-not-equal-expe
Gensim Word2Vec produces different most_similar results through final epoch than end of training,"<p>I'm using gensim's Word2Vec for a recommendation-like task with part of my evaluation being the use of callbacks and the <code>most_similar()</code> method. However, I am noticing a huge disparity between the final few epoch callbacks and that of immediately post-training. In fact, the last epoch callback may often appear worthless, while the post training result is as best as could be desired.</p>
<p>My during-training tracking of most similar entries utilizes gensim's <code>CallbackAny2Vec</code> class. It follows the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">doc example</a> fairly directly and roughly looks like:</p>
<pre><code>class EpochTracker(CallbackAny2Vec):

  def __init__(self):
    self.epoch = 0

  def on_epoch_begin(self, model):
    print(&quot;Epoch #{} start&quot;.format(self.epoch))

  def on_epoch_end(self, model):
    
    print('Some diagnostics')
    # Multiple terms used in the below
    e = model.wv
    print(e.most_similar(positive=['some term'])[0:3]) # grab the top 3 examples for some term

    print(&quot;Epoch #{} end&quot;.format(self.epoch))
    self.epoch += 1
</code></pre>
<p>As the epochs progress, the <code>most_similar()</code> results given by the callbacks to not seem to indicate an advancement of learning and seem erratic. In fact, often the callback from the first epoch shows the best result.</p>
<p>Counterintuitively, I also have an additional process (not shown) built into the callback that does indicate gradual learning. Following the similarity step, I take the current model's vectors and evaluate them against a down-stream task. In brief, this process is a sklearn <code>GridSearchCV</code> logistic regression check against some known labels.</p>
<p>I find that often the last <code>on_epoch_end</code> callback appears to be garbage. Or perhaps some multi-threading shenanigans. However, if directly after training the model I try the similarity call again:</p>
<pre><code>e = e_model.wv # e_model was the variable assignment of the model overall
print(e.most_similar(positive=['some term'])[0:3])
</code></pre>
<p>I tend to get beautiful results that are in agreement with the downstream evaluation task also used in the callbacks, or are at least vastly different than that of the final epoch end.</p>
<p>I suspect I am missing something painfully apparent or <code>most_similar()</code> has an unusual behavior with epoch-end callbacks. Is this a known issue or is my approach flawed?</p>
","nlp, gensim, word2vec, word-embedding","<p>What version of Gensim are you using?</p>
<p>In older versions â€“ pre 4.0 if I remember correctly? â€“ the <code>most_similar()</code> operation relies on a cached pre-computed set of unit-normalized word-vectors that in some cases will be frozen when you first try a <code>most_similar()</code>.</p>
<p>Thus, incremental updates to vectors won't be reflected in results, unless something happens to flush that cache - which happens at then <em>end</em> of training. But, since mid-training checks weren't an originally-envisioned usage, more-frequent flushing doesn't happen unless forced.</p>
<p>I think if you're sure to use the latest Gensim, the problem may go away - or reviewing this older project issue may provide ideas if you're stuck on an older version: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2260"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2260</a></p>
<p>(Your other mid-training learning process â€“ if it's accessing the non-normalized per-word vectors directly, rather than via <code>most_similar()</code> â€“ is likely succeeding because it's skipping that normed-vector cache.)</p>
",1,0,122,2023-01-13 03:03:18,https://stackoverflow.com/questions/75104456/gensim-word2vec-produces-different-most-similar-results-through-final-epoch-than
Word2Vec empty word not in vocabulary,"<p>I'm currently required to work on a multilingual text classification model where I have to classify whether two sentences in two languages are semantically similar. I'm also required to use Word2Vec for word embedding.</p>
<p>I am able to generate the word embedding using Word2Vec, however, when I'm trying to convert my sentences to vectors with a method similar to <a href=""https://stackoverflow.com/a/71847207/13328625"">this</a>. I get an error saying</p>
<blockquote>
<p>KeyError: &quot;word '' not in vocabulary&quot;</p>
</blockquote>
<p>Here is my code snippet</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
nltk.download('punkt')
tokenized_text_data = [nltk.word_tokenize(sub) for sub in concatenated_text]

model = Word2Vec(sentences=tokenized_text_data, min_count=1)

# Error happens here
train_vectors = [model.wv[re.split(&quot; |;&quot;, row)] for row in concatenated_text]
</code></pre>
<p>For context, concatenated_text is the sentences from two languages concatenated together with semi-colon as the delimiter. Hence, why the function <code>re.split(&quot; |;&quot;)</code>.</p>
<p>I guess the important thing now is to understand why the error is telling me that an empty string <code>''</code> is not in the vocabulary.</p>
<p>I did not provide the sentences cause the dataset is too big and I can't seem to find which word of which sentence is producing this error.</p>
","python, word2vec","<p>It turns out it was because of the delimiter that I concatenated myself all along. There are other semicolons in the sentence dataset, and with how <code>re.split(&quot; |;&quot;)</code> works, it will split the sentence such as <code>ice cream ; bread ; milk</code> into a list of <code>['ice', 'cream', '', '', 'bread', '', '', 'milk']</code>. Hence why the error <code>word '' not in vocabulary</code>.</p>
<p>I hope this would benefit someone in the future!</p>
",1,0,242,2023-01-27 10:34:55,https://stackoverflow.com/questions/75256900/word2vec-empty-word-not-in-vocabulary
Sentiment analysis feature extraction,"<p>I am new to NLP and feature extraction, i wish to create a machine learning model that can determine the sentiment of stock related social media posts. For feature extraction of my dataset I have opted to use Word2Vec. My question is:</p>
<p>Is it important to train my word2vec model on a corpus of stock related social media posts - the datasets that are available for this are not very large. Should I just use a much larger pretrained word vector ?</p>
","word2vec, feature-extraction, nlp-question-answering","<p>The only way to to tell what will work better for your goals, within your constraints of data/resources/time, is to try alternate approaches &amp; compare the results on a repeatable quantititave evaluation.</p>
<p>Having training texts that are properly representative of your domain-of-interest can be quite important. You may need your representation of the word 'interest', for example, to represent that of stock/financial world, rather than the more general sense of the word.</p>
<p>But quantity of data is also quite important. With smaller datasets, <em>none</em> of your words may get great vectors, and words important to evaluating new posts may be missing or of very-poor quality. In some cases taking some pretrained set-of-vectors, with its larger vocabulary &amp; sharper (but slightly-mismatched to domain) word-senses may be a net help.</p>
<p>Because these pull in different directions, there's no general answer. It will depend on <em>your</em> data, goals, limits, &amp; skills. Only trying a range of alternative approaches, and comparing them, will tell you what should be done for your situation.</p>
<p>As this iterative, comparative experimental pattern repeats endlessly as your projects &amp; knowledge grow â€“ it's what the experts do! â€“ it's also important to learn, &amp; practice. There's no authority you can ask for any certain answer to many of these tradeoff questions.</p>
<p>Other observations on what you've said:</p>
<ul>
<li><p>If you don't have a large dataset of posts, and well-labeled 'ground truth' for sentiment, your results may not be good. All these techniques benefit from larger training sets.</p>
</li>
<li><p>Sentiment analysis is often approached as a classification problem (assigning texts to bins of 'positive' or 'negative' sentiment, operhaps of multiple intensities) or a regression problem (assigning texts a value on numerical scale). There are many more-simple ways to create features for such processes that do <em>not</em> involve word2vec vectors â€“ a somewhat more-advanced technique, which adds complexity. (In particular, word-vectors only give you features for individual words, not texts of many words, unless you add some other choices/steps.) If new to the sentiment-analysis domain, I would recommend <em>against</em> starting with word-vector features. Only consider adding them later, after you've achieved some initial baseline results without their extra complexity/choices. At that point, you'll also be able to tell if they're helping or not.</p>
</li>
</ul>
",0,0,77,2023-02-02 16:39:57,https://stackoverflow.com/questions/75326292/sentiment-analysis-feature-extraction
Word2Vec convert a sentence,"<p>I have trained a Word2Vec model using gensim, I have a dataset of tweets that I would like to convert to vectors. What is the best way to convert a sentence to a vector + how can this be done using a word2vec model.</p>
","gensim, word2vec","<p>Formally, the word2vec algorithm only gives you a vector per word, <strong>not</strong> per longer text (like a sentence or paragraph or tweet or article).</p>
<p>One quick &amp; easy baseline approach for turning longer texts into vectors is to just average together the vectors of each word. Recent versions of Gensim have a helper method <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer""><code>get_mean_vector()</code></a> to do this on <code>KeyedVectors</code> model objects (sets-of-word-vectors):</p>
<pre class=""lang-py prettyprint-override""><code>text_vector = kv_model.get_mean_vector(list_of_words)
</code></pre>
<p>Of course, such a simpleminded average has no way to model the effects of word-order/grammar. Words may tend to cancel each other out rather than have the compositional effects of real language, and the space of possible multiword-text meanings is much larger than the space of single-word meanings â€“ so just collapsing the text into the same coordinate system as words may lose a lot.</p>
<p>More sophisticated ways of vectorizing text rely on model far more more sophisticated than plain word2vec, such as deep/recurrent neural networks for modelling longer ranges of text.</p>
",1,0,449,2023-02-15 16:32:25,https://stackoverflow.com/questions/75462764/word2vec-convert-a-sentence
PySpark ArrayIndexOutOfBoundsException error during model fit: How can I diagnose and fix the issue?,"<p>I am working on a PySpark project where I'm trying to fit a MultilayerPerceptronClassifier model to my text data  using the fit method.I am using the Word2ve model provided bu Mllib to extract features .  However, I keep running into an ArrayIndexOutOfBoundsException error when I try to run the fit method. Specifically, the error message says :</p>
<pre><code>Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-1-d46ecefd3281&gt; in &lt;module&gt;
     41 # Use Word2Vec to generate word embeddings
     42 word2Vec_pipeline = Pipeline(stages=[tokenizer, word2Vec, labelIndexer, mlp])
---&gt; 43 word2Vec_model = mlp.fit(trainingData_word2Vec)
     44 word2Vec_predictions = word2Vec_model.transform(testData_word2Vec)
     45 

~\Documents\spark-3.3.1-bin-hadoop3\python\pyspark\ml\base.py in fit(self, dataset, params)
    203                 return self.copy(params)._fit(dataset)
    204             else:
--&gt; 205                 return self._fit(dataset)
    206         else:
    207             raise TypeError(

~\Documents\spark-3.3.1-bin-hadoop3\python\pyspark\ml\wrapper.py in _fit(self, dataset)
    381 
    382     def _fit(self, dataset: DataFrame) -&gt; JM:
--&gt; 383         java_model = self._fit_java(dataset)
    384         model = self._create_model(java_model)
    385         return self._copyValues(model)

~\Documents\spark-3.3.1-bin-hadoop3\python\pyspark\ml\wrapper.py in _fit_java(self, dataset)
    378 
    379         self._transfer_params_to_java()
--&gt; 380         return self._java_obj.fit(dataset._jdf)
    381 
    382     def _fit(self, dataset: DataFrame) -&gt; JM:

~\anaconda3\lib\site-packages\py4j\java_gateway.py in __call__(self, *args)
   1319 
   1320         answer = self.gateway_client.send_command(command)
-&gt; 1321         return_value = get_return_value(
   1322             answer, self.gateway_client, self.target_id, self.name)
   1323 

~\Documents\spark-3.3.1-bin-hadoop3\python\pyspark\sql\utils.py in deco(*a, **kw)
    188     def deco(*a: Any, **kw: Any) -&gt; Any:
    189         try:
--&gt; 190             return f(*a, **kw)
    191         except Py4JJavaError as e:
    192             converted = convert_exception(e.java_exception)

~\anaconda3\lib\site-packages\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--&gt; 326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328                     format(target_id, &quot;.&quot;, name), value)

Py4JJavaError: An error occurred while calling o181.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 39) (DESKTOP-HILGIEG executor driver): java.lang.ArrayIndexOutOfBoundsException
    at java.lang.System.arraycopy(Native Method)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
    at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)
    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
    at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)
    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)
    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)
    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)
    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:136)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
    at org.apache.spark.rdd.RDD.count(RDD.scala:1274)
    at org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)
    at org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)
    at org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)
    at org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)
    at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
    at scala.util.Try$.apply(Try.scala:213)
    at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
    at org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)
    at org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)
    at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
    at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException
    at java.lang.System.arraycopy(Native Method)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
    at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)
    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
    at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)
    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)
    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)
    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)
    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:136)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
</code></pre>
<p>I'm not sure what's causing this error, but it seems like it might be related to how I'm handling my data or how I'm using PySpark. I've checked my data and my code, but I can't seem to find any null values . Can anyone provide some insight into what might be going wrong here and how I can fix it?</p>
<p>I traid to fit the MultilayerPerceptronClassifier of Mllib using features extracted by Word2vec and i keep getting java.lang.ArrayIndexOutOfBoundsException. Although I have checked the null values and I have none.</p>
","pyspark, word2vec, apache-spark-mllib, mlp","<p>If you are encountering an ArrayIndexOutOfBoundsException error in PySpark when trying to fit a machine learning model using the fit method, one possible solution is to add a normalizer to your data before calling the model.</p>
<p>The ArrayIndexOutOfBoundsException error can occur when your data is not normalized or scaled appropriately, which can cause the model to try to access array indices that are out of bounds. i fixed  this error by adding  a normalizer in my  data using PySpark's Normalizer class.</p>
",0,0,546,2023-03-17 13:08:16,https://stackoverflow.com/questions/75767968/pyspark-arrayindexoutofboundsexception-error-during-model-fit-how-can-i-diagnos
correct syntax for sklearn&#39;s LogisticRegression where the feature data is arrays and there&#39;s more than one feature,"<p>I have a word2vec encoding for a set of queries and documents. Some queries and documents are relevant to one another and some are not. I am trying to train a logistic regression model to recognise whether a document is relevant to a query.</p>
<p>Currently I have a pandas data frame named training_data that looks like this (simplified version):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>query vector</th>
<th>doc vector</th>
<th>relevant</th>
</tr>
</thead>
<tbody>
<tr>
<td>[1,6,4]</td>
<td>[3,6,2]</td>
<td>1</td>
</tr>
<tr>
<td>[5,2,1]</td>
<td>[1,1,1]</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Where all the vectors for query and doc vectors columns are the same length.
I have a different similar set-up for my test set also.</p>
<p>My question is, what is the correct syntax for feeding this data into sklearns Logistic Regression model. I am able to do it if the data is a single number and I am able to make one column into a list and feed that in, but how can I do that for 2 sets of features?</p>
<p>for example, if I ignore the query vector column for now I can just do:</p>
<pre><code>
y = training_data[&quot;relevant&quot;]

X = list(training_data[&quot;doc vector&quot;])

clf = LogisticRegression()
clf.fit(X, y)

</code></pre>
<p>and that works, but how can I add the query vector column into this? If I try and go straight from the data frame into the model without converting to a list I get &quot;ValueError: setting an array element with a sequence.&quot;.</p>
<p>I've tried a few combinations of things including making a 2d array representing the 2 columns but that gave me the same error as above. Help!</p>
","scikit-learn, linear-regression, logistic-regression, gensim, word2vec","<p>If you have one set of <code>N</code> numeric features, then another set of <code>M</code> numeric features, you'd generally concatenate them together into one combined  &quot;flat&quot; set of <code>(N+M)</code> feature to make them appropriate for a Scikit-Learn model that expects a single flat array of features as its <code>X</code> inputs.</p>
<p>There are many ways to do that, manually or with the assistance of other library code.</p>
<p>The most simple approach from your setup would probably be a bit of idiomatic Python like:</p>
<pre class=""lang-py prettyprint-override""><code>X = [list1 + list2 for list1, list2 
                   in zip(training_data[&quot;query vector&quot;], 
                          training_data[&quot;doc vector&quot;])]
</code></pre>
<p>If you were building a more extensive Scikit-Learn pipeline, within its helper classes, the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion"" rel=""nofollow noreferrer""><code>FeatureUnion</code></a> class is a utility for combining the results of two different feature-extraction methods. A demo of its use is in the example <a href=""https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html"" rel=""nofollow noreferrer"">Concatenating multiple feature extraction methods</a>.</p>
<p>(Note that in the case of your dataframe's rows, each of the two &quot;feature extractions&quot; could just be pulling the existing arrays from their two different cells. So, any 'transformers' you might use, in that style of approach, might do little more than taking some rough not-yet-correctly structured example â€“ like a Pandas row, or a tuple of your 2 arrays â€“ and indexing to access the chosen input elements.)</p>
",0,0,494,2023-03-27 11:30:36,https://stackoverflow.com/questions/75855226/correct-syntax-for-sklearns-logisticregression-where-the-feature-data-is-arrays
Why does the loss of Gensim Word2Vec model deteriorate in every epoch?,"<p>I'm training a Word2vec model using Gensim Word2Vec on twitter data. The loss of the model deteriorates in every epoch. The first epoch gives the lowest loss. Why is it so? Code is shared below:</p>
<pre><code>loss_list = []
class callback(CallbackAny2Vec):
     
    def __init__(self):
        self.epoch = 0
          
    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_list.append(loss)
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch = self.epoch + 1

model = Word2Vec(df['tweet_text'], vector_size=300, window=10, epochs=30, hs=0, negative = 1, compute_loss=True, callbacks=[callback()])
embedding_size = model.wv.vectors.shape[1]
print(&quot;embedding size---&gt;&quot;, embedding_size)
vocab = model.wv.index_to_key
print(&quot;minimum loss {} at epoch {}&quot;.format(min(loss_list), loss_list.index(min(loss_list))))
</code></pre>
<p>The output is:</p>
<pre><code>Loss after epoch 0: 527066.375
Loss after epoch 1: 1038087.0625
Loss after epoch 2: 1510719.75
Loss after epoch 3: 1936163.875
Loss after epoch 4: 2364015.5
Loss after epoch 5: 2779299.75
Loss after epoch 6: 3183956.25
Loss after epoch 7: 3570054.5
Loss after epoch 8: 3966524.75
Loss after epoch 9: 4335994.5
Loss after epoch 10: 4706316.0
Loss after epoch 11: 5046213.0
Loss after epoch 12: 5410604.5
Loss after epoch 13: 5754962.0
Loss after epoch 14: 6080469.0
Loss after epoch 15: 6428622.5
Loss after epoch 16: 6771707.0
Loss after epoch 17: 7105302.0
Loss after epoch 18: 7400089.0
Loss after epoch 19: 7732032.0
Loss after epoch 20: 8059942.5
Loss after epoch 21: 8408386.0
Loss after epoch 22: 8685176.0
Loss after epoch 23: 8959723.0
Loss after epoch 24: 9242788.0
Loss after epoch 25: 9506676.0
Loss after epoch 26: 9752588.0
Loss after epoch 27: 10013168.0
Loss after epoch 28: 10288152.0
Loss after epoch 29: 10550915.0
embedding size---&gt; 300
minimum loss 527066.375 at epoch 0
</code></pre>
","nlp, gensim, word2vec, word-embedding","<p>Unfortunately, the code which totals-up loss for reporting in the Gensim <code>Word2Vec</code> model has a number of known bugs &amp; deviations from reasonable user expectations. You can see an overview of the problems, with links to a number of more-specific bugs, in the project's bug tracking issue [#2617][1].</p>
<p>Among other problems, the default loss reported is a running tally across all epochs â€“ you'd have to do extra comparisons, or resets-to-<code>0.0</code>, to get per-epoch loss. And, insufficient precision in the running tally variables means other inaccuracies that become noticeable in large epochs or large runs.</p>
<p>These bugs <em>don't</em> affect the effectiveness of training, only the accuracy of <code>get_latest_training_loss()</code>. reporting.</p>
<p>Manually resetting the internal tally to <code>0.0</code> at the start of each epoch, from your own callback, may improve the reporting enough for your purposes, if your jobs aren't especially large.</p>
<p>However, other things to note about your apparent setup:</p>
<ul>
<li><p>Keep in mind that a full epoch's loss can hint about whether more SGD training will be beneficial on the model's internal training goals, but is <em>not</em> a reliable indicator of the quality of the final word-vectors for other downstream uses. A model with more loss might give better vectors, a model with less loss might (through overfitting) give word-vector that are less generally-useful for typical purposes. So don't rely on loss as a guide to other meta-optimization, only the choice of <code>epochs</code>/<code>alpha</code> or potential early-stopping.</p>
</li>
<li><p><code>min_count=1</code> is essentially always a mistake with <code>Word2Vec</code>, giving you not just bad vectors for the words that only appear 1 (or a few) times, but also making the other word-vectors, for more common words, worse than they'd be with a more sensible <code>min_count</code> choice. This is especially the case if you truly have enough data to justify large <code>vector_size=300</code> vectors.</p>
</li>
<li><p>The atypical parameter <code>negative=1</code> is also almost certainly sub-optimal, and <code>window=10</code> is another deviation from defaults that will usually only make sense if you've got some repeatable quantitative quality evaluation that can assure you it's an improvement over the default.</p>
</li>
</ul>
",1,0,127,2023-04-01 18:14:07,https://stackoverflow.com/questions/75908046/why-does-the-loss-of-gensim-word2vec-model-deteriorate-in-every-epoch
"Could not build wheels for word2vec, which is required to install pyproject.toml-based projects","<p>the error appears after trying to install word2vec in Jupyter Notebook:</p>
<p><code>!pip install word2vec</code></p>
<pre><code>
  Compiling: gcc C:\Users\Mikhail\AppData\Local\Temp\pip-install-npzfmr51\word2vec_1d22636dcb264535b7c0bcd56e9c8d55\word2vec\includes\win32/word2vec.c -o Scripts\word2vec.exe -O2 -Wall -funroll-loops
  error: [WinError 2] ÐÐµ ÑƒÐ´Ð°ÐµÑ‚ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
  [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for word2vec
ERROR: Could not build wheels for word2vec, which is required to install pyproject.toml-based projects
</code></pre>
<p>I have already tried to reinstall python, update pip, install specific realises of Python. Nothing helps. I know that for many people installation of python 3.8 solves the problem, but that's not my case as well.</p>
","python, pip, word2vec","<p>Unless you are on an uncommon OS configuration, you should generally <em>not</em> need to be building your own local wheels for popular libraries like Gensim.</p>
<p>Rather, the right choice of local versions &amp; remote package-repositories should result in the installation of prebuilt, tested packages well-matched to your local system.</p>
<p>So, even if you could make this local build work (by tinkering with the right build-tools, or addressing whatever that error-reported-in-Cyrillic-is), seeing such a build proceed suggests things are suboptimal.</p>
<p>How did you &quot;reinstall Python&quot;? Did you replace the system Python (which could introduce other issues), or make a separate installation for the specific use of your project?</p>
<p>If the initial error was because you were originally using some leading-edge Python, then rolling to another better-supported Python (eg 3.8.x) should help â€“ but you'd have to make sure that was the Python being used by your Jupyter service, for your specific notebook... which might not be the case.</p>
<p>In general, also, it's best to use project-specific Python virtual environments, rather than the system Python, for more control over versions/libraries, without cross-interference from the competing needs of the system Python and other projects.</p>
<p>In particular, the <code>conda</code> tool offers fine-grained control of multiple Python versions for multiple independent virtual environments - so your system Python can remain whatever the OS prefers, while you use some other version just for Jupyter or individual notebooks. (I prefer to start with the <code>miniconda</code> install â€“ <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html"" rel=""nofollow noreferrer"">https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html</a> â€“ so as not to bring in excess unneeded libraries, and keep things very explicit as to how I build up each new virtual environment for each coding project's needs.)</p>
",0,0,1730,2023-04-05 16:51:11,https://stackoverflow.com/questions/75942103/could-not-build-wheels-for-word2vec-which-is-required-to-install-pyproject-toml
Word2Vec - same context word different label,"<p>The link is the tutorial : <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec</a>
Here the sentence is &quot;The wide road shimmered in the hot sun&quot;.
I have two questions.</p>
<ol>
<li>It uses negative sampling but here, the same context word shimmered has two different labels and the target word &quot;road&quot; is also for the negative sampling.</li>
<li>I heard that skip-gram has weight on nearby words that is the context word close to the target word will have a higher weight but it does not show in this tutorial. I wonder which one is correct for skip-gram. Thanks
<a href=""https://i.sstatic.net/vChv4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vChv4.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>I have tried to use Chat-Gpt and other skip-gram tutorial.</p>
","nlp, word2vec","<p>Regarding your (1) question:</p>
<p>Because the negative-sampling candidates are chosen randomly from the whole vocabulary, there is always a chance that the center target word (here <code>'road'</code>) or the positive-example nearby-context-word (here <code>'shimmering'</code>) will also be chosen as negative examples.</p>
<p>In a tiny toy-sized example like this, with a total vocabulary of just 8 words, that chance is pretty high!</p>
<p>By contrast, in typical vocabularies of tens- to hundreds-of-thousands of words â€“ which are required to have a model that really works rather than just illustrates the process â€“ such random picks of the target or positive word are very rare.</p>
<p>But in practice, this has negligible effect on the final results. The positive word <code>'shimmering'</code> often <em>won't</em> be in the negative examples. And even when it is, its appearance as the positive example will, on net, still nudge its vector differently than the other negative-examples.</p>
<p>And in a realistically-sized training corpus, with many examples of each word's usage in context, and with multiple training epochs, it's even less of a problem. Even if the intended positive example appears, by bad luck, <em>many</em> times among the randomly-drawn negative examples, overall, the necessary meaning-influenced distinguishing updates to the model still happen. So typical implementations don't even worry about the rare cases where the same positive word is drawn as a negative example as well: to try to check for, and  special handle, that rare case is more expensive than any theoretical slight benefit it could offer.</p>
<p>(Also note: <code>'road'</code> is a perfectly legitimate negative example for a center target word <code>'road'</code> â€“ most words <em>don't</em> appear near themselves, and for the few that do, the <code>('road', 'road')</code> skip-gram will appear many times, tugging the <code>'road'</code> vector appropriately.)</p>
<p>Regarding your (2) question:</p>
<p>Typical full-featured word2vec implementations do essentially weight nearer words more heavily, but in a somewhat non-intuitive way, for efficiency. No matter what <code>window</code> size you've specified, the <em>actual</em> window that it uses, for a given center target word, will be some random value from <code>1</code> to the <code>window</code> size you've chosen. That is, for a <code>window=3</code>, it will actually sometimes use an effective value of <code>window=1</code>, and sometimes <code>window=2</code>, and sometimes <code>window=3</code>.</p>
<p>This ensures that the immediate nearest-neighbors are <em>always</em> used for skip-gram pairs, while those that are a full <code>window</code> positions away are only used <code>1/window</code> of the time. This has roughly the same effect, over many texts &amp; epochs, as if they were each assigned a scaling factor â€“ but by performing less calculation, rather than more calculation (every position every time, with a multiplicative scaling factor).</p>
<p>The original <code>word2vec.c</code> reference code from the Google researchers who published the word2vec algorithm did this â€“ calling the effective, reduced window value <code>b</code> â€“ and the Python <code>gensim</code> library re-implementation based on that code keeps these effective-window values in a array called <code>reduced_windows</code>.</p>
<p>I'd guess that <code>tensorflow</code> word2vec implementations that are more complete also do this. As you've noted, this small illustrative tutorial â€“ which notes at its top that it is not an exact implementation of the usual approach â€“ doesn't seem to do any such weighting (via dynamic-window-shrinking or otherwise).</p>
<p>Where such window-shrinking is used, it's used for both the skip-gram and continuous-bag-of-words modes.</p>
<p><strong>Update:</strong> regarding your comment questions about ordering issues:</p>
<p>Note that data that's not real natural language text â€“ like say, lists of co-ordered products â€“ may not have the same token-frequency distributions, &amp; co-occurrence patterns, as natural-language. The initial written-up uses &amp; evaluations were mainly on real natural-language text.</p>
<p>So while word2vec has since been shown to often work well on such other token-streams, remember it's a bit of a different domain that examples that use meaningful natural-language sentences/paragraphs/etc â€“ so may benefit from trying quite different parameters.</p>
<p>In particular, early word2vec implelmentations hard-coded the negative-sampling exponent parameter at a value of <code>0.75</code>, which workd well in the early natural-langauge experiments. (In the article you link, this parameter is named <em>É‘</em>, but in Gensim it's called <code>ns_exponent</code>.)</p>
<p>There's been work (footnote 14 in the article you link) suggesting that in other domains, like recommendation, values very different from <code>0.75</code> may be better â€“ even negative values. Both <code>-0.5</code> &amp; <code>1.0</code> worked better on some of that paper's evaluations than the old <code>0.75</code> hardcoded default.</p>
<p>I say all this to highlight: if working with natural-language, you may not want to tinker with <code>ns_exponent</code> much if at all, but using word2vec further afield, trying a wider range of values makes sense.</p>
<p>As you note, for tokens where ordering <em>isn't</em> meaningful â€“ perhaps because the logs/DB returned them in some random, or sorted, order that's arbitrary with respect to the original generating user's actual actions â€“ you also might make different choices for <code>window</code> &amp; other parameters.</p>
<p>For example, Gensim's recent versions offer a <code>shrink_windows</code> parameter that, with its default <code>True</code> value, does the usual dynamic shrinking of effective windows to essentially weight nearer words higher. If in your data, you're sure the nearness of tokens is truly irrelevant, you can set <code>shrink_windows=False</code>, and the full <code>window</code> size will be used every time.</p>
<p>And if you set <code>window</code> to be some number larger tha twice your largest 'text' (in token count), you'd ensure that for each text, every word is always in every other word's context window. (If your texts are long, that could be expensive, but it may also better truly match the meaning of your token-sets.)</p>
<p>Now, with regard to the choice of CBOW or skip-gram, <em>both</em> of them are influenced by word nearness â€“ whether words appear within the <code>window</code> or not, and (in the default <code>shrink_windows=True</code>) also whether they're near-neighbors or farther-neighbors. But neither are sensitive to exact orderings in a manner that really learns anything from phrases or grammar. While one or the other might work better, or faster, under certain conditions, neither is inherently more ordering-oblivious than the other.</p>
<p>If you have time &amp; a way to score them against each other, they could both be worth trying, and evaluating against each other, but none is necessarily better on 1st principles. The article likely just picked skip-gram because it usually does OK, is probably a little simpler to explain, &amp; it wasn't interested in discussing/checking both modes.</p>
<p>Finally, if your token-sets don't have a meaningful ordering, but might have been <em>given</em> some forced order â€“ say by an alphabetical sort â€“ as part of prepping the corpus, <em>and</em> if either you leave nearer-wieghting on or use a <code>window</code> value smalelr than any of your token-sets, then your model may learn <em>spurious</em> associations. For example, two alphabetically-near product-names would appear more often in each others' context-windows, or be have disproportionate influence on each other by their <em>artifical</em>, process-created nearness. The two different ways to fight this are to either:</p>
<ol>
<li>ensure the windows are large enough, an <code>shrink_windows</code> is off, that to the algorithm all tokens in a single text are equally-near; or:</li>
<li>randomly-shuffle the tokens inside each text at least once before  training, so that even windows that don't cover the whole tesxt aren't oversampling false-associations.</li>
</ol>
<p>(This was a lengthier explanation of the the same considerion that your linked article mentions as &quot;an additional theoretical note&quot; in the &quot;Window size&quot; section.)</p>
",1,0,74,2023-04-25 02:39:13,https://stackoverflow.com/questions/76097304/word2vec-same-context-word-different-label
Convert vector to a word with Fasttext,"<p>I made a model with a dataset with Fasttext and I can convert every word to a vector.
But now I want to convert a vector to its unique word.</p>
<p>For example, I have this vector that is for (<strong>&quot;the&quot;</strong>) word and I want to convert it to its word with my Fasttext model.</p>
<pre class=""lang-py prettyprint-override""><code>[-0.0193,  0.1951, -0.1819, -0.3403,  0.3106,  0.2078, -0.0274,
  0.0346, -0.0239,  0.1478, -0.0802, -0.0720,  0.2250,  0.0943,
 -0.0288, -0.0493,  0.1270, -0.0680, -0.1122,  0.0083, -0.0060,
  0.1109, -0.0454, -0.2186,  0.0731,  0.0368,  0.1594,  0.0640,
                        ....
 -0.1320,  0.2031,  0.1679, -0.0396, -0.2523, -0.0785, -0.0268,
  0.0182, -0.0330, -0.2324, -0.1024, -0.1578,  0.2445, -0.0421,
 -0.0757,  0.0089, -0.2211,  0.0022, -0.2253, -0.0776]
</code></pre>
<p>It's a <code>(,300)</code> dim vector. What should I do?</p>
","python, word2vec, fasttext","<p>An origin vector oesn't necessarily have a unique word. Rather, FastText models can often report a ranked list of the words <em>nearest</em> your target vector. (Of course, if your vector does in fact match the word's vector exactly, it will be the 1st item in this ranked list.)</p>
<p>I can't find a supported method in Facebook's <code>fasttext</code> Python wrapper for this - if it's there, it's not prominent in their docs.</p>
<p>But in the alternative Python 'gensim' library â€“ which can load FastText models from elsewhere â€“ the method to use is <code>.most_similar()</code>:</p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastTextKeyedVectors.most_similar"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastTextKeyedVectors.most_similar</a></p>
<p>Specifically, it can take <em>either</em> words or vectors as the items in its <code>positve</code> &amp; <code>negative</code> parameters. To get the nearest-neighbors of a single vector <code>vec</code>, you could use:</p>
<pre><code>    ranked_neighbors = ft_model.most_similar(positive=[vec])
</code></pre>
<p>The list you get back includes both words and their similarity-scores, so to just get the single best match (no matter how far it is from your target), you could use:</p>
<pre><code>    top_hit = ranked_neighbors[0][0]
</code></pre>
",2,2,469,2023-04-28 12:57:36,https://stackoverflow.com/questions/76129814/convert-vector-to-a-word-with-fasttext
Fine tune a custom word2vec model with gensim 4,"<p>I am new using gensim, especially with gensim 4. To be honest, I found quite hard to understand the docs how to fine-tune a pre-trained word2vec model.
I have a binary pre-trained model saved local. I would like to fine tune this model on new data.</p>
<p>My questions are;</p>
<ul>
<li>how to create the vocab merging both vocabs?</li>
<li>is that the correct approach to fine-tune a word2vec model?</li>
</ul>
<p>So far i have created the following code:</p>
<pre><code># path to pretrained model
pretrained_path = '../models/german.model'

# new data
sentences = df.stem_token_wo_sw.to_list() #Â Pandas column containing text data

# Create new model
w2v_de = Word2Vec(
    min_count = min_count,
    vector_size = vector_size,
    window = window,
    workers = workers,
)

#Â Build vocab
w2v_de.build_vocab(sentences)

# Extract number of examples
total_examples = w2v_de.corpus_count

# Load pretrained model
model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)

# Add previous words from pretrained model
w2v_de.build_vocab([list(model.key_to_index.keys())], update=True)

# Train model
w2v_de.train(sentences, total_examples=total_examples, epochs=2)

# create array of vectors
vectors = np.asarray(w2v_de.wv.vectors)
#Â create array of labels
labels = np.asarray(w2v_de.wv.index_to_key) 

# create dataframe of vectors for each word
w_emb = pd.DataFrame(
    index = labels,
    columns = [f'X{n}' for n in range(1, vectors.shape[1] + 1)],
    data = vectors,
)
</code></pre>
<p>After training I use PCA to reduce the dimensions from 300 to two, in order to plot the word-embedding space.</p>
<pre><code># create pipeline
pipeline = Pipeline(
    steps = [
        # ('scaler', StandardScaler()),
        ('pca', PCA(n_components=2)),
    ]
)

# fit pipeline
pipeline.fit(w_emb)

# Transform vectors
vectors_transformed = pipeline.transform(w_emb)

w_emb_transformed = (
    pd.DataFrame(
        index = labels,
        columns = ['PC1', 'PC2'],
        data = vectors_transformed,
    )
)
</code></pre>
<p>The <code>labels</code> and <code>vectors</code> do only contain the new words, and not the old + new words and so does my plot and PCA values.</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>There are no official Gnesim docs on how to fine-tune a <code>Word2Vec</code> model because there's no well-established/reliable way to do fine-tuning &amp; be sure it's helping.</p>
<p>There's thus no direct support in Gensim, nor standard recipe that Gensim could recommend to non-expert users.</p>
<p>People have patched together approaches, reaching into Gensim steps/models directly, to try to accomplish fine-tuning. But the average quality of such write-ups that I've seen is <em>very poor</em>, with little evaluation of whether the steps are working, or discussion of the tradeoffs and considerations when expanding beyond the write-up's toy setup.</p>
<p>That is: they're often misleading the unaware into thinking this is a well-esablished process, with dependable results, when it's not.</p>
<p>Regarding your process, some comments:</p>
<ul>
<li>Your initial creation of a vocabulary will get all of your corpus's words into the model, with accurate frequency counts based on your corpus. (Frequencies affect how a model does negative-sampling &amp; frequent-word downsampling, and which words get ignored entirely because they appear fewer times than the configured <code>min_count</code>.)</li>
<li>You are then successfully requesting the model's vocabulary expand with the <code>.build_vocab(..., update=True)</code> call - but by providing a mere list of the words in the new corpus, every word gets an effective occurence-count of just <code>1</code>. With sensible values of <code>min_count</code> (such as the default <code>5</code>, or higher when your corpus is larg enough), none of those word from the pre-trained model will be added to the vocabulary.</li>
<li>But even if you did fix this step â€“ either setting <code>min_count</code> unwisely-low, or artificially repeating the words â€“ the <code>build_vocab()</code> step only makes slots for a word, &amp; randomly-initialized its vector to ready the word for training. You're not doing anything to copy over the actual vectors from the <code>model</code> into <code>w2v_de</code>. So all those 'borrowed' words will just be untrained noise in your actual model. And, these words don't have accurate frequency counts to participate properly in training.</li>
<li>When you train, on just your corpus, only your local-corpus words will appear in the corpus, and thus appear in the positive word-to-word training examples. But some of the imported words (if any) will occasionally be chosen as negative-examples, if you're using negative-sampling mode. (But, they won't be chosen at the typical frequencies - because of the lack of frequency info.) So you'll have a weird training run, primarily updating only your corpus's words, sometimes negative-example updating the other words (but never positive-updating them). The randomly-initialized imported words will thus be skewed further, but not in any useful way.</li>
</ul>
<p>At the end, you <em>might</em> have passable vectors for your in-corpus words. (Though: <code>epochs=2</code> is unlikely to be sufficient training unless your corpus is so vary large that every word of interest appears in many, many diverse contexts.) But the words you tried to import will have just junk vectors, having been initialized randomly, never influenced in their weights by your pretrained <code>model</code> at all, just skewed a bit by sometimes appearing as negative examples.</p>
<p>In short: a mess, with the extra non-standard steps attempting <code>fine-tuning</code> doing nothing useful. (If you've copied this pattern from an online resource faithfully â€“ that resource may have been offered by an author that didn't know what they were doing.)</p>
<p>A far surer approach, if you find your corpus is missing words, is to obtain a larger corpus. As one example, if your pretrained vectors were trained on something like Wikipedia, you can just mix your corpus with Wikipedia texts, to have a combined corpus with good usage example of all the same words. (In some cases, you might be able to find corpus-extending materials that are more appropriate for  your project/domain than generic Wikipedia reference text. Alternatively, you might choose to interleave &amp; repeat your corpus to essentially give your texts greater weight, in the combined corpus.)</p>
<p>A straightforward from-scratch training-run on this new extended corpus will co-train all words in the same model, with accurate counts matching words' appearances in the combined corpus.</p>
<p>Another approach that's sometimes used to re-use word-vectors from elsewhere is to learn a projection between your new/small model, and the pretrained/larger model, based on words that are shared between the two models. Then, use that projection to move the extra words needed â€“ in one or the other model â€“ to new positions, that render them comparable, &quot;in the same coordinate space&quot;, to the other imported vectors. There's an example of doing this in the Gensim <code>TranslationMatrix</code> class &amp; demo notebook.</p>
",3,2,910,2023-05-03 08:16:47,https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4
How to interpret word2vec train output?,"<p>Running the code snippet below report an output (3, 60). I wonder what exactly it is reporting?</p>
<p>The code is reproducible..just copy into a notebook cell and run.</p>
<pre><code>from gensim.models import Word2Vec    
sent = [['I', 'love', 'cats'], ['Dogs', 'are', 'friendly']]
w2v_model = Word2Vec(sentences=sent, vector_size=100, window=7, min_count=1,sg=1)
w2v_model.train(sent, total_examples=len(sent), epochs=10)
</code></pre>
<p>(3, 60)</p>
","python, nlp, word2vec","<p>You seem to be using the Gensim Python library for your <code>Word2Vec</code>, &amp; for internal reasons, the <code>.train()</code> method returns just the tuple <code>(trained_word_count, raw_word_count)</code>.</p>
<p>The 1st number happens to be the number of words actually trained on â€“ more on why this is only <code>3</code> for you below â€“ &amp; the 2nd the total raw words passed to training routines â€“ just your 6 words times 10 epochs. But, most users never need to consult these values.</p>
<p>A better way to monitor progress is to turn on logging to the <code>INFO</code> level - at which point you'll see many log lines of the model's steps &amp; progress. By reading these, &amp; over time, you'll start to recognize signs of a good run, or common errors (as when the totals or elapsed times don't seem consistent with what you thought you were doing).</p>
<p>You 3 lines are already a bit off, though:</p>
<ul>
<li>If you pass your training corpus into the constructor, you don't have to also call <code>.train()</code> - that's already done for you, automatically. So, you're trining twice here. (And, if you want <code>epochs=10</code> for that automatic training, you can specify it in the constructor.)</li>
<li>With a tiny toy-sized corpus, word2vec learns no useful vectors â€“ and even the reporting is more likely to reveal oddnesses that are irrelevant to more realistic-sized training runs. I recommend never training on anything less than hundreds-of-thousands of words, so that all your experiments reveal useful things about its usual operation, with minimal distractions from artifacts of unrealistic runs.</li>
<li>In particular, here, since you only have 6 words total, each has a word frequency of ~17% of all words. In any real corpus, such a word would be unrelaistically super-frequent â€“ and thus <em>all</em> your words fall victim to what is (in real corpora) a very useful optimization: probabilistic highly-frequent-word-dropping (tuned by the <code>sample</code> parameter). This is why out of 60 words (6 words times 10 epochs), only <em>3</em> word occurrences were actually trained at all. (With truly frequent words in an adequately-sized corpus, dropping 19-out-of-20 appearances leaves plenty, &amp; the overall model gets improved by spending relatively more effort on rarer words.)</li>
<li><code>min_count=1</code> is essentially always a bad idea with real word2vec workloads, as an words that only appear once can't get good vectors, but <em>do</em> waste model time/state. Ignoring such rare words completely is a standard practice. (If you <em>need</em> vectors for such words, you should find more training material sufficient to demonstrate their varied uses, in context, repeatedly.</li>
</ul>
",1,0,188,2023-05-21 16:52:10,https://stackoverflow.com/questions/76300983/how-to-interpret-word2vec-train-output
Should I Pass Word2Vec and FastText Vectors Separately or Concatenate Them for Deep Learning Model in Smart Contract Vulnerability Detection?,"<p>i have been working with word embedding latly, i have a question. So, here consider taking vulnerability detection in smart contract. So the input is smart contract files labeled with 0 or 1 stating vulnerable or not. Now i m performing 2 different word embedding such as word2vec and fasttext with same input. My question is, it is right to concatenate the vectors of word2vec and fasttext and then fed as input to deep learning model. or should i pass the vectors of the word embedding models separately to the deep learning model for feature extraction and then concatenate the extracted features for classification.</p>
<p>So far, i have performed word embedding using word2vec and passed the vectors obtained to cnn and performed fasttext and passed it to BiGRU model and concatenated the extracted features. My question is can i concatenate the vectors before performing feature extraction using deep learning ? But i m afraid that the concatenation of 2 word embedding models with same input will cause confusion ? that is the same input words will have two different vectors when concatenated. i m so confused. If anybody have insight kindly help. Thanks in advance.</p>
","python, deep-learning, word2vec, word-embedding, fasttext","<p>The generic answer for when you don't know which of multiple different ideas is better, youy try them each separately &amp; see which evaluates as better on your robust, repeatable evaluations.</p>
<p>(If you don't have a way to evaluate which is better, that's a bigger &amp; more foundational thing to address than any other choices.)</p>
<p>Given what you've said, other observations:</p>
<p>The word2vec &amp; FastText algorithms are very similar, with the most experience supporting their use being in the fuzzy sorts of menaings inherent in natural-language text. And, the main advantage of FastText is in being able to synthesize better-than-nothing guess-vectors for words that weren't seen during training, but might be similar in substrings that hint their meaning to other known words.</p>
<p>Smart contract source code (or bytecode) is sufficiently <em>unlike</em> natural language, in its narrow vocabulary, token frequencies, purposes, &amp; rigorous execution model that it's not immediately clear word-vectors could help. Word-vectors often <em>have</em> been useful with language-like token-sets that aren't natural-language, but even there, usually for discovering gradations of meaning. With smart contracts, the difference between &quot;Works as hoped&quot; and &quot;fatally vulnerable&quot; may just be a tiny matter of a single misplaced operation, or subtle missed error case. Those are the kind of highly contextual, ordering-based outcomes that word-vectors simply do not model. (At best, I think you might discover that competent coders tend to use mroe of certain kinds of operations or names than incompetent ones.)</p>
<p>Further, the main advantage of FastText â€“ synthesizing vectors for unknown but morphologically-similar tokens â€“ may be far less relevant for bytecode-analysis, where unknown tokens are rare or even impossible. (Maybe, if you're analyzing source-code including freely chosen variable names, new unknown variable names will have hints of relations to previously-trained names.)</p>
<p>So: word-vectors may the an improper or underpowered tool for doing the sort of high-stakes, subtle classification you're attempting. But, as with the topmost answer: the only way to know, &amp; test ideas of what works or not, is to try each approach, &amp; evaluate it in some fair, repeatable way. (This even includes testing different ways of training the word-vectors from a single algorithm like just word2vec itself: different modes, parameters, preprocessing, etc.)</p>
",0,-1,155,2023-05-23 09:14:30,https://stackoverflow.com/questions/76313091/should-i-pass-word2vec-and-fasttext-vectors-separately-or-concatenate-them-for-d
What is the correct definition of window size for Word2Vec (CBOW and Skip-gram)?,"<p>Which one is the correct definition of the window size in Word2Vec (CBOW and Skip-gram)?</p>
<p>After examining multiple resources on how Word2Vec (CBOW and Skip-gram) works, I discovered that there are two ways in which people define the window size:</p>
<p>The window size is represented by an integer, indicating the number of words before or after the target word (excluding the target word itself). For example, with a window size of 3: &quot;<em>The quick brown</em> <strong>fox</strong> <em>jumps over the</em> lazy dog.&quot; (Resource: <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec</a>)</p>
<p>The window size is represented by an integer, indicating the number of words before and after the target word, including the target word itself. For example, with a window size of 3: &quot;The quick <em>brown</em> <strong>fox</strong> <em>jumps</em> over the lazy dog.&quot; (Resource: <a href=""http://jalammar.github.io/illustrated-word2vec/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-word2vec/</a>)</p>
<p>Which one is correct and why?</p>
",word2vec,"<p>The original <code>word2vec.c</code> example implementation, released by the authors of the paper which introduced &amp; named the word2vec algorithm, used the 1st interpretation: <code>window</code> indicates the number of context words on each side of the the center/target word.</p>
<p>So, <code>window=3</code> means up-to-six words are considered.</p>
<p>Other writeups or implementations might have their own reasons for picking an alternate convention, but that would be in conflict with the early precedent. (Counting the center word also introduces an extra inefficiency in interpreting even-sized parameters: how would <code>4</code> and <code>5</code> mean anything different?) In my experience, most implementations follow the original <code>word2vec.c</code> approach.</p>
<p>Another relevant note: the original implementation used the configured <code>window</code> as the <em>maximum</em> count of words on each side to consider. For each individual micro-example â€“ context-words to center-word â€“ some 'effective window' in the range from 1 to configured-<code>window</code> was chosen. This serves to essentially weight nearer words more highly â€“ the direct neighbors are considered every time, the <code>window</code>-away words only <code>1 / window</code> of the time â€“ by doing <em>less</em> total calcualation comapred to an alternative approach that might, for example, scale-down more-distant words' influence by some flaoting-point multiplication.</p>
",1,1,540,2023-06-04 11:17:51,https://stackoverflow.com/questions/76400122/what-is-the-correct-definition-of-window-size-for-word2vec-cbow-and-skip-gram
How does Word2Vec (CBOW and Skip-gram) create the training data?,"<p>How does Word2Vec (CBOW and Skip-gram) create the training data? Here is my interpretation of how CBOW and Skip-gram create the training data (the crossed-out text indicates that the training example was already added to the training dataset):</p>
<p>CBOW:
<a href=""https://i.sstatic.net/THPRO.png"" rel=""nofollow noreferrer"">CBOW training data</a>
<a href=""https://i.sstatic.net/jhApg.png"" rel=""nofollow noreferrer"">CBOW neural network</a></p>
<p>Skip-gram:
<a href=""https://i.sstatic.net/KqXTP.png"" rel=""nofollow noreferrer"">Skip-gram training data</a>
<a href=""https://i.sstatic.net/yz9Ms.png"" rel=""nofollow noreferrer"">Skip-gram neural network</a></p>
<p>The above interpretation cannot be correct. If it were correct, it would mean that we end up with the exact same training dataset for both CBOW and Skip-gram. Since the neural network architecture is the same for both, this would result in the exact same word vectors, which is clearly not the case in reality.</p>
<p>Although there are several resources that describe Word2Vec (CBOW and Skip-gram) briefly, I found it challenging to locate detailed explanations that provide a clear view of what is actually fed to the neural network.</p>
",word2vec,"<p>When you need details, it's best to go to the source-code of long-used, well-debugged implementations, rather than descriptions/diagrams from sources of unclear competence.</p>
<p>I don't know where your slides come from, but the &quot;CBOW training data&quot; examples of training microexamples seems somewhat confused. For example, the line that readsâ€¦</p>
<blockquote>
<p><strong>UVT</strong> <em>is</em> <strong>great</strong>! â†’ (<strong>UVT</strong>, <em>is</em>) and (<strong>great</strong>, <em>is</em>)</p>
</blockquote>
<p>â€¦would more realistically be shown asâ€¦</p>
<blockquote>
<p><strong>UVT</strong> <em>is</em> <strong>great</strong>! â†’ (mean(<strong>UVT</strong>, <strong>great</strong>), <em>is</em>)</p>
</blockquote>
<p>That is, in CBOW, all words from the context/input window get combined â€“ usually by average, though early implementations also offered a 'sum' option â€“ before being used to predict the center/output word.</p>
<p>So, a more-accurate description of the training-examples created by CBOW of the text <code>['UVT', 'is', 'great']</code> would be:</p>
<blockquote>
<p><strong>UVT</strong> <em>is</em> great! â†’ (mean(<strong>UVT</strong>,), <em>is</em>)</p>
<p><strong>UVT</strong> <em>is</em> <strong>great</strong>! â†’ (mean(<strong>UVT</strong>, <strong>great</strong>), <em>is</em>)</p>
<p>UVT <strong>is</strong> <em>great</em>! â†’ (mean(<strong>is</strong>,), <em>great</em>)</p>
</blockquote>
<p>That is, in CBOW, one pass over a 3-word text generates exactly, and only, 3 micro-examples for the internal neural-network: one for each target word.</p>
<p>But also, a pass over the next 3-word text (<code>['UniBuc', 'is', 'great']</code>) will create exactly 3 more micro-examples for the internal neural-network, <strong>including</strong> the repeated example, not just the 2 examples on that slide.</p>
<p>That is: there is no de-duplication of micro-examples in usual word2vec implementations as the crossing-out implies in your clips: the repetition of frequent <code>context-&gt;target</code> inputs has an important weighting influence. (Though, the parameter <code>sample</code> also controls a probabilistic dropping of very-frequent words, to prevent their influence from overpowering the larger number of less-frequent words.)</p>
<p>Another reason the examples in the slides you've clipped are suboptimal is that with a mere 3-word text, and mere 1-word <code>window</code>, there's not great contrast between effects of window size, extent-of-text, etc.</p>
<p>Reading actual implementation source code, or top answers here, is likely to provide a better understanding than that source.</p>
",1,0,156,2023-06-04 12:14:52,https://stackoverflow.com/questions/76400367/how-does-word2vec-cbow-and-skip-gram-create-the-training-data
How embedding lookup works in Word2Vec,"<p>I am trying to understand Word2Vec. For word input of 5x1 (one hot encoding) and hidden layer of 3 units.I have came across following information from famous sources. The first (monochrome) image says the first column will become the embedding vector when multiplying  3x 5 to 5 x 1 matrix while in 2nd image, 4th row is taken against 1 x 5 one hot encoding. This is confusing. The embedding lookup pics row or column is not understandable. Please help.</p>
<p><a href=""https://i.sstatic.net/xiNeO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xiNeO.jpg"" alt=""enter image description here"" /></a></p>
","machine-learning, nlp, word2vec, word-embedding","<p>There's a lot of custom intros to the word2vec algorithm online that, in my opinion, are quite unhelpful in what they choose to highlight.</p>
<p>So, if you're struggling with a particular one, I'd suggest moving on to some other. And if you have to go someplace else (like StackOverflow) to explain some <em>external</em> writeup, try to provide a link to the full original source as context for understanding what that particular author has adopted as their mental-model.</p>
<p>Further, if your true interest is in understanding actual word2vec implementation, studying working source code may be a better path than more abstract write-ups, for seeing what actually happens.</p>
<p>Although abstractly, you <em>can</em> think of a word-vector lookup as being the multiplication of a one-hot vector times a (count-of-vocabulary x count-of-dimensions) matrix, I've not seen popular implementations (like Google's original <code>word2vec.c</code> release, or the Python Gensim library) do exactly that. So learning that form can mislead when later using, or reviewing the source code of, or implementing real code.</p>
<p>Instead, implementations tend to use the word-token as a key to lookup a row-number inside some sort of <code>dict</code>/hashtable. (No one-hot vector is ever created â€“ except abstractly, in the sense that a simple <code>int</code> can be thought of as representing the one-hot vector with a single one at that int index.)</p>
<p>Then, they use that row-number to access the word's vector, from a matrix that's better considered the &quot;input weights&quot; that lead to a hidden-layer, rather than any &quot;hidden layer&quot; itself. (The &quot;hidden layer&quot; activations, at least in 1:1 modes like skip gram, is then that vector itself.)</p>
<p>That is: despite the abstract description, in implementations, no multiplication occurs. An index-lookup occurs, then a simple row-access-by-that-index occurs, and then you've got a word's vector. (And yes, it's the same result as-f there'd been a one-hot multiplication - at least in a simple skip-gram mode, where the 'input' to the network is a single context word.)</p>
<p>To try to map to that diagram's top (monochrome) half: you have a 5-word vocabulary, where each word has 3 dimensions. The <em>columns</em> of that <code>w00 .. w24</code> table, with 0-based indexes {0,1,2,3,4} are the individual word-vectors. (This varies from most implementations I know, where individual word-vectors are the <em>rows</em> of the model's matrix.)</p>
<p>So, per the top (monochrome) half, you get the 3-dimensional word-vector for the 1st of 5 words by pulling the <em>column</em> that's <code>w[0][0] .. w[2][0]</code>: rows 0-2, column 0</p>
<p>In contrast, per the bottom (multicolor) half, you get the 3-dimensional word-vector for the 4th of 5 words by pulling the <em>row</em> that's <code>cell[3][0] to cell[3][2]</code>: rows 3, columns 0-2</p>
<p>The bottom diagram better fits the implementations I know. There, learned word-vectors â€“ both the in-progress vectors grabbed for adjustment during training, &amp; what's accessed at the end as final word-vectors â€“ are more often stored as the <em>rows</em> of an &quot;input weights&quot; matrix.</p>
<p>That matrix <em>can be thought of</em> as a mapping from one-hot vectors to hidden-layer activations - but really isn't a &quot;hidden layer&quot; itself. And further: in a mode like &quot;CBOW with averaging&quot;, the actual &quot;hidden layer&quot; activations are an <em>average</em> of multiple rows' values.</p>
<p>Of your 2 diagrams, the bottom better represents usual implementations â€“ though again, usually no actual &quot;multiplication by a one-hot&quot; occurs.</p>
<p>Hope this helps!</p>
",2,0,231,2023-06-14 09:39:53,https://stackoverflow.com/questions/76472136/how-embedding-lookup-works-in-word2vec
Trouble getting my gradient descent algorithm to converge (word2vec),"<p>Just for the sake of practising, learning and experimenting, I made a word2vec model from scratch, using the formulas and algorithm of <a href=""https://web.stanford.edu/%7Ejurafsky/slp3/5.pdf"" rel=""nofollow noreferrer"">Jurafsky &amp; Martin</a>. Here is the code:</p>
<pre><code>class Word2Vec:
    def __init__(self, context_window=2, num_dimensions=5, num_negative_samples=2, normalizer_func=None):
        self.num_dimensions = num_dimensions
        self.context_window = context_window
        self.num_negative_samples = num_negative_samples * context_window
        self.normalize = normalizer_func
        self.W = None
        self.V = None

    def train(self, corpus, learning_rate=0.1, decay_rate=0.1, num_epochs=100):
        &quot;&quot;&quot;
        (1) Build the vocabulary from the corpus.
        (2) Initialize the weight matrices.
        (3) Iterate over the number of epochs.
        (4) Update the weights for each training example (target word, list of context words, list of negative samples).
        (5) Add the two trained matrices.
        &quot;&quot;&quot;
        self.losses = list()  # List to store the value of the loss function at each epoch (for informative purposes).
        self.count = 0
        self.last_print = ''
        self.num_epochs = num_epochs

        self.build_vocabulary(corpus)
        self.initialize_weights()

        for epoch in range(self.num_epochs):
            self.total_loss = 0.0  # For informative purposes.
            learning_rate = self.get_learning_rate(learning_rate=learning_rate)
            for target, context, negative_samples in self.generate_training_data(corpus):
                self.update_weights(target, context, negative_samples, learning_rate)
            self.losses.append(self.total_loss)
            print(f&quot;Loss at epoch {epoch}: {self.total_loss}&quot;)

        self.W += self.V

    def build_vocabulary(self, corpus):
        &quot;&quot;&quot;
        Create a list of unique words, where the index represents the word's idx.
        &quot;&quot;&quot;
        self.vocabulary = set()
        for sentence in corpus:
            for word in sentence.split():
                self.vocabulary.add(word)
        self.vocabulary = list(self.vocabulary)

    def initialize_weights(self):
        &quot;&quot;&quot;
        Create two weight matrices, W (target words) and V (context words), with dimensions vocab_size * ndims.
        The value of each dimension is randomly chosen within the range [-0.5/ndims, 0.5/ndims].
        &quot;&quot;&quot;
        vocab_size = len(self.vocabulary)
        self.W = np.random.uniform(low=-0.5, high=0.5, size=(vocab_size, self.num_dimensions))
        self.V = np.zeros((vocab_size, self.num_dimensions))

    def generate_training_data(self, corpus):
        &quot;&quot;&quot;
        Convert each sentence in the corpus into an ordered list of words and yield the target word, context words, and negative samples.
        &quot;&quot;&quot;
        for sentence in corpus:
            self.print_progress(len(corpus) * self.num_epochs)
            sentence_list = sentence.strip().split()
            for target_idx, target in enumerate(sentence_list):
                context = self.get_context(sentence_list, target_idx)
                negative_samples = self.get_negative_samples(context)
                yield target, context, negative_samples

    def get_context(self, sentence, target_idx):
        &quot;&quot;&quot;
        Given a list of words (sentence) and the target word's index within that list, return a list of context words.
        &quot;&quot;&quot;
        context = []
        for i in range(target_idx - self.context_window, target_idx + self.context_window + 1):
            if i != target_idx and i &gt;= 0 and i &lt; len(sentence):
                context.append(sentence[i])
        return context

    def get_negative_samples(self, context):
        &quot;&quot;&quot;
        Select random words from the vocabulary that are not in the context words.
        &quot;&quot;&quot;
        negative_samples = []
        while len(negative_samples) &lt; self.num_negative_samples:
            sample = random.choice(self.vocabulary)
            if sample not in context:
                negative_samples.append(sample)
        return negative_samples

    def update_weights(self, target, context, negative_samples, learning_rate):
        &quot;&quot;&quot;
        Iterate over the context words and negative samples.
        (1) Compute the error.
        (2) Add it to a total loss attribute (for later printing).
        (3) Update the weight matrices.
        &quot;&quot;&quot;
        for context_word in context:
            error = self.compute_error(target, context_word, 1)  # 1 because it's the true label (context word)
            self.total_loss += abs(np.log(error + 1.01))
            self.update_weights_for_word(target, context_word, error, learning_rate)

        for negative_word in negative_samples:
            error = self.compute_error(target, negative_word, 0)  # 0 because it's the true label (negative sample)
            self.total_loss += abs(np.log(error + 1.01))
            self.update_weights_for_word(target, negative_word, error, learning_rate)

    def compute_error(self, target, context_word, label):
        &quot;&quot;&quot;
        For context words, the true label is 1, and the algorithm aims to approximate the dot product (to make them similar according to cosine similarity).
        For negative words, the true label is 0, and the cost is higher if the prediction approaches 1.
        &quot;&quot;&quot;
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))

        target_vector = self.W[self.vocabulary.index(target)]
        context_vector = self.V[self.vocabulary.index(context_word)]
        dot_product = np.dot(target_vector, context_vector)
        prediction = sigmoid(dot_product)
        return prediction - label

    def update_weights_for_word(self, target, word, error, learning_rate):
        &quot;&quot;&quot;
        Update the weights based on the Jurfasky and Martin equation.
        The gradient is equal to error * w/v.
        If the error is negative (context), the vectors are summed; otherwise (negative sample), they are subtracted.
        Note that in this algorithm, the update is done word by word, but it may be possible to optimize the calculations by summing the vectors of context words and negative words.
        &quot;&quot;&quot;
        self.W[self.vocabulary.index(target)] = self.W[self.vocabulary.index(target)] - learning_rate * error * self.V[self.vocabulary.index(word)]
        self.V[self.vocabulary.index(word)] = self.V[self.vocabulary.index(word)] - learning_rate * error * self.W[self.vocabulary.index(target)]

    def print_progress(self, len_corpus):
        self.count += 1
        trained = round((self.count / len_corpus) * 100)
        if trained != self.last_print:
            clear_output(wait=True)
            print(f'{trained}% processed', flush=True)
            self.last_print = trained

    def get_word_vector(self, word):
        if word in self.vocabulary:
            return self.W[self.vocabulary.index(word)].reshape(1, -1)
        else:
            print(f&quot;{word} is not in the vocabulary.&quot;)

    def compute_most_similar_words(self, word, top_k=5):
        word_vector = self.get_word_vector(self.normalize(word))

        if word_vector is None:
            return []

        similarities = np.dot(self.W, word_vector.T)
        top_indices = np.argsort(similarities, axis=0)[::-1][:top_k]
        similar_words = [self.vocabulary[idx] for idx in top_indices.flatten() if idx != self.vocabulary.index(word)]
        return similar_words

    def get_learning_rate(self, learning_rate, min_value=0.0001):
        if len(self.losses) &lt; 2:
            return learning_rate

        if self.losses[-1] &lt; self.losses[-2]:
            return learning_rate
        if self.losses[-1] &gt; self.losses[-2]:
            if learning_rate &gt; min_value:
                learning_rate = learning_rate * 0.5
                print(f'Adjusting learning rate. New value: {round(learning_rate, 2)}')
                return learning_rate

        return learning_rate
</code></pre>
<p>The problem is that I cannot get a loss value lower than 1 when training with corpus of more than 20 sentences, <strong>unless I use a very low learning rate</strong> and <strong>hundreds of epochs</strong> and wait for six o seven hours (which seems too much in comparison to fasttext or solutions like these).</p>
<p>I know that maybe there is no easy solution, but there might be some bug or something that I'm missing which could help me improve this code. In either case, I would appreciate your insights.</p>
<p>Thank you in advance</p>
","python, machine-learning, nlp, logistic-regression, word2vec","<p>Some thoughts:</p>
<p>Until you're training with a corpus of many thousands of unique words, &amp; with many varied usage contexts for every word-of-interest, &amp; with vectors of higher dimensionality (at least 10s-of-dimensions), you won't be at the scale where word2vec provides value, and at the scale where variations of parameters/data/implementation-choices teach relevant/generalizable things about about word2vec.</p>
<p>That is, 'toy-sized' tests will often show nothing interesting, or mislead about the relevant decisions &amp; tradeoffs. As the only scale-of-data you've mentioned is a mere &quot;20 sentences&quot;, and you speak <em>favorably</em> of your loss results at that scale, you may simply be worrying about things that don't matter for word2vec's actual effectiveness.</p>
<p>In particular, word2vec model overall quality is generally <em>not</em> evaluable by checking the training loss. That's only appropriate for determining the point at which more training can't help, when the improvement-in-loss stagnates, hinting at 'model convergence' â€“ as good as it can get at predicting the training data, within its size/parameter constraints.</p>
<p>(Further: typical open-source implementations like the original <code>word2vec.c</code> from Google, or the Python Gensim library, or Facebook's FastText release, don't even consult loss for that stopping decision, or even dynamic adjustment of the internal learning-rate. They instead traditionally just let users specify a fixed number of epochs, with straight-line decay of learning-rate. If training-loss is reported at all, it's only as advisory info for the operator, to guide comparisons to other runs.)</p>
<p>Whether a model's word-vectors â€“ the neural-net's 'input weights' â€“ are good at tasks is a separate matter than its internal training loss, and a model with higher final-epoch training-loss might yield better vectors for real purposes than a model with lower final-epoch training-loss.</p>
<p>In particular, a model that's far oversized for the training data may become incredibly good at predicting the idiosyncracies of the small training set â€“ very low training loss â€“ in ways that are deleterious for other out-of-training language modeling: overfitting.</p>
<p>So any concern that &quot;this run's loss doesn't seem low enough&quot; is likely misguided. Ask instead: when loss stagnated, were the word-vectors useful on robust repeatable evaluations, testing for the qualities your task(s) require?</p>
<p>From a quick glance at your code, some other comments:</p>
<ul>
<li><p>Your exploratory implementation has none of the common optimizations of usual implementations â€“ most especially use of optimized bulk vector operations. Thus this implementation will be far slower, and not an accurate hint as to what algorithms might be competitive for real uses. For example, when Gensim moved from a pure-Python implementation to one which used Cython code to use a BLAS library, its word2vec code ran 80X-120X faster. (Your &quot;6 hours&quot; thus might be &quot;3 minutes&quot; once code is similarly optimized.)</p>
</li>
<li><p>Words which appear only once, or a few times, in a natural-language corpus can be very numerous, by usual Zipfian frequency distributions, but nearly irrelevant to automated understanding of the texts. Yet assigning them all trainable-vectors, &amp; having the model learn weak representations from their idiosyncratic occurrences, consumes lots of memory &amp; training time for no benefit. So typical implementations, unlike yours, <em>ignore</em> all words under some chosen minimum-frequency threshold. Then training time, model size â€“ and the measurable quality of remaining words' vectors for usual tasks! â€“ are all much improved.</p>
</li>
<li><p>I see you're starting with a learning-rate, <code>0.1</code>, that's far higher than the usual defaults chosen by other implementations, often <code>0.025</code> or <code>0.05</code>. You're also using a momentum-based learning-rate adjustment, where open-source implementations often use simple linear decay. Your decisions <em>might</em> be improvements! But they aren't typical, and should ultimately be evaluated on whether they reach convergence faster, and the final word-vectors work better or worse than those from other methods â€“ not any intuitions about what absolute training-loss values should be.</p>
</li>
</ul>
<p>Hope this helps!</p>
",1,-1,104,2023-07-14 22:43:09,https://stackoverflow.com/questions/76691477/trouble-getting-my-gradient-descent-algorithm-to-converge-word2vec
Infer document vectors for pretrained word vectors,"<p>the following questions refers to the implementation of Word2Vec and Doc2Vec algorithms provided by the great gensim package.</p>
<p>I know similar questions have been asked, however, I feel the given answers seem not to be the best solution for my use-case.</p>
<p>I have a large corpus of 110,000 financial reports with an average length of approx. 30,000 tokens. My goal is to train word vectors first. In a next step, I want to infer doc vectors on sentence level and examine if the vector is similar to the average vector of topic words, e.g., sustainability, environmental, emissions.</p>
<p>My first idea was to use the possibility to train word vectors and doc vectors at the same time. However, if I split the reports in sentences, multiple millions of sentences (documents) result which exceeds my memory (32GB) for saving the arrays of words and documents.</p>
<p>The next idea is to treat every report as a single document for training. I read on github that documents only are trained up to a token limit of 10,000 words but I can split a document into parts of 10,000 token size and use the same tag. So far, this results in a trained model which gives me the ability to train meaningful (in the sense they learned word similarity) word vectors and use the infer_vector method later to infer doc vectors for individual sentences. However, it does not feel as a very good solution because I first train a large number of document vectors which are not used for anything.</p>
<p>My desired goal would be to train a Word2Vec model first, use the word vectors for an &quot;empty&quot; Doc2Vec model which gives me access to the infer_vector method when needed. My understanding is, that this is not easily possible because no pre-trained word vectors can be inizialized for a Doc2Vec model, right? I know this is not necessary under common use-cases related to Doc2Vec, but I hope with this question I could clarify why it would make sense in my case.</p>
<p>I also would appreciate guidance how I could use the internal C-functions which are used by the infer_vector method for training a single docvec given word vectors, unfortunately, I have no C experience at all.</p>
<p>Any help or advice would be highly appreciated, and to be honest I hope Gordon Mohr or someone else from the gensim team might read this;)</p>
<p>Best regards
Ralf</p>
","python, gensim, word2vec, doc2vec","<p>The <code>Doc2Vec</code> algoithm, called &quot;Paragraph Vector&quot; in the papers that introduced it, is not initialized from external pretrained word-vectors, nor is creating word-vectors a distinct 1st step of creating a <code>Doc2Vec</code> model from scratch that could somehow be done separately, or cached/reused across runs. So not even the internal inference routines can do anything with just some external word-vectors - they depend on model weights separate from word-vectors, learned from doc-to-word relations seen in training.</p>
<p>(I've occasionally seen some variants/improvised-changes that move a bit in the direction of taking outside word-vectors, but I've not seen evidence such variations outperform the usual approach, and they're not implemented in Gensim.)</p>
<p>In standard <code>Dov2Vec</code>, rather that taking word-vectors as an input, <em>if</em> the chosen mode of <code>Doc2Vec</code> creates typical per-word word-vectors at all, they get co-trained simultaneously with the doc-vectors.</p>
<ul>
<li><p>In the plain &quot;PV-DBOW&quot; mode â€“ <code>dm=0</code> â€“ no typical word-vectors are trained at all, <em>only</em> doc-vectors &amp; the support for inferencing new doc-vectors. This mode is thus pretty fast and often works quite well for broad topical similarity for short docs of dozens to hundreds of words â€“ because the <em>only</em> thing training is trying to do is predict in-doc words from candidate doc-vectors. In this mode, the <code>window</code> parameter is meaningless - every word in a doc affects its doc-vector.</p>
</li>
<li><p>You can optionally add to that PV-DBOW mode interleaved skip-gram word-vector training, by using the non-default <code>dbow_words=1</code> parameter. This co-training, using a shared output (center word) prediction layer, forces the word-vectors &amp; doc-vectors into a shared coordinate system â€“ so that they're directly comparable to each other. The <code>window</code> parameter then affects the skip-gram word-to-word training, just like in word2vec skip-gram training. Training takes longer, by a factor of about the <code>window</code> value â€“ and in fact the model is spending more total computation making the words predict their neighbors than the doc-vector predicting the doc words. So there's a margin at which improving the word-vectors may be 'crowding out' improvement of the doc-vectors.</p>
</li>
<li><p>The PV-DM mode â€“ the default <code>dm=1</code> parameter â€“ inherently uses a combo of condidate doc-vector &amp; neighbor words to predict each center word. That makes <code>window</code> relevant, and inherently puts the word-vectors &amp; doc-vectors into a shared comparable coordinate space, without as much overhead for larger <code>window</code> values as the interleaved skip-gram above. There <em>may</em> still be some reduction in doc-vector expressiveness to accomodate all the word-to-word influences.</p>
</li>
</ul>
<p>Which is best for a particular set of docs, subject domain, and intended downstream use is really a matter for experimentation. As you've mentioned comparing doc-vectors to word-vectors is an aim, only the latter two modes above â€“ PV-DBOW with optional skip-gram, or PV-DM â€“ would be appropriate. (But if you don't absolutely need that, &amp; have time to run more comparisons, I'd still recommend trying plain PV-DBOW for its speed &amp; strength in some needs.)</p>
<p>Let's assume your sentences are an average of 20 tokens each, so your 110k docs * 30k tokens / (20 tokens/sentence) give you 165 million sentences. Yes, holding (say) 300-dimensional doc-vectors (1200 bytes each) in-training for 165 million texts has prohibitive RAM costs: 198 GB.</p>
<p>As you've noted, you could use a model trained for only the 110k docs to then infer doc-vectors for other smaller texts, liek the sentences. You shouldn't worry about those 'wasted' 110k doc-vectors: they were necessary to create the inferencing capability, you <em>could</em> throw them away after training (&amp; inference will still work), and maybe you will have some reason to compare words or sentences or new docs or other doc-fragments to those full-doc vectors.</p>
<p>You could also consider training on chunks larger than sentences but smaller than your full docs, like paragraphs or sections, if you can segment docs that way. You could conceivably even use arbitrary Ntoken chunks, and it might work well - only way to know is to try. This sort of algorithm isn't superâ€“sensitive to small changes in tokenization/text-segmenting, as it's the bulk of the data, and broad relationships, it's modeling.</p>
<p>You can also simultaneously train doc-vectors for different levels of text, by supplying more than one 'tag' (key for looking up the doc-vector post-training) per example text. That is, if your full document with ID <code>d1</code> has 3 distinct sections <code>d1s1</code>, <code>d1s2</code>, <code>d1s3</code>, you could feed it the doc as 3 texts: the 1st section with tags <code>['d1', 'd1s1']</code>, the 2nd with tags <code>['d1', 'd1s2']</code>, the 3rd with tags <code>['d1', 'd1s3']</code>. Then all the texts contribute to the train-tuning of the <code>d1</code> doc-vector, but the subsections only affect the respective subsection-vectors. Whether that'd be appropriate depends on your goals â€“ &amp; the effect of supplying multiple tags varies a bit between modes â€“ but it may also be worth some experiments.</p>
",1,0,154,2023-07-26 16:37:55,https://stackoverflow.com/questions/76773386/infer-document-vectors-for-pretrained-word-vectors
shape of my dataframe(#rows) and that of final embeddings array doesn&#39;t match,"<p>I generated the word embeddings for my corpus(2-D List) then tried to generate the Average Word2Vec embeddings for each of the individual word list(that is for each comment which have been converted into a list though split() method) inside my corpus but the final length of my average word2vec embeddings numpy array and that of the #rows doesn't match i.e. 159571, which is the number of comments.</p>
<p>here's the code for generating the 'final_embeddings' array:</p>
<pre class=""lang-py prettyprint-override""><code>#Building vocabulary
vocabulary = set(model.wv.index_to_key)

final_embeddings = []
for i in flatten_corpus:
    avg_embeddings = None
    for j in i:
      
         if j in vocabulary:

            if avg_embeddings is None:
                avg_embeddings = model.wv[j]
            else:
                avg_embeddings = avg_embeddings + model.wv[j]
    if avg_embeddings is not None:
        avg_embeddings = avg_embeddings / len(avg_embeddings)
        final_embeddings.append(avg_embeddings)

</code></pre>
<ul>
<li>length of flatten_corpus: 159571</li>
<li>length of the above array: 159487 (doesn't match to above number)</li>
</ul>
<p>what am I doing wrong?</p>
","python, machine-learning, nlp, word2vec","<p>You are only appending into your <code>final_embeddings</code> in a code branch that's only sometimes reached: if there's at least one known word in the text.</p>
<p>If any element of <code>flatten_corpus</code> only includes words that aren't in the model, it will simply proceed to the next item in <code>flatten_corpus</code>.</p>
<p>And then, you'll not only be missing those 84 items, but the average vectors in <code>final_embeddings</code> will no longer be aligned at the same slot indexes as their matching texts.</p>
<p>A quick and dirty fix would be to initialize your <code>avg_embeddings</code> to some value that stands-in, as the default, even if none of the words are known. For example:</p>
<pre><code>    avg_embeddings = np.zeros(model.vector_size, dtype=np.float32)
</code></pre>
<p>Of course, having 84 of your per-text summary average vector be zero-vectors may cause other problems down the way, so you may want to think more about what, if anything you should be doing for such texts. Maybe, without word-vectors to model them, they should just be ignored.</p>
<p>Other notes on making code that is easier to debug:</p>
<ul>
<li><p>using descriptive temporary variable names like 'text' &amp; 'word' instead of 'i' &amp; 'j' makes code clearer</p>
</li>
<li><p>you can already test whether a word is inside a set of word-vectors (<code>model.wv</code>, of Gensim class type <code>KeyedVectors</code>) with idiomatic Python membership-checking, so there's no need to create your <code>vocabulary</code> set â€“ instead just check with <code>if word in model.wv:</code>.</p>
</li>
<li><p>the <code>KeyedVectors</code> object has a utility method for getting the average of  the word-vectors of a list-of-words, with other options that could prove helpful: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer""><code>.get_mean_vector()</code></a> â€“ and if you combine that with a Python <em>list comprehension</em>, your code can be replaced by a 1-liner:</p>
</li>
</ul>
<pre><code>final_embeddings = [model.wv.get_mean_vector(text) for text in flatten_corpus]
</code></pre>
",0,0,62,2023-08-16 16:33:44,https://stackoverflow.com/questions/76915495/shape-of-my-dataframerows-and-that-of-final-embeddings-array-doesnt-match
Negative values in data passed to MultinomialNB when vectorize using Word2Vec,"<p>I am currently working on a project where I'm attempting to use Word2Vec in combination with Multinomial Naive Bayes (MultinomialNB) for accuracy calculations.</p>
<pre><code>import pandas as pd
import numpy as np, sys
from sklearn.model_selection import train_test_split
from gensim.models import Word2Vec
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score
from datasets import load_dataset

df = load_dataset('celsowm/bbc_news_ptbr', split='train')
X = df['texto']
y = df['categoria']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

sentences = [sentence.split() for sentence in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)

def vectorize(sentence):
    words = sentence.split()
    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    if len(words_vecs) == 0:
        return np.zeros(100)
    words_vecs = np.array(words_vecs)
    return words_vecs.mean(axis=0)

X_train = np.array([vectorize(sentence) for sentence in X_train])
X_test = np.array([vectorize(sentence) for sentence in X_test])
clf = MultinomialNB()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred, pos_label='positive'))
</code></pre>
<p>However, I've encountered an error:</p>
<pre><code>ValueError(&quot;Negative values in data passed to %s&quot; % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)
</code></pre>
<p>I would appreciate any insights into resolving this issue.</p>
","python, scikit-learn, gensim, word2vec, naivebayes","<h2>The Error</h2>
<p>Each word2vec embedding for a word is a vector whose elements can take any real number value. This means that even after you take the mean of all vectors, there might be some negative values in the final vector.</p>
<p>This is not a problem. However, since you are using Multinomial Naive Bayes (MNB), it is causing problems.</p>
<p>Why? MNB assumes that the data follows a multinomial distribution, a generalization of the binomial distribution. It is based entirely on the idea of counts of successes (1s) and failures (0s). Thus, you can imagine why scikit-learn complains about MNB getting negative values.</p>
<h2>The Solution</h2>
<p>If you want to keep the model as MNB, you will have to do away with the negative values. Some ideas (as per <a href=""https://github.com/ClimbsRocks/machineJS/issues/176"" rel=""nofollow noreferrer"">this</a> link):</p>
<ul>
<li>Remove/filter all negative values from the final vector.</li>
<li>Normalize the vector to [0,1] range using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"" rel=""nofollow noreferrer""><code>MinMaxScaler</code></a>.</li>
</ul>
<p>You can also change the vectorization method from Word2Vec to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a>, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a>. Tfidf will work even though it gives fractional values in the final vector. MNB is not designed to work with fractions, only integers, but it works in practice!</p>
<p>If you are okay with using another model, you can try some model options below:</p>
<ul>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"" rel=""nofollow noreferrer"">Gaussian Naive Bayes</a></li>
<li><a href=""https://scikit-learn.org/stable/modules/svm.html"" rel=""nofollow noreferrer"">Support Vector Machines</a></li>
<li><a href=""https://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow noreferrer"">Decision Trees</a></li>
</ul>
<h2>Code</h2>
<p>Example using MinMaxScaler:</p>
<p>Just switch the line <code>clf = MultinomialNB()</code> to the following.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline

clf = Pipeline([
    ('scaler', MinMaxScaler()),
    ('clf', MultinomialNB()),
])
</code></pre>
<h2>Transformers</h2>
<p>Depending upon your task, you might also want to check out transformers. They have their own vectorization method, generating dense semantic embeddings instead of working at a purely syntactic level as word2vec does. These models are much bigger, and computationally expensive, but will produce much better results if machine learning models fail to satisfy with accuracy.</p>
<h2>Further Readings</h2>
<ul>
<li>A comprehensive scikitlearn guide: <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Binomial_distribution</a></li>
<li><a href=""https://stats.stackexchange.com/questions/169400/naive-bayes-questions-continus-data-negative-data-and-multinomialnb-in-scikit"">https://stats.stackexchange.com/questions/169400/naive-bayes-questions-continus-data-negative-data-and-multinomialnb-in-scikit</a></li>
<li>Transformers: <a href=""https://huggingface.co/learn/nlp-course/chapter1/4"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter1/4</a></li>
</ul>
<p>Feel free to ask any questions!</p>
",2,1,625,2023-11-09 22:41:04,https://stackoverflow.com/questions/77456745/negative-values-in-data-passed-to-multinomialnb-when-vectorize-using-word2vec
Is it normal for all similarities to be positive in a gensim word2vec model?,"<p>Implementing a standard gensim word2vec model (continuous bag of words) on a series of Chinese characters, and for (comparison between chinese homophones and words of similar frequency) our cosine similarities are positive and weirdly high (&gt;0.3), any clue why this is the case? We have vector size set to 300 and min word set to 1, other than that no modifications made to the gensim's standard implementation of word2vec.</p>
<p>Also if anyone has any resources to look into for how to learn how these embeddings are actually generated that would be really helpful. Thank you very much in advance!</p>
","python, trigonometry, gensim, word2vec, similarity","<p>I don't think it's typical for <em>all</em> pairwise comparisons within a model to be positive, but large sets of tokens-of-interest for one specific investigation might all have positive similarities.</p>
<p><code>0.3</code> isn't necessarily a particularly high similarity, but also note such similarity values don't have any absolute interpretation. Rather, they only have meaning compared to the other similarities in the same model.</p>
<p>Depending on other chosen parameters, especially <code>vector_size</code> dimensionality, the <em>very best</em> nearest-neighbors to a token might be of nearly any positive similirity. That token B is most-similar to A, or that B is more-similar than other tokens, is more meaningful &amp; reliable than whether <code>cossim(a_vector, b_vector)</code> is ~<code>0.3</code> or ~<code>0.9</code>.</p>
<p>Separately, <code>min_count=1</code> is almost always a bad idea for these models. A token that only appears with one single context won't get a good vector from this sort of algorithm, but typical natural-lanaguae corpora may have many such one-off or few-off rare words â€“ which altogether soak up a lot of training time to get nothing of value, while also seeving as 'noise' to worsen the vectors of other tokens which <em>do</em> have adequately numerous/varied contexts. Discarding rarer words, per the default <code>min_count=5</code> (or even higer values as soon as your corpus is large enough) is a best practice which results in far more improvement to the remaining tokens' vectors than loss from ignoring rarer words.</p>
<p>And, <code>vector_size=300</code> would only be appropriate if you have a large-enough corpus to justify it. How large is your corpus, in terms of (1) total tokens; (2) unique words (overall &amp; after applying a reasonable <code>min_count</code>); (3) average text length (in token-count). Most of these stats about your corpus will appear in logging output, as Genim <code>Word2Vec</code> works, if you enable Python logging to the <code>INFO</code> level.</p>
<p>If you continue to have problems, you should either expand (edit) this question, or ask a new question, with more details, including:</p>
<ul>
<li>corpus size, per above</li>
<li>exact <code>Word2Vec</code> parameters used</li>
<li>details on how your corpus is preprocessed/tokenized</li>
<li>example code &amp; output showing what you see as the problem - such as sets of token-to-token similarities that seem 'wrong' to you, or the results of some code that demonstrates some unlikely result like &quot;all&quot; similarities being in some tight positive range</li>
</ul>
",0,0,69,2023-11-30 18:23:58,https://stackoverflow.com/questions/77580859/is-it-normal-for-all-similarities-to-be-positive-in-a-gensim-word2vec-model
topic coherence (w2v) and its trend?,"<p>I tried to use <code>w2v</code> topic coherence score to evaluate the topic model based on <code>NMF</code>.
Below is the <code>w2v</code> coherences I have calculated.</p>
<p>And I want to know, is <code>w2v</code> coherence higher better?
Also, why the coherence scores get lower with more topics?</p>
<p><code>w2v</code> scores with different topic numbers:</p>
<p><img src=""https://i.sstatic.net/TpKpD.png"" alt=""w2v scores with different topic numbers"" /></p>
","python, gensim, word2vec","<blockquote>
<p>Is <code>w2v</code> coherence better if it's higher?</p>
</blockquote>
<p>Yes, generally a higher w2v coherence score means better topic quality. It basically says, that the words that make up each topic are more coherent.</p>
<blockquote>
<p>Why does the coherence scores get lower with more topics?</p>
</blockquote>
<p>Usually as you increase the number of topics, the model tries to divide the words into more and more groups. That then leads to less meaningful or more fragmented topics overall. That's a common trade off in topic modeling, which is why you might wanna look into other metrics like <a href=""https://en.wikipedia.org/wiki/Perplexity"" rel=""nofollow noreferrer"">perplexity</a> <em>in addition</em> to your w2v (or others, depending on your dataset)</p>
",1,0,62,2023-12-08 09:13:19,https://stackoverflow.com/questions/77625499/topic-coherence-w2v-and-its-trend
Python word2vec updates,"<p>I am trying to convert this old snippet of code to be in line with the updated version of gensim. I was able to convert the model.wv.vocab to model.wv.key_to_index but am having issues with the model[model.wv.vocab] and how to convert that.</p>
<p>Here is that the code looks like:</p>
<pre><code>model = Word2Vec(corpus, min_count = 1, vector_size = 5 )

#pass the embeddings to PCA
X = model[model.wv.vocab]

pca = PCA(n_components=2)
result = pca.fit_transform(X)

#create df from the pca results
pca_df = pd.DataFrame(result, columns = ['x','y'])
</code></pre>
<p>I have tried this:</p>
<pre><code>#pass the embeddings to PCA
X = model.wv.key_to_index
pca = PCA(n_components=2)
result = pca.fit_transform(X)

#create df from the pca results
pca_df = pd.DataFrame(result, columns = ['x','y'])
</code></pre>
<p>and keep getting errors. Here is what model.wv.key_to_index looks like:</p>
<pre><code>{'the': 0,
 'in': 1,
 'of': 2,
 'on': 3,
 '': 4,
 'and': 5,
 'a': 6,
 'to': 7,
 'were': 8,
 'forces': 9,
 'by': 10,
 'was': 11,
 'at': 12,
 'against': 13,
 'for': 14,
 'protest': 15,
 'with': 16,
 'an': 17,
 'as': 18,
 'police': 19,
 'killed': 20,
 'district': 21,
 'city': 22,
 'people': 23,
 'al': 24,
 'came': 996,
 'donbass': 997,
 'resulting': 998,
 'financial': 999}
</code></pre>
","python, pca, gensim, word2vec","<p>Your question doesn't show which errors you keep getting, which would usually help identify what's going wrong.</p>
<p>However, it looks like your original (older circa Gensim-3.x.x) code's lineâ€¦</p>
<pre><code>X = model[model.wv.vocab]
</code></pre>
<p>â€¦intends to assemble (per scikit-learn <code>PCA</code> api) a <em>array-like of shape (n_samples, n_features)</em>, by looking up every key in <code>model.wv.vocab</code> in <code>model</code> to assemble a new array, where each row is one of the <code>vocab</code> word-vectors.</p>
<p>The most direct replacement for that line would thus be to just use the model's existing internal array of word-vectors:</p>
<pre><code>X = model.wv.vectors
</code></pre>
<p>That is: for this use, you don't need to look up words individually, or create a new array of results. The existing in-model array is already exactly what you need.</p>
<p>Of course if you instead want to use subsets of words, you might want to look up mixtures of word individually. Still, for the specific case of using, say, the first 10 words (as in your sibling answer), you could also just use a numpy array 'view' on the existing array, accessed via Python slice notation:</p>
<pre><code>first_ten_word_vectors = model.wv.vectors[:10]
</code></pre>
<p>(As these models typically front-load the storage ordering of words with the most-frequent words, and the &quot;long-tail&quot; of less-frequent words have worse vectors and less utility, working with just the &quot;top-N&quot; of words, while ignoring other words, often improves overall resource usage <em>and</em> evaluated performance. More isn't alway better, when that 'more' is in the less-informative 'noise' of your training data or later texts.)</p>
<p>Two other unrelated notes on your example code â€“ which I recognize may just be a toy demop of some larger exercise, but still useful to remember:</p>
<ul>
<li><p><code>min_count=1</code> is almost always a bad idea with <code>Word2Vec</code> &amp; similar algorithms. Words in your corpus with just 1, or a few, usage examples won't get good vectors â€“ a single usage context won't be representative of the broad/bushy aspects of the word's real meaning â€“ but <em>will</em>, in aggregate, take up a lot of the model's RAM &amp; training-time, and their <em>bad</em> representations will dilute the <em>good</em> representations for more-frequent words. So higher <code>min_count</code> values that actively discard rarer words often improve all three of {training_time, memory_usage, vector_quality} â€“ and if you don't have enough training texts to use, or increase, the class default <code>min_count=5</code>, you may not have a task at which word2vec will work well until you get more data.</p>
</li>
<li><p>Similarly, the usual strengths of the word2vec algorithm are only shown with <em>high-dimensional</em> word-vectors â€“ typically, at least 50-100 dimensions (and often 300+), with a commensurately-sufficient amount of training data. So, except for testing that code <em>runs</em>, or showing syntax, using any value as low as <code>vector_size=5</code> will often mislead about how word2vec behaves at usual scales. It's not even faintly-illustrative of the useful relationships that appear with more-dimensions &amp; plentiful data.</p>
</li>
</ul>
",0,0,89,2023-12-19 17:18:03,https://stackoverflow.com/questions/77686928/python-word2vec-updates
Load word2vec model that is in .tar format,"<p>I want to load a previously trained word2vec model into gensim. The trouble is the file format. It is not a .bin file format but a .tar file. It is the model / file  <em>deu-ch_web-public_2019_1M.tar.gz</em> from the <a href=""https://wortschatz.uni-leipzig.de/en/download/German"" rel=""nofollow noreferrer"">University of Leipzig</a>. The model is also listed <a href=""https://huggingface.co/inovex/Word2Vec-Finetuning-Service-Base-Models"" rel=""nofollow noreferrer"">on HuggingFace</a> where different word2vec models for English and German are listed.</p>
<p><strong>First I tried:</strong></p>
<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('deu-ch_web-public_2019_1M.tar.gz')
</code></pre>
<p>--&gt; ValueError: invalid literal for int() with base 10: 'deu-ch_web-public_2019_1M</p>
<p><strong>Then I unzipped the file with 7-Zip and tried the following:</strong></p>
<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; ValueError: invalid literal for int() with base 10: 'deu-ch_web-public_2019_1M</p>
<pre><code>from gensim.models import word2vec
model = word2vec.Word2Vec.load('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; UnpicklingError: could not find MARK</p>
<p>Then I got a bit desperate...</p>
<pre><code>import gensim.downloader
model = gensim.downloader.load('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; ValueError: Incorrect model/corpus name</p>
<p>Googling around I found useful information how to load a .bin model with gensim ( <a href=""https://stackoverflow.com/questions/39549248/how-to-load-a-pre-trained-word2vec-model-file-and-reuse-it/39662736#39662736"">see here</a> and <a href=""https://stackoverflow.com/questions/65394022/how-can-a-word2vec-pretrained-model-be-loaded-in-gensim-faster"">here</a> ). Following this <a href=""https://github.com/deeplearning4j/deeplearning4j/issues/4150"" rel=""nofollow noreferrer"">thread</a> it seems tricky to load a .tar file with gensim. Especially if one has not one .txt file but five .txt files as in this case. I found <a href=""https://stackoverflow.com/questions/71733191/how-to-read-and-load-tarfile-to-extract-feature-vector"">one answer how to read a .tar file but with tensorflow</a>. Since I am not familiar with tensorflow, I prefer to use gensim. Any thoughts how to solve the issue is appreciated.</p>
","python, gensim, word2vec, tar.gz","<p>A <code>.tar</code> file is a bundle of one or more directories and files â€“ see <a href=""https://en.wikipedia.org/wiki/Tar_(computing)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Tar_(computing)</a> â€“ and thus not the sort of single-model file that you should expect Gensim to open directly.</p>
<p>Rather, similar to as with a <code>.zip</code> file, you'd use some purpose-specific software to extract any content inside the <code>.tar</code> into individual files â€“ then point Gensim at those, individually, <em>if</em> they're formats Gensim understands.</p>
<p>A typical command-line operation to extract the individual file(s) from a <code>.tar.gz</code> file (which is both tarred &amp; gzipped) would be:</p>
<p>tar -xvzf deu-ch_web-public_2019_1M.tar.gz</p>
<p>That tells the command to e<code>x</code>tract with <code>v</code>erbose reporting while also un-g<code>z</code>ipping the <code>f</code>ile <code>deu-ch_web-public_2019_1M.tar.gz</code>. Then you'll have one or more new local files, which are the actual (not-packaged-up) files of interest.</p>
<p>In some graphical UI file-explorers, like the MacOS 'Finder', simply double-clicking to perform the default 'open' action on <code>deu-ch_web-public_2019_1M.tar.gz</code> will perform this expansion (no <code>tar</code> command-line needed).</p>
<p>But note: the University of Liepzig page you've linked describes these files as 'corpora' (training texts), <em>not</em> trained sets of word-vectors or word2vec models.</p>
<p>And I looked at the &quot;2019 - switzerland - public web file&quot; you're referring-to, and inside is a directory (folder) <code>deu-ch_web-public_2019_1M</code>, with 7 <code>.txt</code> files inside of various formats, and 1 <code>.sql</code>  file. But none of those are any sort of trained word-vectors - just text &amp; text-statistics.</p>
<p>You could use those to train a model yourself. The <code>deu-ch_web-public_2019_1M-sentences.txt</code> is closest to what you need, as 1 million plain-text sentences.</p>
<p>But it's still not yet in a form fully ready for word2vec training. Each line has a redundant line-number at the front, and the text hasn't yet been tokenized into word-tokens (which would potentially remove punctuation, or sometimes keep punctuation as distinct tokens). And, as a mere 15 million words total, it's still fairly small as a corpus for creating a powerful word2vec model.</p>
",0,0,217,2024-02-07 10:41:01,https://stackoverflow.com/questions/77954019/load-word2vec-model-that-is-in-tar-format
What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax?,"<p>I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?</p>
","machine-learning, nlp, word2vec, hierarchical, softmax","<p>While there are many slightly-different ways to think about it, it may help to consider the values in the (trained, frozen) neural network as associated with edges moreso than vertexes (nodes).</p>
<p>The network &quot;projection weights&quot; leading from an (abstract) one-hot encoding of each known word into the network are essentially the actual per-word word-vectors. Assuming a common case of 300d vectors, the 300 edges from the single-word node to the inner nodes are that words.</p>
<p>Then, there's another set of edge-weights from the internal activation to the &quot;output&quot; layer, which is offering the network's training goal of in-context word-projection.</p>
<p>In the (more common &amp; usual default) negative-sampling approach, each output node corresponds to a single predictable word. That's a very easy output-shape to visualize. And the value of negative-sampling is that you only check the activations of the desired word, and <code>n</code> more randomly chosen negative words, to perform your training updates. That's way less calculation than if you checked the output values at all <code>V</code> (size of vocabulary) output nodes, and still works pretty well, and doesn't get more expensive with larger vocabularies (unless you choose for other reasons to also increase your choice of <code>n</code> negative samples).</p>
<p>In hierarchical softmax, the interpretation of the output nodes is more complicated. Rather than the activation at a single node indicating the prediction of a single word, a (varying) set of nodes must have the right on/off activations to communicate the variable-length huffman-code of a word.</p>
<p>So in HS, to backprop the network more towards predicting your desired &quot;positive&quot; word (from one input context), the algorithm considers just those nodes involved in that words nique coding â€“ a smaller set of nodes for the most-ommon words, but a larger set of nodes for rarer words â€“ and nudges each of them more towards the pattern that predicts the desired word. Again, you get the sparse training efficiency of updating only a tiny subset, far smaller than all <code>V</code> nodes, each training-step. But, the cost will vary based on the target word, and grow with the log of <code>V</code> as vocabulary-size grows. Further, as the original/naive assignment of codes is based strictly on word-frequency, quite-dissimilar words may have very-similar codings, perhaps causing more word-to-word interference. (There were hints in the original <code>word2vec.c</code> release of refining the HS word-codings over time to ensure similar words share similar Huffman codings, but I've seen litle followup on that idea, perhaps because of the dominance of negative-sampling.)</p>
<p>So, in an HS network, the weights from the inner-activations, to the output nodes, are tuned to indicate, by Huffman code, which word is the preferred prediction from a context.</p>
<p>In the word2vec implementation I'm most familiar with, Python Gensim, these &quot;hidden to output&quot; weights are not even randomly initialized at the beginning, instead left as 0.0 â€“ and I think this was directly copied from the initialization of Google's <code>word2vec.c</code> release. But, as soon as training begins, the explict random initialization of those &quot;input weights&quot; (initial random input word-vectors) means those weights are immediately perturbed in a way that at 1st is nearly all random but becomes more helpful over the SGD training.</p>
<p>So:</p>
<ul>
<li>those inner weights start 0.0 but quickly start reflecting the influence of the (initially-random) word vectors and training examples</li>
<li>they're usually <em>not</em> harvested from the network after training, like the final word-vectors are â€“ but would be kept around if you wanted to continue training later, and perhaps provide a running start to future training runs</li>
</ul>
<p>(In the negative-sampling case, some research suggested those hidden-to-output weights could also be interpreted as same <code>embedding_size</code> per-word vectors, with some usefulness: see paper by Mitra et al at Microsoft about &quot;Dual Word Embeddings&quot;. But given the varying-length codings of output words in the HS case, extracting/interpretating those output-weights would be trickier.)</p>
<p>In the implementation I'm most familiar with, the It may help to think instead of the shallow neural network's &quot;projection layer&quot; (effectively the word-vectors themselves, as each virtual &quot;single node&quot; 1-hot word has its <code>embedding_size</code> out-weights) and &quot;hidden layer</p>
",2,0,144,2024-04-06 18:15:41,https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"<p>I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data.</p>
<p>However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model.</p>
<p>This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?</p>
","python, nlp, artificial-intelligence, word2vec, word-embedding","<p>As has been pointed out <a href=""https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4?rq=2"">before</a>, there is no &quot;go-to&quot; way for fine-tuning Word2Vec type models.</p>
<p>I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of <a href=""https://sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.</p>
<p>For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">faiss</a>.</p>
",1,0,1340,2024-04-12 17:57:56,https://stackoverflow.com/questions/78317989/is-it-possible-to-fine-tune-a-pretrained-word-embedding-model-like-vec2word
Word vectors trained from word2vec have very small value in all dimension for all words,"<p>I am using word2vec (gensim 4.3.3) on word embedding, results of the word vectors from the saved file 'wv.vectors.npy' shows that all word vectors are small, min of the entire array is -0.003 and max is 0.003, that each word is embedded with a very small vector, which is not expected.
What seems to be the problems, is my corpus or word not good for the application of the word2vec model, or the somethings about the training settings?</p>
<p>I am working with mol2vec (<a href=""https://github.com/samoturk/mol2vec"" rel=""nofollow noreferrer"">https://github.com/samoturk/mol2vec</a>), which embeds molecules into vectors using word2vec. I am trying to retrain the model with my own list of molecules, the &quot;words&quot; are ID numbers (no &quot;real&quot; words, they are just hashed numbers  generated by morgan fingerprints representing sub-structure of the molecule, all words constitute a molecule or a sentence), the corpus look like</p>
<pre><code>2246997334 3696389118 2246699815 2259502203 977461771 2245384272 1506993418 UNK 2245273601 1736287034 387666683 864662311 1542633699 2245277810 954800030 3006711714 864674487 1979311206 264864308 2246699815 3537119515 2246728737 3537119515
864942730 10565946 3217380708 328936174 3237386214 2132511834 2297887526 808456108 3218693969 584893129 864662311 2192318254
</code></pre>
<p>20 million sentences, with 0.3 million unique words</p>
<p>trained with mostly default settings adopted from the source mol2vec (original code use word2vec from older version of gensim, I change some code to adopt with newer version of gensim, which should not affect the performance? )</p>
<pre><code>corpus = word2vec.LineSentence('smiles.cp.unk')
model = word2vec.Word2Vec(corpus, vector_size=300, window=10, min_count=4, workers=-1, sg=1)
</code></pre>
<p>The pretrained model provided by source mol2vec have vectors array with number from -2 to 2. Yet I tried different window size and vector size, all give similar results of small number vector of -0.003 to 0.003.</p>
","python, word2vec","<p><code>workers=-1</code> is not a valid parameter for Gensim's <code>Word2Vec</code> model class. If you're mimicking some online example suggesting that value, it's a bad reference.</p>
<p>If you carefully review the console output of your run â€“ especially if you've set relevant logging levels to WARNING â€“ there'll probably be a message about that, or at the very least you might notice that your training completes instantly (<em>no</em> worker threads), which would not be expected with a corpus of tens of millions of words.</p>
<p><code>workers</code> should be a positive number that's strictly no larger than the number of CPU cores available.</p>
<p>If your system has 4 cores, 3-4 might be good values for <code>workers</code>. (The value that actually achieves fastest training depends a bit on your other parameters and corpus.)</p>
<p>If your system has 8 cores, 4-8 might be good values.</p>
<p>If your system has 16 or more cores, 8-12 are usually good values. Above that count of workers â€“ and certainly above 16 â€“ Python &amp; Gensim contention issues can cause extra worker threads to actually hurt training throughput, in the usual method of using a Python iterator corpus, even if youy have many more cores.</p>
",0,1,46,2025-01-01 00:57:32,https://stackoverflow.com/questions/79321001/word-vectors-trained-from-word2vec-have-very-small-value-in-all-dimension-for-al
