{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def getData(page, tag):\n",
    "    \n",
    "    base_url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "    params = {\n",
    "        \"order\": \"asc\",\n",
    "        \"sort\": \"creation\",\n",
    "        \"tagged\": tag,\n",
    "        \"site\": \"stackoverflow\",\n",
    "        \"pagesize\": 100,\n",
    "        \"page\": page,\n",
    "        \"filter\": \"withbody\",\n",
    "        \"answers\": \"1\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            logging.warning(\"No questions found.\")\n",
    "            return None\n",
    "        \n",
    "        data_list = []\n",
    "        for question in data[\"items\"]:\n",
    "            answer_id = question.get(\"accepted_answer_id\",'')\n",
    "                                     \n",
    "            if answer_id:\n",
    "                answers_url = f\"https://api.stackexchange.com/2.3/answers/{answer_id}\"\n",
    "                answers_params = {\n",
    "                    \"site\": \"stackoverflow\",\n",
    "                    \"filter\": \"withbody\",\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    answers_response = requests.get(answers_url, params=answers_params)\n",
    "                    answers_response.raise_for_status()\n",
    "                    answers_data = answers_response.json()\n",
    "                    answers = answers_data.get('items', False)\n",
    "                    if answers[0].get('is_accepted', False):\n",
    "                        accepted_answer=answers[0]\n",
    "                        data_list.append([\n",
    "                            question.get('title', ''),\n",
    "                            question.get('body', ''),\n",
    "                            \", \".join(question.get('tags', [])),\n",
    "                            accepted_answer.get('body', ''),\n",
    "                            question.get('score', 0),\n",
    "                            question.get('creation_date','')\n",
    "                        ])\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error fetching answers for question {answer_id}: {e}\")\n",
    "        \n",
    "            if data_list:\n",
    "                df = pd.DataFrame(data_list, columns=[\n",
    "                    \"Title\", \"Description\", \"Tags\", \"Accepted Answer\", \"Question Score\", \"Question Time\"\n",
    "                ])\n",
    "\n",
    "                file_name = \"questions_data.csv\"\n",
    "\n",
    "                with open(file_name, 'a', encoding='utf-8') as f:\n",
    "                        df.to_csv(f, header=f.tell() == 0, index=False,lineterminator='\\n')\n",
    "                logging.info(f\"Data appended to {file_name}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request error: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"JSON decode error: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['.net', 'datetime', 'nlp'], 'owner': {'account_id': 411, 'reputation': 5842, 'user_id': 521, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/3893360ae5ac6c06ade059fce126bc51?s=256&d=identicon&r=PG', 'display_name': 'palmsey', 'link': 'https://stackoverflow.com/users/521/palmsey'}, 'is_answered': True, 'view_count': 6470, 'closed_date': 1492171912, 'accepted_answer_id': 631134, 'answer_count': 9, 'score': 27, 'last_activity_date': 1438422109, 'creation_date': 1219445110, 'question_id': 23689, 'link': 'https://stackoverflow.com/questions/23689/natural-language-date-time-parser-for-net', 'closed_reason': 'Not suitable for this site', 'title': 'Natural language date/time parser for .NET?', 'body': '<p>Does anyone know of a .NET date/time parser similar to <a href=\"http://chronic.rubyforge.org/\" rel=\"noreferrer\">Chronic for Ruby</a> (handles stuff like \"tomorrow\" or \"3pm next thursday\")?</p>\\n\\n<p>Note: I do write Ruby (which is how I know about Chronic) but this project must use .NET.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['language-agnostic', 'nlp'], 'owner': {'account_id': 1196, 'reputation': 2297, 'user_id': 1592, 'user_type': 'registered', 'accept_rate': 60, 'profile_image': 'https://www.gravatar.com/avatar/2886583fab3929c95a68e339e73d61d8?s=256&d=identicon&r=PG', 'display_name': 'jeffreypriebe', 'link': 'https://stackoverflow.com/users/1592/jeffreypriebe'}, 'is_answered': True, 'view_count': 6486, 'closed_date': 1451720842, 'answer_count': 4, 'score': 14, 'last_activity_date': 1396481617, 'creation_date': 1219611453, 'last_edit_date': 1233102270, 'question_id': 25332, 'link': 'https://stackoverflow.com/questions/25332/whats-a-good-natural-language-library-to-use-for-paraphrasing', 'closed_reason': 'Not suitable for this site', 'title': 'What&#39;s a good natural language library to use for paraphrasing?', 'body': \"<p>I'm looking for an existing library to summarize or paraphrase content (I'm aiming at blog posts) - any experience with existing natural language processing libraries?</p>\\n\\n<p>I'm open to a variety of languages, so I'm more interested in the abilities &amp; accuracy.</p>\\n\"}\n",
      "{'tags': ['windows-vista', 'nlp', 'speech-recognition', 'multilingual'], 'owner': {'account_id': 1335, 'reputation': 82044, 'user_id': 1782, 'user_type': 'registered', 'accept_rate': 98, 'profile_image': 'https://www.gravatar.com/avatar/4dfc8f56817006ef21327d5ff19ce04f?s=256&d=identicon&r=PG', 'display_name': 'juan', 'link': 'https://stackoverflow.com/users/1782/juan'}, 'is_answered': True, 'view_count': 5658, 'accepted_answer_id': 36684, 'answer_count': 6, 'score': 3, 'last_activity_date': 1253219922, 'creation_date': 1220144928, 'last_edit_date': 1233172076, 'question_id': 36533, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/36533/vista-speech-recognition-in-multiple-languages', 'title': 'Vista speech recognition in multiple languages', 'body': \"<p>my primary language is spanish, but I use all my software in english, including windows; however I'd like to use speech recognition in spanish.</p>\\n\\n<p>Do you know if there's a way to use vista's speech recognition in other language than the primary os language?</p>\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['nlp'], 'owner': {'account_id': 1917, 'reputation': 18035, 'user_id': 2644, 'user_type': 'registered', 'accept_rate': 82, 'profile_image': 'https://www.gravatar.com/avatar/a2d07b1a0a6ddcfed5c18ddfafd5a887?s=256&d=identicon&r=PG', 'display_name': 'pek', 'link': 'https://stackoverflow.com/users/2644/pek'}, 'is_answered': True, 'view_count': 33164, 'closed_date': 1357631818, 'accepted_answer_id': 41448, 'answer_count': 17, 'score': 118, 'last_activity_date': 1353304626, 'creation_date': 1220438173, 'last_edit_date': 1495541392, 'question_id': 41424, 'link': 'https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean', 'closed_reason': 'exact duplicate', 'title': 'How do you implement a &quot;Did you mean&quot;?', 'body': '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\">How does the Google “Did you mean?” Algorithm work?</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>Suppose you have a search system already in your website. How can you implement the \"Did you mean:<code>&lt;spell_checked_word&gt;</code>\" like Google does in some <a href=\"http://www.google.com/search?hl=en&amp;q=spellling&amp;btnG=Search\" rel=\"noreferrer\">search queries</a>?</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'nlp', 'semantics'], 'owner': {'account_id': 2809, 'reputation': 3636, 'user_id': 1925263, 'user_type': 'registered', 'accept_rate': 89, 'profile_image': 'https://www.gravatar.com/avatar/38fa6a316b34296bb2532f09e13e8e2e?s=256&d=identicon&r=PG', 'display_name': 'btw0', 'link': 'https://stackoverflow.com/users/1925263/btw0'}, 'is_answered': True, 'view_count': 49878, 'accepted_answer_id': 63076, 'answer_count': 11, 'score': 65, 'last_activity_date': 1593283252, 'creation_date': 1221481602, 'last_edit_date': 1221483755, 'question_id': 62328, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/62328/is-there-an-algorithm-that-tells-the-semantic-similarity-of-two-phrases', 'title': 'Is there an algorithm that tells the semantic similarity of two phrases', 'body': '<p>input: phrase 1, phrase 2</p>\\n\\n<p>output: semantic similarity value (between 0 and 1), or the probability these two phrases are talking about the same thing</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'cluster-analysis', 'machine-learning', 'nlp'], 'owner': {'user_type': 'does_not_exist', 'display_name': 'adityaw'}, 'is_answered': True, 'view_count': 5257, 'answer_count': 5, 'community_owned_date': 1221503797, 'score': 3, 'last_activity_date': 1398538698, 'creation_date': 1221503797, 'last_edit_date': 1233102218, 'question_id': 65487, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/65487/how-do-you-categorize-based-on-text-content', 'title': 'How Do You Categorize Based On Text Content?', 'body': '<p>How does one automatically find categories for text based on content?</p>\\n'}\n",
      "{'tags': ['algorithm', 'language-agnostic', 'parsing', 'numbers', 'nlp'], 'owner': {'account_id': 6714, 'reputation': 6967, 'user_id': 11414, 'user_type': 'registered', 'accept_rate': 73, 'profile_image': 'https://i.sstatic.net/s8vyO.png?s=256', 'display_name': 'Evgeny Zislis', 'link': 'https://stackoverflow.com/users/11414/evgeny-zislis'}, 'is_answered': True, 'view_count': 16251, 'accepted_answer_id': 653098, 'answer_count': 12, 'community_owned_date': 1237297753, 'score': 51, 'last_activity_date': 1526882586, 'creation_date': 1221551256, 'last_edit_date': 1526882586, 'question_id': 70161, 'content_license': 'CC BY-SA 4.0', 'link': 'https://stackoverflow.com/questions/70161/how-to-read-values-from-numbers-written-as-words', 'title': 'How to read values from numbers written as words?', 'body': '<p>As we all know numbers can be written either in numerics, or called by their names. While there are a lot of examples to be found that convert 123 into one hundred twenty three, I could not find good examples of how to convert it the other way around.</p>\\n\\n<p>Some of the caveats:</p>\\n\\n<ol>\\n<li>cardinal/nominal or ordinal: \"one\" and \"first\"</li>\\n<li>common spelling mistakes: \"forty\"/\"fourty\"</li>\\n<li>hundreds/thousands: 2100 -> \"twenty one hundred\" and also \"two thousand and one hundred\"</li>\\n<li>separators: \"eleven hundred fifty two\", but also \"elevenhundred fiftytwo\" or \"eleven-hundred fifty-two\" and whatnot</li>\\n<li>colloquialisms: \"thirty-something\"</li>\\n<li>fractions: \\'one third\\', \\'two fifths\\'</li>\\n<li>common names: \\'a dozen\\', \\'half\\'</li>\\n</ol>\\n\\n<p>And there are probably more caveats possible that are not yet listed.\\nSuppose the algorithm needs to be very robust, and even understand spelling mistakes.</p>\\n\\n<p>What fields/papers/studies/algorithms should I read to learn how to write all this?\\nWhere is the information?</p>\\n\\n<blockquote>\\n  <p>PS: My final parser should actually understand 3 different languages, English, Russian and Hebrew. And maybe at a later stage more languages will be added. Hebrew also has male/female numbers, like \"one man\" and \"one woman\" have a different \"one\" — \"ehad\" and \"ahat\". Russian also has some of its own complexities.</p>\\n</blockquote>\\n\\n<p>Google does a great job at this. For example:</p>\\n\\n<p><a href=\"http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal\" rel=\"noreferrer\">http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal</a></p>\\n\\n<p>(the reverse is also possible <a href=\"http://www.google.com/search?q=999999999999+in+english\" rel=\"noreferrer\">http://www.google.com/search?q=999999999999+in+english</a>)</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'nlp', 'word-frequency'], 'owner': {'account_id': 9391, 'reputation': 8250, 'user_id': 17328, 'user_type': 'registered', 'accept_rate': 80, 'profile_image': 'https://www.gravatar.com/avatar/0b8f5cf3259414d6b33897ffc6430df2?s=256&d=identicon&r=PG', 'display_name': 'Mark McDonald', 'link': 'https://stackoverflow.com/users/17328/mark-mcdonald'}, 'is_answered': True, 'view_count': 22642, 'accepted_answer_id': 90846, 'answer_count': 8, 'score': 33, 'last_activity_date': 1741687724, 'creation_date': 1221720566, 'last_edit_date': 1434981418, 'question_id': 90580, 'content_license': 'CC BY-SA 3.0', 'link': 'https://stackoverflow.com/questions/90580/word-frequency-algorithm-for-natural-language-processing', 'title': 'Word frequency algorithm for natural language processing', 'body': '<p>Without getting a degree in information retrieval, I\\'d like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  The goal is to get a \"general feel\" of what people are saying over a set of textual comments.  Along the lines of <a href=\"http://wordle.net/\" rel=\"noreferrer\">Wordle</a>.</p>\\n\\n<p>What I\\'d like:</p>\\n\\n<ul>\\n<li>ignore articles, pronouns, etc (\\'a\\', \\'an\\', \\'the\\', \\'him\\', \\'them\\' etc)</li>\\n<li>preserve proper nouns</li>\\n<li>ignore hyphenation, except for soft kind</li>\\n</ul>\\n\\n<p>Reaching for the stars, these would be peachy:</p>\\n\\n<ul>\\n<li>handling stemming &amp; plurals (e.g. like, likes, liked, liking match the same result)</li>\\n<li>grouping of adjectives (adverbs, etc) with their subjects (\"great service\" as opposed to \"great\", \"service\")</li>\\n</ul>\\n\\n<p>I\\'ve attempted some basic stuff using Wordnet but I\\'m just tweaking things blindly and hoping it works for my specific data.  Something more generic would be great.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['string', 'linguistics', 'nlp'], 'owner': {'account_id': 747, 'reputation': 10629, 'user_id': 976, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/ee07c455591be3321bdd8305aab15880?s=256&d=identicon&r=PG', 'display_name': 'Ozgur Ozcitak', 'link': 'https://stackoverflow.com/users/976/ozgur-ozcitak'}, 'is_answered': True, 'view_count': 5849, 'accepted_answer_id': 92033, 'answer_count': 13, 'score': 24, 'last_activity_date': 1320568799, 'creation_date': 1221740420, 'last_edit_date': 1320568799, 'question_id': 92006, 'content_license': 'CC BY-SA 3.0', 'link': 'https://stackoverflow.com/questions/92006/how-do-i-determine-if-a-random-string-sounds-like-english', 'title': 'How do I determine if a random string sounds like English?', 'body': '<p>I have an algorithm that generates strings based on a list of input words. How do I separate only the strings that sounds like English words? ie. discard <strong>RDLO</strong> while keeping <strong>LORD</strong>.</p>\\n\\n<p><strong>EDIT:</strong> To clarify, they do not need to be actual words in the dictionary. They just need to sound like English. For example <strong>KEAL</strong> would be accepted.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['nlp', 'text-analysis'], 'owner': {'account_id': 9101, 'reputation': 9917, 'user_id': 16668, 'user_type': 'registered', 'accept_rate': 64, 'profile_image': 'https://www.gravatar.com/avatar/662b99660e8446a5f56aa754dd7b11a3?s=256&d=identicon&r=PG', 'display_name': 'Alex Weinstein', 'link': 'https://stackoverflow.com/users/16668/alex-weinstein'}, 'is_answered': True, 'view_count': 13430, 'protected_date': 1496512315, 'accepted_answer_id': 126378, 'answer_count': 2, 'score': 13, 'last_activity_date': 1496422293, 'creation_date': 1222192307, 'question_id': 122595, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence', 'title': 'NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence', 'body': '<p>I need your help in determining the best approach for analyzing industry-specific sentences (i.e. movie reviews) for \"positive\" vs \"negative\". I\\'ve seen libraries such as OpenNLP before, but it\\'s too low-level - it just gives me the basic sentence composition; what I need is a higher-level structure:\\n- hopefully with wordlists\\n- hopefully trainable on my set of data</p>\\n\\n<p>Thanks!</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['.net', 'nlp'], 'owner': {'account_id': 9959, 'reputation': 6753, 'user_id': 18619, 'user_type': 'registered', 'accept_rate': 87, 'profile_image': 'https://www.gravatar.com/avatar/0ad58350b46012b50b00c5e61d0dc7d3?s=256&d=identicon&r=PG', 'display_name': 'Alexandre Brisebois', 'link': 'https://stackoverflow.com/users/18619/alexandre-brisebois'}, 'is_answered': True, 'view_count': 323, 'accepted_answer_id': 127503, 'answer_count': 4, 'score': 2, 'last_activity_date': 1233101781, 'creation_date': 1222264104, 'last_edit_date': 1233101781, 'question_id': 127238, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/127238/contextual-natural-language-resources-where-do-i-start', 'title': 'Contextual Natural Language Resources, Where Do I Start?', 'body': '<p>Where can i find some .Net or conceptual resources to start working with Natural Language where I can pull context and subjects from text. I wish not to work with word frequency algorithms.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['language-agnostic', 'parsing', 'nlp'], 'owner': {'account_id': 3370, 'reputation': 7212, 'user_id': 4857, 'user_type': 'registered', 'accept_rate': 96, 'profile_image': 'https://i.sstatic.net/cxMVR.jpg?s=256', 'display_name': 'Ande Turner', 'link': 'https://stackoverflow.com/users/4857/ande-turner'}, 'is_answered': True, 'view_count': 1904, 'accepted_answer_id': 707502, 'answer_count': 8, 'score': 5, 'last_activity_date': 1274560532, 'creation_date': 1222373983, 'last_edit_date': 1274560532, 'question_id': 135777, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/135777/a-stringtoken-parser-which-gives-google-search-style-did-you-mean-suggestions', 'title': 'A StringToken Parser which gives Google Search style &quot;Did you mean:&quot; Suggestions', 'body': '<h2>Seeking a method to:</h2>\\n\\n<h2>Take whitespace separated tokens in a String; return a suggested Word</h2>\\n\\n<p><br>\\n<strong>ie:</strong><br>\\nGoogle Search can take <em>\"fonetic wrd nterpreterr\"</em>,<br>\\nand atop of the result page it shows <em>\"Did you mean: phonetic word interpreter\"</em></p>\\n\\n<p><em>A solution in any of the C* languages or Java would be preferred.</em></p>\\n\\n<p><br>\\n<strong>Are there any existing Open Libraries which perform such functionality?</strong></p>\\n\\n<p><strong>Or is there a way to Utilise a Google API to request a suggested word?</strong></p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['nlp', 'linguistics', 'corpus'], 'owner': {'account_id': 6824, 'reputation': 34478, 'user_id': 11596, 'user_type': 'registered', 'accept_rate': 94, 'profile_image': 'https://www.gravatar.com/avatar/e78cebfe065dac6f667b1b8259d9299c?s=256&d=identicon&r=PG', 'display_name': 'unmounted', 'link': 'https://stackoverflow.com/users/11596/unmounted'}, 'is_answered': True, 'view_count': 1658, 'accepted_answer_id': 138310, 'answer_count': 7, 'score': 5, 'last_activity_date': 1391084995, 'creation_date': 1222395349, 'last_edit_date': 1495540883, 'question_id': 137380, 'content_license': 'CC BY-SA 3.0', 'link': 'https://stackoverflow.com/questions/137380/nlp-building-small-corpora-or-where-to-get-lots-of-not-too-specialized-engl', 'title': 'NLP: Building (small) corpora, or &quot;Where to get lots of not-too-specialized English-language text files?&quot;', 'body': '<p>Does anyone have a suggestion for where to find archives or collections of everyday English text for use in a small corpus?  I have been using Gutenberg Project books for a working prototype, and would like to incorporate more contemporary language.  A <a href=\"https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence#126378\">recent answer</a> here pointed indirectly to a great <a href=\"http://us.imdb.com/Reviews/\" rel=\"nofollow noreferrer\">archive of usenet movie reviews</a>, which hadn\\'t occurred to me, and is very good.  For this particular program technical usenet archives or programming mailing lists would tilt the results and be hard to analyze, but any kind of general blog text, or chat transcripts, or anything that may have been useful to others, would be very helpful.  Also, a partial or downloadable research corpus that isn\\'t too marked-up, or some heuristic for finding an appropriate subset of wikipedia articles, or any other idea, is very appreciated.</p>\\n\\n<p>(BTW, I am being a good citizen w/r/t downloading, using a deliberately slow script that is not demanding on servers hosting such material, in case you perceive a moral hazard in pointing me to something enormous.)</p>\\n\\n<p><strong>UPDATE</strong>:  User S0rin points out that wikipedia requests no crawling and provides <a href=\"http://en.wikipedia.org/wiki/Special:Export\" rel=\"nofollow noreferrer\">this export tool</a> instead.  Project Gutenberg has a policy specified <a href=\"http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages\" rel=\"nofollow noreferrer\">here</a>, bottom line, try not to crawl, but if you need to: \"Configure your robot to wait at least 2 seconds between requests.\"</p>\\n\\n<p><strong>UPDATE 2</strong>  The wikpedia dumps are the way to go, thanks to the answerers who pointed them out.  I ended up using the English version from here: <a href=\"http://download.wikimedia.org/enwiki/20090306/\" rel=\"nofollow noreferrer\">http://download.wikimedia.org/enwiki/20090306/</a> , and a Spanish dump about half the size.  They are some work to clean up, but well worth it, and they contain a lot of useful data in the links.</p>\\n\\n<hr>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['java', 'dsl', 'nlp', 'parsing'], 'owner': {'account_id': 8161, 'reputation': 12742, 'user_id': 14540, 'user_type': 'registered', 'accept_rate': 91, 'profile_image': 'https://www.gravatar.com/avatar/c4ceadae4f61dac8ff3ababc3faa6c42?s=256&d=identicon&r=PG', 'display_name': 'kolrie', 'link': 'https://stackoverflow.com/users/14540/kolrie'}, 'is_answered': True, 'view_count': 14601, 'closed_date': 1580482119, 'accepted_answer_id': 144374, 'answer_count': 6, 'score': 18, 'last_activity_date': 1580481644, 'creation_date': 1222545372, 'last_edit_date': 1233101128, 'question_id': 144339, 'link': 'https://stackoverflow.com/questions/144339/what-would-the-best-tool-to-create-a-natural-dsl-in-java', 'closed_reason': 'Not suitable for this site', 'title': 'What would the best tool to create a natural DSL in Java?', 'body': '<p>A couple of days ago, I read a blog entry (<a href=\"http://ayende.com/Blog/archive/2008/09/08/Implementing-generic-natural-language-DSL.aspx\" rel=\"noreferrer\">http://ayende.com/Blog/archive/2008/09/08/Implementing-generic-natural-language-DSL.aspx</a>) where the author discuss the idea of a generic natural language DSL parser using .NET.</p>\\n\\n<p>The brilliant part of his idea, in my opinion, is that the text is parsed and matched against classes using the same name as the sentences. </p>\\n\\n<p>Taking as an example, the following lines:</p>\\n\\n<pre>\\nCreate user user1 with email test@email.com and password test\\nLog user1 in\\nTake user1 to category t-shirts\\nMake user1 add item Flower T-Shirt to cart\\nTake user1 to checkout\\n</pre>\\n\\n<p>Would get converted using a collection of \"known\" objects, that takes the result of parsing. Some example objects would be (using Java for my example):</p>\\n\\n<pre><code>public class CreateUser {\\n    private final String user;\\n    private String email;\\n    private String password;\\n\\n    public CreateUser(String user) {\\n    this.user = user;\\n    }\\n\\n    public void withEmail(String email) {\\n    this.email = email;\\n    }\\n\\n    public String andPassword(String password) {\\n        this.password = password;\\n    }\\n}\\n</code></pre>\\n\\n<p>So, when processing the first sentence, CreateUser class would be a match (obviously because it\\'s a concatenation of \"create user\") and, since it takes a parameter on the constructor, the parser would take \"user1\" as being the user parameter. </p>\\n\\n<p>After that, the parser would identify that the next part, \"with email\" also matches a method name, and since that method takes a parameter, it would parse \"test@email.com\" as being the email parameter. </p>\\n\\n<p>I think you get the idea by now, right? One quite clear application of that, at least for me, would be to allow application testers create \"testing scripts\" in natural language and then parse the sentences into classes that uses JUnit to check for app behaviors.</p>\\n\\n<p>I\\'d like to hear ideas, tips and opinions on tools or resource that could code such parser using Java. Better yet if we could avoid using complex lexers, or frameworks like ANTLR, which I think maybe would be using a hammer to kill a fly.</p>\\n\\n<p>More than that, if anyone is up to start an open source project for that, I would definitely be interested.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'statistics', 'nlp', 'named-entity-recognition'], 'owner': {'account_id': 8717, 'reputation': 21332, 'user_id': 15842, 'user_type': 'registered', 'accept_rate': 59, 'profile_image': 'https://www.gravatar.com/avatar/58d77327a8f46041462b31c117e5c51a?s=256&d=identicon&r=PG', 'display_name': 'Gregg Lind', 'link': 'https://stackoverflow.com/users/15842/gregg-lind'}, 'is_answered': True, 'view_count': 6591, 'accepted_answer_id': 164722, 'answer_count': 2, 'community_owned_date': 1222973072, 'score': 9, 'last_activity_date': 1692196803, 'creation_date': 1222973072, 'last_edit_date': 1257179414, 'question_id': 163923, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/163923/methods-for-geotagging-or-geolabelling-text-content', 'title': 'Methods for Geotagging or Geolabelling Text Content', 'body': \"<p>What are some good algorithms for automatically labeling text with the city / region  or origin?  That is, if a blog is about New York, how can I tell programatically.  Are there packages / papers that claim to do this with any degree of certainty?  </p>\\n\\n<p>I have looked at some tfidf based approaches, proper noun intersections, but so far, no spectacular successes, and I'd appreciate ideas!  </p>\\n\\n<p>The more general question is about assigning texts to topics, given some list of topics.</p>\\n\\n<p>Simple / naive approaches preferred to full on Bayesian approaches, but I'm open.</p>\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['encoding', 'theory', 'nlp', 'linguistics'], 'owner': {'account_id': 3370, 'reputation': 7212, 'user_id': 4857, 'user_type': 'registered', 'accept_rate': 96, 'profile_image': 'https://i.sstatic.net/cxMVR.jpg?s=256', 'display_name': 'Ande Turner', 'link': 'https://stackoverflow.com/users/4857/ande-turner'}, 'is_answered': True, 'view_count': 2497, 'accepted_answer_id': 173946, 'answer_count': 8, 'community_owned_date': 1223295041, 'score': 4, 'last_activity_date': 1274560520, 'creation_date': 1223131686, 'last_edit_date': 1495535271, 'question_id': 170452, 'content_license': 'CC BY-SA 3.0', 'link': 'https://stackoverflow.com/questions/170452/theory-lexical-encoding', 'title': 'Theory: &quot;Lexical Encoding&quot;', 'body': '<p><strong>I am using the term \"Lexical Encoding\" for my lack of a better one.</strong></p>\\n\\n<p>A Word is arguably the fundamental unit of communication as opposed to a Letter.  Unicode tries to assign a numeric value to each Letter of all known Alphabets.  What is a Letter to one language, is a Glyph to another.  Unicode 5.1 assigns more than 100,000 values to these Glyphs currently.  Out of the approximately 180,000 Words being used in Modern English, it is said that with a vocabulary of about 2,000 Words, you should be able to converse in general terms. A \"Lexical Encoding\" would encode each Word not each Letter, and encapsulate them within a Sentence.</p>\\n\\n<pre><code>// An simplified example of a \"Lexical Encoding\"\\nString sentence = \"How are you today?\";\\nint[] sentence = { 93, 22, 14, 330, QUERY };\\n</code></pre>\\n\\n<p>In this example each Token in the String was encoded as an Integer. The Encoding Scheme here simply assigned an int value based on generalised statistical ranking of word usage, and assigned a constant to the question mark.</p>\\n\\n<p>Ultimately, a Word has both a Spelling &amp; Meaning though.  Any \"Lexical Encoding\" would preserve the meaning and intent of the Sentence as a whole, and not be language specific.  An English sentence would be encoded into <a href=\"https://stackoverflow.com/questions/170452/linguistics-lexical-encoding#174249\">\"...language-neutral atomic elements of meaning ...\"</a> which could then be reconstituted into any language with a structured Syntactic Form and Grammatical Structure.</p>\\n\\n<p>What are other examples of \"Lexical Encoding\" techniques?</p>\\n\\n<hr>\\n\\n<p>If you were interested in where the word-usage statistics come from :<br>\\n<a href=\"http://www.wordcount.org\" rel=\"nofollow noreferrer\">http://www.wordcount.org</a></p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['java', 'nlp', 'named-entity-recognition'], 'owner': {'account_id': 11931, 'reputation': 2640, 'user_id': 23238, 'user_type': 'registered', 'accept_rate': 67, 'profile_image': 'https://i.sstatic.net/5uVzs.jpg?s=256', 'display_name': 'webclimber', 'link': 'https://stackoverflow.com/users/23238/webclimber'}, 'is_answered': True, 'view_count': 23773, 'closed_date': 1398109959, 'accepted_answer_id': 390090, 'answer_count': 4, 'score': 27, 'last_activity_date': 1398108583, 'creation_date': 1223571294, 'question_id': 188176, 'link': 'https://stackoverflow.com/questions/188176/named-entity-recognition-libraries-for-java', 'closed_reason': 'Not suitable for this site', 'title': 'Named Entity Recognition Libraries for Java', 'body': '<p>I am looking for a simple but \"good enough\" Named Entity Recognition library (and dictionary) for java, I am looking to process emails and documents and extract some \"basic information\" like:\\nNames, places, Address and Dates</p>\\n\\n<p>I\\'ve been looking around, and most seems to be on the heavy side and full NLP kind of projects. </p>\\n\\n<p>Any recommendations ?</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['php', 'nlp', 'stemming', 'snowball', 'porter-stemmer'], 'owner': {'account_id': 8483, 'reputation': 838, 'user_id': 15318, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/d25bf284b1ee8e827f1a22f01c66082e?s=256&d=identicon&r=PG', 'display_name': 'Dave', 'link': 'https://stackoverflow.com/users/15318/dave'}, 'is_answered': True, 'view_count': 37295, 'accepted_answer_id': 190885, 'answer_count': 3, 'score': 36, 'last_activity_date': 1441217372, 'creation_date': 1223635409, 'last_edit_date': 1237394608, 'question_id': 190775, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/190775/stemming-algorithm-that-produces-real-words', 'title': 'Stemming algorithm that produces real words', 'body': '<p>I need to take a paragraph of text and extract from it a list of \"tags\".  Most of this is quite straight forward. However I need some help now stemming the resulting word list to avoid duplicates. Example: Community / Communities</p>\\n\\n<p>I\\'ve used an implementation of Porter Stemmer algorithm (I\\'m writing in PHP by the way):</p>\\n\\n<p><a href=\"http://tartarus.org/~martin/PorterStemmer/php.txt\" rel=\"noreferrer\">http://tartarus.org/~martin/PorterStemmer/php.txt</a></p>\\n\\n<p>This works, up to a point, but doesn\\'t return \"real\" words.  The example above is stemmed to \"commun\".</p>\\n\\n<p>I\\'ve tried \"Snowball\" (suggested within another Stack Overflow thread).</p>\\n\\n<p><a href=\"http://snowball.tartarus.org/demo.php\" rel=\"noreferrer\">http://snowball.tartarus.org/demo.php</a></p>\\n\\n<p>For my example (community / communities), Snowball stems to \"communiti\".</p>\\n\\n<p><strong>Question</strong></p>\\n\\n<p>Are there any other stemming algorithms that will do this? Has anyone else solved this problem?</p>\\n\\n<p><em>My current thinking is that I could use a stemming algorithm to avoid duplicates and then pick the shortest word I encounter to be the actual word to display.</em></p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'statistics', 'nlp'], 'owner': {'account_id': 8717, 'reputation': 21332, 'user_id': 15842, 'user_type': 'registered', 'accept_rate': 59, 'profile_image': 'https://www.gravatar.com/avatar/58d77327a8f46041462b31c117e5c51a?s=256&d=identicon&r=PG', 'display_name': 'Gregg Lind', 'link': 'https://stackoverflow.com/users/15842/gregg-lind'}, 'is_answered': True, 'view_count': 9706, 'answer_count': 6, 'score': 21, 'last_activity_date': 1645638404, 'creation_date': 1223644987, 'last_edit_date': 1223673055, 'question_id': 191248, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/191248/latent-dirichlet-allocation-pitfalls-tips-and-programs', 'title': 'Latent Dirichlet Allocation, pitfalls, tips and programs', 'body': '<p>I\\'m experimenting with <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">Latent Dirichlet Allocation</a> for topic disambiguation and assignment, and I\\'m looking for advice.</p>\\n\\n<ol>\\n<li>Which program is the \"best\", where best is some combination of easiest to use, best prior estimation, fast</li>\\n<li>How do I incorporate my intuitions about topicality.  Let\\'s say I think I know that some items in the corpus are really in the same category, like all articles by the same author.  Can I add that into the analysis?</li>\\n<li>Any unexpected pitfalls or tips I should know before embarking?</li>\\n</ol>\\n\\n<p>I\\'d prefer is there are R or Python front ends for whatever program, but I expect (and accept) that I\\'ll be dealing with C.  </p>\\n'}\n",
      "{'tags': ['string', 'nlp'], 'owner': {'account_id': 8234, 'reputation': 5160, 'user_id': 14728, 'user_type': 'registered', 'accept_rate': 96, 'profile_image': 'https://www.gravatar.com/avatar/57317da58d94bc6435e9f51aaf5262b9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Taptronic', 'link': 'https://stackoverflow.com/users/14728/taptronic'}, 'is_answered': True, 'view_count': 37842, 'accepted_answer_id': 481773, 'answer_count': 16, 'score': 58, 'last_activity_date': 1688534830, 'creation_date': 1223779039, 'last_edit_date': 1336289641, 'question_id': 195010, 'content_license': 'CC BY-SA 3.0', 'link': 'https://stackoverflow.com/questions/195010/how-can-i-split-multiple-joined-words', 'title': 'How can I split multiple joined words?', 'body': '<p>I have an array of 1000 or so entries, with examples below:</p>\\n\\n<pre><code>wickedweather\\nliquidweather\\ndriveourtrucks\\ngocompact\\nslimprojector\\n</code></pre>\\n\\n<p>I would like to be able to split these into their respective words, as:</p>\\n\\n<pre><code>wicked weather\\nliquid weather\\ndrive our trucks\\ngo compact\\nslim projector\\n</code></pre>\\n\\n<p>I was hoping a regular expression my do the trick.  But, since there is no boundary to stop on, nor is there any sort of capitalization that I could possibly key on, I am thinking, that some sort of reference to a dictionary might be necessary?  </p>\\n\\n<p>I suppose it could be done by hand, but why - when it can be done with code! =)  But this has stumped me.  Any ideas?  </p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['javascript', 'python', 'nlp'], 'owner': {'account_id': 2809, 'reputation': 3636, 'user_id': 1925263, 'user_type': 'registered', 'accept_rate': 89, 'profile_image': 'https://www.gravatar.com/avatar/38fa6a316b34296bb2532f09e13e8e2e?s=256&d=identicon&r=PG', 'display_name': 'btw0', 'link': 'https://stackoverflow.com/users/1925263/btw0'}, 'is_answered': True, 'view_count': 1250, 'accepted_answer_id': 196950, 'answer_count': 10, 'score': 9, 'last_activity_date': 1300858863, 'creation_date': 1223883132, 'question_id': 196924, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/196924/how-to-ensure-user-submit-only-english-text', 'title': 'How to ensure user submit only english text', 'body': '<p>I am building a project involving natural language processing, since the nlp module currently only deal with english text, so I have to make sure the user submitted content (not long, only several words) is in english. Are there established ways to achieve this? Python or Javascript way preferred.</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['nlp', 'grammar'], 'owner': {'account_id': 26309, 'reputation': 2331, 'user_id': 68336, 'user_type': 'registered', 'accept_rate': 88, 'profile_image': 'https://i.sstatic.net/1UVjt.jpg?s=256', 'display_name': 'Enrico Murru', 'link': 'https://stackoverflow.com/users/68336/enrico-murru'}, 'is_answered': True, 'view_count': 17810, 'closed_date': 1320772230, 'accepted_answer_id': 202771, 'answer_count': 51, 'community_owned_date': 1224082933, 'score': 55, 'last_activity_date': 1495993330, 'creation_date': 1224017473, 'last_edit_date': 1335895584, 'question_id': 202750, 'link': 'https://stackoverflow.com/questions/202750/is-there-a-human-readable-programming-language', 'closed_reason': 'not constructive', 'title': 'Is there a human readable programming language?', 'body': \"<p>I mean, is there a coded language with human style coding?\\nFor example:</p>\\n\\n<pre><code>Create an object called MyVar and initialize it to 10;\\nTake MyVar and call MyMethod() with parameters. . .\\n</code></pre>\\n\\n<p>I know it's not so useful, but it can be interesting to create such a grammar.</p>\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['parsing', 'nlp'], 'owner': {'account_id': 10104, 'reputation': 7393, 'user_id': 18926, 'user_type': 'registered', 'accept_rate': 63, 'profile_image': 'https://www.gravatar.com/avatar/793978804ee7797173c64ea661ad6dda?s=256&d=identicon&r=PG', 'display_name': 'Greg', 'link': 'https://stackoverflow.com/users/18926/greg'}, 'is_answered': True, 'view_count': 18685, 'answer_count': 5, 'score': 31, 'last_activity_date': 1555903599, 'creation_date': 1224043039, 'last_edit_date': 1233101716, 'question_id': 203684, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/203684/how-can-i-use-nlp-to-parse-recipe-ingredients', 'title': 'How can I use NLP to parse recipe ingredients?', 'body': '<p>I need to parse recipe ingredients into amount, measurement, item, and description as applicable to the line, such as 1 cup flour, the peel of 2 lemons and 1 cup packed brown sugar etc. What would be the best way of doing this? I am interested in using python for the project so I am assuming using the nltk is the best bet but I am open to other languages.</p>\\n'}\n",
      "{'tags': ['nlp', 'dcg'], 'owner': {'account_id': 7713, 'reputation': 11653, 'user_id': 13466, 'user_type': 'registered', 'accept_rate': 66, 'profile_image': 'https://www.gravatar.com/avatar/a08215c7e579fbe5bf95484b2d500524?s=256&d=identicon&r=PG', 'display_name': 'kitsune', 'link': 'https://stackoverflow.com/users/13466/kitsune'}, 'is_answered': True, 'view_count': 8734, 'closed_date': 1371659283, 'accepted_answer_id': 215072, 'answer_count': 10, 'score': 43, 'last_activity_date': 1322595029, 'creation_date': 1224251571, 'last_edit_date': 1495540437, 'question_id': 212219, 'link': 'https://stackoverflow.com/questions/212219/what-are-good-starting-points-for-someone-interested-in-natural-language-process', 'closed_reason': 'not constructive', 'title': 'What are good starting points for someone interested in natural language processing?', 'body': '<h1>Question</h1>\\n\\n<p>So I\\'ve recently came up with some new possible projects that would have to deal with deriving \\'meaning\\' from text submitted and generated by users.</p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"nofollow noreferrer\">Natural language processing</a> is the field that deals with these kinds of issues, and after some initial research I found the <a href=\"http://opennlp.sourceforge.net/\" rel=\"nofollow noreferrer\">OpenNLP Hub</a> and university collaborations like the <a href=\"http://attempto.ifi.uzh.ch/site/\" rel=\"nofollow noreferrer\">attempto project</a>. And stackoverflow has <a href=\"https://stackoverflow.com/questions/88984/your-favorite-natural-language-parser\">this</a>.</p>\\n\\n<p>If anyone could link me to some good resources, from reseach papers and introductionary texts to apis, I\\'d be happier than a 6 year-old kid opening his christmas presents!</p>\\n\\n<h1>Update</h1>\\n\\n<p>Through one of your recommendations I\\'ve found <a href=\"http://www.opencyc.org/\" rel=\"nofollow noreferrer\">opencyc</a> (<em>\\'the world\\'s largest and most complete general knowledge base and commonsense reasoning engine\\'</em>). Even more amazing still, there\\'s a project that is a distilled version of opencyc called <a href=\"http://umbel.org/\" rel=\"nofollow noreferrer\">UMBEL</a>. It features semantic data in rdf/owl/skos n3 syntax.</p>\\n\\n<p>I\\'ve also stumbled upon <a href=\"http://antlr.org/\" rel=\"nofollow noreferrer\">antlr</a>, a parser generator for <em>\\'constructing recognizers, interpreters, compilers, and translators from grammatical descriptions\\'</em>.</p>\\n\\n<p>And there\\'s a question on here by me, that lists tons of <a href=\"https://stackoverflow.com/questions/202092/where-can-i-find-free-and-open-data\">free and open data</a>.</p>\\n\\n<p>Thanks stackoverflow community!</p>\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'text', 'nlp', 'analysis', 'lexical-analysis'], 'owner': {'user_type': 'does_not_exist', 'display_name': 'Michael Julson'}, 'is_answered': True, 'view_count': 11035, 'answer_count': 7, 'score': 21, 'last_activity_date': 1302439869, 'creation_date': 1224542302, 'last_edit_date': 1224552868, 'question_id': 220187, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/220187/algorithms-or-libraries-for-textual-analysis-specifically-dominant-words-phra', 'title': 'Algorithms or libraries for textual analysis, specifically: dominant words, phrases across text, and collection of text', 'body': \"<p>I'm working on a project where I need to analyze a page of text and collections of pages of text to determine dominant words.   I'd like to know if there is a library (prefer c# or java) that will handle the heavy lifting for me.  If not, is there an algorithm or multiple that would achieve my goals below.  </p>\\n\\n<p>What I want to do is similar to word clouds built from a url or rss feed that you find on the web, except I don't want the visualization.  They are used all the time for analyzing the presidential candidate speeches to see what the theme or most used words are.  </p>\\n\\n<p>The complication, is that I need to do this on thousands of short documents, and then collections or categories of these documents.  </p>\\n\\n<p>My initial plan was to parse the document out, then filter common words - of, the, he, she, etc..  Then count the number of times the remaining words show up in the text (and overall collection/category).  </p>\\n\\n<p>The problem is that in the future, I would like to handle stemming, plural forms, etc..   I would also like to see if there is a way to identify important phrases. (Instead of a count of a word, the count of a phrase being 2-3 words together)</p>\\n\\n<p>Any guidance on a strategy, libraries or algorithms that would help are appreciated.  </p>\\n\"}\n",
      "{'tags': ['nlp', 'statistics', 'tf-idf', 'oov'], 'owner': {'account_id': 8717, 'reputation': 21332, 'user_id': 15842, 'user_type': 'registered', 'accept_rate': 59, 'profile_image': 'https://www.gravatar.com/avatar/58d77327a8f46041462b31c117e5c51a?s=256&d=identicon&r=PG', 'display_name': 'Gregg Lind', 'link': 'https://stackoverflow.com/users/15842/gregg-lind'}, 'is_answered': True, 'view_count': 3800, 'answer_count': 2, 'score': 9, 'last_activity_date': 1710188735, 'creation_date': 1224615215, 'last_edit_date': 1710188735, 'question_id': 223032, 'content_license': 'CC BY-SA 4.0', 'link': 'https://stackoverflow.com/questions/223032/tf-idf-and-previously-unseen-terms', 'title': 'tf-idf and previously unseen terms', 'body': '<p><a href=\"http://en.wikipedia.org/wiki/Tf-idf\" rel=\"noreferrer\">TF-IDF (term frequency - inverse document frequency)</a> is a staple of information retrieval.  It\\'s not a proper model though, and it seems to break down when new terms are introduced into the corpus.  How do people handle it when queries or new documents have new terms, especially if they are high frequency.  Under traditional cosine matching, those would have no impact on the total match.  </p>\\n'}\n",
      "{'tags': ['nlp', 'information-retrieval', 'text-mining'], 'owner': {'account_id': 6291, 'reputation': 11754, 'user_id': 10522, 'user_type': 'registered', 'accept_rate': 78, 'profile_image': 'https://www.gravatar.com/avatar/8396048b39279996e7ee6c1acdac2bf7?s=256&d=identicon&r=PG', 'display_name': 'Berlin Brown', 'link': 'https://stackoverflow.com/users/10522/berlin-brown'}, 'is_answered': True, 'view_count': 1052, 'accepted_answer_id': 228110, 'answer_count': 3, 'score': 3, 'last_activity_date': 1224774047, 'creation_date': 1224721969, 'last_edit_date': 1224749491, 'question_id': 228042, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/228042/natural-language-text-mining-and-reddit-social-news-site', 'title': 'Natural Language/Text Mining and Reddit/social news site', 'body': \"<p>I think there is a wealth of natural language data associated with sites like reddit or digg or news.google.com.</p>\\n\\n<p>I have done a little bit of research with text mining, but can't find how I could use those tools to parse something like reddit.</p>\\n\\n<p>What kind of applications can you come up with?</p>\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n",
      "INFO:root:Data appended to questions_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tags': ['algorithm', 'nlp'], 'owner': {'user_type': 'does_not_exist', 'display_name': 'rouli'}, 'is_answered': True, 'view_count': 611, 'answer_count': 3, 'score': 4, 'last_activity_date': 1230211278, 'creation_date': 1224955538, 'last_edit_date': 1224969255, 'question_id': 236722, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/236722/how-does-googles-in-quotes-work', 'title': 'How does Google&#39;s In Quotes work?', 'body': '<p>I find Google\\'s <a href=\"http://labs.google.com/inquotes/\" rel=\"nofollow noreferrer\">In Quotes</a> a really nifty application, and as a CS guy, I have to understand how it works. How do you think it turns news articles into a list of quotes attributed to specific persons?\\nSure, there are some mistakes, but their algorithm seems to be smarter than just a simple heuristic or multiple regular expressions. For example, a quote can be attributed to someone even though his/her name was only mentioned in the last paragraph.</p>\\n\\n<p>Any ideas? Any known paper on the subject?</p>\\n'}\n",
      "{'tags': ['c#', 'regex', 'nlp'], 'owner': {'account_id': 3194, 'reputation': 12333, 'user_id': 4555, 'user_type': 'registered', 'accept_rate': 69, 'profile_image': 'https://www.gravatar.com/avatar/a616e70f0c7b7e8e5c35f1e64a558f26?s=256&d=identicon&r=PG', 'display_name': 'Tai Squared', 'link': 'https://stackoverflow.com/users/4555/tai-squared'}, 'is_answered': True, 'view_count': 10234, 'answer_count': 6, 'score': 5, 'last_activity_date': 1300837861, 'creation_date': 1225228953, 'last_edit_date': 1225940352, 'question_id': 244913, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/244913/split-string-into-sentences-using-regular-expression', 'title': 'Split string into sentences using regular expression', 'body': '<p>I need to match a string like \"one. two.    three. four. five.  six. seven. eight. nine. ten. eleven\" into groups of four sentences.  I need a regular expression to break the string into a group after every fourth period. Something like: </p>\\n\\n<pre><code>  string regex = @\"(.*.\\\\s){4}\";\\n\\n  System.Text.RegularExpressions.Regex exp = new System.Text.RegularExpressions.Regex(regex);\\n\\n  string result = exp.Replace(toTest, \".\\\\n\");\\n</code></pre>\\n\\n<p>doesn\\'t work because it will replace the text before the periods, not just the periods themselves.  How can I count just the periods and replace them with a period and new line character?</p>\\n'}\n",
      "{'tags': ['shell', 'grep', 'nlp'], 'owner': {'user_type': 'does_not_exist', 'display_name': 'Simon Rowsby'}, 'is_answered': True, 'view_count': 3028, 'accepted_answer_id': 245122, 'answer_count': 2, 'score': 3, 'last_activity_date': 1227301642, 'creation_date': 1225232197, 'last_edit_date': 1227301642, 'question_id': 245082, 'content_license': 'CC BY-SA 2.5', 'link': 'https://stackoverflow.com/questions/245082/shell-script-to-find-bigrams', 'title': 'Shell script to find bigrams', 'body': \"<p>I'm making a shell script to find bigrams, which works, sort of.</p>\\n\\n<pre><code>#tokenise words\\ntr -sc 'a-zA-z0-9.' '\\\\012' &lt; $1 &gt; out1\\n#create 2nd list offset by 1 word\\ntail -n+2 out1 &gt; out2\\n#paste list together\\npaste out1 out2 \\n#clean up\\nrm out1 out2\\n</code></pre>\\n\\n<p>The only problem is that it pairs words from the end and start of the previous sentence.</p>\\n\\n<p>eg for the two sentences 'hello world.' and 'foo bar.' i'll get a line with ' world. foo'. Would it be possible to filter these out with grep or something?</p>\\n\\n<p>I know i can find all bigrams containing a full stop with grep [.] but that also finds the legitimate bigrams.</p>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "for page in range(1,5):\n",
    "    result = getData(page,'nlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
